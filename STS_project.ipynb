{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copie de STS_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PierrickLeroy/Activity_Detection_From_Electrical_Consumption_Load_Curves/blob/main/STS_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qI3vWPXiYNH"
      },
      "source": [
        "<center><img src=\"data:image/png;base64,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\" /></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsvRF4kE4AIf"
      },
      "source": [
        "<h1><center><strong>Semantic Textual Similarity (STS) project</strong></center></h1>\n",
        "\n",
        "<center>\n",
        "<h3> Project Supervisor: Aina Garí Soler</a></h3>\n",
        "<email>aina.garisoler@telecom-paris.fr</email>  <br/>\n",
        "Year 2021-2022\n",
        "</center>\n",
        "\n",
        "Test3 pierrick\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tShu6pZ8c5o6"
      },
      "source": [
        "## What is Semantic Textual Similarity (STS)?\n",
        "\n",
        "Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two texts. As an NLP task, it typically consists in determining, for two sentences $s_1$ and $s_2$, how similar they are in meaning. Systems must output a continous score $p$ between, for example, 0 (completely unrelated) and 1 (meaning-equivalent). For example, for these two unrelated sentences, $p$ should be close to 0:\n",
        "\n",
        "$s_1$: *The black dog is running through the snow.*\n",
        "\n",
        "$s_2$: *A race car driver is driving his car through the mud.*\n",
        "\n",
        "### Why is it important?\n",
        "\n",
        "There are several NLP applications that can directly benefit from STS predictions. Here are some examples:\n",
        "- Evaluation of **Automatic Summarization** and **Machine Translation** models. These models must output sentences in natural language preserving the meaning of a reference text (the text to be summarized / translated);\n",
        "- **Information Retrieval**: for checking the semantic equivalence between a query and a potential match;\n",
        "- **Plagiarism detection**: for locating passages that are similar in content and potentially contain plagiarism.\n",
        "\n",
        "### Why is it hard? \n",
        "\n",
        "**TL;DR**: Because there are different ways of saying the same thing, and there are subtle ways of saying radically different things.\n",
        "\n",
        "Solving this task involves multiple kinds of non-trivial linguistic knowledge.Relying on counting the common words between $s_1$ and $s_2$ works to some extent, but it is not enough. The example below (taken from [Landauer et al., 1997](http://cetus.stat.cmu.edu/~cshalizi/350/2008/readings/Landauer-Dumais.pdf)), with a very high word overlap, illustrates this. The relations between words in a sentence are also important.\n",
        "\n",
        "$s_1$: *It was not the sales manager who hit the bottle that day, but the office worker with the serious drinking problem.*\n",
        "\n",
        "$s_2$: *That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J6el1_ym14Q"
      },
      "source": [
        "\n",
        "<p align=\"left\">\n",
        "  <img src=\"data:image/png;base64,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\" width=\"100\" />\n",
        "\n",
        "\n",
        "<img src=\"data:image/png;base64,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\" width=\"100\" />\n",
        "\n",
        "\n",
        "\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHZpGsZ-zGfb"
      },
      "source": [
        "In fact, people may often refer to the same thing using different words (**synonyms**, like *happy* and *glad*, or **hypernyms**, like *animal* for *dog*). \n",
        "\n",
        "$s_1$: *I'm happy I managed to fix my dad's bike*\n",
        "\n",
        "$s_2$: *I'm glad I could repair my father's bicycle*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rK3_gqUz7HH"
      },
      "source": [
        "  <img src=\"data:image/png;base64,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\" width=\"100\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUNv1hwFz_e5"
      },
      "source": [
        "The simple addition of one word can change a situation completely:\n",
        "\n",
        "$s_1$: *I enjoyed watching the stars* (⭐)\n",
        "\n",
        "$s_2$: *I enjoyed watching the **TV** stars* (👩‍🎤)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wGnZuAhV83R"
      },
      "source": [
        "## SemEval Workshop: Semantic Evaluation Challenges\n",
        "\n",
        "[SemEval](https://semeval.github.io/) is a series of international workshops that proposes, every year, a number of NLP challenges. These challenges (or \"shared tasks\") target different tasks, all related to semantics. Teams participate by submitting their models, and optionally may write a paper describing their approach. The organizers of each task publish a paper describing the data used, and summarize the participating approaches and their results.\n",
        "\n",
        "There have been multiple editions of STS tasks, each with a different dataset:\n",
        "- [SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity](https://aclanthology.org/S12-1051.pdf)\n",
        "- [*SEM 2013 shared task: Semantic Textual Similarity](https://aclanthology.org/S13-1004.pdf)\n",
        "- [SemEval-2014 Task 10: Multilingual Semantic Textual Similarity](https://aclanthology.org/S14-2010.pdf)\n",
        "-[SemEval-2015 Task 2: Semantic Textual Similarity,\n",
        "English, Spanish and Pilot on Interpretability](https://aclanthology.org/S15-2045.pdf)\n",
        "- [SemEval-2015 Task 1: Paraphrase and Semantic Similarity in Twitter (PIT)](https://aclanthology.org/S15-2001.pdf)\n",
        "- [SemEval-2016 Task 1: Semantic Textual Similarity,\n",
        "Monolingual and Cross-Lingual Evaluation](https://aclanthology.org/S16-1081.pdf)\n",
        "- [SemEval-2017 Task 1: Semantic Textual Similarity\n",
        "Multilingual and Cross-lingual Focused Evaluation](https://aclanthology.org/S17-2001.pdf) \n",
        "\n",
        "\n",
        "For a summary of most of these tasks, including the datasets, you can visit [**this wiki**](http://ixa2.si.ehu.eus/stswiki/index.php/Main_Page).\n",
        "\n",
        "For this project, you will work on solving this task using some of the datasets proposed in these SemEval challenges. You will use different kinds of features, and may find inspiration for your models in the papers above.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0tu6RyFYACP"
      },
      "source": [
        "# Overview of the project\n",
        "\n",
        "Both teams will implement at least 3 kinds of models to solve the standard English STS task:\n",
        "### Common part: \n",
        "1. A traditional ML model relying on simple linguistic and textual features;\n",
        "2. A traditional ML model relying on word representations;\n",
        "3. A neural model (Siamese Bidirectional Long Short-Term Memory Network, BiLSTM) that computes sentence representations.\n",
        "\n",
        "### Team-specific part:\n",
        "One of the models above will be adapted for:\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4a. A multilingual STS task \\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4b. A cross-lingual STS task \n",
        "\n",
        "\n",
        "For (1), you will be asked to use some specific features and will be given hints on other (kinds of) features that you may want to try. You are also expected to check the bibliography to come up with potentially useful features.\n",
        "For models of type (2) and (3), you will mostly be asked to complete portions of code. You are not expected to obtain outstanding results. The goal is rather for you to learn how to solve an NLP task using different approaches and to reflect on pre-processing as well as on the results obtained (do worry, though, if your correlations are all close to 0!)\n",
        "\n",
        "**Places where you need to write code are marked with \"## TO COMPLETE\"**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UubTW9GjuXKS"
      },
      "source": [
        "# Let's start! Common part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS4Ts5dKuYS1",
        "outputId": "f47f6319-53ac-4af3-d587-e8476d868deb"
      },
      "source": [
        "import os\n",
        "from collections import Counter\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import jaccard_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.stats import pearsonr\n",
        "from scipy.spatial.distance import cosine, euclidean\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import jaccard_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "import collections\n",
        "import os\n",
        "from collections import Counter\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from scipy.stats import pearsonr\n",
        "from scipy.spatial import distance \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import jaccard_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtXNN7Jwx02P"
      },
      "source": [
        "\n",
        "\n",
        "## STSBenchmark Dataset\n",
        "\n",
        "For models 1, 2 and 3 we will use the STSBenchmark, which contains portions of the datasets used in different STS tasks between 2012 and 2017. You can download it using [this link](http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz). The dataset comes with a pre-defined train/dev/test split.\n",
        "\n",
        "You can find more details about this benchmark on [this wiki](http://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark).\n",
        "\n",
        "Download the dataset, get acquainted with its format, upload it to Colab and load it using the function below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTzaY2Hb_ft-",
        "outputId": "e9b75687-a5d9-4019-e43b-ff894f8ce926"
      },
      "source": [
        "! tar -xvzf Stsbenchmark.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar (child): Stsbenchmark.tar.gz: Cannot open: No such file or directory\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsROnw9RrjXX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da5e4c89-1beb-489b-c4a4-7844792076f6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrTEA6o5xqQZ",
        "outputId": "6491be33-55ae-4375-e4c0-12caccce6266"
      },
      "source": [
        "def load_data():\n",
        "  data = dict()\n",
        "  for fn in os.listdir(\"/content/drive/MyDrive/Colab Notebooks/stsbenchmark\"):\n",
        "    print(fn)\n",
        "    if fn.endswith(\".csv\"):\n",
        "      with open(\"/content/drive/MyDrive/Colab Notebooks/stsbenchmark/\"+fn) as f:\n",
        "        subset = fn[:-4].split(\"-\")[1]\n",
        "        #print(subset)\n",
        "        data[subset] = dict()\n",
        "        data[subset]['data'] = []\n",
        "        data[subset]['scores'] = []\n",
        "        for l in f:\n",
        "          #genre filename year score sentence1 sentence2 (and sources, sometimes)          \n",
        "          l = l.strip().split(\"\\t\")          \n",
        "          data[subset]['data'].append((l[5],l[6]))\n",
        "          data[subset]['scores'].append(float(l[4])) \n",
        "  return data\n",
        "\n",
        "dataset = load_data()\n",
        "\n",
        "### Having a look at the data...\n",
        "\n",
        "print(\"Some examples from the dataset:\")\n",
        "for i in range(5):\n",
        "  print(\"s1:\", dataset['train']['data'][i][0])\n",
        "  print(\"s2:\", dataset['train']['data'][i][1])\n",
        "  print(\"score:\", dataset['train']['scores'][i], \"\\n\")\n",
        "\n",
        "print(\"\\nNumber of sentence pairs by subset:\")\n",
        "for subset in dataset:\n",
        "  print(subset, len(dataset[subset]['data']))\n",
        "\n",
        "print(\"\\nRange of scores in the training set:\", min(dataset[\"train\"][\"scores\"]), \"-\", max(dataset[\"train\"][\"scores\"]))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correlation.pl\n",
            "sts-dev.csv\n",
            "sts-test.csv\n",
            "sts-train.csv\n",
            "readme.txt\n",
            "LICENSE.txt\n",
            "Some examples from the dataset:\n",
            "s1: A plane is taking off.\n",
            "s2: An air plane is taking off.\n",
            "score: 5.0 \n",
            "\n",
            "s1: A man is playing a large flute.\n",
            "s2: A man is playing a flute.\n",
            "score: 3.8 \n",
            "\n",
            "s1: A man is spreading shreded cheese on a pizza.\n",
            "s2: A man is spreading shredded cheese on an uncooked pizza.\n",
            "score: 3.8 \n",
            "\n",
            "s1: Three men are playing chess.\n",
            "s2: Two men are playing chess.\n",
            "score: 2.6 \n",
            "\n",
            "s1: A man is playing the cello.\n",
            "s2: A man seated is playing the cello.\n",
            "score: 4.25 \n",
            "\n",
            "\n",
            "Number of sentence pairs by subset:\n",
            "dev 1500\n",
            "test 1379\n",
            "train 5749\n",
            "\n",
            "Range of scores in the training set: 0.0 - 5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train']['data'][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqUintbBSWyX",
        "outputId": "93d48129-ffdd-41b2-9cfd-9efbb44b4d87"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('A plane is taking off.', 'An air plane is taking off.'),\n",
              " ('A man is playing a large flute.', 'A man is playing a flute.'),\n",
              " ('A man is spreading shreded cheese on a pizza.',\n",
              "  'A man is spreading shredded cheese on an uncooked pizza.'),\n",
              " ('Three men are playing chess.', 'Two men are playing chess.'),\n",
              " ('A man is playing the cello.', 'A man seated is playing the cello.'),\n",
              " ('Some men are fighting.', 'Two men are fighting.'),\n",
              " ('A man is smoking.', 'A man is skating.'),\n",
              " ('The man is playing the piano.', 'The man is playing the guitar.'),\n",
              " ('A man is playing on a guitar and singing.',\n",
              "  'A woman is playing an acoustic guitar and singing.'),\n",
              " ('A person is throwing a cat on to the ceiling.',\n",
              "  'A person throws a cat on the ceiling.')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV7JIRvqS624"
      },
      "source": [
        "## Word count baseline\n",
        "\n",
        "The very first model you will use is fully implemented. It is a very simple baseline that you can compare to the other models you build. It consists of a Linear Regression model which uses a single feature based on word overlap.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ooIDhPtHYdm"
      },
      "source": [
        "### word overlap baseline\n",
        "def baseline_features(data):\n",
        "  x = []\n",
        "  for s1, s2 in data:\n",
        "    cv = CountVectorizer(binary=True) # binary=True because we use Jaccard score (we want presence/absence information, not counts)\n",
        "    vectors = cv.fit_transform([s1,s2]).toarray()    \n",
        "    x.append(jaccard_score(vectors[0], vectors[1]))\n",
        "  return np.array(x).reshape(-1,1)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49P-LWMduhEn"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "We evaluate the model directly on the test set since we will want to compare its performance to that of the other models. As an evaluation metric, following common practice in this task, we will use Pearson's correlation.\n",
        "\n",
        "You can reuse some of the code below for your next models (and you should use the same evaluation function).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV3bGaitukoN"
      },
      "source": [
        "## evaluation function: it returns Pearson's r\n",
        "def evaluate(predictions, gold_standard):\n",
        "  return pearsonr(predictions, gold_standard)[0] \n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyIC3-girZDX"
      },
      "source": [
        "# Mapping the scores from the [0,5] to the [0,1] range for convenience\n",
        "train_y = np.array(dataset['train']['scores']) / 5\n",
        "dev_y = np.array(dataset['dev']['scores']) / 5\n",
        "test_y = np.array(dataset['test']['scores']) / 5"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSnZe9vXTspf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69f877a9-9e81-4eea-8bcc-4b9eecc59409"
      },
      "source": [
        "train_baseline_x = baseline_features(dataset['train']['data'])\n",
        "test_baseline_x = baseline_features(dataset['test']['data'])\n",
        "\n",
        "# Having a look at the features and y\n",
        "print(train_baseline_x[:10])\n",
        "print(train_y[:10])\n",
        "print(\"Checking the correlation of the word overlap feature with the gold standard scores on the training set:\", pearsonr(train_baseline_x.squeeze(), train_y))\n",
        "\n",
        "# Initializing the model\n",
        "linreg = LinearRegression()\n",
        "# Training\n",
        "linreg.fit(train_baseline_x, train_y)\n",
        "# Predicting\n",
        "predictions = linreg.predict(test_baseline_x)\n",
        "# Evaluating\n",
        "print(\"Pearson's r obtained on the test set:\", evaluate(predictions, test_y))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.66666667]\n",
            " [0.8       ]\n",
            " [0.6       ]\n",
            " [0.66666667]\n",
            " [0.83333333]\n",
            " [0.6       ]\n",
            " [0.5       ]\n",
            " [0.66666667]\n",
            " [0.5       ]\n",
            " [0.55555556]]\n",
            "[1.   0.76 0.76 0.52 0.85 0.85 0.1  0.32 0.44 1.  ]\n",
            "Checking the correlation of the word overlap feature with the gold standard scores on the training set: (0.6116184058994178, 0.0)\n",
            "Pearson's r obtained on the test set: 0.5997660486084755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PreProcessing(object):\n",
        "\n",
        "    \"\"\"\n",
        "        language           = \"english\", \"spanish\", ...\n",
        "        remove_stopwords   = True, False \n",
        "        remove_punct       = True, False\n",
        "        pos_tagging        = True, False [to use only with lemmatization]\n",
        "        lexical_processing =  '', 'lemmatization', 'stemming'\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 language           = \"english\",\n",
        "                 remove_stopwords   = True, \n",
        "                 remove_punct       = True,\n",
        "                 pos_tagging        = False,  \n",
        "                 lexical_processing = '' \n",
        "                ) :\n",
        "\n",
        "        self.language            = language\n",
        "        self.remove_stopwords    = remove_stopwords\n",
        "        self.remove_punct        = remove_punct\n",
        "        self.pos_tagging         = pos_tagging\n",
        "        self.lexical_processing  = lexical_processing\n",
        "        self.lexical_processor   = self.init_lexical_proc()\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "        lexical processor initialization\n",
        "    \"\"\"\n",
        "    def init_lexical_proc(self):\n",
        "        if self.lexical_processing == \"lemmatization\":\n",
        "            return nltk.stem.WordNetLemmatizer()\n",
        "        elif self.lexical_processing == \"stemming\":\n",
        "            # nltk.stem.PorterStemmer()\n",
        "            # nltk.stem.SnowballStemmer(language=self.language)\n",
        "            return nltk.stem.LancasterStemmer() # Algo more recent than PorterStemmer\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "        return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v) \n",
        "    \"\"\"\n",
        "    def get_wordnet_pos(self,treebank_tag):\n",
        "        if treebank_tag.startswith('J'):\n",
        "            return wordnet.ADJ\n",
        "        elif treebank_tag.startswith('V'):\n",
        "            return wordnet.VERB\n",
        "        elif treebank_tag.startswith('N'):\n",
        "            return wordnet.NOUN\n",
        "        elif treebank_tag.startswith('R'):\n",
        "            return wordnet.ADV\n",
        "        else:\n",
        "            return wordnet.NOUN # As default pos in lemmatization is Noun\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "        handle simple token and token with pos tagging\n",
        "    \"\"\"\n",
        "    def lemmatize(self,token):\n",
        "        if isinstance(token,tuple):\n",
        "            tag = self.get_wordnet_pos(token[1])\n",
        "            return self.lexical_processor.lemmatize(token[0], tag)\n",
        "        else:\n",
        "            return self.lexical_processor.lemmatize(token)\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "       Preprocessing the sentence \"string\" \n",
        "       according to the options \n",
        "    \"\"\"\n",
        "    def __call__(self, string):\n",
        "        tok = word_tokenize(string)\n",
        "        if (self.remove_stopwords) :\n",
        "            stop = stopwords.words(self.language)\n",
        "            tok = [ t for t in tok if t not in stop] \n",
        "        if (self.remove_punct) :\n",
        "            tok = [ t for t in tok if t not in punctuation]\n",
        "        if (self.lexical_processing == \"lemmatization\") :\n",
        "            if (self.pos_tagging) :\n",
        "                tok = nltk.pos_tag(tok)\n",
        "            tok = [ self.lemmatize(t) for t in tok ] \n",
        "        elif (self.lexical_processing == \"stemming\") :\n",
        "            tok = [self.lexical_processor.stem(t) for t in tok]\n",
        "        return tok\n"
      ],
      "metadata": {
        "id": "8WLgik3QsfdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_processor_lem = PreProcessing(pos_tagging=True, lexical_processing='lemmatization')\n",
        "pre_processor_stem = PreProcessing(lexical_processing='stemming')\n",
        "pre_processor      = PreProcessing()\n",
        "\n",
        "# test \n",
        "string = \"I am just testing this python-program !!!\"\n",
        "print(pre_processor_lem(string))\n",
        "print(pre_processor_stem(string))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_KKGlvbsp4R",
        "outputId": "73233bb3-a1cc-49d4-ba43-ae6d25619d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'test', 'python-program']\n",
            "['i', 'test', 'python-program']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureExtracting(object):\n",
        "\n",
        "    \"\"\"\n",
        "        vectorizer = tf_idf, wordnet, ...\n",
        "        sim_extractor = cosine_sim, ...\n",
        "    \"\"\"\n",
        "    def __init__(self, vectorizer, sim_extractor='cosine'):\n",
        "        self.vectorizer     = vectorizer\n",
        "        self.sim_extractor = None\n",
        "        self.X             = None\n",
        "        self.init_sim_extractor(sim_extractor)\n",
        "        self.vocab = None\n",
        "        self.n_words = 0\n",
        "\n",
        "    def init_sim_extractor(self, sim_extractor):\n",
        "        def dist2sim(dist_calculator):\n",
        "            def __dist2sim(v1, v2):\n",
        "                return 1 - dist_calculator(v1, v2)\n",
        "            return __dist2sim\n",
        "\n",
        "        if sim_extractor == \"cosine\" : \n",
        "            self.sim_extractor = dist2sim(distance.cosine)\n",
        "        if sim_extractor == \"euclidian\" : \n",
        "            self.sim_extractor = dist2sim(distance.euclidian)\n",
        "        elif sim_extractor == \"jaccard\" : \n",
        "            self.sim_extractor = jaccard_score\n",
        "\n",
        "    def get_wordnet_pos(self,treebank_tag):\n",
        "        if treebank_tag.startswith('J'):\n",
        "            return wordnet.ADJ\n",
        "        elif treebank_tag.startswith('V'):\n",
        "            return wordnet.VERB\n",
        "        elif treebank_tag.startswith('N'):\n",
        "            return wordnet.NOUN\n",
        "        elif treebank_tag.startswith('R'):\n",
        "            return wordnet.ADV\n",
        "        else:\n",
        "            return wordnet.NOUN # As default pos in lemmatization is Nou \n",
        "\n",
        "    def tf_idf(self, data):\n",
        "        X = []\n",
        "        for s1, s2 in data:\n",
        "            tfidf_v = TfidfVectorizer(tokenizer=lambda text: text, preprocessor=lambda text: text)\n",
        "            vectors = tfidf_v.fit_transform([s1,s2]).toarray()    \n",
        "            similarity = self.sim_extractor(vectors[0], vectors[1])\n",
        "            X.append(similarity)\n",
        "            self.X = np.array(X).reshape(-1,1)\n",
        "        return self.X\n",
        "\n",
        "    def wordnet(self,data):\n",
        "        X = []\n",
        "        for s1, s2 in data:\n",
        "            sim_s1_s2 = []\n",
        "            sim_s2_s1 = []\n",
        "\n",
        "            s1_tagged = [ (word,self.get_wordnet_pos(tag)) for (word,tag)  in nltk.pos_tag(s1) ]\n",
        "            s2_tagged = [ (word,self.get_wordnet_pos(tag)) for (word,tag)  in nltk.pos_tag(s2) ]\n",
        "\n",
        "            _s1_synsets = [ wordnet.synsets(word, tag) for (word, tag) in s1_tagged ]\n",
        "            _s2_synsets = [ wordnet.synsets(word, tag) for (word, tag) in s2_tagged ]\n",
        "\n",
        "            s1_synsets = [ _s[0] for _s in _s1_synsets if _s is not None and len(_s)>1 ]\n",
        "            s2_synsets = [ _s[0] for _s in _s2_synsets if _s is not None and len(_s)>1 ]\n",
        "\n",
        "            for synset_1 in s1_synsets:\n",
        "                try: \n",
        "                    sim_s1_s2.append(np.mean([ synset_1.wup_similarity(synset_2) \n",
        "                                        if synset_1.wup_similarity(synset_2) is not None\n",
        "                                        else 0\n",
        "                                        for synset_2 in s2_synsets ]))\n",
        "                except ValueError:\n",
        "                    sim_s1_s2.append(0)\n",
        "            for synset_2 in s2_synsets:\n",
        "                try:\n",
        "                    sim_s2_s1.append(np.mean([ synset_2.wup_similarity(synset_1) \n",
        "                                        if synset_2.wup_similarity(synset_1) is not None\n",
        "                                        else 0\n",
        "                                        for synset_1 in s1_synsets ]))\n",
        "                except ValueError:\n",
        "                    sim_s2_s1.append(0)\n",
        "\n",
        "            sim_s = list(map(lambda _s1, _s2: (_s1+_s2)/2, sim_s1_s2, sim_s2_s1 ))\n",
        "            X.append(np.array(sim_s).mean())\n",
        "            self.X = np.array(X).reshape(-1,1)\n",
        "        return self.X\n",
        "\n",
        "    def overlap_n_n(self, n_gram_min, n_gram_max, data):\n",
        "        X = []\n",
        "        for s1, s2 in data:\n",
        "            overlap_v = CountVectorizer(ngram_range=(n_gram_min, n_gram_max),\n",
        "                                        tokenizer=lambda text: text, \n",
        "                                        preprocessor=lambda text: text)\n",
        "            vectors = overlap_v.fit_transform([s1,s2]).toarray()    \n",
        "            similarity = self.sim_extractor(vectors[0], vectors[1])\n",
        "            X.append(similarity)\n",
        "            self.X = np.array(X).reshape(-1,1)\n",
        "        return self.X\n",
        "\n",
        "    def overlap_tag_n_n(self, n_gram_min, n_gram_max, data):\n",
        "        X = []\n",
        "        # print(data)\n",
        "        for s1, s2 in data:\n",
        "            s1_tagged = [x2 for x1, x2 in nltk.pos_tag(s1)] # On remplace s1 par les TAG des ses tokens\n",
        "            s2_tagged = [x2 for x1, x2 in nltk.pos_tag(s2)] # idem pour s2\n",
        "            X.append((s1_tagged, s2_tagged))\n",
        "        # print(X)\n",
        "        return self.overlap_n_n(n_gram_min, n_gram_max, X)\n",
        "\n",
        "\n",
        "\n",
        "    def build_vocab(self, data):\n",
        "        \"\"\"\"data is already preprocessed\"\"\"\n",
        "        flat_list = [item for sublist in [(s1 + s2)  for s1, s2 in data] for item in sublist]\n",
        "        self.vocab = dict(collections.Counter(flat_list))\n",
        "        self.n_words = len(flat_list)\n",
        "\n",
        "    def ic(self, w):\n",
        "        if w in self.vocab:\n",
        "            return np.log2(self.n_words / self.vocab[w])\n",
        "        else:\n",
        "            raise KeyError(\"Le mot recherché n'est pas dans le vocabulaire\")\n",
        "\n",
        "    def wwc(self, s1, s2):\n",
        "        num = 0\n",
        "        den = 0\n",
        "        for w1 in s1:\n",
        "            if w1 in s2 :\n",
        "                num += self.ic(w1)\n",
        "            den += self.ic(w1)\n",
        "        return num / den\n",
        "\n",
        "    def wwo(self, data):\n",
        "        X = []\n",
        "        for s1, s2 in data :\n",
        "            X.append(2 / (1 / self.wwc(s1, s2) + 1 / self.wwc(s2, s1))) \n",
        "        return X\n",
        "\n",
        "\n",
        "    # for overlap vectorization the keyword must be overlap_n_n\n",
        "    # where n is {1,2,3,...} ( overlap_1_2, overlap_2_4,...)\n",
        "    def extract(self, data):\n",
        "        if self.vectorizer == \"tf_idf\" :\n",
        "            return self.tf_idf(data)\n",
        "        elif self.vectorizer == \"wordnet\":\n",
        "            print(\"3\")\n",
        "            return self.wordnet(data)\n",
        "        elif self.vectorizer.startswith(\"overlap_tag_\"):\n",
        "            splited    = self.vectorizer.split(\"_\")\n",
        "            n_gram_min = int(splited[-2])\n",
        "            n_gram_max = int(splited[-1])\n",
        "            return self.overlap_tag_n_n(n_gram_min, n_gram_max, data)\n",
        "        elif self.vectorizer.startswith(\"overlap_\"):\n",
        "            splited    = self.vectorizer.split(\"_\")\n",
        "            n_gram_min = int(splited[-2])\n",
        "            n_gram_max = int(splited[-1])\n",
        "            return self.overlap_n_n(n_gram_min, n_gram_max, data)\n",
        "        elif self.vectorizer == \"wwo\":\n",
        "            self.build_vocab(data)\n",
        "            return self.wwo(data)"
      ],
      "metadata": {
        "id": "1GvWHuuHOie3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset['train']['data']\n",
        "preprocessor = PreProcessing(language='english',\n",
        "                 remove_stopwords   = False, \n",
        "                 remove_punct       = True,\n",
        "                 pos_tagging        = False,  \n",
        "                 lexical_processing = '') \n",
        "data_preprocessed = [(preprocessor(s1), preprocessor(s2)) for s1, s2 in dataset['train']['data']]"
      ],
      "metadata": {
        "id": "0vYCNbkTJeNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_w = FeatureExtracting('wwo')\n",
        "res = f_w.extract(data_preprocessed)\n",
        "\n"
      ],
      "metadata": {
        "id": "0JVD5P3AuEuy",
        "outputId": "07a1d5d5-700e-4854-f714-adc5f17576f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:135: RuntimeWarning: divide by zero encountered in double_scalars\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5749"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ci-dessous : fonction de distance utilisant le principe de Named Entity Recognition. 2 phrases ne possédant pas les mêmes entités nommées ne sont probablement pas identiques.\n",
        "Possibilité de raffiner en choisissant seulement des types d'entités souhaitées"
      ],
      "metadata": {
        "id": "1XDvjzdvdyU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Intégrer ce code\n",
        "### Difficulté : a besoin d'arriver avant le preprocessing\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "\n",
        "def jaccard_set(list1, list2):\n",
        "    \"\"\"Define Jaccard Similarity function for two sets\"\"\"\n",
        "    intersection = len(list(set(list1).intersection(list2)))\n",
        "    union = (len(list1) + len(list2)) - intersection\n",
        "    return (float(intersection) + 1) / (union +1)\n",
        "\n",
        "def dist_ner(s1, s2):\n",
        "    NER = spacy.load(\"en_core_web_sm\") # A placer en dehors de la fonction idéalement\n",
        "    ner_s1 = [(X.text, X.label_) for X in NER(s1).ents]\n",
        "    ner_s2 = [(X.text, X.label_) for X in NER(s2).ents]\n",
        "    return (1 / jaccard_set(ner_s1, ner_s2)) - 1, ner_s1, ner_s2"
      ],
      "metadata": {
        "id": "fdIk_JduWR2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s1, s2 = 'A plane is taking off.', 'An air plane is taking off.'\n",
        "dist_ner(s1, s2)"
      ],
      "metadata": {
        "id": "IXqnsjD8dAlY",
        "outputId": "eaf2b457-a746-4068-9369-ed95138e84c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, [], [])"
            ]
          },
          "metadata": {},
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s1, s2 = 'Tomorrow will be a good day.', 'Today was a good day.'\n",
        "dist_ner(s1, s2)"
      ],
      "metadata": {
        "id": "JuhKWJ0adphI",
        "outputId": "16221b26-fcc3-4fe6-b244-3eb5d01e9f17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0,\n",
              " [('Tomorrow', 'DATE'), ('a good day', 'DATE')],\n",
              " [('Today', 'DATE'), ('a good day', 'DATE')])"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s1, s2 = 'Tomorrow will be a good evening.', 'Today was a good day.'\n",
        "dist_ner(s1, s2)"
      ],
      "metadata": {
        "id": "ZH3WLq-kdLhe",
        "outputId": "55259931-509c-4ee6-fe5f-6d4e76df186c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4.0,\n",
              " [('Tomorrow', 'DATE'), ('a good evening', 'TIME')],\n",
              " [('Today', 'DATE'), ('a good day', 'DATE')])"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s1, s2 = \"On Saturday, a 149mph serve against Agassi equalled Rusedski's world record.\", \"On Sunday, a 149mph serve against Agassi equalled Rusedski's world record in India.\"\n",
        "dist_ner(s1, s2)"
      ],
      "metadata": {
        "id": "3gD5gcl0Y9FH",
        "outputId": "6ee4c196-07c1-4ca9-c6dd-9e826c2ca837",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.75,\n",
              " [('Saturday', 'DATE'),\n",
              "  ('149mph', 'QUANTITY'),\n",
              "  ('Agassi', 'PERSON'),\n",
              "  ('Rusedski', 'PERSON')],\n",
              " [('Sunday', 'DATE'),\n",
              "  ('149mph', 'QUANTITY'),\n",
              "  ('Agassi', 'PERSON'),\n",
              "  ('Rusedski', 'PERSON'),\n",
              "  ('India', 'GPE')])"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ci- dessous : utilisation d'une API construite pour récupérer les comptes de mots sur google ngram viewer - Weighted Word Overlap feature"
      ],
      "metadata": {
        "id": "UsygGSykgSKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install google-ngram-downloader\n",
        "# from google_ngram_downloader import readline_google_store\n",
        "# from google_ngram_downloader import ngram_to_cooc\n",
        "# readline_google_store?\n"
      ],
      "metadata": {
        "id": "YJGBPrAdSQwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'cat'\n",
        "# iterator = readline_google_store(ngram_len = 1, lang='eng', indices=None, chunk_size=1048576, verbose=False)\n",
        "# fname, url, records = next(iterator)\n",
        "\n",
        "found = False\n",
        "for _, _, records in readline_google_store(ngram_len = 1, lang='eng', indices=None, chunk_size=1048576, verbose=False):\n",
        "    if found:\n",
        "            break\n",
        "    for element in records:\n",
        "        if found:\n",
        "            break\n",
        "        if (element.ngram in vocab) & (element.year == 2008):\n",
        "            print(element)\n",
        "            found = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbLzIviSWHOB",
        "outputId": "116c598d-5e53-4a8c-e9ad-0169571785f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Record(ngram='0.2', year=2008, match_count=148908, volume_count=18664)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = []\n",
        "for s1, s2 in data_preprocessed:\n",
        "    vocab.extend(s1)\n",
        "    vocab.extend(s2)\n",
        "vocab = set(vocab)"
      ],
      "metadata": {
        "id": "I5IG1Km8dixe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'0.2' in vocab "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Fkcv3l3fsqW",
        "outputId": "f69fc419-cfc7-4c0c-c6b8-24d212abfdcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_preprocessed = [(preprocessor(s1), preprocessor(s2)) for s1, s2 in dataset['train']['data']]"
      ],
      "metadata": {
        "id": "Hj1EuBz9cd8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[(s1, s2) for s1,s2 in dataset['train']['data'] if '0.2' in s1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4j9lKzpfyFJ",
        "outputId": "cbfe143e-5de6-47b1-9167-8011cbfbe016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The Dow Jones industrial average <.DJI> added 28 points, or 0.27 percent, at 10,557, hitting its highest level in 21 months.',\n",
              "  'The Dow Jones industrial average <.DJI> rose 49 points, or 0.47 percent, to 10,578.'),\n",
              " (\"The broader Standard & Poor's 500 Index .SPX advanced 2 points, or 0.24 percent, to 977.\",\n",
              "  'The Nasdaq Composite Index .IXIC was off 6.52 points, or 0.39 percent, at 1,645.66.'),\n",
              " ('The tech-heavy Nasdaq composite index fell 3.99, or 0.2 percent, to 1,682.72, following a two-day win of 55.93.',\n",
              "  'The technology-laced Nasdaq Composite Index .IXIC eased 8.52 points, or 0.51 percent, to 1,670.21.'),\n",
              " ('The technology-laced Nasdaq Composite Index <.IXIC> tacked on 5.91 points, or 0.29 percent, to 2,053.27.',\n",
              "  'The technology-focused Nasdaq Composite Index <.IXIC> advanced 6 points, or 0.30 percent, to 2,053, erasing earlier losses.'),\n",
              " ('Of 24 million phoned-in votes, 50.28 percent were for Studdard, putting him 130,000 votes ahead of Aiken.',\n",
              "  'Of the 24 million phone votes cast, Studdard was only 130,000 votes ahead of Aiken.'),\n",
              " ('Hong Kong was flat, Australia , Singapore and South Korea lost 0.2-0.4 percent.',\n",
              "  'Australia was flat, Singapore was down 0.3 percent by midday and South Korea added 0.2 percent.'),\n",
              " ('Hong Kong stocks close down 0.28%',\n",
              "  'Hong Kong stocks open 0.62 pct higher'),\n",
              " ('Tokyo stocks close down 0.26pc', 'Tokyo stocks close down 1.53%'),\n",
              " ('Singapore stocks end up 0.26 percent', 'Singapore stocks end up 0.11 pct'),\n",
              " ('Tokyo stocks open down 0.24%', 'Tokyo stocks open up 0.52%'),\n",
              " ('Singapore shares open 0.28% lower on Thursday',\n",
              "  'Singapore shares open 0.19% higher on Thursday')]"
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDlXcuG7T6RS"
      },
      "source": [
        "## Your turn \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kYeWh9oTtoy"
      },
      "source": [
        "## 1. A model using simple linguistic and textual features\n",
        "\n",
        "Now it's your turn to develop a simple model for the STS task. The model should rely on simple features but without using distributional or distributed representations (for which we will have a dedicated model in (2)). You should:\n",
        "* 1.1 **Preprocess** the sentences as you deem appropriate:\\\n",
        "[Tokenize](https://www.nltk.org/api/nltk.tokenize.html), lower-case, [remove stopwords](https://scikit-learn.org/stable/modules/feature_extraction.html#using-stop-words)\\*... you could also try with [stemming](https://www.nltk.org/howto/stem.html) or [lemmatization](https://www.geeksforgeeks.org/python-lemmatization-with-nltk/), [pos-tagging](https://www.nltk.org/book/ch05.html) or [removing punctuation](https://www.geeksforgeeks.org/string-punctuation-in-python/), for example.\n",
        "* 1.2 Come up with different kinds of **features**. \\\n",
        "This is the most important part. See below for more details.\n",
        "* 1.3 Choose **at least a couple of different models** from sklearn (for example, LinearRegression or RandomForestRegression)\n",
        "* 1.4 **Train** different feature and model combinations on the training set.\n",
        "* 1.5 **Evaluate** them all on the **development set**.\n",
        "* 1.6 **Evaluate** the best conifiguration (i.e. the one which obtained the best result on the development set) on the **test set**.\n",
        "\n",
        "\\* Not all stopwords lists are the same and you might want to adapt yours depending on the application. However, for the sake of simplicity, you can just use the stopwords list provided in NLTK [like shown here](https://awhan.wordpress.com/2016/06/05/scikit-learn-nlp-list-english-stopwords/).\n",
        "\n",
        "### About the features\n",
        "**You should implement at least 6 different features**, of which **there must be, at least**:\n",
        "- One **WordNet**-based feature (for example, does $s_2$ contain synonyms or hypernyms of words in $s_1$? how many of them?) You can use [nltk's wordnet interface](https://www.nltk.org/howto/wordnet.html).\n",
        "- One **TF-IDF** based feature. You can fit a [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html?highlight=tfidf#sklearn.feature_extraction.text.TfidfVectorizer) on the whole training set, treating a sentence as a document. Then you can calculate the similarity (cosine, euclidean...) between the representations of $s_1$ and $s_2$.\n",
        "- One more complex **overlap** feature (based on n-gram overlap, $n>1$). You can use the *ngram_range* parameter of [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Your model can, *additionally*, include the word overlap feature used by the baseline model, leaving it as is or modifying it (for example trying cosine similarity).\n",
        "\n",
        "More ideas: consider exploiting [the PPDB resource](https://aclanthology.org/S12-1060.pdf), using the overlap of [syntactic roles](https://spacy.io/usage/linguistic-features) or pos-tags, [word alignment](https://aclanthology.org/Q14-1018.pdf) features (for example with [this tool](https://github.com/ma-sultan/monolingual-word-aligner), or something simpler), a Machine Translation evaluation metric (like [BLUE](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/)), the edit distance...\n",
        "\n",
        "IMPORTANT: You should check the SemEval reports listed at the beginning (for example [this one](https://aclanthology.org/S12-1060.pdf)) or model descriptions (like [this one](https://aclanthology.org/S17-2028.pdf)) to see what features other models have relied on in the past and/or how they were implemented. \n",
        "\n",
        "NOTE: Empty functions are just there for your orientation, feel free to organize the code as it suits you, adding as many functions as needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idljrR58IKKa"
      },
      "source": [
        "#### Preprocess all the data\n",
        "\n",
        "def preprocess_dataset(dataset):\n",
        "  ## TO COMPLETE\n",
        "  return preprocessed_dataset\n",
        "\n",
        "#### Extract features\n",
        "def extract_features(preprocessed_dataset):\n",
        "  '''Similarly to baseline_features, this function could return an array with all features, or a DataFrame'''\n",
        "  ## TO COMPLETE\n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWQg0ACTIQj3"
      },
      "source": [
        "##### Build and train different models. You can do a little feature ablation (i.e. removing one feature at a time)\n",
        "##### to see the usefulness of the different features.\n",
        "\n",
        "## TO COMPLETE\n",
        "\n",
        "##### Evaluate the models on the dev set. You can use the evaluate() function above.\n",
        "\n",
        "## TO COMPLETE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fduut5aIIfvR"
      },
      "source": [
        "#### Evaluate the best model on the test set. \n",
        "\n",
        "## TO COMPLETE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBI8d_vwXlTZ"
      },
      "source": [
        "#### A bit of reflection\n",
        "\n",
        "In the report, you can write:\n",
        "- Why did you preprocess the data in this way? If you tried multiple preprocessing variants, which one worked best and why do you think that is the case?\n",
        "- Why did you choose these features? Did you find them somewhere?\n",
        "- What were the most useful features in the end? Why do you think this is the case?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK4RYqyrpgA7"
      },
      "source": [
        "## 2. A model relying on distributional and distributed word representations\n",
        "\n",
        "The second model that you'll build will rely on a different kind of features, obtained from word vector representations. You have to:\n",
        "- 2.1 **Preprocess** the text. Be careful, you might want to follow a different strategy this time (would stemming be a good idea?)\n",
        "- 2.2 **Build/download word representations** (more on this below)\n",
        "- 2.3 **Aggregate** the representations of words in a sentence (for example, by taking their average, or the sum) to obtain a sentence representation. \n",
        "- 2.4 Try **two kinds of features** in a simple ML model: \\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;- (a) the concatenation of the sentence representations themselves;\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;- (b) different similarity/distance measures (cosine, euclidean...)\n",
        "- 2.5 As done before, **train** the models on the training set, **evaluate** them on the development set, and finally **evaluate** the best model on the test set.\n",
        "\n",
        "### About the word representations\n",
        "You **must use two kinds** of word representations: distributional and distributed.\n",
        "All word representations that we will use are obtained based on the distributional hypothesis (Harris, 1954): the idea that semantically similar words tend to occur in the same (or similar) contexts. Or, as Firth (1959) put it, \"*you shall know a word by the company it keeps*\".\n",
        "\n",
        "#### **Distributional representations**\n",
        "\n",
        "\n",
        "We will build distributional representations for words by collecting co-occurrence counts from the sentences. First, we need to create a **vocabulary** $V$, which is basically a list of words seen in the training set for which we want to build a vector representation.\n",
        "Let $|V|$ be the size of our vocabulary. We will create a matrix $M$ of size $|V| \\times |V|$. $M_{ij}$ will contain the number of times word $i$ *co-occurred* with word $j$. The definition of *co-occurrence* is flexible. In this exercise we will define it as *appearing in the same sentence*. \n",
        "Most of the code to obtain them is already there.\n",
        "\n",
        "Once we have obtained the co-occurrence counts, we can still modify the matrix in order to make the most of them. One way of doing so is calculating the Pointwise Mutual Information (PMI), which quantifies the interdependence between two variables (= two words), or how expected the co-occurrence of two words is.\n",
        "\n",
        "$$\n",
        "\\text{PMI}(x,y) = \\log \\left( \\frac{P(x,y)}{P(x)P(y)} \\right)\n",
        "$$\n",
        "\n",
        "$P(x,y)$ is the number of times words $x$ and $y$ have co-occurred. $P(x)$ is the probability of word $x$ (= all co-occurrences where $x$ appears).\n",
        "\n",
        "#### **Distributed representations**\n",
        "We will use out-of-the-box word embeddings. These are dense word representations that have been trained on large amounts of text using a neural model with a language-model-like objective. Concretely, for this exercise we will use pre-trained [word2vec](https://towardsdatascience.com/word2vec-explained-49c52b4ccb71) word embeddings.\n",
        "<br/><br/>\n",
        "\n",
        "An important part of the code is provided so you'll mainly have to take care of aggregating the word embeddings of the words in a sentence.\n",
        "There are different libraries to use word embeddings. For this experiment we will be using [magnitude](https://github.com/plasticityai/magnitude) because of its high speed and because its ability of inducing vectors for unknown words.\n",
        "You can download the word embeddings we'll use [here](http://magnitude.plasticity.ai/word2vec/medium/GoogleNews-vectors-negative300.magnitude). Uploading them to Google Drive will make uploading them to Colab faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu0K5-FmvPPm"
      },
      "source": [
        "### Distributional representations: experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7p_0JAoD-bE"
      },
      "source": [
        "#### Building distributional representations\n",
        "sws = stopwords.words('english')\n",
        "sws = set(list(sws) + [p for p in punctuation])\n",
        "\n",
        "def put_sentences_together(dataset):\n",
        "  all_sentences = []\n",
        "  for s1, s2 in dataset[\"train\"][\"data\"]:\n",
        "    all_sentences.extend([s1, s2])\n",
        "  return all_sentences\n",
        "\n",
        "def create_vocabulary(dataset, count_threshold=1, voc_threshold=None, stopwords=set(), lowercase=False):\n",
        "    \"\"\"    \n",
        "    Function using word counts to build a vocabulary \n",
        "    Params:\n",
        "        corpus (list of list of strings): corpus of sentences\n",
        "        count_threshold (int): minimum number of occurences necessary for a word to be included in the vocabulary\n",
        "        voc_threshold (int): maximum size of the vocabulary \n",
        "        stopwords: a set of words which are excluded from the vocabulary\n",
        "        lowercase: bool. If True, all words are lowercased (which results in a smaller, more compact vocabulary)\n",
        "    IMPORTANT: the vocabulary includes \"UNK\", which is a placeholder for an unknown word and it will later be assigned a zero vector.\n",
        "    Returns:\n",
        "        vocabulary (dictionary): keys: list of distinct words across the corpus\n",
        "                                 values: indexes corresponding to each word sorted by frequency        \n",
        "    \"\"\"    \n",
        "    corpus = put_sentences_together(dataset)\n",
        "    word_counts = {}\n",
        "    for sent in corpus:\n",
        "        for word in word_tokenize(sent):\n",
        "            if lowercase:\n",
        "              word = word.lower()          \n",
        "            if word not in word_counts:\n",
        "                word_counts[word] = 0\n",
        "            word_counts[word] += 1\n",
        "\n",
        "    # Create a dictionary called <filtered_word_counts> (with words as keys and their frequencies as values).\n",
        "    # Include only those words that appear more than <count_threshold> times,\n",
        "    # and which are not in the set of stopwords.\n",
        "    ## TO COMPLETE\n",
        "    filtered_word_counts = {word: count for word, count in word_counts.items() if (count>=count_threshold)}\n",
        "  \n",
        "    # Create a list called <words> sorting the words from highest to lowest frequency\n",
        "    ## TO COMPLETE\n",
        "    #filtered_word_counts = dict(sorted(filtered_word_counts.items(), key=lambda item: item[1], reverse=True)) # tri par valeurs decroissantes\n",
        "    #words = [w for w in filtered_word_counts.keys()]\n",
        "    #words = sorted(filtered_word_counts.keys(), key=word_counts.get, reverse=True) + ['UNK']\n",
        "    words = sorted(filtered_word_counts.keys(), key=filtered_word_counts.get, reverse=True) + ['UNK'] # tri par nb d'occ decroissant\n",
        "\n",
        "    if voc_threshold is not None:\n",
        "        words = words[:voc_threshold] \n",
        "        if ('UNK' not in words) : \n",
        "          words = words + ['UNK']\n",
        "    vocabulary = {words[i] : i for i in range(len(words))}\n",
        "    return vocabulary, {word: filtered_word_counts.get(word, 0) for word in vocabulary}\n",
        "\n",
        "def co_occurence_matrix(dataset, vocab, lowercase=False, stemming=False, lemmatization=False):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "        dataset: output of load_data()\n",
        "        vocab: first output of create_vocabulary(). These are the words that will be included in the matrix        \n",
        "    Returns:\n",
        "        matrix (array of size (len(vocab), len(vocab))): the co-occurrence matrix, using the same ordering as the vocabulary given in input\n",
        "    \"\"\" \n",
        "    l = len(vocab)\n",
        "    all_sentences = put_sentences_together(dataset)\n",
        "    M = np.zeros((l,l))\n",
        "    for sent in all_sentences:\n",
        "        # Preprocessing\n",
        "        if (lowercase) : \n",
        "          sent = sent.lower()\n",
        "\n",
        "        if (lemmatization):\n",
        "          lemmatizer = WordNetLemmatizer() # sortir cette instruction de la boucle for\n",
        "          sent = \" \".join([lemmatizer.lemmatize(m) for m in word_tokenize(sent)])\n",
        "\n",
        "        elif (stemming):\n",
        "          stemm = LancasterStemmer()\n",
        "          sent = \" \".join([stemm.stem(m) for m in word_tokenize(sent)])\n",
        "\n",
        "        sent = word_tokenize(sent)\n",
        "        sent_idcs = [vocab.get(word, len(vocab)-1) for word in sent]\n",
        "        for i, idx in enumerate(sent_idcs):            \n",
        "            for j, ctx_idx in enumerate(sent_idcs[i+1:]):\n",
        "              M[idx][ctx_idx] +=1\n",
        "              M[ctx_idx][idx] +=1\n",
        "    return M  \n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anSbFG_zpko9"
      },
      "source": [
        "# Build vocabulary\n",
        "## TO COMPLETE\n",
        "vocab = create_vocabulary(dataset, count_threshold=2, voc_threshold=None, stopwords=set(), lowercase=True)\n",
        "\n",
        "# Build co-occurrence matrix\n",
        "M = co_occurence_matrix(dataset, vocab[0], lowercase=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemples de phrases similaires / disimilaires dans le dataset\n",
        "subsize = 100\n",
        "np.array(dataset['train']['data'][:subsize])[np.array(dataset[\"train\"][\"scores\"][:subsize]) <= 0.6]"
      ],
      "metadata": {
        "id": "I2jpbo2WoLte",
        "outputId": "45d71299-3537-4cc0-b13e-70f4cf1eb20f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['A man is smoking.', 'A man is skating.'],\n",
              "       ['A woman is writing.', 'A woman is swimming.'],\n",
              "       ['A man is dancing.', 'A man is talking.'],\n",
              "       ['A man is fishing.', 'A man is exercising.'],\n",
              "       ['Two boys are driving.', 'Two bays are dancing.']], dtype='<U56')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "M"
      ],
      "metadata": {
        "id": "B5rWnkYRm98Q",
        "outputId": "0968c76a-78c7-4db6-85c5-a1ffd6c40e58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[8.800e+01, 6.568e+03, 4.644e+03, ..., 0.000e+00, 0.000e+00,\n",
              "        3.055e+03],\n",
              "       [6.568e+03, 5.652e+03, 2.172e+03, ..., 0.000e+00, 0.000e+00,\n",
              "        1.891e+03],\n",
              "       [4.644e+03, 2.172e+03, 3.708e+03, ..., 0.000e+00, 1.000e+00,\n",
              "        2.845e+03],\n",
              "       ...,\n",
              "       [0.000e+00, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
              "        0.000e+00],\n",
              "       [0.000e+00, 0.000e+00, 1.000e+00, ..., 0.000e+00, 0.000e+00,\n",
              "        2.000e+00],\n",
              "       [3.055e+03, 1.891e+03, 2.845e+03, ..., 0.000e+00, 2.000e+00,\n",
              "        3.116e+03]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#(~M.any(axis=0)).any() \n",
        "#np.where(~M.any(axis=0))[0] # colonne nulle\n",
        "#(~M.any(axis=1)).any() \n",
        "#np.where(~M.any(axis=1))[0] # ligne nulle \n",
        "#print(list(vocab[0].keys())[list(vocab[0].values()).index(14407)])  # UNK"
      ],
      "metadata": {
        "id": "medPQM8RGBvc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvH2A6WDW34I"
      },
      "source": [
        "# A function that converts the matrix values to PMI\n",
        "def pmi(M, positive=True):\n",
        "    sum_vec = M.sum(axis=0)\n",
        "    sum_tot = sum_vec.sum()\n",
        "    with np.errstate(divide='ignore'):\n",
        "        pmi = np.log((M * sum_tot) / (np.outer(sum_vec, sum_vec)))   #print((np.outer(sum_vec, sum_vec) == 0).any()) #division par 0 par endroits       \n",
        "    pmi[np.isinf(pmi)] = 0.0  # log(0) = 0\n",
        "    # A voir : \n",
        "    #pmi[np.isnan(pmi)] = 0.0  # divisions par 0 --> valeur nan\n",
        "    if positive:\n",
        "        pmi[pmi < 0] = 0.0\n",
        "    return pmi\n",
        "\n",
        "# To apply the transformation:\n",
        "PMI_M = pmi(M)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PMI_M"
      ],
      "metadata": {
        "id": "PX-ARhOKnQEy",
        "outputId": "e17d84ed-2e3d-46d2-eeb6-87462279bdda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.59886282, 0.14676802, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.59886282, 0.65425595, 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.14676802, 0.        , 0.02181614, ..., 0.        , 0.25043992,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.25043992, ..., 0.        , 0.        ,\n",
              "        1.16768406],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 1.16768406,\n",
              "        0.29606753]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEcxnd-faB8L"
      },
      "source": [
        "Here's the part where you get to combine the vectors of the words making up a sentence into a sentence representation. You can complete and modify the function `assign_distributional_vectors` to your convenience, trying different things:\n",
        "- Choose **what words to include** and which not to include. You can use a postag filter* to, for example, include only words that are richer in content (nouns, verbs and adjectives)\n",
        "- Choose **an operation** to combine the word representations. In doing that, take into account that words that are not in the vocabulary should be assigned the id of the 'UNK' token, which corresponds to a zero vector. Alternatively you can leave out all unknown words.\n",
        "- Try using the **concatenation** of the representations of $s_1$ and $s_2$ as features, and one or multiple **similarity/distance metrics** as **features**.\n",
        "\n",
        "\\* For postagging I recommend using the universal tagset for its simplicity. Below I included the code to load the postagger. Check out the tag list at 2.3 [here](https://www.nltk.org/book/ch05.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDSzPLtj0_6V",
        "outputId": "27ea8431-d5d3-4d46-b951-fce39f88b709",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from traitlets.traitlets import ForwardDeclaredInstance\n",
        "# Loading the postagger\n",
        "nltk.download('universal_tagset')\n",
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def euclidean_dist(s1, s2) : \n",
        "  return np.linalg.norm(s1 - s2)\n",
        "\n",
        "def assign_distributional_vectors(data, M, vocab, sim_or_dist=True, postag=False, lowercase=False, agg=lambda x: np.sum(x, axis=0), lemmatization=False, stemming=False, distance_measure=lambda x,y: euclidean_dist(x,y)):\n",
        "#def assign_distributional_vectors(data, M, vocab, sim_or_dist=True, postag=False, lowercase=False, agg=lambda x: np.sum(x, axis=0)):\n",
        "  '''This functions assigns each sentence a vector and optionally calculates the similarity/distance \n",
        "  between the representations of s1 and s2.\n",
        "  Parameters\n",
        "    data: list of tuples (like dataset['train']['data'])\n",
        "    M: a matrix of distributional representations for all words in the vocabulary\n",
        "    vocab: first output of create_vocabulary(). These are the words that will be included in the matrix        \n",
        "    sim_or_dist: bool. If True, we will use a similarity or distance as the only feature. If False,\n",
        "    we will use the concatenation of the representations of s1 and s2.\n",
        "    postag: whether we want to apply a postag-based filter to obtain sentence representations\n",
        "    lowercase: bool. If True, words are lowercased. You should set it to True if the vocabulary is lowercased.\n",
        "  Returns:\n",
        "    features: an array with the data transformed into features '''\n",
        "\n",
        "  if sim_or_dist:\n",
        "    features = np.zeros((len(data), 1))\n",
        "  else:\n",
        "    features = np.zeros((len(data), M.shape[1]*2))\n",
        "\n",
        "  PMI_M = pmi(M, positive=True) #representation vectorielle des mots du vocabulaire\n",
        "\n",
        "  for i, (s1, s2) in enumerate(data):\n",
        "    # Tokenize, lowercase if lowercase=True, and if postag=True, postag s1 and s2    \n",
        "    ## TO COMPLETE\n",
        "\n",
        "    # Preprocessing\n",
        "    if (lowercase) :\n",
        "      s1, s2 = s1.lower(), s2.lower()\n",
        "    \n",
        "    if (lemmatization):\n",
        "      lemmatizer = WordNetLemmatizer() # sortir cette instruction de la boucle for\n",
        "      s1 = \" \".join([lemmatizer.lemmatize(m) for m in word_tokenize(s1)])\n",
        "      s2 = \" \".join([lemmatizer.lemmatize(m) for m in word_tokenize(s2)])\n",
        "    \n",
        "    elif (stemming):\n",
        "      stemm = LancasterStemmer()\n",
        "      s1 = \" \".join([stemm.stem(m) for m in word_tokenize(s1)])\n",
        "      s2 = \" \".join([stemm.stem(m) for m in word_tokenize(s2)])\n",
        "\n",
        "    s1_tok, s2_tok = word_tokenize(s1), word_tokenize(s2)\n",
        "    \n",
        "    # Now create two lists, one for each sentence, with the word representations that you want to use\n",
        "    # You can go through the words (or word, pos) in each sentence and decide whether you keep their representation or not\n",
        "    ## TO COMPLETE\n",
        "    #PMI_M = pmi(M, positive=True) #representation vectorielle des mots du vocabulaire\n",
        "\n",
        "    s1vecs, s2vecs = [], []\n",
        "    if (postag) :\n",
        "      keep_pos = ['NN', 'VBP', 'VBZ', 'VBG', 'VB', 'JJ'] #nouns, verbs and adjectives\n",
        "      s1_pos, s2_pos = pos_tag(s1_tok), pos_tag(s2_tok)\n",
        "      for (w, pos) in s1_pos :\n",
        "        if (pos in keep_pos) :\n",
        "          s1vecs.append(PMI_M[vocab.get(w, vocab['UNK'])]) # on ajoute la representation vectorielle du mot si dispo, sinon vect 0\n",
        "      for (w, pos) in s2_pos : \n",
        "        if (pos in keep_pos) : \n",
        "          s2vecs.append(PMI_M[vocab.get(w, vocab['UNK'])])\n",
        "    else : \n",
        "      for w in s1_tok :\n",
        "        s1vecs.append(PMI_M[vocab.get(w, vocab['UNK'])])\n",
        "      for w in s2_tok :\n",
        "        s2vecs.append(PMI_M[vocab.get(w, vocab['UNK'])])\n",
        "\n",
        "    # It is possible that some sentences will not have any word representation available.\n",
        "    # We assign them a 0-vector in this case (be careful, because this could result in a cosine of NaN)\n",
        "    if not s1vecs:\n",
        "      s1vecs = [np.zeros(M.shape[1])]  # or PMI_M[vocab['UNK']]\n",
        "    if not s2vecs:\n",
        "      s2vecs = [np.zeros(M.shape[1])]    \n",
        "    \n",
        "    # Aggregate the representations of words in a sentence, for example by averaging them\n",
        "    ## TO COMPLETE \n",
        "    #print(s1vecs)\n",
        "    s1vec = agg(s1vecs) #s1vecs.mean(axis=0)\n",
        "    s2vec = agg(s2vecs) #s2vecs.mean(axis=0)\n",
        "\n",
        "    # Fill in features[i] with the desired feature (one or more similarity/distance measures if sim=True, \n",
        "    # a concatenation of the representations otherwise)\n",
        "    ## TO COMPLETE\n",
        "    if (sim_or_dist) :\n",
        "      features[i] = distance_measure(s1vec, s2vec)\n",
        "      #features[i] = np.linalg.norm(s1vec - s2vec), features[i] = cosine(s1vec, s2vec)\n",
        "    else : \n",
        "      features[i] = np.concatenate((s1vec,s2vec))\n",
        "   \n",
        "  return features\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vec_house = PMI_M[vocab[0].get('house', vocab[0]['UNK'])]\n",
        "vec_habitation = PMI_M[vocab[0].get('habitation', vocab[0]['UNK'])]\n",
        "vec_dog = PMI_M[vocab[0].get('dog', vocab[0]['UNK'])]\n",
        "vec_cat = PMI_M[vocab[0].get('cat', vocab[0]['UNK'])]\n",
        "vec_butterfly = PMI_M[vocab[0].get('butterfly', vocab[0]['UNK'])]"
      ],
      "metadata": {
        "id": "hLDoXRs-Q4Yc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.linalg.norm(vec_house - vec_dog), np.linalg.norm(vec_house - vec_cat), np.linalg.norm(vec_dog - vec_cat) #, np.linalg.norm(vec_home - vec_habitation), np.linalg.norm(vec_house - vec_butterfly), np.linalg.norm(vec_cat - vec_butterfly)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwpPj7ZXRJcv",
        "outputId": "1ea3d9b3-a28b-4202-d0c3-7d848d10e9c4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64.24002963863944, 62.490031643369946, 51.549568971922085)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#assign_distributional_vectors(dataset['train']['data'][:3], M, vocab[0], sim_or_dist=True, postag=True, lowercase=True, agg=lambda x: np.mean(x, axis=0))\n",
        "#dataset['train']['scores'][:3]"
      ],
      "metadata": {
        "id": "CnnGCZcGqbOR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain train_x, dev_x, test_x\n",
        "# Try different combinations (using M, PMI_M, different aggregation functions, with and without postag filtering...)\n",
        "## TO COMPLETE\n",
        "from nltk.stem import WordNetLemmatizer, LancasterStemmer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "train_y = np.array(dataset['train']['scores']) / 5\n",
        "dev_y = np.array(dataset['dev']['scores']) / 5\n",
        "test_y = np.array(dataset['test']['scores']) / 5\n",
        "\n",
        "def evaluate_model(dataset=dataset, count_threshold=2, voc_threshold=None, stopwords=set(), postag=False, \\\n",
        "                   lemmatization=False, stemming=False, size=-1, sim_or_dist=False, lowercase=False, PMI=False, \\\n",
        "                   agg=lambda x: np.mean(x, axis=0), regression=\"linear\", distance_measure=lambda x,y: euclidean_dist(x,y)):\n",
        "    \"\"\"regression : \"linear\" or \"rf\" (random forest)\"\"\" \n",
        "    \n",
        "    # Create vocab\n",
        "    print(\"Creating vocab and co-occ matrix\")\n",
        "    vocab = create_vocabulary(dataset, count_threshold=count_threshold, voc_threshold=voc_threshold, stopwords=set(), lowercase=lowercase)\n",
        "    \n",
        "    # Build co-occurrence matrix # dataset['train']['data'][:size]\n",
        "    M = co_occurence_matrix(dataset, vocab[0], lowercase=lowercase, lemmatization=lemmatization, stemming=stemming)\n",
        "\n",
        "    # Train, Dev, Test\n",
        "    print(\"Assigning vectors\")\n",
        "    train_x = assign_distributional_vectors(dataset['train']['data'][:size], M, vocab[0], sim_or_dist=sim_or_dist, postag=postag, lowercase=lowercase, agg=agg, lemmatization=lemmatization, stemming=stemming, distance_measure=distance_measure)\n",
        "    dev_x = assign_distributional_vectors(dataset['dev']['data'][:size], M, vocab[0], sim_or_dist=sim_or_dist, postag=postag, lowercase=lowercase, agg=agg, lemmatization=lemmatization, stemming=stemming, distance_measure=distance_measure)\n",
        "    test_x = assign_distributional_vectors(dataset['test']['data'][:size], M, vocab[0], sim_or_dist=sim_or_dist, postag=postag, lowercase=lowercase, agg=agg, lemmatization=lemmatization, stemming=stemming, distance_measure=distance_measure)\n",
        "    print(\"Normalizing scores\")\n",
        "    train_y = np.array(dataset['train']['scores'][:size]) / 5\n",
        "    dev_y = np.array(dataset['dev']['scores'][:size]) / 5\n",
        "    test_y = np.array(dataset['test']['scores'][:size]) / 5\n",
        "\n",
        "    # Initializing the model\n",
        "    if (regression == \"linear\"):\n",
        "      linreg = LinearRegression()\n",
        "      print(\"train_x[:size] \", train_x[:size])\n",
        "      print(\"isnan(ar)\", np.isnan(train_x[:size]).any())\n",
        "\n",
        "      linreg.fit(train_x[:size], train_y[:size])\n",
        "      predictions = linreg.predict(dev_x[:size])\n",
        "      print(\"Coeff regression lineaire : \", linreg.coef_, \" - intercept : \", linreg.intercept_)\n",
        "\n",
        "    else : # regression ==\"rf\"\n",
        "      regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "      regr.fit(train_x[:size], train_y[:size])\n",
        "      predictions = dev_x[:size]\n",
        "\n",
        "    # Evaluating\n",
        "    print(\"Evaluating regression model\")\n",
        "    pears = evaluate(predictions[:size], dev_y[:size])\n",
        "    print(\"Pearson's r obtained on the dev set:\", pears)\n",
        "    return train_x, dev_x, test_x, pears\n"
      ],
      "metadata": {
        "id": "Bv8AT62oPe37"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resultats = []\n",
        "\n",
        "### Features = vecteurs de phrases\n",
        "# Basic : no stop words, no pos, no lemmat, no stemm, no dist, no lowercase, no pmi, reg lineaire\n",
        "res = evaluate_model(dataset=dataset, count_threshold=2, voc_threshold=None, stopwords=set(), postag=False, lemmatization=False, stemming=False, size=30, sim_or_dist=False, lowercase=False, PMI=False, agg=lambda x: np.mean(x, axis=0), regression=\"linear\")\n",
        "print(\"Pearson : \", res[-1])\n",
        "resultats.append(res[-1])\n",
        "\n",
        "# # idem mais avec sum()\n",
        "# res = evaluate_model(dataset=dataset, count_threshold=2, voc_threshold=None, stopwords=set(), postag=False, lemmatization=False, stemming=False, size=30, sim_or_dist=False, lowercase=False, PMI=False, agg=lambda x: np.sum(x, axis=0), regression=\"linear\")\n",
        "# print(\"Pearson : \", res[-1])\n",
        "# resultats.append(res[-1])\n",
        "\n",
        "# # idem (mean()) mais avec random forest()\n",
        "# res = evaluate_model(dataset=dataset, count_threshold=2, voc_threshold=None, stopwords=set(), postag=False, lemmatization=False, stemming=False, size=30, sim_or_dist=False, lowercase=False, PMI=False, agg=lambda x: np.mean(x, axis=0), regression=\"rf\")\n",
        "# print(\"Pearson : \", res[-1])\n",
        "# resultats.append(res[-1])\n",
        "\n",
        "\n",
        "# # PMI & lower\n",
        "# res = evaluate_model(dataset=dataset, count_threshold=2, voc_threshold=None, stopwords=set(), postag=False, lemmatization=False, stemming=False, size=30, sim_or_dist=False, lowercase=True, PMI=True, agg=lambda x: np.sum(x, axis=0), regression=\"linear\")\n",
        "# resultats.append(res[-1])\n",
        "# print(\"Pearson : \", res[-1])\n",
        "\n",
        "# # idem sum()\n",
        "# res = evaluate_model(dataset=dataset, count_threshold=2, voc_threshold=None, stopwords=set(), postag=False, lemmatization=False, stemming=False, size=30, sim_or_dist=False, lowercase=True, PMI=True, agg=lambda x: np.sum(x, axis=0), regression=\"linear\")\n",
        "# resultats.append(res[-1])\n",
        "# print(\"Pearson : \", res[-1])\n",
        "\n",
        "# # idem (mean()) mais avec rf\n",
        "# res = evaluate_model(dataset=dataset, count_threshold=2, voc_threshold=None, stopwords=set(), postag=False, lemmatization=False, stemming=False, size=30, sim_or_dist=False, lowercase=True, PMI=True, agg=lambda x: np.mean(x, axis=0), regression=\"rf\")\n",
        "# resultats.append(res[-1])\n",
        "# print(\"Pearson : \", res[-1])\n",
        "\n",
        "# # PMI & lower & stemming\n",
        "# import nltk\n",
        "# nltk.download('wordnet')\n",
        "# res = evaluate_model(dataset=dataset, count_threshold=2, voc_threshold=None, stopwords=set(), postag=False, lemmatization=True, stemming=False, size=30, sim_or_dist=False, lowercase=True, PMI=True, agg=lambda x: np.mean(x, axis=0), regression=\"linear\")\n",
        "# resultats.append(res[-1])\n",
        "# print(\"Pearson : \", res[-1])\n",
        "\n",
        "# # PMI & lower & lemmatization\n",
        "# res = evaluate_model(dataset=dataset, count_threshold=2, voc_threshold=None, stopwords=set(), postag=False, lemmatization=True, stemming=False, size=30, sim_or_dist=False, lowercase=True, PMI=True, agg=lambda x: np.mean(x, axis=0), regression=\"linear\")\n",
        "# resultats.append(res[-1])\n",
        "# print(\"Pearson : \", res[-1])\n",
        "\n",
        "# # PMI & lower & postag\n",
        "# res = evaluate_model(dataset=dataset, count_threshold=2, voc_threshold=None, stopwords=set(), postag=True, lemmatization=False, stemming=False, size=30, sim_or_dist=False, lowercase=True, PMI=True, agg=lambda x: np.mean(x, axis=0), regression=\"linear\")\n",
        "# resultats.append(res[-1])\n",
        "# print(\"Pearson : \", res[-1])\n",
        "\n",
        "\n",
        "# ### Feature = distance\n",
        "# # PMI & lower & postag & sim or dist\n",
        "# res = evaluate_model(dataset=dataset, count_threshold=2, voc_threshold=None, stopwords=set(), postag=True, lemmatization=False, stemming=False, size=30, sim_or_dist=True, lowercase=True, PMI=True, agg=lambda x: np.mean(x, axis=0), regression=\"linear\")\n",
        "# resultats.append(res[-1])\n",
        "# print(\"Pearson : \", res[-1])\n",
        "\n",
        "# # PMI & lower & postag & sim or dist & stemming\n",
        "# res = evaluate_model(dataset=dataset, count_threshold=2, voc_threshold=None, stopwords=set(), postag=True, lemmatization=False, stemming=True, size=30, sim_or_dist=True, lowercase=True, PMI=True, agg=lambda x: np.mean(x, axis=0), regression=\"linear\")\n",
        "# resultats.append(res[-1])\n",
        "# print(\"Pearson : \", res[-1])\n",
        "\n",
        "\n",
        "### Features = vecteurs de phrases"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig1eE7otUCJX",
        "outputId": "f9abd70f-962d-4336-ba21-dcefc91c198c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating vocab and co-occ matrix\n",
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.33708258 0.01922778 0.53414367 ... 0.         0.         0.0734461 ]\n",
            " [0.70033481 0.03938426 0.894298   ... 0.         0.         0.        ]\n",
            " [0.4966451  0.05107072 0.6511207  ... 0.15919371 0.30913498 0.10886438]\n",
            " ...\n",
            " [0.56111996 0.08001309 0.6209091  ... 0.21889135 0.21131327 0.07144943]\n",
            " [0.67643183 0.03409997 0.88275512 ... 0.         0.         0.        ]\n",
            " [0.62838541 0.06379912 0.75824579 ... 0.         0.         0.        ]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 0.00060998  0.0029025  -0.00189889 ...  0.00042302 -0.00172205\n",
            "  0.00012098]  - intercept :  1.512887920722259\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.5665312125058032\n",
            "Pearson :  0.5665312125058032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ci-dessous : partie ajoutée par Pierrick\n",
        "\n",
        "**Optimisation des hyperparamètres**\n",
        "\n",
        "\n",
        "*   Remplacement du dataset complet par des mini batch pour diminuer le temps de réponse\n",
        "*   Utilisation du framework Optuna"
      ],
      "metadata": {
        "id": "F8UrFHddcrpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "import optuna"
      ],
      "metadata": {
        "id": "WGGLQ8WTa1Ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.keys())\n",
        "print(dataset[\"train\"].keys())"
      ],
      "metadata": {
        "id": "5ZHAgF8pb61V",
        "outputId": "c512fd44-26b4-42a0-a5c0-3067c0fe608c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['dev', 'test', 'train'])\n",
            "dict_keys(['data', 'scores'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Création d'un minidataset pour pouvoir itérer plus vite les fonctions d'évaluation du modèle\n",
        "### Paramètre : minibatch_ratio, le coefficient de réduction du dataset\n",
        "\n",
        "def random_mini_batches(X,Y, minibatch_ratio):\n",
        "    m = len(Y)           # number of examples\n",
        "    minibatch_size = int(m * minibatch_ratio)\n",
        "    # Lets shuffle X and Y\n",
        "    permutation = list(np.random.permutation(m))[:minibatch_size]            # shuffled index of examples\n",
        "    shuffled_X = [X[idx] for idx in permutation]\n",
        "    shuffled_Y = [Y[idx] for idx in permutation]\n",
        "    return shuffled_X , shuffled_Y\n",
        "\n",
        "def random_mini_dataset(dataset, mini_ratio):\n",
        "    d_res = {}\n",
        "    for d in dataset.keys():\n",
        "        mini_X, mini_y = random_mini_batches(dataset[d][list(dataset[d].keys())[0]], dataset[d][list(dataset[d].keys())[1]], mini_ratio)\n",
        "        d_res[d] = {}\n",
        "        d_res[d][\"data\"] = mini_X\n",
        "        d_res[d][\"scores\"] = mini_y\n",
        "    return d_res\n",
        "\n",
        "mini_dataset = random_mini_dataset(dataset, 0.1)"
      ],
      "metadata": {
        "id": "LLUPEg-genCi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    \n",
        "    # Dataset -> réduction d'un facteur\n",
        "    minidataset = random_mini_dataset(dataset, 0.1)\n",
        "\n",
        "    # Hyperparamètres - définition de l'espace de recherche\n",
        "    trial_count_threshold = trial.suggest_int(\"count_threshold\", 1, 10)  # minimum number of occurences necessary for a word to be included in the vocabulary\n",
        "    trial_postag = trial.suggest_categorical(\"postag\", [True, False]) # Binaire utilisation du POSTAG\n",
        "    trial_voc_threshold = trial.suggest_int(\"voc_threshold\", 1000, 10000) # Maximum size of the vocabulary\n",
        "    # trial_stopwords = trial.suggest_categorical(\"stopwords\", [set(), set(['.'])]) # stopwords: a set of words which are excluded from the vocabulary\n",
        "    \n",
        "    ## A CONTINUER\n",
        "\n",
        "\n",
        "    # return evaluate_model(dataset=minidataset, count_threshold=trial_count_threshold, voc_threshold=trial_voc_threshold, stopwords=trial_stopwords, postag=trial_postag\n",
        "    #                       , lemmatization=False, stemming=False, size=30, sim_or_dist=False\n",
        "    #                       , lowercase=False, PMI=False, agg=lambda x: np.mean(x, axis=0), regression=\"linear\")[2]\n",
        "    return evaluate_model(dataset=minidataset, count_threshold=trial_count_threshold, voc_threshold=trial_voc_threshold, stopwords=set()\n",
        "    , postag=False, lemmatization=False, stemming=False, size=30\n",
        "    , sim_or_dist=False, lowercase=False, PMI=False, agg=lambda x: np.mean(x, axis=0), regression=\"linear\")[3]"
      ],
      "metadata": {
        "id": "zTzMNHQLluhC"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation de l'étude\n",
        "study = optuna.create_study(direction = \"maximize\")\n",
        "study.optimize(objective, n_trials = 10)"
      ],
      "metadata": {
        "id": "Z5i1unzOKx0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Boucle d'optimisation - possible de la relancer plusieurs fois\n",
        "for i in range(100):\n",
        "    try :\n",
        "        study.optimize(objective, n_trials = 1)\n",
        "        print(\"yes\")\n",
        "    except: # Gestion de l'erreur NaN sur train_X\n",
        "        pass"
      ],
      "metadata": {
        "id": "PCPXYWVEQ7_Z",
        "outputId": "e88586c3-6f4b-4f38-cade-fe26b2d5eb5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:46:58,601]\u001b[0m Trial 138 finished with value: -0.15122919319147674 and parameters: {'count_threshold': 6, 'postag': False, 'voc_threshold': 6003}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[6.52484617e-01 8.51472320e-01 1.16561089e-01 ... 1.95700318e-02\n",
            "  1.15080878e-01 0.00000000e+00]\n",
            " [1.01508631e-03 3.60211787e-02 8.72449696e-02 ... 1.87341615e-01\n",
            "  3.68493117e-01 9.38957456e-02]\n",
            " [3.95128599e-03 3.46870207e-02 9.37404006e-04 ... 2.37151740e-01\n",
            "  0.00000000e+00 7.11532949e-02]\n",
            " ...\n",
            " [2.24063133e-02 7.80739011e-02 4.51962646e-04 ... 1.77327666e-01\n",
            "  0.00000000e+00 1.30565663e-01]\n",
            " [0.00000000e+00 0.00000000e+00 7.53271076e-04 ... 1.86194050e-01\n",
            "  0.00000000e+00 5.80818388e-02]\n",
            " [0.00000000e+00 0.00000000e+00 6.69574290e-04 ... 2.05616750e-01\n",
            "  0.00000000e+00 8.51795713e-02]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-2.12265863e-02 -1.48466317e-02  9.84747445e-03 -6.27901034e-03\n",
            "  4.46508907e-03  1.19688918e-02  4.79183337e-03  8.47113813e-03\n",
            "  3.97236611e-03  2.32605474e-03 -1.25559485e-02 -1.22586102e-02\n",
            "  1.57502913e-02 -1.34523492e-02  1.80570304e-02  5.97691195e-03\n",
            "  1.22708844e-02  9.37356203e-03 -3.50418222e-02 -1.59562420e-02\n",
            " -3.04466204e-03 -1.55760193e-02 -4.47097723e-03  5.82800712e-04\n",
            "  2.60915410e-02 -2.84856222e-02 -1.54691276e-02  1.56826077e-03\n",
            " -9.45132968e-04 -1.05066073e-02 -3.37350196e-04  2.59521595e-02\n",
            " -3.16699680e-04 -5.69439264e-02 -3.11680642e-03  2.53545268e-02\n",
            " -3.36864022e-03 -2.43056060e-02  2.03949082e-02  1.71746141e-03\n",
            "  5.53680208e-03  2.35339828e-02  6.01804318e-04 -3.53318770e-02\n",
            " -7.07461001e-03 -4.40204310e-02 -4.90918523e-03 -1.58462582e-02\n",
            "  2.36875150e-02  1.18720766e-02  1.92298592e-02 -6.12133497e-02\n",
            "  1.48284812e-02 -2.58201971e-02 -3.90978428e-02 -2.68450980e-02\n",
            " -1.52273411e-02  4.03032120e-02 -1.20751507e-02 -2.53594685e-02\n",
            "  7.10449058e-02 -2.79308013e-02  6.13878307e-02 -7.85550193e-03\n",
            " -9.54437325e-03  1.28570924e-02  3.02407568e-02 -1.45151242e-02\n",
            " -3.49012852e-02 -6.57686459e-04 -8.80844754e-03  2.32506329e-02\n",
            "  3.96911990e-02  2.14150249e-02  3.24606014e-02  1.43196190e-02\n",
            " -2.25869626e-02 -1.83632286e-02 -4.17207312e-02  6.76832632e-03\n",
            " -3.60025188e-02  1.02646738e-03  1.79571170e-02  1.91314251e-02\n",
            " -3.52539831e-02 -6.70184883e-03 -2.13433496e-02 -1.07393440e-02\n",
            "  8.21965215e-03  8.00808694e-03  6.72437290e-03 -3.23385971e-02\n",
            "  1.89679915e-02  3.42822890e-02  5.58269932e-02  8.16374620e-03\n",
            " -1.58453824e-02 -1.54404516e-02  3.03238545e-02  3.16565056e-02\n",
            "  2.16014340e-02 -1.02982028e-02 -4.74607252e-02  5.18104165e-03\n",
            "  3.71123628e-03 -4.48770401e-03 -7.07035238e-02  2.95007162e-04\n",
            " -4.46489133e-02  3.84099854e-03 -1.84586937e-02  1.30267542e-03\n",
            " -1.57462426e-02 -5.70349373e-02 -2.65647465e-02 -2.83001213e-04\n",
            "  2.22865117e-02  1.33158414e-02  1.99058596e-02 -3.37729129e-02\n",
            " -8.11900793e-03  3.90769979e-02 -5.91046975e-02  1.97184088e-02\n",
            " -1.88921439e-02 -4.20911744e-02 -1.62381158e-02 -5.50383276e-04\n",
            "  1.44581166e-03  3.83435679e-02  1.74107367e-02 -2.43592721e-02\n",
            " -6.29619216e-02 -2.62206131e-02 -5.06487828e-02  1.64392531e-02\n",
            "  5.55781934e-02  2.14809685e-02 -2.86389320e-03 -3.38523955e-02\n",
            " -8.14852014e-03  7.99337079e-04  2.87408699e-02 -4.96691162e-02\n",
            " -8.25602544e-03  4.38994425e-02  1.04492224e-02 -8.31187081e-03\n",
            "  4.16329514e-02 -6.66924488e-02  2.97296229e-03  4.86857718e-03\n",
            " -1.47191159e-03  4.44847013e-02 -9.89954098e-03 -5.06289461e-02\n",
            "  9.24155654e-02 -3.85218941e-03 -2.32903323e-02  1.38582392e-03\n",
            "  2.96697118e-02 -1.11087336e-02  2.84429839e-02 -1.42095758e-02\n",
            "  8.29764119e-03 -4.75805319e-03  1.52298099e-02  4.64726208e-03\n",
            "  9.31128701e-03 -2.11875866e-02  1.53173228e-02 -3.82158215e-02\n",
            "  6.16678855e-03  2.81787433e-02  5.97247029e-02 -4.00484352e-02\n",
            "  3.10163998e-03  3.19167250e-02 -1.48285754e-02 -1.31304255e-02\n",
            " -2.25538120e-03  2.18476467e-02 -8.31720171e-03  2.27809578e-04\n",
            " -9.06327861e-03 -2.39828944e-02 -4.22691843e-02  5.84754343e-03\n",
            "  4.15091832e-02 -3.01262203e-02  2.86630531e-02  1.01577304e-02\n",
            " -2.87062845e-02  2.28997245e-02  1.97723506e-02  9.58553405e-03\n",
            " -2.41038052e-04 -3.30396785e-02  2.91731430e-02  3.40532078e-03\n",
            " -1.65191568e-04 -1.71965236e-02 -3.55587185e-02  8.65760078e-03\n",
            " -6.41665596e-03 -1.24148586e-02 -2.64775642e-02  2.46454573e-03\n",
            " -1.64769346e-02  4.09351127e-04  1.08185583e-02 -7.49979866e-03\n",
            "  2.80941497e-02  1.48777059e-03 -2.79684266e-03 -1.61388665e-02\n",
            " -1.61388665e-02 -8.55655786e-03  1.20230620e-02  6.87914317e-02\n",
            "  2.93789653e-02  1.94683489e-02  6.64803786e-03  1.10813952e-02\n",
            " -2.67236061e-02  1.97960349e-02  2.02834948e-02 -7.96608259e-03\n",
            "  2.07815166e-02 -1.01735839e-02  5.76847673e-03 -2.80052671e-02\n",
            "  3.86168208e-02  2.95151324e-02  3.82313405e-02  1.34715143e-03\n",
            "  1.51262273e-02 -3.87353791e-04  2.04825049e-02 -3.42661887e-04\n",
            " -9.94640526e-03 -3.62397673e-02 -2.74991854e-02  1.68408543e-02\n",
            "  3.64583064e-02 -5.13304112e-02  6.52670567e-03 -3.10858485e-02\n",
            " -4.74736987e-02 -1.38960892e-03  4.20511801e-04  5.89832417e-03\n",
            "  2.90631052e-02  1.12032644e-02 -3.52483878e-05 -1.02333993e-01\n",
            "  2.64450187e-02 -5.27128976e-03 -5.16954146e-03 -2.37034671e-02\n",
            "  5.81351123e-03  3.74457075e-02  1.20270185e-02  2.88998784e-02\n",
            " -2.70185946e-02  3.05700178e-02  2.28620042e-02  2.38291055e-02\n",
            " -3.41623157e-02  1.52362959e-02 -2.59596505e-02  2.46863088e-02\n",
            "  1.51156343e-02  7.76735128e-02  7.35987104e-03 -7.53485306e-03\n",
            " -6.01173785e-05 -1.69644557e-02  2.70624365e-02 -1.43914159e-02\n",
            "  3.98000107e-03  3.62546496e-03 -5.70421598e-03 -1.36114462e-02\n",
            "  2.65781242e-03  2.15068523e-02  4.37143341e-03  4.27666217e-02\n",
            "  1.04829479e-02  1.44965084e-02  1.95268628e-02  7.36377462e-04\n",
            "  9.75650730e-04 -1.60517287e-02 -1.18544241e-02 -7.88263852e-03\n",
            " -2.21186253e-02  3.03327386e-02 -8.10720455e-03  2.49538281e-02\n",
            " -1.14017200e-02  1.58515399e-02 -7.39269534e-03  1.54633614e-02\n",
            " -4.03139256e-04 -2.97618174e-03  2.67596982e-02  1.73777263e-02\n",
            " -3.35980214e-02  1.42272218e-02 -2.37847749e-02  1.07456440e-02\n",
            " -1.28191629e-02  2.13997817e-02 -1.53141018e-02 -1.98430386e-02\n",
            "  2.20234119e-02  1.48042398e-02 -2.79074343e-02  2.92013934e-02\n",
            " -1.84741512e-02 -4.67754049e-02  4.69227025e-02 -3.05286504e-02\n",
            " -3.08955266e-02  1.51073914e-02  2.84281595e-02 -5.95815853e-03\n",
            "  1.15997590e-02  3.57396885e-02  3.09894600e-03 -1.12249437e-02\n",
            "  1.68725109e-02  3.54789553e-02  2.96845947e-02  3.31482514e-02\n",
            "  2.13671902e-02  9.99230294e-03  1.06633502e-02  2.42434192e-02\n",
            "  3.95410141e-02  2.49493687e-02  1.72100575e-02  2.03795711e-02\n",
            "  1.36351007e-02 -9.15847535e-03  3.27835980e-02 -2.02298868e-02\n",
            " -2.70006637e-02  9.55488862e-04  3.93928521e-03  9.94410929e-02\n",
            " -1.82515471e-02  4.77187934e-03  2.16685038e-02  3.47268121e-02\n",
            "  9.72319539e-03  1.80173307e-02 -2.56141791e-02 -1.25257568e-02\n",
            "  4.39854884e-02  2.88858982e-03  5.15186152e-03  2.16381802e-02\n",
            "  5.73716050e-02  1.10512275e-02  1.81127296e-02 -1.84122214e-02\n",
            "  7.41415728e-02 -1.32481369e-02 -2.03389300e-02 -3.27982903e-03\n",
            "  2.55835234e-02  1.11589400e-02  4.18594356e-03  2.95113664e-03\n",
            "  5.06400247e-03  4.93020373e-03  6.26069625e-03  1.26903964e-02\n",
            "  2.89448463e-02 -4.91141728e-03  2.41407595e-02 -2.57466847e-02\n",
            " -4.83897347e-02  4.83462454e-02  8.27598548e-03 -7.39631766e-03\n",
            " -2.38405515e-02 -1.80302475e-02  9.92396648e-03 -3.76371217e-03\n",
            "  1.91652717e-02  9.08947488e-03  7.44365485e-03  1.20799955e-02\n",
            " -2.08755938e-02  1.32456305e-02 -4.41938357e-03  1.94321027e-02\n",
            " -2.27104600e-03  1.03876560e-02  1.07682464e-02 -3.19185967e-02\n",
            "  2.78387549e-03 -1.72358262e-02 -1.03981190e-03 -2.26584145e-02\n",
            "  6.30225160e-03 -2.29474092e-02  3.48378482e-03  4.63730007e-02\n",
            "  2.98070065e-02 -1.57319748e-02  4.89021962e-02 -4.92821945e-02\n",
            "  4.02441628e-03  2.61656411e-02  9.02844163e-03 -4.40017147e-02\n",
            "  3.27599869e-02  1.05036494e-02 -6.71899362e-03 -2.12557704e-02\n",
            " -5.18334651e-02 -3.96644697e-02 -1.13406259e-02  1.31151429e-02\n",
            "  1.75289624e-02 -2.76340807e-02 -1.35371351e-02 -2.75155509e-02\n",
            "  1.28807242e-02  2.19446973e-03  3.99503988e-02  4.85983942e-02\n",
            "  1.00123855e-02 -1.41376959e-03  2.55487294e-02 -3.95618528e-02\n",
            "  1.25959219e-02  4.33485378e-03  2.56751021e-03  4.23399638e-02\n",
            "  2.67772540e-02  6.26407306e-03 -4.02865482e-02  1.08745196e-02\n",
            "  1.42361651e-02  1.89248019e-02  2.72226056e-02 -3.87457996e-02\n",
            "  1.65793491e-02  5.89113058e-03 -1.12237877e-02 -9.05513024e-03\n",
            "  1.09132081e-02  1.47168073e-02  4.24377651e-03 -2.71328470e-02\n",
            " -7.17953260e-02  4.39727607e-02 -9.00645340e-03  2.76626056e-02\n",
            " -3.90936281e-03 -3.24195725e-02  3.25572301e-02 -3.93631550e-03\n",
            " -5.74422961e-02 -4.33343124e-03 -2.41044955e-02  2.47573733e-02\n",
            "  6.70482132e-03 -3.42190267e-02  1.70492909e-02  1.70229357e-02\n",
            " -1.18755853e-02 -6.21355790e-03 -2.42680203e-03 -1.67120060e-03\n",
            " -3.26621006e-02 -2.13712773e-02  1.77033306e-02  5.57542255e-04\n",
            "  1.36356061e-02  2.15884337e-02 -2.20378659e-02  1.49242003e-02\n",
            "  5.51162230e-03  1.83198030e-03 -2.51567171e-03  3.87050843e-03\n",
            "  7.06388718e-03  2.76945131e-02  2.84933225e-02 -1.18014024e-02\n",
            " -1.73696482e-03 -1.73696482e-03  3.88131891e-03 -9.00102763e-03\n",
            "  3.03777415e-02  3.98693078e-02  3.51489622e-03  2.17283886e-02\n",
            "  2.73087244e-02 -8.89684359e-03  1.75319084e-02  1.34764260e-02\n",
            " -3.58468354e-02  8.63803922e-03  1.62787581e-02  1.54572795e-02\n",
            "  1.83743728e-02 -7.46840410e-02  5.56890677e-02 -3.72548719e-02\n",
            "  2.30485461e-02  9.32430264e-03 -1.97190342e-02  1.63552684e-02\n",
            "  7.00905801e-03  5.20236496e-03 -3.39564139e-02 -1.82644667e-02\n",
            " -1.13935928e-02 -6.63884959e-03 -9.87861279e-03  1.68035460e-02\n",
            "  2.31309106e-02 -1.03328865e-02  7.48123036e-04  6.24305048e-03\n",
            "  5.79552751e-02  3.82141750e-02  1.79599402e-02 -2.11123101e-03\n",
            " -3.36011991e-02 -4.00666154e-03 -7.17550891e-03  4.47284572e-02\n",
            " -2.21923509e-02  2.30448166e-02  3.52301836e-02  2.37656207e-02\n",
            "  1.66827342e-02  4.32137247e-03 -1.13988650e-02  5.59804074e-03\n",
            " -1.62341593e-02  1.96389030e-02  1.30639671e-02  1.51131628e-02\n",
            "  1.45260495e-02 -5.25618169e-02  1.27494191e-02 -2.53071820e-02\n",
            " -2.97671891e-02 -1.98017135e-02  1.32314343e-02  6.07004394e-02\n",
            "  2.39913420e-02  1.43107656e-03]  - intercept :  0.49572694118508703\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.15122919319147674\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:46:59,223]\u001b[0m Trial 139 finished with value: -0.07968089028501292 and parameters: {'count_threshold': 6, 'postag': False, 'voc_threshold': 8642}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[1.05861143e-02 3.98517174e-02 6.61936345e-02 ... 2.54875054e-01\n",
            "  1.76113601e-02 5.17813971e-02]\n",
            " [3.01090318e-01 4.81437765e-01 1.60764968e-02 ... 4.35798447e-02\n",
            "  1.83344482e-01 8.90794057e-03]\n",
            " [0.00000000e+00 0.00000000e+00 3.54314757e-03 ... 0.00000000e+00\n",
            "  0.00000000e+00 5.23664190e-02]\n",
            " ...\n",
            " [1.47094283e-04 3.74244617e-02 3.54314757e-03 ... 0.00000000e+00\n",
            "  0.00000000e+00 1.30831610e-01]\n",
            " [3.81442288e-01 5.07614927e-01 2.23146416e-01 ... 2.56435414e-01\n",
            "  0.00000000e+00 1.04325014e-02]\n",
            " [3.75960745e-01 5.51048969e-01 4.46373172e-02 ... 2.93069044e-01\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 2.22797183e-02  1.49572777e-02  1.41637156e-02  1.28474257e-03\n",
            "  1.55291357e-04  1.55603703e-02  2.05270767e-02  2.05217636e-02\n",
            " -1.63991398e-03  1.65328418e-02  1.41318191e-02  8.64486266e-03\n",
            "  6.68215538e-03  3.00016107e-02  1.69582757e-02  2.04564527e-02\n",
            " -7.96086363e-04  7.62090476e-03  2.23441623e-02  1.69923270e-02\n",
            "  2.60210735e-03 -1.44902034e-02  2.00505782e-02 -6.35479873e-03\n",
            "  1.67298954e-02  1.67903523e-02  3.33002371e-03 -3.09926558e-02\n",
            " -2.92384646e-03 -1.48010808e-02 -1.11959652e-02  1.00576003e-02\n",
            "  1.44873818e-02 -1.70507595e-02  4.62757523e-03 -2.76949236e-02\n",
            " -7.18729046e-03 -1.51438676e-02 -7.00671471e-03  1.25586201e-02\n",
            " -2.82753713e-02 -2.69266543e-02 -1.63581984e-03  2.95666922e-03\n",
            "  1.82615582e-02  3.31848523e-02  1.24202049e-02  1.90055303e-02\n",
            " -3.23862318e-02  9.31539099e-03  4.72814467e-02  2.96363561e-02\n",
            " -4.43145238e-03  3.66667906e-02  2.34339789e-04  1.59872441e-02\n",
            " -7.79831305e-03 -1.15575915e-03 -1.24960786e-03  7.14002811e-03\n",
            "  1.36502862e-02  3.10024855e-02 -3.57591021e-03 -1.06837255e-02\n",
            "  2.10783468e-02  8.67039036e-03 -3.95499491e-03 -2.32207473e-02\n",
            " -6.10193834e-03 -2.15286583e-02  2.18181280e-02  9.89952711e-04\n",
            "  1.03839621e-02 -4.34033822e-03 -2.41439036e-02  5.79074032e-02\n",
            " -3.59993915e-02  1.73121826e-03  6.52995697e-03  4.72317755e-02\n",
            " -1.53328451e-02  2.06427463e-02  3.39545954e-02 -3.83253601e-03\n",
            "  3.80110287e-02  3.81819604e-02  1.26929253e-02 -3.23434887e-02\n",
            " -8.32454896e-03 -5.59447472e-03  1.41511189e-02  5.53279018e-03\n",
            " -2.18263234e-02  2.09647840e-02  2.23587660e-04  3.11247782e-02\n",
            "  1.95936765e-02  1.16657992e-02  9.51917040e-03 -5.96721690e-03\n",
            "  1.01741387e-02 -1.85539248e-03  1.25434046e-03 -3.56758979e-02\n",
            "  2.63231414e-02  3.66815996e-03  5.22128847e-02  2.44357588e-02\n",
            " -1.68057802e-02  2.85781550e-02  5.65253982e-02 -4.93010161e-03\n",
            " -2.45361881e-02  2.64053373e-02 -5.39917287e-02 -1.04181418e-02\n",
            "  5.05553145e-03  4.48735287e-02  3.69623315e-02  1.95986046e-02\n",
            " -1.49888780e-02 -3.87458306e-03 -7.45902857e-03  7.44812808e-02\n",
            "  1.43911971e-03  5.95589782e-03 -2.07981611e-02  4.64926565e-03\n",
            "  1.59048597e-02  3.96442162e-02  2.34355147e-02  6.41892935e-03\n",
            " -1.15241119e-02 -2.70767324e-02  2.51907497e-03  2.48032473e-02\n",
            "  2.69228415e-02  4.86035054e-02  7.86540810e-03  1.75906041e-02\n",
            "  3.38739498e-02 -1.83740027e-02 -4.63758767e-02  3.47869526e-02\n",
            " -2.08162431e-02  3.67649419e-03  2.61077143e-02  2.27877225e-02\n",
            "  1.03870770e-03 -1.98308822e-02  2.25254405e-02  5.92889296e-02\n",
            "  3.14800818e-02 -3.54308446e-03 -1.67431983e-02  8.92056348e-03\n",
            "  3.30057584e-02  2.15638667e-02  1.60763802e-02  8.15977121e-02\n",
            "  1.36777326e-02  1.93488116e-02 -1.56729513e-02 -7.70995204e-03\n",
            "  2.39089480e-02 -2.03349499e-03  2.15582437e-02  3.44493882e-02\n",
            "  2.91880754e-02  1.72972297e-02  4.57322446e-02 -1.66702453e-02\n",
            " -3.71123418e-03 -1.34626512e-02  3.12375384e-03  4.06582515e-02\n",
            "  3.26750411e-02  8.98598123e-03 -1.29521565e-02 -3.66596809e-03\n",
            "  4.54853581e-02  1.89467391e-02  2.31234421e-02 -2.62926882e-02\n",
            " -5.26655612e-03  4.01018222e-02  7.96489269e-03 -5.96494911e-02\n",
            "  1.31453238e-02  1.57864142e-02  1.62998068e-02  1.91956058e-02\n",
            "  2.52306820e-02 -1.87492990e-03  3.34833807e-02  2.89703617e-02\n",
            " -1.70864133e-02  1.93401063e-02  2.65709932e-02  6.76077809e-02\n",
            " -7.44040824e-03  6.32441680e-03 -4.89331649e-02 -5.68140621e-02\n",
            " -1.73149206e-02 -2.41234954e-02  2.98086261e-04  1.98104558e-02\n",
            "  6.02323753e-02  6.94150040e-03  8.11601735e-03  5.68978236e-02\n",
            "  2.11953629e-02  4.68623326e-02 -2.32626863e-02  1.46607449e-02\n",
            "  6.76017369e-02  2.76283557e-02  3.54451683e-02  2.62249967e-02\n",
            " -2.07349271e-02  1.60660745e-02  1.57735885e-02 -4.22273020e-03\n",
            "  2.65685757e-02  4.53122338e-02  1.21135408e-02  1.14001704e-01\n",
            "  2.72358917e-02  1.40411907e-02  2.07780324e-02  5.37856270e-02\n",
            "  8.29323847e-03 -7.98238794e-03 -1.36314001e-02  9.45174034e-04\n",
            " -2.08756638e-02 -1.22037082e-02 -3.56741571e-02  5.55275441e-03\n",
            "  2.23201066e-02 -9.66265076e-03  2.94855758e-02 -1.21625399e-02\n",
            "  1.27342893e-02  3.44044118e-02  1.77149097e-02 -1.89341573e-02\n",
            "  1.81998525e-02  2.26215919e-02  5.29484597e-02 -1.42140856e-03\n",
            " -2.55915707e-03  5.90094492e-04  6.06684728e-03 -3.61318657e-03\n",
            "  3.92815190e-02  1.86986277e-02 -8.66442925e-03  8.05838018e-02\n",
            "  4.25867490e-03 -6.14211712e-03  2.57575990e-02  1.21164730e-02\n",
            "  1.08188370e-02 -1.80836346e-02  1.35659013e-02  1.86780289e-02\n",
            "  1.22662581e-02 -5.94465192e-03 -3.82887289e-02 -9.58203241e-04\n",
            "  2.19377879e-02 -5.62749653e-03 -4.22644737e-02 -1.54909094e-02\n",
            "  1.01617645e-02  6.86260272e-02  2.50775210e-02  2.82767036e-02\n",
            "  2.10895483e-02  1.23482659e-02 -1.38355550e-05  9.98740433e-03\n",
            "  4.46782650e-02  1.55993475e-02  2.09436275e-03 -3.00868280e-03\n",
            " -6.10179146e-02  2.92618213e-02  1.60195793e-02  1.44278748e-02\n",
            "  1.44278748e-02  1.44278748e-02  7.03163818e-02  6.88108248e-03\n",
            " -1.54457060e-03  2.35654500e-02  2.63024799e-03  1.05839471e-02\n",
            "  1.64237985e-02  1.33699725e-02 -1.42810506e-02 -1.26447266e-02\n",
            "  4.83064811e-03 -1.19301167e-02  2.13247000e-02  3.55293374e-03\n",
            " -1.99778682e-02 -4.46187013e-04  1.15248495e-02  2.30745318e-02\n",
            "  1.63615990e-02  9.54616897e-03  1.25560659e-02 -8.48003031e-03\n",
            " -3.61213751e-03  7.27204803e-03 -4.07985918e-03  2.54031722e-02\n",
            " -4.74145905e-03 -3.28809567e-02  7.14346346e-03 -4.29788416e-02\n",
            " -3.21858191e-02  1.74976791e-02 -1.05188831e-04 -7.45667734e-03\n",
            "  6.88448896e-03  8.93143661e-03  5.97464250e-03  2.15698215e-03\n",
            "  3.03681813e-04 -1.04179116e-02  7.10788030e-03  2.76098203e-03\n",
            "  3.73619026e-04 -7.15624740e-03 -1.54507079e-02 -1.60160128e-02\n",
            " -1.24086281e-03  2.17977340e-02  4.10366826e-02 -1.98659390e-02\n",
            "  3.36518344e-02  7.72178984e-03 -5.67055163e-02  8.55201004e-03\n",
            "  4.08394825e-02 -7.68362597e-03  1.27644156e-02 -6.19128869e-03\n",
            " -4.65288254e-03 -3.31729251e-02 -3.53250444e-02  1.22626695e-02\n",
            "  1.41966572e-02 -2.58384648e-02  1.58341682e-02  1.30210701e-02\n",
            "  6.08011221e-04 -2.65752071e-02 -2.39913247e-02  2.15128622e-03\n",
            " -5.54483042e-02 -3.74226414e-02 -3.86235061e-02  1.23856735e-02\n",
            " -4.69122968e-02 -1.37538323e-02  3.86646192e-03  8.41518776e-03\n",
            "  1.73629668e-02 -2.77948164e-02  2.72398379e-03 -5.66473022e-02\n",
            " -8.84683078e-03 -4.67869471e-03  5.99185075e-03  7.88938198e-03\n",
            " -1.78700513e-03 -1.55334264e-02  2.20658775e-02 -2.75032319e-02\n",
            " -6.30153733e-03 -1.66232791e-02 -4.08164889e-02 -7.27281808e-03\n",
            " -4.02796265e-05  4.58069498e-03  6.75029361e-03 -3.34763362e-02\n",
            "  2.53693207e-02  1.19561194e-02 -4.27041364e-02  4.73942450e-03\n",
            " -4.62021939e-02 -1.04827430e-02 -4.73250175e-02 -2.00533455e-02\n",
            " -2.66978220e-02  3.71034913e-03 -5.30562862e-03 -1.12107422e-02\n",
            "  1.33407328e-02 -2.88427477e-02  8.38435063e-03  5.88611072e-02\n",
            "  6.22392504e-03 -2.96631646e-02 -2.73764516e-02 -4.94067155e-02\n",
            " -6.05216310e-03 -5.79615162e-02  1.97424211e-03  1.27730515e-02\n",
            " -7.08217757e-02 -7.21831715e-02 -3.73444274e-02 -1.15954301e-03\n",
            "  2.14693450e-02  1.89664842e-02 -2.20788975e-02 -5.21375711e-02\n",
            "  3.04279163e-02  2.09747853e-02 -7.89839330e-04  9.02407885e-03\n",
            "  1.62704297e-02 -4.45553009e-02 -4.11620638e-02  1.98509993e-02\n",
            "  1.25431638e-02 -1.76551863e-02  1.10619344e-02  1.41766723e-02\n",
            " -1.00590498e-02  2.11378543e-02 -5.94469887e-02 -3.56282618e-02\n",
            "  3.95913532e-02 -2.40667075e-02  7.46636136e-03 -8.71057782e-03\n",
            " -7.99825278e-03 -9.99394426e-03  6.55546769e-03  1.00603461e-02\n",
            " -7.57346729e-03 -8.03842680e-03 -2.21711534e-02 -9.36455957e-03\n",
            " -1.55707093e-02  1.85770370e-02 -3.02683991e-03 -1.16272965e-03\n",
            "  6.99776137e-03 -6.90284514e-02  1.51952381e-02 -1.71092231e-02\n",
            "  8.17385586e-03 -1.89553090e-02 -1.38521126e-02  2.07308490e-02\n",
            "  2.07917569e-02  3.79540319e-02 -5.33634487e-03 -4.45183998e-04\n",
            " -7.22761819e-03 -1.73640769e-02 -8.18344875e-03 -2.68105679e-02\n",
            "  2.49429084e-02  4.61334890e-02 -1.51531256e-03 -2.19086399e-02\n",
            " -4.93044401e-03 -4.32368620e-02 -1.36334131e-02  3.16683785e-02\n",
            " -1.68394897e-02 -5.87349420e-03  2.57933399e-02  1.46777020e-02\n",
            " -3.24096140e-02  1.47237047e-02  2.75563981e-02 -6.46437843e-02\n",
            "  1.57596594e-02 -2.23698008e-02  4.14981282e-03 -6.29553172e-03\n",
            " -9.71826189e-03  5.32841346e-03  1.27993082e-02  2.36322407e-02\n",
            "  1.84139110e-02 -6.60852114e-03 -1.49015046e-02 -6.70325355e-02\n",
            " -5.55399939e-02 -6.17299612e-03 -4.70995689e-02  5.32576502e-03\n",
            " -5.69758079e-03  3.66537829e-02  7.81310921e-03  1.29533318e-02\n",
            " -1.20184328e-02  2.25641958e-02  2.64526381e-02 -3.06628996e-03\n",
            " -3.87450377e-02  3.38784858e-02 -6.01921555e-02  2.48000980e-02\n",
            "  2.51068944e-02 -2.36575191e-03  1.96552757e-03 -1.32109547e-02\n",
            "  3.38688217e-02  4.39823199e-02  2.33648569e-02  2.91591474e-02\n",
            "  9.22025210e-02  1.81027412e-02 -2.55649893e-03 -2.28915353e-02\n",
            " -9.90973294e-03  7.50394969e-03 -1.10278440e-02 -2.75183625e-02\n",
            " -1.13425342e-02 -3.74991240e-02 -9.67154524e-03 -2.10299384e-02\n",
            "  3.31709430e-02 -3.43262991e-03 -1.94313550e-02 -1.57264170e-04\n",
            " -2.01784871e-02 -1.92727553e-02 -4.57213691e-02 -2.70023298e-02\n",
            " -5.45813561e-02  7.57264472e-03  1.26776883e-02  3.72592615e-02\n",
            " -1.49258438e-02 -4.46719904e-02 -2.88878722e-03  5.95958887e-03\n",
            " -3.29841380e-02  1.03228136e-02  2.22988203e-03 -1.89921922e-02\n",
            " -1.89653132e-02  1.24353081e-02 -3.80261522e-02  2.24201903e-02\n",
            " -9.17939555e-05  1.56344796e-02 -5.39004471e-02  2.76432281e-02\n",
            "  2.71106370e-02 -1.66418536e-02  8.22490656e-03 -3.84817015e-03\n",
            " -1.55450088e-02  2.67766467e-03 -3.99408395e-02 -9.30814438e-03\n",
            " -2.54465613e-02  8.76216014e-03  1.86329782e-02 -5.68983968e-02\n",
            " -3.08692189e-02 -3.33399307e-04  1.38060174e-02  1.46964522e-02\n",
            "  4.08058715e-03  1.49359656e-02 -5.74984525e-02 -9.48103086e-05\n",
            " -2.55886327e-02 -4.10130719e-02  8.65335964e-03  1.20434938e-02\n",
            " -2.08987169e-03 -2.08987169e-03 -2.08987169e-03  4.62429744e-02\n",
            " -4.06171805e-03 -3.20745368e-03]  - intercept :  0.4882988232599115\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.07968089028501292\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:46:59,798]\u001b[0m Trial 140 finished with value: 0.18074801337477106 and parameters: {'count_threshold': 4, 'postag': False, 'voc_threshold': 5588}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.47219963 0.01031534 0.60831338 ... 0.00307821 0.02435202 0.01057632]\n",
            " [0.         0.0045071  0.0128048  ... 0.09266387 0.04188866 0.11178241]\n",
            " [0.27133455 0.05382206 0.40217985 ... 0.09714426 0.13557093 0.06408835]\n",
            " ...\n",
            " [0.51165586 0.00353266 0.81549135 ... 0.         0.         0.        ]\n",
            " [0.         0.01031414 0.01138204 ... 0.17730216 0.11155764 0.074035  ]\n",
            " [0.         0.01709873 0.02855018 ... 0.00846507 0.03695807 0.03949289]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-0.00079381 -0.00040987  0.00018933 ... -0.00400107  0.00467706\n",
            "  0.00269631]  - intercept :  0.662736236318693\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.18074801337477106\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:00,404]\u001b[0m Trial 141 finished with value: -0.042924709578610366 and parameters: {'count_threshold': 7, 'postag': False, 'voc_threshold': 6499}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.0230812  0.05225995 0.12869046 ... 0.15384568 0.18279671 0.04666054]\n",
            " [0.35177947 0.4946126  0.03558955 ... 0.01991449 0.04569918 0.00444789]\n",
            " [0.         0.         0.01506637 ... 0.03203892 0.24372894 0.16909821]\n",
            " ...\n",
            " [0.00664886 0.02748642 0.04396347 ... 0.28699364 0.18279671 0.03754301]\n",
            " [0.38342783 0.46569972 0.03026137 ... 0.02459759 0.03323576 0.00571371]\n",
            " [0.         0.         0.01761596 ... 0.04805838 0.36559341 0.03558316]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-1.45852564e-02  1.91949498e-02  1.02864623e-02  3.15257977e-02\n",
            " -6.20051255e-02  2.26686909e-03 -6.13332147e-02  7.00884034e-03\n",
            "  2.56033489e-03  1.05608207e-02  5.98334785e-03  5.01359779e-04\n",
            " -9.10206730e-03  2.77395084e-02  8.73461619e-03 -3.76538840e-02\n",
            "  1.46984586e-02  5.41308235e-02  1.85935034e-02  1.33778009e-02\n",
            "  1.50829994e-02  5.89968560e-03  1.96189347e-02 -2.73057991e-02\n",
            " -1.88388999e-02  3.55491787e-02 -5.67681175e-03 -2.28954943e-02\n",
            "  5.30425952e-02  3.25744999e-02  2.06352482e-02  4.56443907e-02\n",
            " -4.27714453e-02 -2.79114916e-03 -2.20538719e-02  1.28120233e-02\n",
            " -7.10766236e-03  2.18869281e-02  6.01319656e-03 -1.98332893e-02\n",
            " -1.56324782e-02  1.80928681e-02 -1.65519733e-02 -2.60023541e-04\n",
            "  9.98741569e-03  1.27741154e-02  4.28866986e-03  4.07741847e-02\n",
            " -2.85296302e-02 -4.30007606e-02 -3.30216998e-02 -3.25325633e-03\n",
            "  5.70370019e-02  2.38050929e-02  2.24837481e-02  1.32946713e-02\n",
            "  1.12608953e-02  2.72629488e-02  2.74267383e-02  5.69155602e-02\n",
            "  2.05952069e-02  6.32783904e-02  5.75647828e-02  1.42984029e-02\n",
            "  5.66647158e-03 -1.09206308e-02 -2.84067489e-02  5.03271819e-02\n",
            "  2.55416573e-04  2.06499613e-03 -1.50856469e-03  3.51088725e-02\n",
            "  4.27536207e-02 -5.74429045e-03  3.12659070e-02 -4.76907113e-02\n",
            "  1.78874601e-02  5.92736377e-03  4.29187105e-02  3.04474260e-02\n",
            "  2.91288668e-02 -6.40964484e-02  9.60122531e-03  4.71709255e-03\n",
            "  2.66300240e-02  6.37268762e-02  1.64078562e-02  7.44389300e-02\n",
            "  3.21555341e-02  6.38922099e-02  2.19370534e-03  4.83169660e-03\n",
            " -5.00257638e-02  3.46062661e-02  5.53008901e-03  1.62586559e-02\n",
            "  2.89275235e-02  1.47045669e-02 -4.12785019e-03  9.43187127e-03\n",
            " -6.34739989e-03  4.99460628e-02 -9.63568466e-03  1.08829406e-02\n",
            "  9.53164511e-02 -1.15477089e-02  3.09305612e-02 -3.24596041e-02\n",
            " -1.40969216e-02  2.96021338e-02  4.03495819e-02  3.37124277e-02\n",
            " -5.12036844e-02  2.53827917e-03 -6.19748589e-03 -1.74067105e-02\n",
            "  4.62701610e-03  3.25815100e-02 -3.23361914e-02  1.51578183e-02\n",
            "  6.87273482e-02  1.57221173e-02  3.96255179e-02  8.25567368e-03\n",
            " -1.45678893e-02  2.12687404e-02  3.90190456e-02 -6.32522226e-03\n",
            " -1.40566933e-03  3.12144698e-02 -3.79173388e-04 -1.42795409e-02\n",
            " -5.02921842e-02  1.01362108e-02 -3.53779055e-02 -1.45329568e-03\n",
            "  8.06363381e-02  7.45326246e-03 -3.71684832e-03 -5.14686366e-03\n",
            "  5.23820771e-02  1.79149346e-02  1.73891186e-02 -2.12072485e-02\n",
            " -4.75355991e-02 -5.09642041e-02 -2.17118822e-03 -1.32293276e-02\n",
            " -1.54219012e-02 -1.53699578e-02  3.28361523e-02  2.16672062e-02\n",
            "  2.41765702e-02  1.90638764e-02  6.62521521e-03  5.04743024e-03\n",
            "  2.78113561e-02 -4.40167739e-02 -1.55987778e-02 -5.03123911e-02\n",
            "  4.74441713e-03  2.18401423e-02  1.02637455e-02 -3.68986779e-02\n",
            "  4.23795698e-02 -1.22948045e-02  6.38165518e-04 -2.64919656e-03\n",
            " -1.90003566e-03 -2.38296139e-02 -3.27942769e-03  1.39780460e-02\n",
            " -4.20608533e-02  1.81628732e-03 -4.71938606e-02 -8.37648207e-03\n",
            "  3.49200852e-02 -1.08389013e-02  8.64961934e-03 -5.18334818e-02\n",
            " -1.83604158e-02 -3.57451256e-02 -4.35282813e-03 -3.13783512e-02\n",
            " -2.43100093e-02  1.48326978e-02  9.92216141e-03  2.22026945e-02\n",
            " -1.56116635e-03 -1.34684020e-02 -5.94421238e-03  2.30988304e-04\n",
            " -8.31982418e-02 -1.40971184e-02 -2.09585605e-02  5.72789942e-02\n",
            "  4.04236872e-02  4.04236872e-02  4.04236872e-02 -2.48819264e-02\n",
            "  1.55662925e-02  7.22086007e-03  3.22398259e-02  8.70224746e-03\n",
            " -2.67609685e-02 -7.15986464e-02  1.50849100e-02  1.76216587e-02\n",
            " -1.02719196e-02 -4.65137024e-04  5.14513426e-02  7.46405641e-02\n",
            "  6.13090543e-03 -4.20096925e-02  2.11024805e-02 -4.02506667e-02\n",
            "  2.46990233e-02 -1.79896440e-02 -1.88540099e-02 -3.10703068e-02\n",
            " -2.76300837e-03 -2.30749765e-02 -1.69155035e-02  2.54320230e-02\n",
            "  1.04730611e-02  5.13270634e-03  3.79002827e-02  2.26911054e-02\n",
            "  1.36648652e-02 -3.32741238e-02 -1.77456265e-02 -8.80843260e-03\n",
            "  5.08907385e-04 -8.07341731e-03  3.75509083e-03  1.79230123e-03\n",
            "  9.15975538e-02 -1.49905990e-02 -6.16583713e-03 -1.72839805e-02\n",
            "  1.15013784e-03 -3.64001567e-02 -6.14811377e-03  3.38566097e-02\n",
            "  6.93528483e-03 -2.36340573e-02  3.97646209e-03 -3.81416358e-03\n",
            "  1.45398541e-02 -4.77398301e-02 -7.51887469e-03 -1.28991286e-01\n",
            "  1.59473320e-02  6.37868888e-03  1.74470415e-02  1.96779934e-02\n",
            " -1.99564656e-02  1.11227175e-02 -7.88582987e-03  7.16881026e-03\n",
            " -9.58502052e-02 -5.47455740e-04  5.63865748e-03  1.08740026e-02\n",
            "  2.70067535e-03  4.84371551e-03 -7.15410338e-03  2.15495039e-02\n",
            " -7.00985250e-03 -4.08372208e-02  2.94427307e-02 -3.17207567e-02\n",
            " -2.86829978e-02  6.54810339e-02  4.16425390e-02  2.54923806e-02\n",
            "  3.69252751e-02 -3.85954260e-02 -2.30858503e-02  1.96761017e-02\n",
            "  2.01152368e-02  2.67106950e-03 -4.99208925e-03  2.41792558e-02\n",
            " -2.76261271e-02 -1.06604745e-02  4.85754931e-03  4.42951806e-04\n",
            "  1.04929226e-02  2.04935670e-02  1.71342752e-02  2.09710058e-02\n",
            "  3.14450346e-02  8.51753194e-04 -5.18682009e-02  4.39080370e-02\n",
            "  3.95860906e-05  7.70006595e-02  1.86979175e-03 -1.17435943e-03\n",
            " -8.99083018e-03  2.02949022e-02  2.65395685e-02 -1.65465314e-02\n",
            "  1.41205626e-02 -3.15205343e-02  5.06937127e-02  4.08197652e-02\n",
            " -2.03063199e-03  6.20220410e-03  3.51914444e-02  4.84833701e-03\n",
            "  3.97466552e-02  3.20623288e-02 -6.08277777e-02 -1.84665333e-02\n",
            "  3.20112438e-02  6.15376051e-03 -8.47133692e-03  4.03104163e-02\n",
            " -6.01993269e-02  1.21957564e-03  2.27166709e-02  4.90473546e-02\n",
            "  3.04999776e-02 -6.13077747e-03 -5.14807150e-02  9.69560606e-03\n",
            "  1.82404999e-02  2.28016362e-02  5.73912924e-02 -2.63535599e-02\n",
            "  2.08186003e-02 -4.61192071e-02  4.50986910e-02  1.64757689e-02\n",
            " -1.96411718e-02 -6.66308196e-03  3.30400182e-02  6.93621013e-04\n",
            "  3.32087630e-02  6.60349979e-02 -7.19928193e-02  1.97198370e-03\n",
            "  9.02860256e-03 -6.26854453e-03  6.29295954e-03 -1.93265896e-02\n",
            "  1.13151154e-02  6.15082823e-02 -2.92447362e-02  3.41411539e-02\n",
            " -1.02978089e-01  9.97631210e-03  5.77096524e-02 -2.30452411e-03\n",
            " -1.32515771e-02 -6.25448383e-02  1.78401865e-03  1.08996994e-02\n",
            " -7.80906225e-02  7.07995179e-04  8.85808029e-02 -3.90708975e-02\n",
            "  7.66455326e-03  5.14015167e-03 -1.56133119e-02 -2.76131348e-02\n",
            "  3.41941829e-03  2.60887546e-02  7.96683019e-03  2.18976842e-02\n",
            " -4.79153083e-03 -4.59437710e-02  2.31308483e-02 -1.66434248e-02\n",
            " -1.12270972e-02 -3.24969063e-02  1.24864146e-02  1.21859459e-02\n",
            "  9.34680048e-03  6.64234373e-02 -3.36129831e-03  4.72991308e-03\n",
            " -6.87883469e-03  9.83629940e-03 -2.27184715e-02  4.55858877e-03\n",
            " -4.86628607e-02  1.03884293e-03 -3.39150067e-02 -1.79494892e-02\n",
            " -4.58586554e-03  4.49867201e-04  1.13035937e-02  3.15627321e-02\n",
            " -1.82299093e-02  2.90698364e-02  1.76471377e-02 -3.19537258e-02\n",
            " -1.04912113e-02  2.94079890e-02  5.72470026e-02 -3.04761012e-02\n",
            "  1.71699801e-02 -5.54765745e-02  7.76635570e-03  2.39720278e-02\n",
            " -3.63871362e-02 -6.35994558e-02 -1.16027470e-02  2.64727016e-02\n",
            " -1.05271940e-03  1.83689135e-02 -2.00828275e-02  1.00506134e-03\n",
            "  7.43237519e-02 -5.72226454e-02 -8.86273575e-03 -9.93996901e-02\n",
            "  5.04089846e-03  5.14034297e-02 -1.64471515e-02  3.56655307e-03\n",
            " -3.86373486e-02 -9.41160865e-03 -3.21780438e-02  1.30539498e-02\n",
            " -5.20353512e-02 -3.05444484e-02 -6.85363439e-05  3.53054781e-02\n",
            " -4.54207041e-02  8.20755285e-03 -5.05186442e-03  2.11964199e-03\n",
            "  2.49594736e-02 -8.53172408e-02 -1.24519968e-02 -3.63675308e-02\n",
            "  3.99913684e-02 -1.96112119e-03 -1.96112119e-03 -1.96112119e-03\n",
            " -1.39363237e-02  1.58417016e-02 -1.08541384e-02  4.06183758e-02\n",
            " -6.77341786e-03 -7.04449638e-02 -4.67798961e-02  4.60659219e-02\n",
            "  1.80728777e-02 -3.10478872e-02 -1.27625751e-02 -2.27087231e-02\n",
            " -4.94098571e-02  1.28024955e-02 -3.89079337e-02  2.23798165e-02\n",
            "  4.32250961e-02  2.88570300e-02 -4.30693289e-03  1.77057354e-02\n",
            " -3.13423487e-02 -1.06301847e-03  3.32024585e-03 -3.33413923e-04\n",
            " -2.24075773e-02  1.84467237e-02  6.06125670e-02  3.77191271e-02\n",
            "  1.26694059e-02  2.19842706e-03  7.11004671e-03  2.27438637e-02\n",
            " -1.09261215e-02 -1.39309093e-02 -3.18123277e-02  1.40894568e-02\n",
            "  1.56640101e-02  1.20240922e-01 -1.20517275e-02 -8.62180900e-02\n",
            " -6.06102917e-03  1.70926274e-02 -5.27416822e-02  3.17137630e-02\n",
            "  4.30859045e-02  6.54644335e-03]  - intercept :  0.44900458969788193\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.042924709578610366\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:00,995]\u001b[0m Trial 142 finished with value: -0.06532208198752286 and parameters: {'count_threshold': 7, 'postag': True, 'voc_threshold': 9100}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[3.74415618e-01 1.34783687e-01 4.73023886e-01 ... 1.29869008e-01\n",
            "  1.05447609e-01 1.05786185e-02]\n",
            " [2.98130786e-02 8.17834802e-02 7.82992072e-02 ... 3.11685619e-01\n",
            "  0.00000000e+00 5.35012537e-02]\n",
            " [3.21731782e-02 6.41841244e-02 9.33874423e-02 ... 1.59773273e-01\n",
            "  0.00000000e+00 4.58461589e-02]\n",
            " ...\n",
            " [4.62474053e-01 2.38207015e-01 4.96289235e-01 ... 8.65793385e-02\n",
            "  1.36166296e-01 9.45679680e-03]\n",
            " [6.29999488e-02 3.55226652e-04 1.12115213e-01 ... 2.30878236e-01\n",
            "  0.00000000e+00 3.84871258e-02]\n",
            " [7.46460813e-02 7.32152922e-02 2.14625837e-02 ... 2.96843446e-01\n",
            "  5.62707444e-02 6.10315794e-02]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 7.89243530e-03  2.40396523e-02 -6.23648617e-03 -4.52695628e-02\n",
            "  5.96911855e-03  3.55812057e-02 -8.49573235e-03 -1.05042842e-02\n",
            " -3.42909121e-02  2.50573704e-02  3.30187223e-02 -5.09301021e-02\n",
            "  4.07460874e-02  1.73686321e-02 -1.75390677e-02 -4.68159555e-02\n",
            " -5.98948044e-02 -3.04553809e-02 -3.54250764e-02  1.04196253e-01\n",
            "  1.71688609e-02  8.25902450e-03 -6.15392939e-03 -1.67591910e-02\n",
            "  1.06593803e-02 -4.58425799e-02  3.69731419e-03  8.42053194e-02\n",
            "  3.00076131e-05 -3.14181764e-02 -1.50860821e-02 -2.10221517e-02\n",
            " -8.68638409e-02  6.55243883e-03 -9.58673862e-03  2.17167101e-02\n",
            "  8.14946521e-03  7.39939332e-02 -2.65981512e-02  4.06812910e-02\n",
            "  6.42996405e-02 -6.03828823e-02 -2.50033980e-02 -1.65347470e-02\n",
            "  9.10997544e-03 -2.38210084e-02 -2.85422728e-02 -2.20232381e-02\n",
            "  2.03728985e-02  4.15733870e-02  1.36798153e-01  1.25680709e-03\n",
            "  1.44549955e-02 -1.24513450e-01  1.94632184e-02  3.64163848e-02\n",
            " -5.13839570e-02  4.63854956e-04 -7.00067888e-02 -1.20923204e-02\n",
            "  1.82919809e-02 -5.15182443e-02  5.84707074e-02  2.62627876e-02\n",
            "  1.27959000e-02 -2.38217502e-02 -2.72372519e-02 -5.66430123e-02\n",
            " -2.58115187e-02  6.68675931e-02  1.44307721e-02  6.36678029e-03\n",
            " -7.21783956e-02  1.10492829e-02 -2.45492860e-02 -6.87810615e-02\n",
            " -1.71540298e-02  3.98681055e-02 -3.66873772e-02  2.83752859e-02\n",
            " -2.78935610e-02 -3.75386048e-02 -7.46291466e-02  5.58632694e-02\n",
            "  6.18050267e-03 -2.93673190e-03 -4.66849715e-02 -1.74315561e-02\n",
            "  1.92567714e-02 -2.23525325e-03  2.23106259e-02 -2.10928179e-03\n",
            "  4.75369525e-02 -5.18496891e-02  6.79863930e-02 -4.89306991e-02\n",
            "  4.21176387e-02  4.21176387e-02 -3.99059689e-03 -5.25662410e-02\n",
            " -3.31258903e-02  2.55440334e-02 -4.31821044e-03  6.66698383e-02\n",
            " -2.89807646e-02  5.67166426e-02 -3.62151165e-02 -2.06799155e-02\n",
            " -7.18340694e-03  3.22884540e-02  4.20043857e-02 -6.05646269e-02\n",
            " -1.28041593e-02 -6.64535807e-02 -3.97702825e-03 -4.26354581e-03\n",
            "  1.79784593e-03  3.80760506e-02 -4.78797973e-03  3.33488523e-03\n",
            "  3.47822518e-02  5.50532588e-02  6.41265485e-03  2.94844634e-03\n",
            " -7.65531342e-02  2.94043559e-02  2.17870748e-02  1.06242391e-02\n",
            " -4.21481756e-02 -4.19209195e-02  2.90473732e-02 -6.06095981e-02\n",
            " -1.15220793e-01  2.19836696e-02  6.70750679e-02 -4.78908098e-02\n",
            "  5.57721493e-02 -4.65113454e-02 -4.50663174e-03  5.41140389e-02\n",
            "  3.36204453e-03  3.11927978e-02 -1.03898480e-02 -3.02280568e-02\n",
            " -3.77129365e-03  4.62104302e-02  3.43676638e-02 -1.94236462e-02\n",
            " -3.27129922e-03  8.52618602e-03 -5.65527013e-02 -3.40845955e-02\n",
            " -2.75069278e-02  2.55098638e-02  3.20231355e-02  1.11087049e-02\n",
            "  3.12440774e-02 -1.98539060e-02  1.52688361e-02  7.05197991e-02\n",
            " -3.13106239e-02 -1.33604123e-02 -9.53241976e-02  3.00106215e-02\n",
            " -2.37683371e-02  2.91426082e-02 -8.19011055e-02  8.02862952e-02\n",
            "  1.86804557e-03 -7.34194412e-02  5.53129663e-02 -1.10083146e-01\n",
            " -1.44530044e-02  2.40316832e-02 -6.85178948e-02 -5.28396352e-02\n",
            " -4.03836318e-02 -2.68253243e-04  6.37484879e-03  5.28861765e-02\n",
            " -2.04439626e-02 -1.07811830e-02 -1.57406749e-01  2.78749685e-02\n",
            " -1.33969656e-02  1.00764701e-02 -3.23254903e-02 -4.71858186e-03\n",
            " -2.21815251e-02 -8.21504850e-03 -9.04386352e-02 -4.31651988e-02\n",
            " -2.81339218e-02 -8.72715686e-03 -3.80966081e-03  4.14591863e-02\n",
            "  5.02979116e-02  5.37386428e-02  8.47452265e-02 -1.98485706e-02\n",
            "  2.02477602e-02 -9.23270880e-03 -4.03638098e-02 -3.00983557e-02\n",
            "  5.61923928e-02 -3.78785743e-02 -2.05336608e-02  3.51779591e-02\n",
            " -1.85374179e-02 -4.80804216e-02 -9.88351857e-02  3.49465901e-02\n",
            "  6.02815813e-02 -1.77295680e-02  6.10818600e-02  9.39775832e-03\n",
            "  3.29114406e-02  3.76123049e-02  1.38564483e-02 -1.10806862e-01\n",
            " -8.29709066e-02  8.84259773e-02 -4.39728594e-02  4.89506752e-02\n",
            "  6.65629236e-03  4.44325178e-02 -4.55739754e-02  4.39062681e-02\n",
            "  4.72711578e-02 -8.60792570e-02  4.16092021e-02  9.33641292e-03\n",
            " -4.45485910e-02 -1.20012299e-03  3.74787203e-02 -5.74049642e-02\n",
            " -5.81688771e-02  1.13594208e-02  5.70608931e-02  6.59649792e-03\n",
            "  5.04229281e-02 -1.72093127e-02 -7.20674520e-03 -7.21515972e-04\n",
            " -4.45931598e-03 -3.00959254e-02 -5.91791378e-02  1.45182205e-02\n",
            " -2.43807357e-02 -1.28099191e-02 -2.59352670e-02  1.60084282e-02\n",
            "  3.15429051e-02 -6.20966345e-03  4.35318288e-02 -1.30688441e-03\n",
            "  2.83585300e-02 -2.47142790e-02 -1.95371113e-02 -5.25893110e-02\n",
            " -4.64410373e-02 -2.17300217e-02  3.05385435e-02  7.58939996e-03\n",
            "  1.66537619e-02  7.10682752e-03  1.18457676e-02 -9.49181111e-03\n",
            " -8.84331054e-03 -1.53881777e-02  1.19971934e-02 -3.92452010e-02\n",
            " -1.10028210e-02 -3.20796913e-02 -3.70962546e-03 -6.41350238e-02\n",
            "  1.89686819e-02 -1.23973068e-02 -5.99763362e-03 -3.88108611e-02\n",
            " -7.22913120e-02 -5.14606311e-02  1.75046767e-02 -1.36422901e-02\n",
            "  1.27019216e-03  3.26752608e-02  1.65813817e-02 -1.92103941e-02\n",
            "  5.91152127e-02  2.80941554e-02 -6.01798064e-03  1.09124744e-02\n",
            " -8.51565369e-03  4.35852075e-02 -2.22292202e-03 -2.27213898e-02\n",
            " -1.06430856e-01 -2.26942144e-02  4.66830575e-02 -7.06850353e-02\n",
            " -7.62866877e-03 -5.23657275e-02  5.99004303e-02  2.05353075e-02\n",
            "  4.54215598e-02 -4.85757675e-02 -3.65415605e-02  1.27672592e-02\n",
            "  2.79829456e-02  9.74731517e-03 -1.15959762e-02  4.18469411e-04\n",
            "  1.23195912e-02 -2.32093809e-02 -4.41564669e-02 -4.24419202e-02\n",
            " -6.92621128e-02  9.52186850e-02 -2.88890172e-02  1.74169888e-02\n",
            " -4.45942039e-02 -1.23880010e-03 -4.93534506e-02  3.73272665e-02\n",
            " -1.71964346e-02 -2.39512588e-02  2.07995124e-02  1.63629362e-02\n",
            " -3.93364408e-02 -2.26171751e-02 -7.12913612e-02  9.41324144e-03\n",
            " -3.39076091e-02  3.56239971e-02  9.66588835e-02  1.02284498e-01\n",
            " -1.67809631e-02 -1.02161099e-02 -5.64827460e-02  7.17412305e-03\n",
            "  7.17412305e-03 -4.60996742e-02 -6.09469300e-02  8.07405350e-03\n",
            " -1.41266465e-02  7.88586839e-02 -3.85095868e-02  4.84486800e-02\n",
            " -1.95048124e-02 -1.44917302e-02  3.00993416e-02  5.23917549e-03\n",
            "  2.87979155e-02 -6.50866787e-02 -5.72523327e-02 -9.58577662e-02\n",
            " -1.09519669e-02 -5.06245553e-03 -2.56065371e-02  5.81398873e-02\n",
            "  7.61998678e-02  1.81868417e-02 -8.79955172e-03  6.29334517e-02\n",
            " -2.33133999e-02 -3.50955797e-02 -1.74189337e-02 -1.17258513e-01\n",
            "  4.21160940e-02  2.16260123e-02  2.28928180e-02 -5.89342209e-02\n",
            "  5.23716578e-02  3.28014191e-02  1.67703855e-02 -5.40221327e-02\n",
            " -1.18500611e-02  9.19426321e-02 -9.70407417e-02 -1.34649260e-01\n",
            "  5.23059603e-03  1.38090856e-03  6.89585857e-02  1.96430393e-02\n",
            "  1.88325160e-02 -4.74622526e-02  2.93097903e-02  2.92547395e-02\n",
            "  1.77881722e-02  3.82222626e-02 -3.65866227e-02 -5.11662173e-02\n",
            " -4.45204908e-02  3.20242520e-02 -9.00590358e-03 -2.70708506e-02\n",
            "  6.92978041e-03  2.75834936e-02  1.83360896e-02  6.92613458e-02\n",
            " -6.32802032e-02 -9.06625592e-02  9.44177277e-03  1.18000315e-02\n",
            " -2.06168834e-03 -6.83247430e-02 -9.98299573e-03 -3.47720993e-02\n",
            " -6.35780879e-03 -7.41641148e-02  5.94740240e-02  1.94496654e-02\n",
            " -7.67612805e-02  7.08264857e-02 -1.48171230e-02 -4.58044246e-02\n",
            "  3.75801968e-02 -9.96097156e-03 -3.17849134e-02 -2.38630235e-02\n",
            "  7.91872872e-04 -2.63968603e-02  2.43528743e-02 -2.60949337e-02\n",
            "  2.86928806e-02 -5.15778306e-02 -9.31074240e-03  1.45753027e-03\n",
            " -1.11825280e-02 -7.60608797e-03  2.28445360e-03  1.96795459e-03\n",
            "  3.27756066e-02 -4.24597106e-02 -3.76455798e-03 -2.36410450e-02\n",
            " -4.92071444e-03  2.89190418e-02  1.82855453e-02  1.39293509e-02\n",
            " -1.33840311e-02  7.86951050e-02  1.19030724e-01 -5.30907302e-02\n",
            "  3.27330239e-02  5.98478308e-02 -5.71924566e-02  7.61062954e-02\n",
            " -1.35180948e-01 -3.82171386e-02 -4.37296259e-02  5.55533270e-02\n",
            " -8.34144039e-02 -1.93038041e-02  1.46928436e-02  2.87001635e-02\n",
            " -3.27795804e-02 -7.08709402e-02 -2.26382471e-02 -1.47434565e-02\n",
            "  3.55309908e-02 -2.00847115e-03 -9.34494580e-02 -5.52130662e-02\n",
            " -7.42408151e-03  7.50571465e-03  1.74966759e-02 -9.14437773e-02\n",
            "  7.98877043e-03 -5.39770464e-02  6.31371896e-02 -8.05335488e-03\n",
            " -2.17578838e-02 -1.64196347e-02 -8.26620735e-03 -9.95830979e-03\n",
            " -6.76658660e-02 -2.32955347e-02  5.19470558e-02 -4.04787323e-03\n",
            " -1.77853716e-02  1.53511025e-02  1.45491113e-02  1.10360039e-02\n",
            "  2.69646991e-02 -1.76732648e-03]  - intercept :  0.9312010763287404\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.06532208198752286\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:01,589]\u001b[0m Trial 143 finished with value: -0.14896718443888265 and parameters: {'count_threshold': 5, 'postag': False, 'voc_threshold': 9448}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.12206389 0.12546877 0.09075319 ... 0.31338015 0.00950832 0.04609262]\n",
            " [0.         0.04472025 0.02623942 ... 0.01084103 0.0096546  0.09530643]\n",
            " [0.31427013 0.24813633 0.22273775 ... 0.04451095 0.01195331 0.0270934 ]\n",
            " ...\n",
            " [0.33280345 0.34778882 0.08393645 ... 0.21776589 0.00760665 0.01950123]\n",
            " [0.43762984 0.26082672 0.39533991 ... 0.05656376 0.00261479 0.00592668]\n",
            " [0.06091694 0.21242106 0.14023583 ... 0.         0.00929702 0.07204408]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-6.97742168e-03 -5.26768047e-03 -7.68245841e-03  6.56909010e-03\n",
            "  1.12147375e-02 -1.08410618e-02  5.54725029e-03  1.19756009e-03\n",
            " -1.29520773e-03  2.05654555e-03 -1.68754631e-02  1.30962464e-03\n",
            "  5.47356026e-03  6.23851488e-04  8.41484414e-03 -1.32607408e-03\n",
            " -9.27882212e-03  9.82205622e-03 -7.38809102e-03 -5.12405179e-03\n",
            " -1.08224930e-02 -3.36366267e-02 -3.29644760e-04  8.43901121e-03\n",
            "  1.54345856e-02  1.37884717e-02 -1.00678018e-02  1.20472522e-02\n",
            "  1.17225029e-02  1.12081936e-02  6.34810666e-04 -1.65373777e-02\n",
            "  3.24047062e-03  4.59185908e-03 -8.41964776e-03  1.21901160e-02\n",
            " -4.20841805e-03 -5.02395405e-02 -1.47160586e-03 -2.29400683e-03\n",
            "  2.22839009e-02 -1.35836135e-02  6.03382645e-03  4.58823001e-03\n",
            "  6.13155894e-03 -2.63096914e-02  6.71318893e-04  7.04875422e-03\n",
            "  2.70070418e-02 -2.63448262e-02  5.87333734e-03  4.98934167e-03\n",
            "  1.61614915e-02 -7.66857501e-03  8.19176987e-03 -1.73174992e-02\n",
            " -1.12776919e-02 -2.87176942e-02 -1.92411447e-02 -1.22268347e-02\n",
            "  1.48669830e-04 -1.31251715e-02  7.45925738e-03 -1.85487241e-02\n",
            " -1.51431366e-03 -1.20659225e-03 -2.09150877e-02 -1.28374737e-02\n",
            "  3.90867637e-02 -2.89628844e-03  6.64278631e-03  3.08727018e-03\n",
            "  2.02105040e-02 -1.99001863e-02 -1.39298733e-02  2.54388041e-02\n",
            "  5.10257016e-03  2.39834954e-02 -1.40896363e-02  4.87851586e-03\n",
            " -1.13898767e-02  1.90031154e-02  9.15414209e-03  1.43220993e-02\n",
            " -2.55951610e-02 -1.12352713e-02  8.56724604e-03 -4.85508516e-03\n",
            " -7.30341765e-03 -6.29548946e-03 -3.59557449e-02  8.74729402e-03\n",
            "  6.61519343e-03  1.52598471e-02 -2.96238175e-03  2.43334367e-02\n",
            " -4.57810162e-02 -2.60423636e-02  6.02524723e-03  8.25326560e-03\n",
            " -5.71171779e-02 -3.47004216e-02 -7.36665489e-03  4.94948522e-03\n",
            " -1.55053430e-03 -2.65575882e-03  2.27038064e-02 -2.49689098e-02\n",
            " -1.38552729e-02  2.76376556e-02 -7.31627785e-03  9.56407980e-03\n",
            " -7.25407969e-03 -3.35087862e-02  1.92582081e-02  1.44680495e-02\n",
            " -1.96042350e-02 -3.69423042e-03  6.35636552e-03  1.18165966e-02\n",
            "  5.79496903e-03 -7.95382987e-03  3.75146036e-02  1.34909262e-02\n",
            " -1.58442461e-04  1.33396401e-02 -1.28170217e-02  4.91845641e-03\n",
            "  1.01819914e-02  2.73612812e-03  2.45020564e-02  5.41404739e-04\n",
            "  1.05172773e-02 -5.88094566e-03 -2.79967362e-03 -1.40312362e-02\n",
            " -2.85802966e-02 -1.86549319e-02  1.05563812e-02 -1.98894302e-02\n",
            "  3.00173208e-03  2.85206635e-02 -1.21187699e-02 -1.30116669e-02\n",
            " -2.31265070e-03 -1.26583598e-02 -1.25496229e-02  1.54972029e-02\n",
            " -1.40331936e-02 -6.25043423e-03 -1.01799419e-03 -2.44989577e-03\n",
            "  7.01156083e-03  3.54905958e-02 -2.37088710e-02 -3.00440471e-02\n",
            "  1.24653807e-02 -3.25076067e-02 -2.93964048e-02 -7.60035304e-03\n",
            "  3.26279267e-02  1.05696334e-02  3.60334967e-02  1.95293562e-02\n",
            " -1.60539813e-02  1.64091315e-03 -2.56840685e-03  3.89804427e-02\n",
            " -8.75183176e-03 -6.43788639e-03 -3.29469724e-02  1.53574616e-02\n",
            "  1.77919222e-02 -2.82680400e-02 -3.98381090e-02 -8.84639273e-04\n",
            "  1.81627366e-03  1.44691283e-02 -7.62152512e-03 -1.06445824e-02\n",
            "  1.92823377e-02 -2.89373257e-03  3.21798299e-02  1.31859812e-02\n",
            "  2.54391413e-03  1.24556986e-02 -4.62524327e-04 -6.35471159e-03\n",
            "  4.64353808e-03  2.33050466e-03 -1.90003461e-02 -3.87636784e-03\n",
            " -1.00533271e-02  1.55329610e-02  1.71083987e-02  2.44235625e-02\n",
            "  1.85623889e-02 -1.69532776e-02 -7.86390524e-03 -3.18458678e-02\n",
            "  2.25625056e-03 -2.65748899e-02 -4.27879212e-04  1.94482359e-02\n",
            " -8.77928802e-03  6.69887353e-03 -1.19592322e-02  2.22128315e-02\n",
            "  8.75676607e-03  2.21684119e-02 -2.07434663e-02 -1.45946908e-02\n",
            " -6.68801944e-03  7.32497921e-03 -2.16433642e-02  1.79118310e-02\n",
            " -3.40813817e-02  1.29825945e-02 -1.64457330e-02 -3.30011832e-03\n",
            "  3.92743903e-02  1.99006581e-02 -1.82071838e-03  1.47427575e-02\n",
            " -3.07527992e-03 -2.02018092e-02 -2.12824563e-02 -2.12737786e-02\n",
            "  5.34837055e-04  3.41368360e-02  8.59163916e-04  4.18060200e-03\n",
            " -1.52890929e-02 -2.87888979e-02 -2.16250974e-02 -1.64818330e-02\n",
            "  7.41753094e-03  5.18417893e-03  6.58800368e-03 -1.29363377e-02\n",
            "  6.99980675e-03 -2.05032192e-02 -7.35946356e-03  9.58602895e-03\n",
            " -3.87845601e-02 -1.18434198e-03 -3.67444218e-02 -1.32907388e-02\n",
            " -1.78460610e-03  8.94542113e-03 -2.94375608e-02 -6.79985678e-03\n",
            "  1.99817643e-02  1.99817643e-02 -2.47535677e-03  9.19957439e-03\n",
            "  3.25434672e-04  2.04117200e-02 -2.87457532e-02  1.18757419e-02\n",
            "  3.56383614e-02  3.04827162e-02  7.56699615e-04  2.42207370e-03\n",
            "  2.83960047e-03 -7.12347613e-03  6.77503554e-03  1.32565571e-02\n",
            " -1.48109745e-02 -4.13001254e-03  6.90397336e-03  2.89633155e-02\n",
            "  2.32661544e-02  2.57388308e-02  3.36554582e-03 -1.31137565e-02\n",
            " -2.06343232e-02 -1.02438760e-02 -5.71634968e-03  4.13361179e-02\n",
            "  1.44895575e-02  7.74609618e-03 -9.11559976e-03  2.12611356e-02\n",
            "  3.24043351e-02  1.57216060e-03  1.25351070e-02  2.38709647e-02\n",
            "  1.38034794e-02  5.76361632e-03  1.56120212e-02 -1.30585618e-02\n",
            "  7.05554915e-02  2.77145272e-02  3.06434283e-02 -6.21419596e-03\n",
            " -2.23261168e-02 -1.17799866e-02 -7.68811776e-03  4.21507393e-03\n",
            "  7.07624250e-03 -1.27156702e-02 -1.41102754e-02 -2.69722007e-02\n",
            " -4.46347055e-02  1.67927763e-02 -1.35299048e-02 -1.19990486e-02\n",
            "  9.02229556e-03  2.16164725e-02 -9.30517118e-03 -1.65171648e-02\n",
            "  2.55220528e-03 -2.97152508e-02 -1.28227714e-03 -1.23040491e-02\n",
            " -1.01264583e-02 -2.73685332e-02  9.59110303e-03  2.78429830e-02\n",
            " -4.80910138e-03 -2.14805428e-02  2.59353178e-02 -6.00184611e-03\n",
            " -2.52766700e-02  3.19570141e-03 -1.50561467e-02  2.26932945e-02\n",
            " -3.39612867e-02  5.78882753e-03 -2.64303462e-02  1.37489017e-02\n",
            " -1.00942910e-02 -8.11785445e-04 -4.11975480e-02  3.37621963e-02\n",
            " -7.35086049e-03  1.52139786e-02  2.99236116e-03 -1.75785398e-02\n",
            " -4.20951868e-02  2.22958202e-02 -2.25016543e-02 -9.21363585e-03\n",
            "  5.42874828e-02 -6.83455446e-03  1.09296636e-02 -3.72186218e-02\n",
            " -3.39080393e-02  6.68167456e-03  5.53813283e-03 -2.10307887e-03\n",
            " -2.26839997e-02  9.63123448e-03 -2.11360286e-02  1.39211276e-02\n",
            " -3.61751738e-04  2.55576463e-02 -3.23486099e-02  8.10761519e-03\n",
            "  1.36477718e-02 -7.21639754e-03 -3.81132117e-02  1.38906584e-03\n",
            "  1.67601280e-04  2.05309720e-02 -4.98176872e-03  2.07079604e-05\n",
            "  4.48129400e-03 -1.32098663e-03 -4.51717898e-03 -1.59500130e-02\n",
            "  4.90368327e-03  2.27495111e-03 -5.33405668e-03 -3.83703999e-03\n",
            "  2.39045350e-03 -1.27007004e-02  4.98812579e-02 -1.77557163e-02\n",
            " -2.43211886e-03 -5.57604816e-03  2.30623772e-03  1.19811981e-03\n",
            " -5.06602366e-03  4.64412145e-04  1.26030662e-02 -2.71799435e-03\n",
            " -1.43196520e-02 -9.46933773e-03  6.37490541e-03  1.41503146e-02\n",
            " -7.47184983e-03  1.39769046e-02 -5.84180480e-03 -3.25074159e-02\n",
            "  4.14706205e-02 -4.32359268e-03  2.55274944e-02 -1.22876069e-02\n",
            " -1.48659333e-02 -1.89376132e-02 -8.13595648e-04  5.10834399e-04\n",
            "  1.89267274e-03 -1.38406190e-02 -6.97152001e-03 -4.78666014e-03\n",
            "  2.31962699e-03 -1.11704735e-02 -1.31231787e-02 -3.01357495e-02\n",
            " -2.05044779e-02  2.17893023e-02  1.74229217e-03 -3.48772236e-02\n",
            "  1.32371221e-03  7.51924589e-03  6.13567090e-03  2.11682800e-02\n",
            " -1.00849734e-02  2.84192446e-03 -3.41707781e-03 -2.00159425e-02\n",
            " -2.93564559e-03 -6.72186969e-03  3.20657404e-03 -1.33724441e-02\n",
            "  1.86424701e-03 -7.34857964e-03 -1.79982660e-02 -3.18984679e-02\n",
            "  7.74894167e-03  3.79160450e-03 -1.20316984e-02  4.79937838e-03\n",
            " -1.19233823e-02 -3.10103939e-02 -1.32329306e-02 -5.66994378e-03\n",
            " -2.33868781e-02 -3.84580719e-02  3.65038908e-02 -9.87024285e-03\n",
            " -2.29610069e-02  1.81967181e-02  1.60287943e-02 -3.13760133e-02\n",
            " -2.39154996e-02  5.04806407e-03  6.78321015e-03  2.87256514e-02\n",
            " -1.33800020e-02 -1.77220849e-02 -1.79178202e-02 -6.70501467e-03\n",
            "  1.06110961e-02 -5.43437671e-03 -1.52588176e-02 -4.50986549e-03\n",
            "  2.43712183e-02 -7.99055506e-03  1.80236027e-02 -4.53637973e-03\n",
            " -1.85945825e-03 -6.21127412e-03 -1.56153111e-02 -1.48150136e-02\n",
            " -1.30977668e-02  1.16787994e-02 -2.56688871e-02 -9.78743654e-03\n",
            " -2.29940448e-04 -1.28626177e-03 -4.08277476e-02 -1.57056527e-02\n",
            " -2.36260888e-02  2.28811434e-03  9.38554941e-03 -6.05548096e-03\n",
            " -7.61633515e-03  1.37948034e-02 -2.22938946e-02  2.85104044e-02\n",
            " -1.76214652e-02 -7.50849752e-03  2.86667549e-02 -2.10034816e-03\n",
            " -2.18502063e-03  2.95134402e-03 -6.22441359e-03 -2.28606596e-02\n",
            " -2.66613465e-03 -3.33580809e-03 -1.28177804e-03 -4.86505737e-03\n",
            "  2.05093618e-02 -3.07233922e-03  1.75773963e-02  1.02352157e-02\n",
            " -5.64028997e-03  1.68011489e-03 -2.57961742e-02 -1.04657747e-02\n",
            "  1.58897572e-02  6.01507916e-03 -9.15691521e-03  5.01759995e-03\n",
            "  3.29451085e-03 -2.13321885e-02  4.83803540e-02  8.10859396e-03\n",
            "  3.91942598e-03 -1.04916544e-02 -1.39875859e-02  2.44717010e-03\n",
            " -9.60740662e-04 -2.43357699e-02  1.91052807e-02 -2.12427644e-02\n",
            "  4.83269615e-02  2.46876745e-02 -1.32600771e-02  5.82893029e-02\n",
            "  6.55888406e-04 -7.92917761e-03 -1.31093056e-03  2.50551742e-02\n",
            " -3.46348739e-02  7.68633791e-03 -1.25204211e-02 -1.62351490e-02\n",
            " -1.82071613e-02 -1.61307331e-02  7.67759200e-03  1.05304228e-02\n",
            "  1.65751044e-02  1.64750474e-02 -1.23671724e-02 -1.90490652e-03\n",
            " -1.11493114e-02 -2.39711706e-04 -8.28223445e-04 -1.64926911e-02\n",
            " -4.34258612e-02 -1.37293054e-02 -3.10715451e-03  9.25576713e-03\n",
            " -1.93960669e-02  4.55722956e-03  3.64213347e-03 -1.90020617e-02\n",
            " -1.87616946e-02  2.03863593e-02  8.76428069e-03 -1.97758272e-03\n",
            "  2.44810427e-02  8.35281538e-03 -1.91527854e-02 -9.86706131e-03\n",
            "  2.40324757e-02  1.19353177e-02 -1.57046354e-02 -3.88545560e-02\n",
            " -1.91914699e-02 -1.83512097e-03 -1.91224577e-02 -1.87099302e-02\n",
            " -1.08170696e-02  2.14163537e-02 -9.58454277e-04 -1.88134728e-02\n",
            " -5.90657744e-03 -3.06845183e-02  4.88394305e-03  1.23972731e-02\n",
            "  7.67596788e-03  6.31088208e-03 -8.28152034e-03  1.15576964e-03\n",
            " -1.37734249e-02  1.70318334e-02  1.49900852e-02 -1.32551494e-02\n",
            " -3.63514534e-02  4.11817471e-02 -1.99977441e-02 -1.41135827e-02\n",
            " -1.77031367e-03  5.75513406e-02  1.61003226e-02 -1.18528184e-02\n",
            " -2.38849917e-03  9.69084961e-03  3.49364737e-03  1.70817641e-02\n",
            " -3.41906292e-03  2.90597988e-04 -2.16550573e-02  2.77834517e-02\n",
            " -2.64012316e-02 -3.45864133e-04 -4.56521490e-03  3.83443850e-02\n",
            "  2.04941526e-03 -3.15455677e-02 -1.85978328e-02 -9.63422505e-03\n",
            " -1.27719826e-02  1.35093329e-02 -4.23473557e-02 -9.92776694e-03\n",
            " -1.06184414e-02 -3.22031334e-02 -1.30436849e-02 -2.36790468e-02\n",
            " -2.34169083e-02 -1.61519137e-02 -2.45862196e-02  3.32864925e-02\n",
            " -3.48450666e-02  1.21179402e-02  2.58712213e-03  3.59291033e-03\n",
            " -1.01265581e-02 -1.12210296e-02  5.89056968e-03  5.89056968e-03\n",
            " -1.64364352e-02 -5.39250871e-03  1.25437906e-02  1.14713775e-02\n",
            " -2.13878028e-02 -5.35331901e-04  3.32018298e-03  1.76622791e-02\n",
            " -2.33486452e-02  8.96208314e-03  4.06798451e-02 -2.70109816e-02\n",
            "  6.18301989e-03  8.76146083e-03 -1.79865031e-02  5.59738160e-03\n",
            " -3.79001594e-02 -5.78733205e-03  1.11487110e-03 -2.23791559e-03\n",
            " -9.55844605e-03  3.34391214e-03 -1.86914959e-02 -4.82116417e-03\n",
            "  8.99529647e-03  1.82623059e-02  2.23620884e-02 -2.22700212e-02\n",
            "  2.57043563e-02  1.09305443e-02  3.45544841e-02  1.23687957e-02\n",
            " -1.34969812e-04  1.10264535e-02  1.93603983e-02 -1.20866498e-02\n",
            "  3.05244115e-02 -8.29267565e-03  3.75920685e-02  2.12462823e-02\n",
            "  5.85000887e-03  9.06513656e-03  5.34958327e-02 -1.34449889e-02\n",
            " -2.52314017e-02  1.95389214e-03 -1.58851894e-02 -2.38546396e-02\n",
            " -2.65109555e-02  7.13845198e-02 -1.37159070e-02  4.00314414e-03\n",
            "  4.72403133e-02 -1.22658343e-02  1.22363353e-02 -6.98913356e-03\n",
            " -7.20072089e-03 -2.45971181e-02  1.34412279e-02 -2.17804126e-02\n",
            " -7.93860731e-03  1.45136510e-02 -1.58047012e-02 -1.43877690e-02\n",
            "  8.36683305e-03 -6.05810933e-03 -2.34599132e-02 -2.09727970e-02\n",
            "  9.57457806e-03  1.08338124e-02 -3.18139016e-02 -2.64157125e-02\n",
            "  1.21393126e-02  3.22223425e-02 -8.22860021e-03 -2.58544612e-02\n",
            " -1.45290471e-02  7.37229006e-04  4.07285566e-03  1.07180513e-02\n",
            " -2.99877537e-02  7.05544024e-03  1.61002825e-02  1.03701722e-02\n",
            "  2.17405044e-02 -9.10828628e-03 -3.08173929e-02  5.50648780e-03\n",
            " -7.41447372e-03 -2.72913279e-02  4.34395332e-02 -1.52075359e-02\n",
            " -2.80732655e-03 -1.30081870e-02 -3.64423018e-02 -5.61505605e-03\n",
            " -9.97544527e-03  1.01957994e-02 -1.25445928e-02  4.37867498e-04\n",
            " -4.03583076e-03  1.62008610e-02 -5.08934199e-03  1.34888718e-02\n",
            "  3.65806647e-02  1.48907414e-02 -1.62776876e-02 -2.53961008e-02\n",
            " -2.39644210e-03 -4.05223611e-04  2.28797335e-02  1.43060962e-02\n",
            " -2.05685571e-02  1.96809864e-02 -3.67979953e-03  2.99850876e-03\n",
            " -6.46096931e-03  2.19345915e-03 -1.39228745e-02  7.59713187e-03]  - intercept :  0.7445811680317167\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.14896718443888265\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:02,156]\u001b[0m Trial 144 finished with value: 0.15102347865776303 and parameters: {'count_threshold': 10, 'postag': True, 'voc_threshold': 9745}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.3113248  0.02408371 0.47833244 ... 0.42830102 0.63674222 0.01000522]\n",
            " [0.09978447 0.05817536 0.05853347 ... 0.05838242 0.14383413 0.03040617]\n",
            " [0.         0.         0.         ... 0.         0.         0.11159265]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.         0.         0.05780008]\n",
            " [0.         0.03914459 0.0055586  ... 0.         0.         0.07066076]\n",
            " [0.22113771 0.12291706 0.08024517 ... 0.8601253  0.43927801 0.01305332]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-0.05706982  0.03287464 -0.01250341 -0.02288777  0.06268036 -0.10347424\n",
            "  0.08005993 -0.02653145 -0.33427132 -0.13884633  0.0351567   0.04223512\n",
            " -0.01248149 -0.1537366  -0.06084161 -0.10718049 -0.00548557 -0.01898286\n",
            "  0.10437578 -0.04317175 -0.00885046 -0.04542862 -0.09480342 -0.00949817\n",
            " -0.05301033 -0.01785476 -0.06092137 -0.09149191 -0.02982105  0.06329373\n",
            " -0.11682858  0.05755583 -0.02501085 -0.01212376 -0.00471645  0.02322695\n",
            "  0.00899702 -0.08370129 -0.02815612 -0.16432503 -0.13513785 -0.11598689\n",
            " -0.04701756  0.05465146 -0.01273915  0.08468227 -0.30326047 -0.27650485\n",
            " -0.06464807 -0.06159117 -0.32255552  0.040921   -0.17339627  0.09948924\n",
            "  0.02409776  0.09532743  0.01019643 -0.12228159  0.01928546 -0.12214928\n",
            "  0.03956736  0.13394289 -0.10033727  0.069708    0.12817769  0.07181283\n",
            "  0.15980173 -0.10781515 -0.01977636  0.04765177 -0.00461226  0.06678266\n",
            "  0.13856425  0.02849359  0.06396113  0.13505601  0.02189184 -0.19848624\n",
            " -0.19633541  0.14631796 -0.03632421 -0.00947069 -0.00084028 -0.01677123\n",
            "  0.13939788 -0.01055246 -0.02936789  0.02255903  0.22619881  0.12392273\n",
            "  0.02859219  0.05825678 -0.09919661 -0.21459111 -0.03783394 -0.01217181\n",
            " -0.21613247 -0.02487473  0.19064699  0.17936684  0.04188775  0.02334137\n",
            "  0.02507089 -0.03839281 -0.06551759  0.0030299  -0.04264487  0.0045295\n",
            " -0.12946197  0.0226617   0.00487599  0.02584596  0.07517247  0.11047172\n",
            "  0.05770036  0.04112567 -0.23331252  0.03023359  0.14501633  0.14044963\n",
            " -0.38125507  0.09203523 -0.08171932 -0.01625298 -0.05618184 -0.00219697\n",
            " -0.1423534  -0.10680479  0.03130939 -0.04391474 -0.04391474 -0.04391474\n",
            " -0.02351885 -0.11737999 -0.03448985  0.0876574  -0.00867209 -0.13877368\n",
            "  0.1052338  -0.00892873 -0.15594403  0.06727353  0.02418856 -0.06061765\n",
            " -0.06381789 -0.00286836  0.0074925   0.00399349  0.00192061  0.02807358\n",
            " -0.01224066  0.03505512 -0.00790491 -0.15994615 -0.00132745  0.08253379\n",
            " -0.02927988  0.0324178   0.08236526 -0.08473337  0.18420785 -0.10723953\n",
            "  0.03986881 -0.07554658 -0.02789369 -0.02799855  0.0144549   0.00140923\n",
            "  0.01301019  0.11831293  0.05679997 -0.00136901 -0.14642921  0.0751266\n",
            "  0.0476443  -0.0016469   0.16990352 -0.01206708 -0.10022156  0.09727213\n",
            " -0.01845253 -0.03065982 -0.03717837 -0.051602   -0.10283422  0.03987843\n",
            "  0.02115476 -0.00761579  0.01086303  0.07387755 -0.02821076  0.14902304\n",
            " -0.16455499 -0.02507573  0.01955053  0.08763492 -0.00490541 -0.06196599\n",
            "  0.07967898  0.07273959 -0.103042    0.06169583 -0.04172333  0.01802262\n",
            " -0.0364005   0.08470333  0.12698882 -0.00422613  0.06253398  0.05006958\n",
            "  0.17661641  0.11792382 -0.22552545 -0.134854   -0.03216258  0.06984501\n",
            " -0.14954892  0.2841016   0.0148339   0.05575516  0.21786175  0.02288287\n",
            "  0.07010391 -0.09281082 -0.05095033  0.00126488  0.03396447 -0.12667124\n",
            " -0.1145836   0.02450122  0.08857852  0.09702587  0.01350074  0.11752091\n",
            " -0.05903716 -0.02275328 -0.1000041  -0.00658726 -0.04957139 -0.04572899\n",
            " -0.12646076  0.10326761 -0.20172611  0.28658319  0.18385043  0.04663437\n",
            "  0.01265801  0.01185453  0.00250783 -0.069614    0.07035071  0.22014188\n",
            " -0.00221645 -0.00395878 -0.02696151  0.04831846 -0.09899637  0.21729363\n",
            "  0.09154327 -0.01424202  0.09447722 -0.06480634  0.11533276  0.11562801\n",
            "  0.01488654  0.0064551  -0.04313293  0.01445329 -0.00547101 -0.00103805\n",
            "  0.14137135  0.0884277   0.05819496 -0.08552409 -0.00351004 -0.00351004\n",
            " -0.00351004  0.15541938  0.03964354 -0.07022522 -0.00857463  0.07791961\n",
            " -0.25194863  0.00169271  0.0335641  -0.12362921  0.12502356 -0.00734047\n",
            " -0.06295948  0.01017526]  - intercept :  0.5077328609272531\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.15102347865776303\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:02,742]\u001b[0m Trial 145 finished with value: -0.1774583400197315 and parameters: {'count_threshold': 6, 'postag': False, 'voc_threshold': 2228}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.         0.04037182 0.06010835 ... 0.18129118 0.00543462 0.13301052]\n",
            " [0.02752838 0.04443741 0.03647674 ... 0.14787243 0.25116691 0.04215963]\n",
            " [0.07178077 0.01875839 0.07576334 ... 0.15103771 0.29695953 0.04350173]\n",
            " ...\n",
            " [0.04343745 0.12209539 0.08601564 ... 0.12887831 0.09987747 0.04601879]\n",
            " [0.39823226 0.02450601 0.66284646 ... 0.28915057 0.00407596 0.01495217]\n",
            " [0.04105388 0.14840794 0.04151813 ... 0.07730276 0.09414333 0.04392375]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 1.44742623e-02  1.84709924e-03  8.12943433e-03 -1.01004250e-03\n",
            "  5.61823288e-03  2.38309831e-02  1.21663262e-02 -1.61723163e-03\n",
            " -3.69579493e-03  8.06691431e-03  2.30721983e-02  1.72944526e-03\n",
            "  1.35249940e-03  7.94120412e-03 -6.80715138e-04 -6.65757658e-03\n",
            " -2.35760236e-03  2.15460023e-02 -3.93605351e-04  3.11486543e-03\n",
            " -1.28183976e-02  2.19473589e-02 -3.78559037e-03 -5.01220768e-05\n",
            "  1.11573183e-02  1.48636840e-03 -7.00330420e-03  1.61819285e-02\n",
            "  7.97619508e-03 -2.18129099e-03 -7.32246070e-03  1.69498260e-02\n",
            " -6.79690995e-03  4.64603165e-03 -2.02270569e-03  2.53568091e-02\n",
            " -1.17573523e-02 -1.27020052e-02  1.36403989e-02  2.03803504e-02\n",
            "  2.45521774e-03  4.78052996e-03  1.07254132e-02  3.69828490e-03\n",
            " -9.83918509e-03 -1.38272558e-02  7.33561699e-03  1.31371426e-03\n",
            " -1.79545089e-02 -9.26225816e-03 -2.76934761e-03  1.52411445e-02\n",
            "  4.65538506e-03  1.57715429e-02  2.27942185e-02 -2.96629620e-02\n",
            " -3.84150414e-03  2.13103153e-03  3.67498761e-04 -1.18437433e-02\n",
            "  8.77408541e-03 -2.31161501e-02  1.32545933e-02  1.83967709e-02\n",
            " -2.71378397e-03  1.46323497e-02  4.61963048e-02  9.58322314e-03\n",
            " -1.84697637e-02  5.43837416e-04 -1.40903438e-02 -2.93340714e-03\n",
            "  8.90951616e-03  2.75245073e-02  2.67836819e-02  1.29607631e-02\n",
            " -3.30062139e-02 -4.39843808e-03 -7.98700247e-03 -2.16531501e-02\n",
            "  1.49022960e-02  9.02948409e-03  3.28327253e-02 -2.87413439e-02\n",
            " -4.79249990e-03 -8.82370010e-03  8.72330312e-03  1.03268325e-02\n",
            " -1.07685543e-02  4.42271471e-03 -5.64936953e-03 -1.50031779e-02\n",
            " -1.20588148e-02 -5.17558313e-03 -3.30451152e-02 -2.80494320e-03\n",
            "  3.33853064e-03 -6.26559950e-03  2.89237193e-04 -2.74904542e-02\n",
            " -1.78356158e-03 -6.10174411e-03 -2.69350749e-02 -1.23010926e-02\n",
            "  1.74491824e-02 -1.88960152e-03  1.35417352e-02  2.98493868e-03\n",
            " -1.60260245e-02 -1.38874200e-02 -1.17393612e-02 -1.97089724e-03\n",
            " -1.45482561e-02 -1.26530572e-02  8.50948842e-03 -4.49900017e-03\n",
            " -1.06962690e-03 -3.92535523e-02  2.57344637e-02  1.24884484e-02\n",
            "  2.98635187e-02  1.22901217e-02  1.23431057e-02  1.35657852e-02\n",
            " -5.52833154e-03 -2.60917206e-02  1.35581286e-03  2.42781085e-02\n",
            "  8.51065608e-03 -9.11271164e-03 -1.19280504e-02  3.56352892e-03\n",
            "  1.34649796e-02  4.45372847e-03 -7.12263390e-03 -3.60855809e-02\n",
            "  1.43767412e-02 -2.03275194e-02  6.78810146e-03  9.23839786e-03\n",
            "  5.83858176e-03  1.41643338e-02 -2.78770146e-02  2.67010552e-02\n",
            "  6.02348059e-04 -5.16027094e-03  1.47674308e-02  2.16176259e-02\n",
            "  1.88341831e-02  9.53251597e-03  1.68735292e-03  3.02424760e-02\n",
            " -4.11165390e-02  3.73626668e-03 -1.13264420e-02 -1.09164546e-02\n",
            "  1.44807544e-02  1.59259280e-02 -3.19784483e-03 -1.42371200e-02\n",
            "  1.67937213e-02  8.51886287e-03  6.23474913e-03  3.87611130e-03\n",
            " -2.18445221e-02 -1.73487412e-02 -2.06611965e-02 -7.11083123e-03\n",
            " -4.52013465e-03  2.17797869e-02  1.08140905e-02  1.49210467e-02\n",
            "  1.81793097e-03  1.97112446e-02  5.21053250e-03  2.14796315e-03\n",
            "  5.14768107e-03 -3.13028906e-04  2.30495610e-02 -4.69984091e-03\n",
            " -8.48763067e-03 -2.09058675e-02  6.18032181e-04 -2.93137518e-02\n",
            "  1.02391410e-02 -1.53017630e-02 -1.11380194e-02  1.05334551e-02\n",
            "  7.72868460e-03 -2.13810428e-02  4.73910872e-03 -1.15619032e-02\n",
            " -5.50085182e-03 -5.18897753e-03 -7.21583782e-04 -9.41745242e-03\n",
            "  1.75626412e-03 -2.57953773e-02 -1.57395447e-02 -4.53683875e-03\n",
            "  1.29815741e-02 -1.51144797e-02 -1.04337735e-02  5.38669120e-03\n",
            " -9.04577677e-03  6.61891449e-03  1.20256017e-02  1.94342467e-02\n",
            " -3.26369625e-02 -1.24317420e-02  1.89499812e-02 -1.91981849e-02\n",
            " -2.06560464e-02 -1.82954946e-02  9.60663170e-03  7.95999524e-04\n",
            "  8.07167948e-03  6.80911657e-03  3.22569622e-03 -8.75915034e-03\n",
            "  1.57007069e-02 -6.18178934e-03 -3.85643294e-02 -1.05920517e-03\n",
            " -2.79053876e-03 -2.11984768e-05  4.72874087e-03 -1.81383749e-05\n",
            " -3.19901289e-03 -3.99818382e-03 -2.81418689e-02  1.89235904e-02\n",
            " -1.02416835e-02  1.27235981e-02 -2.85019412e-02 -7.93009207e-03\n",
            " -1.91543237e-02 -1.78746376e-02  3.32798585e-02 -3.76999884e-02\n",
            " -1.40438396e-02 -1.39841049e-02  1.76524545e-02  8.66895545e-04\n",
            " -1.59385166e-02 -4.50034819e-02 -3.06814888e-03  1.06534441e-02\n",
            " -1.26073987e-02 -2.33254057e-02  6.68761463e-03 -1.43226913e-02\n",
            " -1.47219885e-04  8.26937102e-03  7.94439550e-04  3.34585080e-02\n",
            "  1.16564573e-02  7.77940482e-04 -3.09853916e-02 -7.75148509e-03\n",
            "  2.34729945e-02  9.69090030e-03 -1.59425108e-02  3.20576831e-02\n",
            "  7.44265194e-03 -2.79075529e-02 -6.30484698e-03 -6.30484698e-03\n",
            " -6.30484698e-03 -4.63846024e-03 -3.84918636e-03  1.02268467e-02\n",
            "  1.84503606e-02  7.77386100e-03  1.49913716e-02  5.10533471e-03\n",
            " -9.32060736e-04  1.92673144e-03 -2.01286293e-04 -1.05011661e-02\n",
            " -1.57641912e-02  6.63329714e-03 -8.41376924e-03 -2.53670198e-02\n",
            " -2.13417897e-02 -3.31377787e-02  2.49033172e-02 -3.64884392e-03\n",
            "  1.50676174e-02 -4.87283823e-03 -1.08052975e-02  3.09813044e-04\n",
            " -1.78069319e-02 -5.44979483e-02 -1.05716633e-02  1.77600750e-03\n",
            "  1.54767923e-03 -5.89560685e-03 -2.47562606e-02  1.43634839e-02\n",
            " -2.17294032e-02 -2.21967996e-02 -8.85889670e-03 -1.53945714e-02\n",
            " -2.37853084e-02  1.81267474e-02  1.31589285e-02  1.11009989e-02\n",
            "  1.11009989e-02  1.11009989e-02 -2.00225265e-02 -1.15776880e-02\n",
            "  1.94278334e-02 -2.68083296e-02  1.48441934e-02  7.38828981e-03\n",
            "  4.57097857e-03 -1.75228368e-02 -4.82004772e-03 -1.61959758e-03\n",
            " -1.34222105e-02  3.44711533e-02 -1.64816382e-02 -8.01688649e-02\n",
            " -2.61482902e-02  1.15292357e-02  2.38812549e-04  5.50236668e-03\n",
            " -3.64873068e-03  1.82891588e-02  5.03043365e-03 -3.35453430e-03\n",
            "  7.98744125e-04 -5.62434448e-04 -3.69033572e-03  5.56391858e-03\n",
            "  2.79204677e-04  1.76224892e-02 -1.01112444e-02 -2.04623375e-02\n",
            " -1.36185844e-02 -8.24992890e-03 -8.85457740e-03 -9.93253899e-03\n",
            " -2.51813183e-02 -8.53351657e-03 -1.81574779e-02 -2.79440933e-02\n",
            "  2.55394282e-02  1.28794404e-02 -1.01864848e-02 -1.63837288e-02\n",
            " -8.16051298e-03 -1.45276467e-02  2.52754978e-02  4.59224441e-03\n",
            " -2.39565169e-03 -1.51599241e-02 -3.77085160e-02 -1.50686287e-03\n",
            " -1.38452141e-02  3.05193792e-03 -3.86838146e-03 -1.88653940e-02\n",
            "  2.68574910e-03 -9.38510863e-03 -5.98075063e-03 -1.12398575e-02\n",
            "  3.44187574e-03 -3.77667338e-03  1.69599095e-02 -1.02705675e-02\n",
            " -2.03007733e-02 -1.51844698e-04 -2.56748497e-04  2.60275426e-02\n",
            " -5.26206803e-02  1.81646160e-02 -1.01178184e-02  5.55763601e-04\n",
            " -3.82450568e-03  2.47266798e-03 -2.62867166e-02 -7.29251100e-03\n",
            "  2.13282227e-02 -2.56035128e-02 -1.39246885e-02  4.80149603e-04\n",
            " -1.95615069e-02  3.14161144e-02 -2.71843651e-03  2.38375392e-02\n",
            " -2.52512880e-02 -1.46039223e-02 -3.21402236e-02 -1.25671965e-02\n",
            " -1.47081974e-03  8.49382757e-03 -7.48619129e-03 -3.08790468e-03\n",
            "  4.69280041e-03  1.93646326e-02  2.57681811e-02 -2.93793921e-02\n",
            " -2.23491926e-02  1.80332183e-03 -4.12538189e-02  5.22975153e-03\n",
            " -4.44242713e-02  1.46988799e-02 -1.87787579e-02 -3.04358652e-02\n",
            " -2.68766532e-03 -6.83245476e-03  8.18342721e-03 -1.70548758e-02\n",
            "  1.89972795e-02  6.76278474e-03 -2.25849451e-02 -2.45026440e-02\n",
            " -1.29994097e-03  1.46753044e-02 -1.88792113e-02 -1.64257520e-04\n",
            "  1.31093223e-02  8.94459864e-03 -6.68020496e-03  9.98567470e-03\n",
            " -2.58044503e-02  2.61248435e-03 -5.47371399e-03  5.07687073e-04\n",
            " -9.31188276e-03 -2.09043801e-02 -3.93837438e-02  2.76426533e-02\n",
            " -6.11929056e-03 -1.09582870e-02 -1.20973148e-02 -1.73226260e-02\n",
            " -1.82404113e-02 -9.87752679e-03  4.30808914e-04  8.86638854e-03\n",
            " -2.66484991e-02  6.32967702e-03  1.06364651e-02  2.50940361e-02\n",
            "  1.33625474e-02  1.19293433e-02 -2.01118259e-02  9.20723812e-03\n",
            "  8.49686404e-03 -1.72078688e-02  1.36633034e-02 -4.40101398e-02\n",
            " -2.51241405e-03 -1.27659366e-02 -2.33029893e-02  1.16463827e-02\n",
            "  1.44150633e-02 -2.03841925e-02 -7.39299608e-02  1.33646565e-03\n",
            " -1.31859091e-02 -2.36609269e-02  1.40891287e-02 -3.63590598e-02\n",
            "  2.59362183e-02  4.20687108e-03  7.08362609e-03  5.29137571e-03\n",
            " -8.09221584e-03 -6.49812163e-03 -4.85755242e-02  2.94868839e-03\n",
            " -1.48532567e-02  2.46886650e-02 -1.14680033e-02 -4.92263808e-02\n",
            "  1.25445029e-02 -1.12404809e-02 -1.61536462e-02 -1.03020885e-02\n",
            "  1.63333646e-02 -1.15632996e-02 -2.51924210e-02  2.79732793e-02\n",
            " -4.69613931e-02 -4.12444976e-03 -1.14926722e-02 -1.85299958e-02\n",
            "  4.79598726e-03 -1.41173270e-02 -2.47789503e-02  7.22245487e-03\n",
            " -3.01411323e-02  6.45358710e-03 -1.44262088e-02  5.66142496e-03\n",
            " -4.59460838e-03 -1.01364012e-02  1.08235107e-02  5.53142288e-03\n",
            " -2.03856329e-03 -4.47599062e-03  6.57041610e-03 -4.89485880e-02\n",
            " -1.34616719e-02 -8.63786148e-03 -4.96375448e-02  8.53295810e-03\n",
            " -2.24617956e-02 -2.46402435e-02  9.77821974e-03  7.94550327e-03\n",
            "  7.69264624e-03 -3.71117484e-02 -4.21851572e-02 -4.62027909e-03\n",
            "  1.73855948e-02  4.82808563e-03  2.82193223e-02 -3.73749161e-02\n",
            " -2.20242744e-03 -4.40156711e-02 -1.35655388e-02  6.75553003e-03\n",
            " -4.73225597e-03 -4.45388745e-03 -1.70261409e-02 -2.95603988e-02\n",
            " -2.93561109e-02 -3.47837549e-02 -3.18282188e-02  7.67548176e-03\n",
            " -2.57282861e-02  1.33774348e-02 -1.24351146e-02  9.96000335e-03\n",
            " -1.34299537e-02  1.83028466e-02 -2.98603764e-04  5.52214165e-04\n",
            " -2.06655618e-02  1.16073065e-02 -1.96094732e-02 -3.41993419e-02\n",
            " -2.15039616e-02 -2.81466730e-02  9.74673624e-03 -1.92343433e-02\n",
            "  1.78453593e-02  1.83232984e-02  1.05840301e-02 -6.69280816e-03\n",
            " -2.06281485e-03 -7.40142185e-03  8.38153841e-03 -8.48676425e-03\n",
            "  9.03008966e-03 -1.34731905e-02  8.01824900e-03 -1.65038970e-02\n",
            " -4.07840494e-03  5.08020007e-03 -3.94094609e-02 -3.51267364e-02\n",
            " -1.08249693e-03 -1.91378542e-03  1.33792144e-02 -1.91879232e-02\n",
            " -5.91185058e-02 -1.01629735e-03  1.61179343e-02 -2.40920733e-02\n",
            " -2.59221031e-02  8.33970997e-03 -1.00239705e-02  1.19467321e-02\n",
            " -1.41664561e-03 -5.40113904e-02  7.35452015e-03 -3.97911516e-02\n",
            "  3.30322408e-03 -1.56518453e-02 -1.45176478e-02  9.55194478e-03\n",
            " -3.96682314e-02 -1.81456429e-02 -1.01732667e-03 -1.79058905e-02\n",
            " -2.72673096e-02 -2.22325214e-02 -2.22325214e-02 -2.22325214e-02\n",
            " -1.50697763e-02 -2.72514169e-02 -8.70079786e-04  1.61213965e-02\n",
            " -1.49540109e-02  6.94879050e-03  1.82710186e-02 -1.37437879e-02\n",
            " -1.27536822e-02  7.67993659e-05 -4.30483999e-02 -4.25922372e-02\n",
            " -1.22451185e-02 -2.05169939e-03 -2.93955167e-02 -5.07234754e-03\n",
            " -1.11182742e-02 -2.27942943e-02 -1.53618558e-02 -2.32005868e-02\n",
            " -6.27480479e-03 -3.51670434e-02  1.35764629e-02 -2.61798170e-02\n",
            " -3.08822221e-02 -8.60953422e-03 -8.82073554e-03  8.69018183e-03\n",
            " -6.19436135e-03  2.90565143e-03 -1.87646114e-02 -2.00677999e-02\n",
            " -2.54724609e-02 -8.37888069e-03 -3.08120401e-03 -8.24908756e-03\n",
            "  1.60579365e-02 -6.75274562e-03 -9.22157412e-03 -9.22157412e-03\n",
            " -9.22157412e-03 -1.76054934e-02 -3.47495572e-02 -1.80065467e-02\n",
            " -2.65923762e-02 -1.11882682e-02 -3.39190540e-02 -3.86271047e-03\n",
            " -3.80341492e-03  6.61773344e-03 -8.05998913e-03 -5.38185678e-03\n",
            "  1.23164986e-03  3.45720984e-02 -3.99468910e-02 -4.57629486e-02\n",
            " -1.09494427e-02  7.47810993e-04]  - intercept :  1.140652118505551\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.1774583400197315\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:03,313]\u001b[0m Trial 146 finished with value: 0.22687595816572845 and parameters: {'count_threshold': 10, 'postag': False, 'voc_threshold': 1347}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.05743407 0.06239665 0.06675851 ... 0.06884926 0.05515333 0.03270538]\n",
            " [0.34430424 0.63135195 0.10421907 ... 0.22030146 0.00221788 0.01656386]\n",
            " [0.05108775 0.04836964 0.07094392 ... 0.06160197 0.29418387 0.04107577]\n",
            " ...\n",
            " [0.00928825 0.03091607 0.02384971 ... 0.         0.00328575 0.05260957]\n",
            " [0.         0.         0.0156374  ... 0.         0.00473148 0.07258835]\n",
            " [0.         0.         0.01250992 ... 0.         0.00443576 0.06082459]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-0.01150993 -0.0322229   0.0145133  -0.00737203  0.03837901  0.03884636\n",
            "  0.01725501  0.04007192  0.03486265  0.03582226  0.05619523 -0.11838001\n",
            "  0.05675635  0.05843779 -0.0030637  -0.09000369 -0.00483801  0.03368473\n",
            " -0.01542863  0.03222102  0.03338732 -0.0258038  -0.01202423 -0.03890471\n",
            "  0.02049517  0.02519365  0.04417042 -0.08136002  0.0322076  -0.00174705\n",
            "  0.03848244  0.040846    0.0037184   0.0220357   0.02677434  0.0438039\n",
            " -0.13453889 -0.04473812  0.06606416 -0.07803106  0.05656609 -0.12453582\n",
            " -0.06535215 -0.05643796  0.02055695 -0.03248656  0.06425302 -0.05806042\n",
            "  0.05600094 -0.10649371  0.06071773 -0.05485323  0.02669688  0.02213848\n",
            " -0.03507817  0.10260774  0.05945228 -0.12438713  0.04541364  0.11823694\n",
            " -0.06159811  0.07131489  0.03703344  0.054441   -0.13214718 -0.03121474\n",
            " -0.05226426 -0.0478128  -0.03108669  0.01687378  0.00480588  0.06117315\n",
            " -0.01059617 -0.01752249 -0.01752249  0.00401962  0.01259454 -0.02843913\n",
            " -0.10761457 -0.06623306  0.01736686  0.06152199  0.07499828  0.01591634\n",
            " -0.01646873  0.01166859 -0.09328526 -0.02576913 -0.12334063 -0.06591404\n",
            " -0.05696476 -0.02419814 -0.00237345 -0.00641306 -0.08427155 -0.02764021\n",
            " -0.0016698   0.01151614 -0.12459836  0.06793634 -0.07696988 -0.01031757\n",
            "  0.09801419 -0.10513277  0.05062407 -0.11305572 -0.10936564 -0.09132043\n",
            " -0.01071027 -0.00501648 -0.10686579 -0.07304736  0.06473296 -0.04162837\n",
            "  0.04218823 -0.07286227 -0.08455692 -0.07682975  0.05883123  0.01477219\n",
            " -0.09197341 -0.04944753 -0.04317624 -0.17120841 -0.16811915  0.01125304\n",
            " -0.06678492  0.04965693 -0.06353845 -0.0930739  -0.02903296 -0.03714603\n",
            "  0.03496299  0.0281544   0.02203644  0.01534778 -0.15103715  0.04605264\n",
            " -0.02705032  0.06037738  0.10837491 -0.00558726 -0.0060066  -0.04346657\n",
            "  0.03046921 -0.06472815 -0.03517228  0.02501989  0.01916654  0.04087761\n",
            "  0.01002086  0.04894665  0.06780943 -0.07305448  0.03871008  0.01005608\n",
            " -0.01783217  0.00724522  0.10054443  0.01622019  0.03340857 -0.09136046\n",
            "  0.00138051  0.00121403  0.06679525  0.01314478 -0.00364545 -0.0299124\n",
            "  0.05015919  0.16634013 -0.02607803 -0.01626394 -0.07013051 -0.03991871\n",
            "  0.04657419 -0.04783341 -0.022904    0.00429567  0.07125322  0.05052065\n",
            " -0.11222431 -0.06268396  0.06186074 -0.02558122 -0.04608754  0.02075738\n",
            " -0.03337434 -0.00418387  0.01170927  0.08143141 -0.01222887  0.00908738\n",
            "  0.04078476 -0.04690781 -0.03920847  0.05130108  0.08877338 -0.00850601\n",
            "  0.05784604 -0.03648804 -0.06114007 -0.03780942 -0.25314759 -0.14076581\n",
            "  0.06061153  0.06747444  0.01272198 -0.20267174  0.0350416   0.03050034\n",
            "  0.09477274 -0.05884225  0.10955721  0.03572202  0.05437412  0.00190204\n",
            "  0.00190204 -0.23695447  0.04806624 -0.04372007  0.02810441 -0.07433696\n",
            "  0.02102735 -0.09496182 -0.02297177  0.04616416 -0.0691223   0.04768009\n",
            "  0.09671528  0.00362713 -0.02427181  0.11335787 -0.11087658 -0.13213025\n",
            " -0.03854108  0.07173477  0.00666129 -0.02517741 -0.11145773  0.0537336\n",
            " -0.18253045  0.06500431 -0.11580695  0.03651651  0.00374321  0.02207981\n",
            "  0.01691651  0.0336801  -0.20169942  0.0488451  -0.16425913 -0.22334187\n",
            "  0.03197346  0.05232658  0.09811604 -0.05241138  0.03116512  0.02140145\n",
            " -0.00487189  0.0129447  -0.130115    0.1100957  -0.07380437 -0.03031401\n",
            "  0.05033937  0.0202127   0.04101132 -0.0076817  -0.08796377 -0.05677007\n",
            "  0.07437543  0.03709646  0.03819592 -0.02411069 -0.02813238  0.00812014\n",
            "  0.02958295 -0.00842457  0.03615806  0.06465963 -0.00171707  0.02971596\n",
            "  0.06799993 -0.01191941]  - intercept :  1.218269758626302\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.22687595816572845\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:03,876]\u001b[0m Trial 147 finished with value: -0.3044966736316322 and parameters: {'count_threshold': 9, 'postag': False, 'voc_threshold': 1067}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.00000000e+00 6.95727303e-03 2.03020834e-04 ... 0.00000000e+00\n",
            "  5.35623930e-02 3.91178794e-02]\n",
            " [1.51262965e-01 5.16571369e-02 5.66890429e-02 ... 4.68358302e-01\n",
            "  7.21310092e-02 1.17630169e-02]\n",
            " [0.00000000e+00 8.34872763e-03 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 5.74284836e-02]\n",
            " ...\n",
            " [0.00000000e+00 6.25521729e-03 0.00000000e+00 ... 5.56312257e-01\n",
            "  0.00000000e+00 6.97558149e-02]\n",
            " [3.21224999e-01 6.51968580e-02 7.41411898e-01 ... 1.39364519e-02\n",
            "  3.26862031e-01 1.47105580e-02]\n",
            " [0.00000000e+00 2.10033896e-02 1.00717625e-02 ... 2.69078747e-01\n",
            "  0.00000000e+00 5.98259834e-02]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 0.01197916 -0.02032576 -0.06880084 -0.01339463  0.03074982 -0.00631144\n",
            "  0.032167    0.11250967 -0.01175925  0.01109113 -0.03575911 -0.00036117\n",
            "  0.00125952  0.04590966 -0.00506474  0.0118677   0.07391617  0.00608996\n",
            " -0.02058202 -0.04728979 -0.02357721  0.10530123  0.0025575  -0.00585867\n",
            "  0.0816615   0.07324622  0.04369453 -0.01235609  0.07021025 -0.00074415\n",
            "  0.05350508 -0.00537523  0.02931622  0.00762497  0.12162878 -0.0113243\n",
            "  0.05776393  0.09700745 -0.01464614  0.01991565  0.09312874 -0.00227441\n",
            "  0.07964695  0.01817887  0.09402383  0.06679317  0.0109285   0.02700511\n",
            "  0.01506266  0.0291254   0.00826631 -0.01386033 -0.0862928  -0.12267995\n",
            "  0.09116224 -0.05399341 -0.03209344  0.02662241  0.00861177 -0.08852341\n",
            " -0.08454663  0.04675743 -0.04939049  0.04908915 -0.05174551 -0.05955736\n",
            "  0.1654728  -0.00624156 -0.03317083 -0.05600433  0.0222623  -0.04624003\n",
            " -0.00103646 -0.05747041 -0.04175983  0.03163494 -0.04805003  0.0697136\n",
            "  0.02331154 -0.02816176  0.00973923  0.01161356  0.03462527 -0.13196375\n",
            "  0.04341283  0.11241478 -0.05685568 -0.01204091  0.01234608 -0.09548676\n",
            "  0.01264509  0.03075518  0.11962187 -0.04727912 -0.06149575  0.01346285\n",
            "  0.0042333  -0.06179466 -0.13680797  0.05029472  0.05194598  0.04646233\n",
            "  0.09960641  0.00927379  0.07916375  0.08431404  0.08734019 -0.0923546\n",
            " -0.00832618 -0.08371352 -0.00853786 -0.04761966  0.06332117 -0.07488288\n",
            " -0.06912037 -0.07442958 -0.0306254  -0.05603311  0.00531414  0.06426461\n",
            " -0.04815256  0.00976605 -0.09479985 -0.01549083  0.03887238 -0.06482518\n",
            "  0.08849726 -0.05373572  0.04838351 -0.05353086 -0.07115321 -0.05791668\n",
            "  0.00534362  0.045703    0.06943528  0.00232364 -0.03741526  0.06594106\n",
            " -0.02139666 -0.01365647 -0.05717372 -0.02807712 -0.02938525 -0.02019267\n",
            " -0.13359122  0.01487138  0.00251684  0.04538314  0.05207069  0.00801917\n",
            " -0.00547848 -0.02039    -0.00170114 -0.02160628  0.00652621 -0.01060424\n",
            "  0.11647669  0.04260721 -0.02579203 -0.00254834  0.014513    0.01538543\n",
            " -0.03158645 -0.0137963  -0.02622043 -0.05546285 -0.0067293   0.18371209\n",
            "  0.03386339  0.00905297  0.03250244 -0.0454231   0.01303618  0.04826425\n",
            "  0.04598849  0.0599555   0.02205313  0.00021766  0.05349219 -0.07625716\n",
            "  0.11268093  0.01262014 -0.03995243 -0.07918198 -0.02196418 -0.01513241\n",
            " -0.03612348 -0.05653165  0.09567054 -0.01594539  0.00067238 -0.04802056\n",
            "  0.06044884 -0.11192242  0.09140435  0.06455323  0.09344276 -0.0164063\n",
            " -0.01183382 -0.09758372 -0.02943525  0.15878058  0.06836529  0.01371957\n",
            "  0.0787079   0.02437236  0.08823092 -0.06604558  0.0248939  -0.01555435\n",
            " -0.06701168  0.08943643  0.0068318  -0.0297692  -0.01689333 -0.01743787\n",
            " -0.06044295  0.00128153  0.0417286   0.01383648 -0.03331859  0.01456666\n",
            " -0.07101076  0.03867529 -0.09401649  0.03217356  0.16046667  0.0435421\n",
            " -0.01477049 -0.09137676  0.02016591 -0.02627021  0.08855656 -0.04674927\n",
            " -0.04358903  0.074063   -0.0846566   0.00296168 -0.07333095 -0.0005381\n",
            " -0.16676304 -0.06037462  0.00634004  0.04486165 -0.08940567  0.02637622\n",
            "  0.00499209 -0.05657183 -0.052455   -0.03927212  0.05016706 -0.09819126\n",
            " -0.0490404  -0.12765329  0.09020404  0.01876666 -0.02670999  0.06259461\n",
            "  0.08687275  0.00403512 -0.07140763 -0.06725402 -0.03292093  0.06842944\n",
            "  0.02273153  0.00901333  0.03980906 -0.07246733  0.09705942 -0.02688294\n",
            " -0.06722915  0.05773038 -0.00275059 -0.18501686  0.02890208  0.09756784\n",
            " -0.06222266 -0.01549343 -0.01180137 -0.03404186 -0.04585863 -0.15004977\n",
            " -0.03739582 -0.01950236  0.15942286 -0.02498746  0.00093224  0.02889443\n",
            " -0.0105695  -0.07240311  0.00607327 -0.04649181  0.02613795  0.09468477\n",
            "  0.0194094  -0.05279772 -0.10969678  0.05383043 -0.06404924  0.07916122\n",
            " -0.01627353 -0.04748362 -0.01586157  0.08668801 -0.01366825  0.01302282\n",
            "  0.03709022  0.03153414 -0.04886963 -0.02276847 -0.07378384  0.07193289\n",
            " -0.02752996 -0.03254885  0.04466954 -0.08435674 -0.03240877 -0.02819821\n",
            " -0.0071298  -0.00626764]  - intercept :  0.6748167283404882\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.3044966736316322\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:04,439]\u001b[0m Trial 148 finished with value: 0.08086563987413332 and parameters: {'count_threshold': 7, 'postag': False, 'voc_threshold': 1832}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.59272356 0.01046137 0.13815995 ... 0.         0.15716924 0.00565251]\n",
            " [0.53835257 0.03884623 0.68144867 ... 0.         0.15716924 0.00946286]\n",
            " [0.         0.01680503 0.0228621  ... 0.         0.02114218 0.03391506]\n",
            " ...\n",
            " [0.29353219 0.01766353 0.04775205 ... 0.00812281 0.07138472 0.00565251]\n",
            " [0.         0.06886863 0.0152414  ... 0.24949174 0.12408933 0.04950659]\n",
            " [0.21125873 0.01928422 0.12200712 ... 0.08319831 0.45620648 0.02705387]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 1.39924173e-02 -3.10072321e-02 -1.34985949e-02  5.46656372e-02\n",
            "  4.42127028e-02  3.81128307e-03  6.91279380e-02  4.31391930e-03\n",
            " -3.81720387e-02 -5.56073156e-02 -5.51962039e-02 -1.20250639e-02\n",
            " -3.80496420e-02  7.07117713e-03 -3.12442289e-02 -4.08941514e-02\n",
            " -5.49781568e-02 -1.39087592e-02 -7.46826993e-03  1.33781656e-02\n",
            " -2.15495973e-02  3.41640681e-02 -4.91680922e-03 -3.01552697e-02\n",
            " -1.02364325e-01  1.50253163e-02 -2.43008865e-02  1.87589639e-02\n",
            " -1.05659866e-02 -6.21232735e-02  3.43134193e-02 -3.29859016e-03\n",
            "  4.64041980e-03 -9.64453409e-03  1.35292633e-02 -2.77220743e-02\n",
            " -2.83372086e-02 -5.55857476e-02 -2.83748720e-02 -1.96212168e-02\n",
            " -5.24958196e-02  4.47781069e-02 -4.42313652e-02 -2.32900926e-02\n",
            " -5.26151367e-03 -2.29127650e-02 -4.34464238e-02 -1.05176295e-02\n",
            " -9.24884100e-02 -7.53870230e-02  2.08669965e-02  4.14154440e-03\n",
            " -3.20300683e-02  2.75554395e-02 -1.56057086e-02 -3.31093364e-02\n",
            " -8.05405571e-03 -6.84950296e-03 -4.63641457e-02  9.31342652e-02\n",
            " -8.50283716e-03  8.30806633e-02 -6.78477416e-03 -2.04913338e-02\n",
            " -2.47108851e-03  9.21929924e-03  2.00429601e-02  4.97393214e-02\n",
            " -1.55977602e-02  1.32755047e-02 -1.86139795e-02 -9.78157676e-03\n",
            " -5.03324407e-03  1.03316896e-02 -2.89066367e-02  4.20238364e-02\n",
            " -5.11381192e-02 -4.30578038e-02  9.05321205e-02 -3.42448260e-03\n",
            " -1.88265233e-02 -7.81985748e-02  1.60035380e-02  3.31186109e-02\n",
            "  3.61048113e-02  1.96081309e-02 -4.08273956e-02 -6.73835396e-02\n",
            " -4.94320015e-03 -1.29674773e-02  2.83924596e-02  2.52892234e-02\n",
            " -3.26036910e-02  1.03346803e-02 -5.97423438e-03  3.03142807e-02\n",
            " -5.42481570e-03  6.54970941e-03  6.06391387e-03 -3.66956900e-02\n",
            " -2.65232403e-02  3.73352092e-02  1.35638259e-02 -7.40924458e-03\n",
            " -3.58136407e-02  1.87029593e-02  3.71477733e-03 -7.36783454e-02\n",
            "  5.91060100e-02 -1.86432572e-02  6.43045407e-02  4.54979843e-02\n",
            "  1.22240613e-02  2.89528585e-02  3.64415160e-02  8.74650909e-03\n",
            "  5.14684024e-02 -3.88916618e-02 -1.56593955e-02 -8.50753284e-03\n",
            " -2.26906427e-02  2.76217720e-02 -2.39124919e-02  1.27452299e-02\n",
            " -3.36044553e-02 -6.08083347e-03  2.78152855e-02  4.70697099e-02\n",
            "  9.93201398e-03  9.93201398e-03  9.93201398e-03  6.74923710e-02\n",
            " -4.46830018e-02 -2.60474859e-04  2.52659687e-02  2.72723411e-02\n",
            " -1.05540827e-01  1.14862728e-02 -1.04206035e-03 -3.57868527e-02\n",
            " -5.99339164e-02 -4.09334325e-02  4.45591189e-02  3.05818763e-02\n",
            "  9.85053540e-03 -2.25353209e-02  1.48785229e-02  3.28793471e-02\n",
            "  9.66952872e-02 -7.05045855e-02  6.16699525e-02 -3.01788911e-02\n",
            " -4.81130637e-02  6.38233281e-02  7.76844175e-02  8.23546253e-02\n",
            " -4.26908913e-02  2.72931650e-02  5.89814623e-02 -8.35895591e-02\n",
            " -9.25082912e-03 -4.14269695e-02  5.54193536e-02 -2.44604616e-02\n",
            "  5.42471261e-02  3.68346178e-02 -3.85994137e-02  5.50009699e-02\n",
            "  2.23822370e-02 -3.64709789e-02  1.26303839e-02 -3.09825096e-03\n",
            " -5.45241289e-03  3.18345126e-04 -4.15958253e-02 -3.81519173e-02\n",
            " -4.27325318e-04  8.32450172e-02 -1.87450528e-02 -4.02684662e-02\n",
            "  7.98900127e-04  4.22631434e-02 -1.47854328e-02  5.77368123e-02\n",
            "  3.15407738e-02 -3.63645649e-02  5.56395988e-02  5.26760167e-02\n",
            " -5.79265917e-02  2.40831993e-03 -5.58237629e-02  3.54740332e-02\n",
            " -6.03601372e-02  2.36035362e-03 -2.09195609e-02  3.12320433e-02\n",
            "  4.37514031e-02  2.26588792e-02  3.44319490e-02  3.81411998e-02\n",
            " -1.78307562e-02 -1.35044396e-01  8.33234624e-03  5.63425351e-02\n",
            " -4.23549217e-02 -3.13783124e-03  1.02015499e-02  7.28020277e-02\n",
            " -7.82478727e-03  2.95031162e-02  9.67694238e-03  3.88940244e-02\n",
            "  2.62277433e-02  1.60865154e-04  3.48968330e-02 -1.17962760e-02\n",
            " -1.36913008e-02  2.42960770e-02  9.06202409e-03 -1.67897516e-02\n",
            " -7.41008672e-04 -5.91902185e-02 -2.22318732e-02 -1.58573664e-02\n",
            "  4.47490515e-02  8.00110660e-03 -4.46507405e-03 -1.32531286e-02\n",
            "  9.11218234e-03  4.80539123e-02 -2.95674091e-02 -5.95981367e-02\n",
            " -1.01043895e-01 -3.62897997e-03  9.13455989e-03  2.72993341e-02\n",
            " -1.12032544e-02  4.56243228e-02 -8.80523983e-02 -2.65128228e-03\n",
            " -3.18180069e-02 -1.40154129e-02  1.05139716e-02 -1.19798481e-02\n",
            " -1.72475000e-02 -6.81245378e-05 -3.45553356e-02 -4.39902243e-03\n",
            " -5.61821358e-03 -3.43766507e-02 -2.03399297e-03 -3.39659977e-02\n",
            " -4.16532361e-02 -8.02562938e-03 -5.19704168e-02  1.24320911e-02\n",
            "  3.23054236e-02 -1.03171330e-01  2.86069025e-02  1.38465227e-02\n",
            " -1.91099579e-02  4.06486932e-02  8.13655060e-03 -2.36907606e-02\n",
            " -9.46406122e-02  9.30625546e-03 -2.30245583e-02 -2.92611568e-02\n",
            "  5.98004845e-03 -3.58315374e-02  4.23289753e-02  4.23475592e-02\n",
            "  3.07093771e-02 -5.37044899e-03  3.35148643e-02 -1.85155198e-03\n",
            " -6.26237833e-03 -5.98905041e-02 -7.91078615e-03  1.83654876e-02\n",
            " -7.51542587e-02 -9.09149156e-02 -5.51852829e-03 -2.22108022e-02\n",
            "  3.56758094e-02 -3.83850266e-02 -5.02016648e-02  5.93084453e-02\n",
            " -5.25882023e-02 -3.35268992e-02  1.04442347e-02  2.72994495e-02\n",
            " -4.87108044e-02  1.18290769e-02  3.19162161e-03 -5.75908737e-02\n",
            " -2.34775808e-02  1.73802871e-02 -8.95307361e-03  2.44918598e-03\n",
            "  3.41656788e-02 -3.60208270e-03 -2.56349955e-02  2.44433110e-02\n",
            " -8.18643332e-02  2.24216201e-02 -1.39623481e-02  2.58826625e-02\n",
            " -1.89579694e-02  4.84216984e-02 -1.00479290e-01 -4.22162766e-02\n",
            "  5.16663037e-02 -3.59693249e-02  2.01267571e-02  8.10285734e-03\n",
            " -5.14531110e-02 -2.11994040e-02  8.86142324e-02  2.33705319e-02\n",
            "  2.01711397e-03 -5.72103442e-03  2.25770741e-04  8.32834754e-02\n",
            " -2.36332062e-03  1.13487372e-02  1.39166560e-02 -6.34057870e-02\n",
            " -5.62382998e-02 -8.79478933e-03  5.22397839e-03  2.78639018e-02\n",
            " -1.83315117e-02  2.04078858e-02  1.97258450e-03 -1.97538812e-02\n",
            " -3.85072920e-02 -1.46973677e-02 -1.07818491e-02 -3.99521717e-03\n",
            "  4.64425474e-02  2.74635428e-02 -1.60871567e-02 -9.91654991e-03\n",
            "  3.72293894e-02 -9.40564126e-03  1.21053448e-02  4.77557897e-02\n",
            "  6.34665457e-02 -5.11457422e-02  4.13020411e-02  4.49912376e-02\n",
            " -6.61963009e-03 -3.45270025e-02 -4.35431046e-02  5.46493226e-02\n",
            "  8.12597666e-02  1.16617809e-02  2.53102727e-04 -1.77947617e-02\n",
            " -4.04116621e-03 -1.21259047e-03 -5.98422865e-03  1.83404942e-03\n",
            " -5.23303886e-02  1.26951911e-02  4.06655916e-02 -4.36854749e-02\n",
            " -6.95155510e-04 -6.95155510e-04 -6.95155510e-04 -3.83866278e-02\n",
            "  3.09778453e-02  7.87136315e-04 -3.69479143e-02  2.12141700e-02\n",
            " -2.82985833e-02  4.67804132e-03 -3.71383080e-02  7.73550507e-03\n",
            " -5.45553650e-03 -1.32328017e-02  3.26647167e-02  5.03665479e-02\n",
            "  1.73770389e-02 -2.21863384e-02 -1.08690569e-02 -3.71131239e-03\n",
            " -3.38293436e-02  7.10951053e-02  5.18715240e-02  1.04570963e-02\n",
            "  2.59283362e-02  1.59956161e-02 -3.57398164e-02  3.56726216e-02\n",
            " -5.54489455e-02 -4.63993604e-02  2.59156302e-03  2.79247794e-02\n",
            " -4.06740794e-03 -1.13827900e-02  6.02854874e-02 -2.04135236e-02\n",
            "  1.31357485e-03  6.43138105e-03 -2.69445668e-02 -2.67677115e-02\n",
            "  1.34921855e-02 -1.60384138e-02 -1.15718276e-01 -3.96173625e-03\n",
            "  2.18685861e-02 -3.52513507e-02  3.72307545e-03  4.78381040e-03\n",
            " -3.30647448e-03  5.89384758e-02  1.98388785e-02 -1.42496925e-02\n",
            "  9.58038768e-03  2.47157896e-02  2.04139056e-02  9.20245366e-03\n",
            "  2.63608434e-02 -1.11339370e-02  7.14571650e-02  7.37161809e-03\n",
            " -3.36152236e-02  7.41794764e-03 -3.48191072e-02  4.58516560e-02\n",
            " -1.31685558e-01 -7.34938087e-03  5.16483480e-02  4.29396741e-02\n",
            " -1.26119460e-02  6.32714911e-02  8.67487280e-03  6.24039379e-02\n",
            "  2.26519816e-02 -8.56741685e-02  8.14473328e-03  2.74431904e-02\n",
            " -5.28007156e-02 -4.79366515e-02 -1.02390359e-02  1.78709691e-02\n",
            " -4.81038201e-02 -8.25645083e-02  3.00747609e-02 -2.36003542e-02\n",
            "  1.59308843e-02 -5.24240091e-03  2.86290344e-02  6.62999112e-02\n",
            " -2.51182199e-02 -6.45665230e-03 -7.47154156e-03 -1.29677180e-02\n",
            " -9.87756851e-03 -7.16227134e-03  1.46250363e-02 -3.86384333e-02\n",
            "  3.14795222e-02  8.03393402e-02 -3.18233226e-02 -3.03194851e-03\n",
            "  6.87439515e-03  3.20102837e-02 -3.02700398e-02 -1.31084751e-02\n",
            " -4.59540327e-02 -4.59515394e-03  1.23635882e-02 -4.40332138e-03\n",
            " -8.72206949e-03 -1.27165787e-02 -1.49330862e-02  2.82449424e-03]  - intercept :  0.6575797929483492\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.08086563987413332\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:05,001]\u001b[0m Trial 149 finished with value: -0.1372425526554277 and parameters: {'count_threshold': 8, 'postag': False, 'voc_threshold': 1620}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.21860713 0.21053071 0.02140586 ... 0.         0.65068081 0.01411432]\n",
            " [0.43154276 0.55711999 0.20050937 ... 0.14130591 0.52249169 0.00467346]\n",
            " [0.0532392  0.08816366 0.0367667  ... 0.10362433 0.0867213  0.04455281]\n",
            " ...\n",
            " [0.44729176 0.47534369 0.04387712 ... 0.         0.14592552 0.01587861]\n",
            " [0.0360892  0.05766591 0.05424912 ... 0.20724867 0.22591713 0.03927648]\n",
            " [0.         0.02317853 0.00813678 ... 0.14130591 0.0651925  0.04343033]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-3.71977608e-02 -5.36978734e-03  3.04828039e-02 -1.39910335e-02\n",
            "  4.16180070e-02 -4.37028346e-02  2.47298906e-02 -3.38277996e-02\n",
            " -3.50826437e-02  1.08460378e-05 -9.23346999e-04  7.09639315e-02\n",
            " -3.55006852e-02  5.93620115e-02  7.35518264e-03  1.98861713e-02\n",
            "  8.99144879e-02 -3.53908318e-02 -3.15022506e-02  1.20056635e-02\n",
            " -2.24471880e-02 -2.15815506e-02 -3.84745998e-03  1.64105987e-02\n",
            " -6.80895593e-02  4.04546091e-03 -1.40497669e-02 -1.56639752e-02\n",
            " -3.81534028e-02  2.17086986e-02  9.03279015e-04  2.39153632e-02\n",
            " -3.21184924e-02 -1.87166092e-02 -2.92116888e-02  1.14496189e-02\n",
            " -5.16646338e-03 -4.31135824e-03  9.81772464e-02 -4.58547641e-02\n",
            " -6.70305818e-02  2.38284242e-03 -7.70438526e-03  3.75624941e-02\n",
            "  1.67791105e-03 -2.84682311e-02 -8.53691008e-03 -1.23687269e-02\n",
            " -5.46980254e-02 -6.77258796e-02 -6.21577905e-03  4.58109872e-02\n",
            "  8.15870666e-02  1.16348111e-02  8.18406850e-02 -3.57742956e-02\n",
            " -7.76179469e-02  4.94346325e-02  1.58742191e-02  1.42958081e-03\n",
            "  4.01138255e-03  2.60867238e-02  1.16285588e-02  4.12763830e-02\n",
            "  3.18553777e-02  1.37608862e-02  1.50776269e-02  2.19018867e-02\n",
            " -5.41571539e-03  8.74954710e-04  5.25053551e-02 -1.67854929e-02\n",
            "  1.26747769e-01  3.69245856e-02  2.62172749e-03  4.68599936e-02\n",
            " -2.41867302e-02 -5.55724680e-02 -1.26792140e-02 -3.89771602e-04\n",
            " -3.20995542e-02  3.27637759e-02  4.92522432e-02  3.64047059e-02\n",
            "  3.38187589e-02 -5.08028613e-02 -9.93050458e-03 -1.19043075e-02\n",
            " -6.61798070e-04  7.02053490e-03  1.12252790e-01  3.23035307e-02\n",
            "  2.60274063e-01 -2.01986302e-02  5.66443369e-02 -1.13494232e-02\n",
            " -2.03516365e-02  1.48390199e-03 -3.94894506e-02 -8.32870810e-02\n",
            " -3.81967682e-02 -4.19185028e-02  5.27616652e-02 -3.93086711e-02\n",
            " -7.22009660e-02  5.97108783e-02 -1.79589714e-02 -2.50898279e-02\n",
            "  4.69985029e-02  4.72202830e-02 -3.44623508e-02 -4.40619356e-02\n",
            "  7.20166006e-02  1.27045459e-03 -1.06407042e-02  9.77589109e-02\n",
            " -7.16447296e-03 -4.20097769e-02  9.06537099e-02  5.74651293e-02\n",
            "  1.36579539e-02  2.53736039e-02  1.90537343e-02 -4.80207852e-02\n",
            "  8.34331054e-02  3.02025300e-05  8.19842331e-02  8.89537795e-03\n",
            " -3.63858538e-02  5.27024510e-02 -4.48666896e-02  2.34573693e-02\n",
            "  1.84293782e-01  3.09528921e-02  2.53548804e-01  3.04903487e-02\n",
            "  1.39736373e-01  2.57721414e-02  4.75466026e-02  1.09054422e-01\n",
            " -1.92757140e-02  1.67779539e-01 -1.61773492e-02 -1.03762918e-01\n",
            " -1.65341824e-02 -2.79514739e-02 -6.90229277e-02  1.04981401e-01\n",
            "  2.00249687e-01 -7.70215355e-02 -4.84010620e-02  1.00205034e-02\n",
            "  2.24407384e-02 -9.89810747e-02  5.44384688e-02  2.19037048e-01\n",
            "  1.43950183e-01 -2.93296063e-02  8.53310374e-02  5.75334975e-04\n",
            "  4.56281062e-02  2.06180940e-01 -5.44846532e-02 -3.55365456e-02\n",
            " -5.12478518e-02 -8.77469014e-03 -4.14469987e-02  4.41516641e-02\n",
            "  1.70587899e-01  8.30649843e-02  6.24567235e-02 -2.98637528e-02\n",
            "  2.48438758e-02 -6.02234948e-02 -1.20810874e-01  3.45508792e-02\n",
            "  2.05884588e-02  1.28726046e-01 -1.09844326e-01 -7.85616197e-02\n",
            "  3.01730179e-02 -7.72175541e-03  1.14377448e-01  5.85943744e-02\n",
            " -4.03495620e-02 -2.47648338e-02 -1.79131796e-02 -1.77045385e-02\n",
            "  2.44150153e-01  1.81996692e-01 -8.41369329e-02 -1.85144599e-02\n",
            "  8.72720543e-02 -3.34193243e-02  6.01465431e-03  2.28700026e-02\n",
            " -2.05261356e-02 -1.65068475e-02  5.47994151e-02  1.56538118e-02\n",
            " -6.03495562e-02 -3.01992265e-02 -2.57907393e-02 -3.23319696e-03\n",
            "  1.74969749e-02  1.37659988e-02  1.93338871e-02 -3.21854062e-02\n",
            " -3.83182659e-02  1.76212602e-02  4.62851713e-02  2.95853923e-02\n",
            " -4.86610896e-02  3.48833293e-02  4.50040763e-02 -2.96738326e-02\n",
            " -2.47964425e-02 -3.90020705e-02  4.74340102e-02 -3.64786821e-02\n",
            " -9.00310209e-03 -2.98118318e-02 -7.24423441e-02 -8.80156142e-03\n",
            " -5.41256433e-02  1.64050725e-02 -3.88111255e-02  5.13984897e-03\n",
            "  2.85115731e-02 -5.57328269e-02  4.28470205e-02 -4.82112983e-02\n",
            " -2.03712873e-04  5.74661459e-03  4.26527327e-02  3.61672127e-04\n",
            " -4.01701831e-02  1.78384217e-02 -3.53702527e-02 -5.51841336e-02\n",
            " -5.75827057e-03 -2.20832090e-02  7.98984392e-03  1.63626837e-02\n",
            " -1.15321971e-01 -4.12526639e-02 -2.07214134e-02 -6.44129092e-03\n",
            "  2.17798878e-02 -1.54784789e-02  9.38787829e-02  2.64470679e-02\n",
            " -1.00529118e-01 -8.37865905e-02  2.61877303e-02  3.46891071e-03\n",
            "  3.90247585e-02 -3.46682052e-03 -1.67353895e-02  2.39156202e-02\n",
            " -1.87101087e-02 -3.42858967e-02 -5.28820480e-03  1.56994929e-04\n",
            " -1.71676282e-03 -6.84251642e-04 -6.32458623e-03  3.66755257e-03\n",
            "  3.35015167e-02 -4.26909088e-02  1.64803793e-01  7.78943619e-02\n",
            " -3.24210936e-02  4.53548380e-02 -3.23338886e-03  6.82575995e-03\n",
            " -5.02685642e-02  6.00973977e-02  6.82674137e-03  2.85761723e-02\n",
            "  3.09166083e-02  4.55926662e-02  1.00340769e-01  4.86003691e-03\n",
            " -4.13743436e-03 -3.58528350e-03  5.30306428e-02 -4.08816924e-02\n",
            " -2.33335342e-02 -1.08882181e-02 -3.11710756e-02  2.92401205e-02\n",
            " -5.62862786e-02 -2.82539388e-03 -1.27925078e-02  3.14710099e-02\n",
            " -1.18459057e-02 -5.76487310e-03 -1.25595183e-02 -3.84800594e-02\n",
            "  5.70733094e-02  5.06987386e-03  6.31584042e-02  5.18456970e-02\n",
            " -4.43511989e-03  4.31469624e-02 -6.59331956e-02  2.28749259e-02\n",
            "  8.29731671e-02  4.08943258e-03 -2.04574067e-02  1.97710312e-02\n",
            " -3.19567152e-04  1.13151227e-01  6.47128965e-02  5.26842988e-02\n",
            " -5.10380194e-02  3.34857683e-02  1.09077692e-01  1.42596637e-02\n",
            " -5.14588606e-02 -1.97080788e-02  3.06615933e-02 -3.17001032e-02\n",
            " -9.15195456e-02  2.04080809e-02 -3.62313452e-02  3.53525231e-02\n",
            "  1.78383001e-02  7.22516662e-02  8.68803707e-02 -2.38645731e-03\n",
            " -4.47597813e-02  1.75974149e-02  2.19782784e-02  6.88665005e-02\n",
            " -3.07980185e-02  1.05569720e-02  1.32928562e-02 -2.87347492e-02\n",
            " -1.84216543e-03  1.04673111e-01 -1.11950564e-02  2.71602107e-02\n",
            " -7.27628415e-03  8.40602602e-02 -4.82383964e-02  3.42938584e-02\n",
            "  1.25948408e-01 -3.50384805e-02 -5.18594386e-02  1.31654747e-02\n",
            " -3.43706738e-02 -1.99407327e-02 -3.32278588e-03  5.81266160e-03\n",
            "  5.68734430e-02  9.79030495e-02  4.87931148e-02 -6.95365641e-05\n",
            " -9.24434236e-02  4.75524524e-02  2.03781081e-02  3.78385323e-02\n",
            "  1.30127939e-03 -4.15639716e-02  1.83605209e-02 -9.58889298e-02\n",
            "  2.17464869e-02  3.10954438e-02 -5.15880496e-02  4.31311024e-02\n",
            "  1.30069298e-01 -1.64283115e-02 -3.22026725e-04  4.25764639e-02\n",
            "  7.59096735e-02 -4.10133371e-03  7.44379905e-02  5.81306850e-03\n",
            " -5.40588468e-02 -1.14655910e-02  1.09496381e-02  8.85159134e-02\n",
            "  1.13685196e-01  7.76775467e-02  2.75295680e-02  4.67657534e-03\n",
            " -7.70304978e-02  8.58420604e-03 -3.13796206e-02  1.50491615e-02]  - intercept :  -0.004291276302854419\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.1372425526554277\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:05,580]\u001b[0m Trial 150 finished with value: -0.003787657654180661 and parameters: {'count_threshold': 9, 'postag': False, 'voc_threshold': 1248}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.         0.         0.         ... 0.00714863 0.         0.03229286]\n",
            " [0.05805925 0.07114593 0.13642254 ... 0.00986928 0.         0.04747478]\n",
            " [0.30188469 0.0255409  0.55164598 ... 0.02762827 0.         0.        ]\n",
            " ...\n",
            " [0.2132796  0.04188908 0.2689252  ... 0.4804325  0.         0.01825493]\n",
            " [0.         0.         0.06085181 ... 0.00612739 0.         0.03459478]\n",
            " [0.         0.05083521 0.03122094 ... 0.00595719 0.         0.03497843]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-1.99081039e-02  1.63649491e-02 -4.80821366e-02 -4.78217921e-03\n",
            "  7.00841278e-03  1.47772653e-03 -2.79212267e-02 -3.30292648e-02\n",
            "  3.18393236e-02  1.51653859e-02 -2.47468758e-02  6.67810593e-02\n",
            " -1.33981726e-01 -5.21455093e-02 -3.72026259e-02 -3.27447698e-02\n",
            "  4.67199569e-02 -4.60261007e-02  1.60323489e-01  1.33902419e-02\n",
            " -3.35439920e-02  1.65809604e-02 -1.22583941e-02  3.75394651e-03\n",
            "  3.31313236e-02  2.45356812e-02 -1.67956101e-02 -1.34037714e-02\n",
            "  4.70388635e-02  3.14756237e-02  4.90512739e-02 -2.12141370e-01\n",
            " -1.50467886e-02 -1.49065561e-02 -1.53399060e-02 -9.80746004e-02\n",
            " -1.81861925e-02 -3.87728320e-02  2.74339688e-02 -3.59308908e-02\n",
            " -5.25261322e-02 -7.44495001e-02  1.18758116e-01 -7.13027276e-02\n",
            " -4.17483713e-02 -8.66075166e-02  7.48086444e-02 -1.66403683e-01\n",
            " -5.77297159e-03 -1.14483304e-02  1.07319023e-01  6.61890226e-02\n",
            " -4.55126736e-03 -1.30273360e-01  5.11504331e-02  9.26216809e-02\n",
            " -6.10170268e-02  1.69439833e-02  1.12606261e-02 -6.77619851e-03\n",
            "  1.20994420e-01 -1.30145939e-01  6.41844513e-03 -7.47931936e-03\n",
            "  2.11168601e-02 -1.15441735e-01 -5.17514240e-02 -1.87100797e-01\n",
            " -1.53345927e-01 -1.38959005e-01  3.46167662e-03 -3.26002666e-02\n",
            "  5.52746883e-02  7.27367707e-02  5.67105961e-02 -4.63299749e-02\n",
            " -1.80546617e-01 -3.28011670e-02 -5.02387190e-02 -2.04063509e-01\n",
            " -1.26166279e-01 -1.37749713e-02 -7.65285667e-02 -3.04087315e-02\n",
            "  5.42803037e-02  1.03783045e-02 -1.75517833e-02 -3.60186295e-02\n",
            "  1.03240939e-01  6.67560617e-04 -1.47501236e-02 -2.59076103e-02\n",
            " -4.63006041e-02  8.04966332e-02  2.73028947e-02 -2.29646682e-03\n",
            " -1.28758551e-01 -8.51026118e-02 -1.81078079e-02 -3.01040894e-02\n",
            "  1.76643631e-02  3.24853252e-02  2.92523908e-02 -1.32253401e-01\n",
            "  4.69919378e-02 -1.15943177e-02  6.44163937e-03 -8.69797940e-03\n",
            "  2.66190634e-02 -1.18552971e-01  8.08664740e-02  1.44581587e-01\n",
            "  4.88736758e-02 -1.13127420e-02 -9.81496369e-02  3.26297513e-02\n",
            " -9.17739044e-02  7.46772969e-03  7.13087130e-02  2.77695420e-02\n",
            "  1.35259410e-03  4.18329541e-02 -1.41523581e-01  6.96845948e-02\n",
            "  7.01892319e-03 -3.00167936e-02  6.46608838e-02  1.78782320e-02\n",
            "  5.81972890e-02 -1.27616958e-02 -5.27623233e-02 -2.82955419e-02\n",
            "  4.69672782e-05  1.34751235e-01 -7.65299657e-02 -4.38453068e-02\n",
            " -3.14032358e-02  1.37617435e-02  1.38187346e-02  1.71152097e-01\n",
            "  6.04863946e-02 -5.39871787e-02  1.33170366e-02 -5.89897189e-03\n",
            "  2.14806527e-02  3.68421773e-02 -1.69521859e-03  4.52084753e-02\n",
            " -1.19691397e-02 -3.22697588e-03 -1.05543663e-02 -5.44808991e-02\n",
            "  5.24522938e-02 -2.23136260e-02 -3.72731681e-02 -2.77940008e-02\n",
            "  1.06304853e-02  2.08579694e-02  1.50253628e-02 -5.67069329e-02\n",
            " -1.27217985e-02 -7.60086418e-03 -3.61240617e-03 -4.17326093e-02\n",
            " -8.46032520e-02  3.05878397e-02  4.30525939e-02 -5.72617133e-02\n",
            " -9.00216545e-04 -1.22398480e-01  9.85646047e-03  4.53590400e-02\n",
            " -1.19348956e-02  4.61813512e-02  1.74540552e-02  8.43005200e-03\n",
            "  8.34270726e-02 -2.49412954e-03 -3.09088144e-02 -7.53714700e-03\n",
            " -1.19733405e-02 -1.92889556e-02 -2.41246824e-02 -4.35524965e-02\n",
            "  6.00435468e-02 -4.82848250e-02 -5.52059135e-02  1.14343414e-01\n",
            " -7.13817913e-03  1.03427451e-01 -1.67760153e-01  8.41220145e-02\n",
            " -7.91722212e-02 -4.01395987e-02 -9.10217589e-02  9.59491302e-02\n",
            " -3.77925969e-03  6.56944001e-03 -8.08344771e-02  1.16361067e-01\n",
            "  7.68478848e-02 -3.30914114e-02 -2.88855728e-02  1.10389236e-01\n",
            " -3.43467243e-02 -8.13630575e-02  8.13774844e-04  5.22232845e-02\n",
            " -2.06306220e-02  8.52649788e-02  7.17164907e-04  1.00073275e-01\n",
            " -7.97301285e-03 -1.88213983e-03 -4.40557477e-02 -4.85176028e-02\n",
            "  2.87973285e-02 -1.01428398e-01  8.63664066e-02  2.46891173e-02\n",
            " -8.49377089e-02  1.03310346e-02 -1.13117997e-01 -4.79940290e-02\n",
            "  1.61339317e-02 -1.20578585e-01 -9.79207952e-02  6.06039333e-02\n",
            " -3.64051923e-02  3.84865931e-02 -1.42982259e-02 -1.54736175e-01\n",
            " -6.67321174e-02  8.19454638e-02 -5.76622626e-02 -6.07564689e-02\n",
            "  5.91004567e-02  1.09359707e-02 -9.90900994e-02  1.39351154e-03\n",
            "  1.44604556e-02 -2.61647383e-02 -8.00811972e-03  2.78923324e-01\n",
            "  2.02100779e-01 -1.09241958e-01 -1.80289732e-02 -5.14166678e-02\n",
            "  9.52371209e-02  2.23117676e-02 -2.21170434e-02  1.62044734e-02\n",
            " -6.55517199e-02  6.88504640e-02  1.31977845e-01 -1.42987237e-03\n",
            "  6.10000746e-02  2.50145748e-02  6.10655872e-02  9.08833389e-02\n",
            "  3.38580411e-03  6.05008539e-02 -4.49584044e-02 -2.45428798e-02\n",
            "  9.33548616e-02  1.13898890e-01  1.23502961e-02  1.03925814e-01\n",
            "  5.56659831e-02 -3.09786311e-02 -6.61992712e-02  3.60032653e-02\n",
            "  7.20059227e-02  1.91026090e-01 -3.21017533e-03  2.37549373e-02\n",
            "  6.25654465e-02  2.59026871e-02 -5.19543517e-02 -5.73730274e-02\n",
            " -2.35914994e-02  3.02062780e-03  1.43708106e-01  3.26460286e-02\n",
            " -1.73714487e-02 -1.15218503e-02 -6.54571533e-02  3.39476353e-02\n",
            " -1.02138539e-01  8.60980754e-02  2.71321657e-02  2.06449729e-02\n",
            " -7.48433119e-02 -5.63967523e-02 -3.71284468e-02  4.68838280e-02\n",
            "  4.39364008e-02  6.80691095e-03]  - intercept :  0.7756150027533528\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.003787657654180661\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:06,128]\u001b[0m Trial 151 finished with value: 0.0012650710620235558 and parameters: {'count_threshold': 10, 'postag': False, 'voc_threshold': 7270}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.04870694 0.09283113 0.02226818 ... 0.         0.         0.02512363]\n",
            " [0.00868578 0.03110413 0.04885119 ... 0.16436843 0.         0.04505771]\n",
            " [0.46655744 0.71780525 0.07764027 ... 0.28855426 0.         0.01004945]\n",
            " ...\n",
            " [0.11570401 0.08458551 0.04497308 ... 0.14932638 0.         0.03639706]\n",
            " [0.03099532 0.06945548 0.02657959 ... 0.         0.         0.04320029]\n",
            " [0.21706703 0.30098066 0.08706212 ... 0.42565299 0.         0.01416864]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-2.98573006e-02 -1.74276458e-02  3.21878453e-02 -2.90350891e-02\n",
            " -9.95965872e-03 -4.75937306e-03  2.54246274e-03 -7.31308521e-04\n",
            "  1.56023191e-02 -3.87557858e-03  2.29176914e-02 -3.48591568e-02\n",
            "  1.28291087e-02 -3.95215796e-02 -7.96571907e-02 -8.37729106e-02\n",
            " -5.72921772e-03  3.56243186e-03 -7.77722657e-02  2.45178190e-02\n",
            " -1.87927200e-02  5.41395740e-05  5.24362598e-02 -1.36055819e-02\n",
            " -6.59729295e-02 -2.10984456e-02  3.80761814e-02  9.31174865e-03\n",
            "  2.00331567e-02 -1.21704174e-02  3.03564716e-04 -8.71597030e-02\n",
            " -4.59334994e-02 -9.83889028e-02 -8.20090990e-02 -2.25064842e-02\n",
            "  3.00036807e-02  7.10654814e-02 -1.51497532e-02 -5.01545897e-03\n",
            " -4.85267454e-02  7.53086591e-02  1.28242334e-02  5.20949957e-02\n",
            "  2.40480841e-02  5.42289652e-02 -8.64561741e-03  1.78295686e-02\n",
            "  8.05209470e-03 -6.72513731e-02 -1.36831118e-01  1.00418482e-02\n",
            "  2.03372476e-02  4.26857854e-02 -6.62733724e-02  2.60677947e-03\n",
            " -6.74749404e-02 -7.86012934e-02  1.40084233e-02 -6.06061736e-02\n",
            "  4.89211895e-02 -2.05230035e-02 -5.64014078e-02  1.17781731e-02\n",
            "  5.03460021e-02 -5.16014894e-02  4.39536910e-02  1.90280725e-02\n",
            "  9.31191072e-02  1.25813707e-02 -5.76022051e-02  2.77641296e-04\n",
            "  3.48562325e-02 -5.15551615e-03 -2.32705885e-02  8.86536582e-02\n",
            "  2.10114585e-02 -8.96057345e-02  9.39834766e-02 -2.42646982e-02\n",
            " -9.23212145e-02 -3.40286529e-02  3.70501248e-02 -6.10861626e-02\n",
            " -5.82036128e-02  8.05834677e-03 -1.25848816e-02 -1.01178946e-01\n",
            " -1.00997386e-02  9.53509963e-02  4.68061990e-02  8.58069787e-02\n",
            " -3.82383000e-02  7.15936103e-02  3.88470524e-02  6.04603249e-02\n",
            "  2.89036535e-03  4.94274393e-02  6.49493735e-02 -2.17705178e-02\n",
            " -1.41514663e-01 -1.89122727e-03  4.44204153e-02 -4.37457071e-02\n",
            "  4.79006353e-04 -5.49502586e-02 -4.88604452e-02  1.75898597e-02\n",
            "  4.49028138e-02 -4.10188215e-02  6.17913583e-02  5.60362349e-03\n",
            " -8.59127409e-02  6.21989875e-02  3.16917962e-02 -1.84567397e-02\n",
            " -1.05547805e-02  1.23579797e-02  1.42752125e-02 -4.24161096e-02\n",
            "  1.44027376e-01 -6.35906117e-02 -7.36160428e-03 -1.45403439e-01\n",
            " -1.20198926e-01  1.06479838e-01 -1.29862017e-02 -4.29600156e-02\n",
            "  1.14975729e-01 -1.43232908e-01  8.61401090e-03  1.00759935e-02\n",
            " -4.15200889e-03 -1.36310885e-03  9.58053554e-02  1.57748938e-02\n",
            " -6.06737531e-02  7.13459182e-02 -3.87897975e-02 -5.95934356e-02\n",
            " -6.27720237e-03  5.84023666e-03  4.24920327e-03  3.23358149e-02\n",
            " -3.96313557e-02  4.12796044e-03  4.08180398e-03 -4.09483311e-02\n",
            "  5.50889967e-02  2.14218783e-02 -4.54969602e-02  9.36382003e-02\n",
            "  9.92074867e-03 -1.87101812e-02 -2.58909776e-02 -3.12622220e-02\n",
            " -1.42238978e-01  2.34929011e-02  1.25197926e-02  2.45825728e-02\n",
            "  3.55889117e-02  4.97598689e-02 -2.94915596e-02 -1.18285350e-01\n",
            "  3.53356010e-02 -1.07478266e-01 -1.24496028e-01  1.32271964e-01\n",
            " -2.36063598e-02  9.25249275e-02 -1.72467782e-01 -4.37064860e-02\n",
            "  5.76157551e-02  1.07836220e-01 -1.20673263e-02  2.54756785e-02\n",
            "  1.62428314e-02  3.37660015e-02  1.75446584e-02  2.96220876e-02\n",
            "  1.24745963e-01 -3.19708499e-02  5.62775329e-02 -1.64048477e-02\n",
            "  8.45151689e-02  3.78151736e-02  3.14400712e-02  3.83365589e-02\n",
            " -3.56710086e-03 -4.74729110e-02  3.18310674e-02 -3.39738524e-02\n",
            "  2.01486319e-02 -1.28896863e-01 -4.23663058e-02 -5.59954942e-02\n",
            "  1.62146967e-03 -8.25790061e-02 -4.10410026e-03 -1.16763422e-02\n",
            " -1.33197950e-02  3.44496247e-02 -8.36884694e-02  2.98786705e-02\n",
            " -8.82246058e-02 -8.30712051e-02  2.41236815e-02  3.87015956e-02\n",
            "  2.70787912e-03  1.14336485e-01 -7.35846821e-02  1.04614701e-01\n",
            " -4.30032140e-02  5.93359239e-02  1.73021480e-02  4.56464840e-03\n",
            "  1.82239946e-02  1.02535933e-01 -1.26089433e-01  8.10242688e-02\n",
            " -1.50934342e-02  7.66746538e-03 -2.10890258e-02  5.21424350e-02\n",
            "  2.41119985e-02  6.12726193e-02 -6.52942204e-02 -3.02240961e-02\n",
            "  5.43251016e-02 -1.27572778e-03 -6.12266238e-02  1.47567387e-02\n",
            "  6.18045111e-02  5.33416743e-02  1.37127587e-01  1.71939595e-02\n",
            "  7.59105395e-02  4.89108810e-02  2.62226235e-02 -1.04692166e-02\n",
            "  1.52803504e-01  6.37613862e-02 -2.08126900e-01 -1.63519777e-02\n",
            " -6.97889299e-02 -4.48490175e-02 -8.68178562e-03  5.90930815e-02\n",
            "  1.28822663e-02  3.30992139e-03  8.41572212e-02  1.01966516e-01\n",
            "  8.53878121e-02 -1.78216844e-01  1.24022427e-01  2.17011564e-01\n",
            "  8.31841202e-02 -8.76233494e-02  1.51916085e-01 -1.57278346e-02\n",
            " -1.13158581e-01  1.13070168e-01  1.33557468e-01  1.65989095e-01\n",
            "  1.30733024e-01  3.55012533e-02  1.23433645e-01 -4.43916580e-02\n",
            " -1.12793683e-01  1.02935046e-01  3.16482341e-02 -1.87365019e-03\n",
            " -1.05025023e-01  8.69795467e-03  1.05847948e-01  1.12873059e-01\n",
            " -8.32062084e-02 -1.41962937e-02 -3.29770524e-02  8.71319464e-02\n",
            " -1.55982101e-02  2.97089798e-04]  - intercept :  0.504964472836635\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.0012650710620235558\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:06,686]\u001b[0m Trial 152 finished with value: 0.17229701342546322 and parameters: {'count_threshold': 4, 'postag': True, 'voc_threshold': 1014}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.39160848 0.71466322 0.19369407 ... 0.17031768 0.04854022 0.00354442]\n",
            " [0.10604846 0.17004058 0.15497311 ... 0.1487882  0.14473075 0.04159693]\n",
            " [0.75293486 0.97731205 0.10496745 ... 0.01969434 0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.23472582 ... 0.35715987 0.25039877 0.08091698]\n",
            " [0.25128653 0.35449025 0.06526163 ... 0.         0.         0.        ]\n",
            " [0.36694575 0.63790318 0.05205479 ... 0.08951237 0.07894405 0.01601118]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 0.00670473  0.01823861  0.00329553 ... -0.00216828  0.00132876\n",
            " -0.00011302]  - intercept :  0.6625125935836762\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.17229701342546322\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:07,286]\u001b[0m Trial 153 finished with value: 0.05462979310331442 and parameters: {'count_threshold': 5, 'postag': False, 'voc_threshold': 9792}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.         0.         0.02452306 ... 0.29057735 0.19746297 0.10932917]\n",
            " [0.         0.00858641 0.01818262 ... 0.         0.06842166 0.04572251]\n",
            " [0.04268417 0.1649617  0.02342846 ... 0.55292974 0.05985484 0.06825191]\n",
            " ...\n",
            " [0.36048503 0.47902007 0.04768399 ... 0.03892187 0.0642914  0.03729488]\n",
            " [0.16988913 0.19214847 0.070683   ... 0.05543517 0.04044578 0.03398013]\n",
            " [0.1081399  0.09964919 0.12390391 ... 0.0221258  0.01995161 0.04223863]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 4.69795803e-03  1.14747917e-02  3.05949830e-03 -3.28298946e-03\n",
            "  5.21084465e-03 -5.91519714e-03  5.07918342e-03  2.32388111e-02\n",
            " -5.31334175e-03 -5.45614513e-03  9.59305747e-03  1.56911899e-03\n",
            "  2.13337118e-02  8.51026959e-03  1.43943101e-02  6.32226249e-03\n",
            "  1.03190529e-02  3.01162995e-02  5.66485782e-03  1.60851107e-02\n",
            " -8.31635676e-03  1.83746563e-03  6.29981514e-03  1.14829861e-03\n",
            " -2.94923930e-03  5.82841423e-03 -2.66711418e-03  7.50081846e-03\n",
            "  1.40818620e-02  1.94680826e-02 -1.57812330e-03 -8.41620780e-03\n",
            "  1.51086978e-02 -4.21980466e-03  7.08259181e-03  1.41099382e-02\n",
            " -3.96536137e-03  9.46134877e-04  1.32870754e-03  2.51327532e-02\n",
            "  3.61997601e-03  5.71552981e-03  7.85430722e-03 -3.33695938e-03\n",
            "  2.24263275e-02  8.70085859e-03 -6.11991744e-03  3.23659888e-02\n",
            "  4.97703686e-03  4.97131346e-02 -5.53603918e-03 -1.22594235e-02\n",
            "  2.15965910e-02  1.34598768e-02 -8.53272860e-03  3.98555459e-03\n",
            "  1.38448277e-02  4.82813873e-03 -1.39685268e-02 -1.13684272e-02\n",
            "  2.64880694e-03  1.01130790e-02  1.41561095e-02  1.22493307e-02\n",
            " -3.55941541e-03 -1.30280074e-02 -7.58617074e-03 -3.06436099e-02\n",
            "  9.99413746e-03  8.87083168e-03 -8.44410742e-03  1.33784194e-02\n",
            " -1.10373559e-03 -2.88030133e-02 -1.85606329e-02  5.78877459e-04\n",
            " -2.29693598e-04 -7.51708000e-03 -1.18789285e-03  2.49407228e-03\n",
            "  3.76498039e-03  1.19650018e-02  5.40277573e-03  6.68864945e-03\n",
            " -2.43287462e-02  1.58615118e-02 -4.26568891e-03  8.68180600e-03\n",
            "  1.97164370e-02 -2.73802054e-03 -9.21663955e-03 -2.21897957e-02\n",
            "  2.50045055e-02  8.28959603e-03  7.34114057e-03 -1.01864122e-02\n",
            "  8.29425999e-04 -5.59131708e-03 -7.30730910e-03  2.74251784e-03\n",
            " -2.87667526e-03  2.65254058e-03  3.48075461e-02 -4.25808068e-03\n",
            " -6.25047121e-03 -1.10420042e-02 -1.93693232e-03  6.69615091e-03\n",
            "  8.38029993e-03 -1.84980599e-02  3.54277374e-03  6.20030977e-05\n",
            " -2.30149175e-02  1.54493243e-02 -8.22879444e-03  3.41997970e-02\n",
            "  6.51657593e-03  1.92567361e-02  3.92480454e-02  1.05169554e-02\n",
            "  7.07443177e-03 -7.54794029e-03  1.61816502e-02  1.47960132e-02\n",
            "  1.90380679e-02 -1.41061366e-02 -9.35974214e-03  8.03010180e-03\n",
            "  4.88776939e-03 -9.10649085e-03 -3.76701375e-06 -2.82267448e-02\n",
            "  1.33020986e-02  1.79710265e-02 -1.62100483e-02  9.71735039e-03\n",
            " -2.91620522e-02  4.44103964e-03  2.07812896e-03 -3.28680055e-02\n",
            "  1.84333926e-02 -1.04004203e-02 -2.62679211e-03  1.02848702e-02\n",
            " -5.43697490e-03  8.16337640e-03  4.59918930e-04  1.09328479e-02\n",
            " -1.50171694e-02 -5.21252966e-03  7.81593822e-03 -2.18488087e-02\n",
            " -9.72658318e-03 -9.27175545e-04  9.12483786e-03 -1.16190394e-02\n",
            "  7.58209633e-05  7.82072577e-03  8.77843262e-03  8.11658957e-04\n",
            "  2.70397742e-02 -8.09045072e-03 -3.00926071e-02 -3.18800695e-03\n",
            "  1.48027093e-02 -1.05502256e-02 -9.85155534e-03 -1.40284747e-02\n",
            "  2.94115264e-03  2.10379585e-02  5.14978708e-03  7.70370469e-03\n",
            " -9.53030072e-03  4.19991775e-03  3.50522325e-02  1.07279772e-02\n",
            " -1.41533598e-02  6.99172941e-03 -8.78729937e-03  3.64037851e-02\n",
            "  3.56789556e-03  1.24834399e-03 -1.34093949e-02  1.52112476e-02\n",
            "  1.38643887e-02  1.68048505e-03  1.43790128e-02 -4.24063096e-02\n",
            "  8.57234419e-03 -2.89418488e-02 -1.52332540e-02 -1.10124396e-02\n",
            "  2.19202816e-02  1.24804716e-03 -3.25004068e-03  1.27295482e-02\n",
            " -8.79889226e-03 -1.19420368e-02  5.37116586e-02 -1.94659701e-02\n",
            "  7.71908503e-03  8.91422947e-03 -5.89209320e-03  3.73254627e-02\n",
            "  5.09167705e-03 -1.55340434e-02 -1.05282374e-03  2.91095218e-03\n",
            " -3.03778852e-02 -5.59485742e-03  1.29497573e-02  9.96267152e-03\n",
            " -1.17880506e-02 -1.81484703e-02 -2.40363583e-04  1.49465083e-02\n",
            " -3.00820404e-02  1.32632078e-03 -1.04285739e-03 -1.07715437e-02\n",
            " -2.45216195e-02 -4.30295854e-04  4.60979680e-03 -2.08289825e-02\n",
            "  1.23781809e-02  1.75334571e-03 -2.16861415e-03  1.43175358e-02\n",
            "  9.92517275e-03 -1.92266029e-02 -3.67201036e-02  1.71348665e-02\n",
            "  5.11286999e-03  8.51400815e-03  1.55968642e-02  5.63738027e-02\n",
            " -3.66189090e-02 -1.34933111e-03 -1.03728528e-02  1.15149420e-02\n",
            "  1.34240314e-02 -1.00692112e-02  1.86289532e-02  6.35916900e-03\n",
            "  1.97063462e-02  2.32322191e-02 -1.07508200e-02 -1.93853356e-02\n",
            " -5.30798970e-03  5.27513985e-03  4.66897913e-03 -2.16141857e-02\n",
            "  1.51907149e-03 -5.43313118e-04  5.81138983e-03 -1.30008914e-03\n",
            " -5.89869280e-03 -1.53576257e-02 -1.67137031e-02 -5.66821953e-03\n",
            "  4.03275074e-03 -2.48380475e-02 -7.72077782e-03 -4.25898136e-03\n",
            " -2.48322847e-03 -2.05578570e-02 -7.56191982e-04  1.30741729e-02\n",
            " -3.66571199e-03 -1.79549448e-02 -3.63058230e-03 -6.50935243e-04\n",
            " -1.17461973e-02 -7.66352951e-03 -5.52937736e-02  4.04338579e-03\n",
            "  1.88555713e-02  2.53877491e-02  7.70589827e-03  3.43213148e-03\n",
            " -1.45322707e-02  3.00192273e-02 -1.70097975e-03  6.91333715e-03\n",
            "  1.13151398e-02  6.27947237e-03  2.03625175e-03 -1.61620704e-02\n",
            " -9.83882625e-03 -7.86599297e-03 -3.12435418e-02  2.41421728e-02\n",
            "  1.53675705e-02 -1.56912526e-02  5.55495739e-03 -6.96517460e-03\n",
            "  1.85272096e-02 -3.45487522e-03  2.21102037e-02 -5.26776789e-03\n",
            "  2.45843055e-02 -4.95543454e-05 -2.05194616e-02 -3.03748825e-02\n",
            "  2.00674628e-02 -3.15334831e-02 -9.99142276e-03 -8.29591607e-03\n",
            " -1.38523624e-02 -1.03663781e-02 -5.28408514e-03  8.23010001e-03\n",
            "  1.28108102e-02 -4.91646455e-02  1.51513524e-03 -1.28334085e-02\n",
            "  1.47251130e-02 -4.23791735e-03  6.18339860e-02 -1.02488482e-03\n",
            " -9.58875148e-03 -9.58875148e-03 -9.58875148e-03 -9.59478062e-03\n",
            " -9.59478062e-03  3.21600537e-02 -5.40128017e-02  2.53697177e-05\n",
            " -5.83448475e-03  2.10488938e-02 -1.57215370e-02  3.90915572e-03\n",
            " -1.22246526e-02  2.44943530e-02  1.30797575e-02 -1.83439807e-02\n",
            " -2.73648283e-02  1.32719280e-02  1.01549075e-02  1.63427590e-02\n",
            "  1.91926045e-05  5.88264237e-02 -2.63088370e-02  1.22791810e-02\n",
            " -3.99248845e-03 -2.73317232e-03 -3.70389725e-02 -2.17918178e-02\n",
            " -1.89755179e-02 -9.73213386e-03 -2.83349207e-03 -2.83349207e-03\n",
            " -2.57076621e-02  3.70651356e-04  1.82537531e-02  1.34046750e-02\n",
            " -6.42579932e-03  1.19306949e-02  1.19306949e-02  1.19306949e-02\n",
            " -3.22212560e-02 -6.94189594e-03 -2.42959851e-02 -1.08051169e-02\n",
            " -1.11332601e-02 -1.93981073e-02 -9.07479614e-03  5.48007716e-03\n",
            " -2.36824609e-03 -2.27350886e-02  3.21675803e-02 -1.26208459e-02\n",
            " -9.03085657e-03  1.91108095e-02  5.55815348e-02  1.95204063e-03\n",
            " -1.67504440e-02 -2.58033409e-02  1.72832163e-02  1.30234224e-02\n",
            " -8.39593425e-03 -2.37978885e-03 -1.20692354e-02  9.70644742e-03\n",
            " -5.44662108e-04  5.81681403e-03  2.10587329e-02  4.17650028e-04\n",
            " -9.75308946e-03  5.84769638e-03  6.75017738e-03  1.37122688e-02\n",
            " -1.47621510e-02 -3.62410110e-03  1.15989403e-02  7.03211921e-03\n",
            "  1.33733866e-02 -1.02296972e-02  2.74757461e-02  1.51031947e-04\n",
            " -1.50805793e-02 -4.74916439e-03  2.44359967e-02 -1.50944959e-03\n",
            " -2.33065151e-04  2.81271550e-03 -2.32232528e-02 -1.41840406e-02\n",
            " -1.70159660e-02  3.48312003e-03 -1.85917642e-03 -8.79573351e-03\n",
            " -1.09205985e-02 -1.04610778e-03 -1.35898312e-02 -2.23049850e-02\n",
            " -5.50167813e-03  1.14855883e-02 -6.75733075e-03  7.05502346e-03\n",
            " -5.82778804e-03  7.83086304e-03 -1.17925469e-03 -1.49840043e-02\n",
            " -1.43921852e-02  6.59651099e-03 -1.25353621e-02 -1.33891131e-02\n",
            "  4.36592216e-03 -9.49098170e-03 -7.45595277e-03  5.97888768e-03\n",
            "  1.45328020e-02  1.65187197e-02 -3.44605130e-03 -9.92270855e-03\n",
            "  2.65372686e-02 -1.28180839e-03 -1.05544704e-02  5.98468955e-03\n",
            " -2.02473760e-02 -1.63349059e-02 -2.24456724e-03  8.70779208e-03\n",
            " -1.45136822e-03 -3.81378464e-03  9.69079326e-03  3.95753738e-03\n",
            " -3.34065061e-04 -1.32051360e-02 -4.81294245e-03  1.27839672e-02\n",
            " -1.96361493e-02  1.44059334e-02 -4.14619728e-02  1.52858136e-02\n",
            "  1.54214658e-02 -7.39779087e-03  2.10961677e-02 -1.36896908e-02\n",
            "  1.85902525e-02  2.47672132e-03  3.66798211e-02  1.00718588e-02\n",
            " -9.77025713e-03 -1.20827759e-02 -9.25728675e-03 -2.28008824e-03\n",
            " -4.34969122e-03  9.66775879e-03 -7.75177769e-04  1.91599877e-02\n",
            "  4.24485250e-03 -7.92578725e-03  1.97007357e-02  1.75704638e-02\n",
            "  7.04205844e-03 -1.01208805e-02 -2.92765379e-02  5.35605343e-03\n",
            " -8.42011863e-04 -5.44308249e-03  1.58505990e-02 -3.22634355e-03\n",
            " -1.32314952e-02  1.12721087e-02  9.87561252e-03 -5.24443879e-02\n",
            " -1.56079344e-02 -2.76336960e-03 -1.82961829e-02 -3.13174028e-03\n",
            "  1.52596773e-03 -2.54770739e-02  8.00498430e-03 -1.41414035e-02\n",
            "  1.18689865e-02  1.61654960e-03  2.62199598e-02  1.02571753e-02\n",
            " -1.15607658e-02  1.04312737e-02 -1.64113707e-02  1.17091199e-02\n",
            " -8.65568146e-03  4.67562209e-04 -4.04632855e-03  5.17244062e-03\n",
            " -7.57781845e-03  8.38576213e-03  8.91025270e-03 -1.14754203e-02\n",
            "  1.14080876e-02  2.10227669e-02 -1.94503455e-03  2.21490642e-03\n",
            " -1.35147104e-02 -1.25768901e-02  1.54567685e-02  2.27246196e-02\n",
            " -4.22684091e-03  5.16102521e-03 -1.92218628e-02 -1.66045636e-02\n",
            " -1.40062768e-02  6.58224363e-03  8.13906675e-03 -1.04056279e-02\n",
            "  1.17716925e-02 -3.02881116e-02  6.87420885e-03  2.81693712e-02\n",
            " -1.56136626e-02  5.52823247e-03  3.06526687e-02  8.14223278e-03\n",
            " -9.62986714e-03 -4.23402769e-02  1.77725323e-02 -1.23599469e-03\n",
            "  1.52361518e-02  4.71062378e-03 -3.24504123e-02  9.46451255e-03\n",
            "  1.15303887e-02 -1.36931386e-02 -2.04147637e-03  5.71640184e-03\n",
            "  2.84342466e-03  8.36019718e-04 -3.60820721e-03 -1.90208000e-02\n",
            " -2.87501165e-02  7.76154272e-03 -4.61592726e-03  6.01976198e-03\n",
            "  1.19728361e-02  1.49422462e-02 -3.14647260e-02 -1.30578375e-02\n",
            "  8.35142352e-04 -1.66977407e-02 -2.44656198e-02  1.26866368e-03\n",
            " -1.79163722e-03  1.98133457e-03  1.05191862e-02 -1.65322258e-02\n",
            "  4.71466894e-03 -1.27795254e-02 -1.61838126e-02  2.05920891e-02\n",
            " -1.60194351e-02 -1.74918090e-02 -1.77312447e-02  3.55244360e-02\n",
            " -6.54697751e-04  2.76783320e-03 -2.09049645e-02  1.64252723e-02\n",
            "  2.01147421e-03  1.49464776e-02  6.42758668e-03 -1.03576852e-02\n",
            " -1.58424499e-02  1.66335961e-02  1.00584580e-02 -1.36925119e-02\n",
            "  6.84225522e-03  7.08198087e-03  8.89441145e-03 -1.95906359e-02\n",
            "  9.51735639e-04 -1.10487794e-02  5.93019300e-03  8.35324475e-03\n",
            "  8.95342649e-03 -8.13280625e-03  6.62896384e-03  5.39621507e-03\n",
            " -1.52755911e-02 -1.03919641e-02  1.40937229e-03 -1.90557588e-02\n",
            " -2.63196916e-03  6.69987646e-03  3.80728373e-03 -5.50425211e-03\n",
            " -1.91573481e-02 -1.96985805e-02 -1.03295827e-02  5.75819893e-03\n",
            " -2.63819237e-03  2.51894424e-03  1.09969219e-02  1.02780264e-02\n",
            " -8.83472351e-03 -1.29735797e-03 -1.38973996e-02 -4.25665910e-02\n",
            " -1.18857198e-02 -1.14539247e-02  8.32802555e-03 -1.34012364e-02\n",
            "  2.03799958e-02  3.48879750e-03 -7.40246673e-03 -1.33283414e-02\n",
            " -2.73514669e-02  1.41501895e-02  6.81865646e-03 -2.13900463e-02\n",
            " -1.42996768e-03 -2.11876370e-02  8.26532816e-03  1.95977621e-03\n",
            "  1.07870518e-02 -7.58817233e-03  3.10563761e-03  3.09926583e-03\n",
            " -2.85304198e-02 -2.83090156e-02  3.17519278e-02 -7.47064587e-04\n",
            " -7.26080000e-04 -1.00706941e-02  2.28655961e-02 -1.06926234e-02\n",
            " -2.48172652e-02 -9.18752224e-03 -1.69968291e-03 -5.04450874e-03\n",
            "  7.21933722e-03  1.67073583e-02  3.23400054e-02  1.46892510e-02\n",
            "  2.52843969e-02  2.72058513e-03 -3.26007531e-02 -3.37108558e-02\n",
            "  5.45623265e-03 -9.79336999e-03 -3.81680971e-02  6.56580977e-03\n",
            "  6.20621966e-04  1.04510726e-02  1.90715086e-03 -1.27062318e-02\n",
            " -4.20422030e-02 -2.61888776e-03  1.14364185e-02 -2.64474518e-02\n",
            "  5.87173104e-03 -9.12519867e-03 -1.29055856e-02 -9.98716638e-03\n",
            "  4.46797656e-03  4.40928770e-03 -3.10188759e-02  1.69784713e-02\n",
            " -7.13956710e-03  9.13159715e-03  1.58605694e-02  6.14918415e-03\n",
            "  1.12362179e-02  2.48988999e-02 -2.01619277e-02  6.66937304e-03\n",
            "  1.49267488e-02  5.74745493e-03 -6.40088695e-03 -1.94436477e-02\n",
            "  2.27276038e-02  7.02227100e-03 -1.04235257e-02 -9.33120069e-04\n",
            " -1.00395696e-02  4.01530739e-03 -3.02153911e-03  1.01694501e-02\n",
            " -3.28510874e-03  1.90365684e-02  8.63725654e-03  4.20162545e-03\n",
            "  8.88759903e-03 -2.76260700e-02 -1.83304429e-02  5.65382330e-03\n",
            "  7.62084909e-03  1.28259744e-02 -2.41920946e-04  2.06040331e-02\n",
            " -4.55689258e-03 -2.11561426e-02  5.24119138e-03  5.19687819e-03\n",
            " -2.26500329e-02 -7.75022475e-04 -4.90708712e-03 -4.90102743e-03\n",
            " -2.51077425e-02 -3.56060412e-02  1.09382763e-02  1.11880103e-02\n",
            " -2.98325874e-03 -1.63351795e-03  1.01772172e-02  1.36665438e-02\n",
            "  4.31382638e-03  3.79350380e-02 -3.43297866e-03 -4.04047271e-03\n",
            " -4.04047271e-03 -4.04047271e-03 -4.04047271e-03 -4.04047271e-03\n",
            "  1.47151993e-03 -2.53735602e-02  4.39135422e-03 -1.67380154e-02\n",
            " -3.79460255e-03 -1.45929127e-02  1.72450967e-02 -1.30928149e-02\n",
            "  8.20108106e-03  4.57888009e-02 -2.00241056e-02 -4.28260370e-03\n",
            "  8.90233923e-03  2.14741718e-02  7.15380076e-03  1.48393549e-02\n",
            " -1.25745272e-02  6.08672794e-03 -1.77153048e-02  2.18412294e-02\n",
            "  1.80407932e-02  5.08848084e-03 -2.35939743e-03 -3.37867828e-03\n",
            " -1.91492107e-03  3.23108399e-03  3.23108399e-03 -6.67191494e-04\n",
            " -1.87002464e-02  8.40004469e-03 -9.80946918e-03  1.33562847e-02\n",
            " -2.19105196e-02 -2.19105196e-02 -2.19105196e-02 -4.57289278e-02\n",
            " -1.32723924e-02  1.65006812e-04  6.99747723e-03 -1.63226839e-03\n",
            "  2.05674769e-03  9.62111929e-03  1.01915910e-03 -2.00688305e-02\n",
            " -6.31392843e-03  1.38495980e-02 -2.02301557e-03 -1.30952651e-02\n",
            " -6.68374559e-03 -2.37324766e-03 -3.69099289e-03 -8.67641912e-04\n",
            "  1.66241510e-02 -2.14029094e-03  1.79973350e-03 -1.59757122e-02\n",
            "  4.88667351e-03 -3.72021165e-03 -6.13812300e-03 -8.18288399e-03\n",
            "  2.19963086e-02 -4.13102778e-02 -5.96600888e-03 -1.98002316e-03\n",
            "  1.60986893e-02  2.06621142e-02 -1.18869130e-02  1.93806032e-02\n",
            "  2.08207409e-02  3.93363812e-03  2.03673437e-02  9.72548359e-03\n",
            " -1.13713618e-02  2.41184179e-02 -1.04097046e-03  1.94121768e-03\n",
            " -5.14935480e-03 -2.07287506e-03 -8.09689894e-03  9.91233429e-03\n",
            "  6.02334950e-03  2.10835996e-03 -3.43587957e-02  2.80545530e-02\n",
            " -1.04071684e-02 -4.46833788e-05]  - intercept :  0.653248391828466\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.05462979310331442\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:07,852]\u001b[0m Trial 154 finished with value: -0.31599508617369076 and parameters: {'count_threshold': 10, 'postag': False, 'voc_threshold': 9224}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.03258141 0.03040612 0.05962987 ... 0.         0.08445944 0.03782312]\n",
            " [0.01484132 0.03192643 0.0470404  ... 0.         0.         0.03736408]\n",
            " [0.31705652 0.01658516 0.06484562 ... 0.         0.09816144 0.00445838]\n",
            " ...\n",
            " [0.01629071 0.08875002 0.0423387  ... 0.         0.0501035  0.01719663]\n",
            " [0.0039263  0.03995455 0.04661017 ... 0.29846141 0.03108741 0.08373261]\n",
            " [0.04029427 0.15186398 0.03848612 ... 0.56180971 0.02120936 0.02651943]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 1.79624338e-03  4.40279730e-02 -1.41049476e-02  4.73544471e-02\n",
            " -1.13353047e-01  3.69015995e-02 -9.79853595e-02  5.15816102e-02\n",
            " -4.47616260e-02 -4.54505426e-02 -4.47446337e-02  1.14473935e-02\n",
            " -9.77774959e-02  6.49402856e-03 -4.68208653e-02  1.03896307e-01\n",
            "  8.52561400e-02 -4.09159013e-02  9.86975549e-02 -8.06870227e-02\n",
            "  8.23601462e-02 -8.87907936e-02 -1.06217314e-01  2.11128235e-02\n",
            "  2.22422766e-02 -1.12460214e-01 -5.28714358e-02  1.36501275e-01\n",
            " -6.29450632e-02  5.19773388e-02  2.00945985e-04  1.54541096e-01\n",
            "  9.89868968e-03 -8.05847592e-02 -2.20733976e-01  2.13068519e-01\n",
            "  6.38167957e-02  4.42718400e-02 -9.52240791e-02  2.67525862e-02\n",
            "  5.99082929e-02  2.85285689e-02  1.87364161e-01  2.21413933e-01\n",
            "  8.35419286e-02 -1.02949942e-01  7.16189989e-02 -3.82768905e-02\n",
            "  5.68679794e-02 -2.08464245e-01 -7.62927392e-03 -2.68489701e-02\n",
            " -2.10355928e-01  1.20457760e-01 -1.00009660e-01  4.02014363e-02\n",
            "  7.07389488e-02 -1.01267558e-02 -8.33064937e-02 -4.92296100e-02\n",
            " -7.47651400e-02 -8.32553640e-02 -4.29564586e-02  1.11054169e-01\n",
            " -6.00343266e-03  1.30087851e-01 -8.30460927e-03 -4.74689600e-03\n",
            "  5.37211522e-02 -1.07334803e-01 -1.11047911e-01  5.09444921e-03\n",
            " -3.77775954e-02 -2.08635928e-01 -1.48907809e-01 -1.41609106e-01\n",
            " -1.47206320e-03 -3.78023816e-02 -1.26055771e-01 -2.05737245e-02\n",
            "  1.28939755e-01 -3.70671031e-02  5.36507152e-02  1.93749525e-01\n",
            "  1.41103847e-01 -2.19572894e-01 -1.80983456e-02 -7.74771290e-02\n",
            " -4.18794072e-02  4.15128008e-02 -1.48813075e-01 -5.72025320e-02\n",
            " -5.64494536e-02  1.33086262e-01  1.71361715e-01  3.73864497e-02\n",
            "  2.24167387e-01  1.18252548e-01  1.15752710e-01  1.67937442e-01\n",
            "  5.92184948e-02  1.54318055e-02  1.74957937e-01  5.76658771e-02\n",
            "  7.53321475e-02 -1.33900416e-02 -2.01844442e-01  1.60850609e-01\n",
            " -1.18558044e-01 -9.90797188e-02 -1.59737204e-01  1.33817601e-01\n",
            " -6.97848726e-03 -1.53608225e-02 -1.62130178e-01  1.05535301e-01\n",
            "  2.32160387e-02  1.15202063e-01 -7.48191043e-02 -6.92658177e-02\n",
            " -1.46801056e-01  3.20828206e-02 -1.03977799e-01  1.44211248e-01\n",
            " -1.06561813e-02  4.14459295e-03 -1.30969602e-01  2.11876646e-02\n",
            " -1.34417843e-01 -6.37764242e-02  3.37225440e-02 -1.17177938e-01\n",
            "  1.44124703e-01 -6.38873249e-02  4.70839813e-02 -7.14544217e-02\n",
            " -1.02102178e-01  2.22583122e-01  7.39287192e-02  1.39156128e-02\n",
            "  2.58048299e-02  3.62419422e-02 -1.87731115e-02  1.31936316e-01\n",
            " -1.06705502e-01  1.01026708e-01  2.91156884e-02 -6.50387421e-03\n",
            " -7.63175036e-02 -6.05677861e-02 -5.65304310e-02  5.72334477e-02\n",
            "  4.14307917e-02  8.09386844e-02 -1.60822892e-01 -1.97297657e-02\n",
            "  5.98473413e-02 -1.11336044e-01 -1.62108088e-01 -6.42753568e-02\n",
            " -1.60288063e-01 -4.38523204e-02  1.23933766e-01  1.32169703e-01\n",
            "  6.64453904e-02 -1.39648074e-01  2.42794940e-02  1.38189343e-02\n",
            "  3.76496547e-03 -1.59953441e-02 -1.22955714e-01  1.34928489e-02\n",
            " -1.26692167e-03 -9.28884293e-02 -8.00405798e-02  1.82673245e-01\n",
            "  1.05759016e-01  7.19679614e-02  1.48680450e-01 -1.06498581e-01\n",
            "  9.17911725e-02 -5.47403721e-03  4.92483754e-01  3.14454044e-02\n",
            " -1.05093843e-01  6.23938674e-02  1.10744060e-01 -4.49371204e-02\n",
            " -1.33094699e-01  1.41368184e-01  5.66782511e-02 -2.02247312e-02\n",
            " -1.06692313e-01  2.85828103e-01  2.16388365e-01  1.44330272e-01\n",
            " -1.31176676e-02  5.85651915e-02  6.12543892e-02 -2.69627599e-02\n",
            " -1.04096281e-01  1.61126251e-01  1.72576715e-01  1.79881456e-01\n",
            "  1.46661873e-01 -7.07582903e-03 -5.83048377e-02 -1.17424364e-01\n",
            "  1.98450567e-02  4.58010105e-02  7.60204794e-02  3.57767058e-01\n",
            "  7.14015988e-02 -8.02684461e-02  8.20213597e-02  2.43246033e-02\n",
            " -1.48329356e-01  1.18524827e-01 -1.68293925e-01 -1.72416347e-01\n",
            " -3.95194839e-02  2.43544003e-01  1.83144779e-01  2.03539628e-01\n",
            "  1.50963733e-01 -9.11114267e-02  3.56070508e-01  3.73328722e-01\n",
            "  4.45559743e-02 -1.03415283e-01 -8.88772249e-02 -1.89248884e-01\n",
            "  2.27685164e-01  3.73695139e-01  4.24904625e-01  3.44586829e-02\n",
            "  1.17240723e-01  1.22716502e-01  3.16488678e-02  2.47495989e-01\n",
            "  1.27828552e-01 -3.59559010e-02  2.99898999e-01 -9.70041866e-02\n",
            "  3.77179248e-02 -6.93166271e-02 -3.10444274e-01  1.14645992e-01\n",
            " -9.30844944e-03  1.13455956e-01 -6.28234782e-02  2.72769024e-02\n",
            "  2.35968461e-01  1.52331980e-01  3.16362556e-01 -7.15697565e-02\n",
            " -7.62253391e-02  3.37439930e-02  1.24119720e-01  7.81927185e-02\n",
            " -1.74257988e-01  5.02079534e-02  5.21093939e-02  3.75285433e-01\n",
            "  1.22007403e-01  2.07270535e-01 -7.70984243e-02 -1.45426812e-01\n",
            " -4.54497754e-02 -6.15979204e-02  2.90306851e-01 -4.13965935e-02\n",
            "  1.45666815e-01 -5.20180561e-02 -6.88983721e-02  1.13799338e-01\n",
            "  1.28632562e-01  1.19024415e-01  3.21439446e-02  8.34067039e-02]  - intercept :  -0.0766889288378535\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.31599508617369076\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:08,387]\u001b[0m Trial 155 finished with value: 0.30495823726529664 and parameters: {'count_threshold': 7, 'postag': True, 'voc_threshold': 5785}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.00542336 0.02021966 0.03564796 ... 0.19285682 0.11653541 0.06222114]\n",
            " [0.0298072  0.11293082 0.04923881 ... 0.07248434 0.08988371 0.04765228]\n",
            " [0.         0.00470451 0.         ... 0.38382131 0.1936524  0.05440377]\n",
            " ...\n",
            " [0.0186973  0.05399897 0.05201597 ... 0.09933275 0.22926491 0.03981163]\n",
            " [0.22164847 0.04254101 0.35048973 ... 0.06408822 0.04997794 0.02243163]\n",
            " [0.         0.00261362 0.02254324 ... 0.10497154 0.0227317  0.1016096 ]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-2.90608740e-02  1.69224193e-02 -2.60810652e-02 -1.72564927e-02\n",
            " -4.05216539e-02  1.45048961e-04  1.14229041e-02  1.90657426e-02\n",
            " -1.03409960e-02 -2.67057478e-03 -3.12973455e-02 -6.26565166e-02\n",
            "  4.38189137e-03  1.81523578e-02  1.80545088e-02 -2.24996697e-02\n",
            " -4.55364150e-02 -3.81088945e-02  5.06432333e-03  4.12826512e-03\n",
            " -2.12703131e-02  2.80922060e-03 -5.70265588e-04 -7.69920118e-03\n",
            " -7.99939248e-03  1.36526449e-02 -3.22945243e-03 -3.37024235e-02\n",
            "  6.44880633e-03  9.99744147e-03  1.75762462e-02 -1.86024676e-02\n",
            "  9.79022467e-03  8.75872784e-03  3.88543187e-02 -9.55341811e-03\n",
            " -1.23567595e-02  2.30747772e-02 -3.86359865e-02  8.75046686e-02\n",
            " -1.12607328e-02 -2.15842001e-02  3.19652269e-03  1.03608915e-02\n",
            " -2.38714699e-02 -1.49688093e-02 -5.06916921e-03 -2.29598502e-02\n",
            "  2.00166564e-02 -8.52848653e-03 -1.43053237e-03 -6.89149617e-02\n",
            " -1.81300072e-02  1.82410424e-02  3.99857590e-02 -1.68719092e-02\n",
            " -9.20610906e-03 -4.51370076e-02 -2.72216939e-02  3.54183762e-02\n",
            "  9.47601131e-04  2.02032718e-02  2.05773671e-02 -3.53468858e-03\n",
            "  1.22133795e-02 -1.23917292e-03 -2.20826581e-02 -1.79188501e-02\n",
            " -5.71779246e-02 -2.60874689e-02  1.49489774e-02  4.24542207e-02\n",
            " -2.91130256e-02  2.10862732e-03  5.10367883e-03 -1.71067302e-03\n",
            " -2.31255491e-03  1.54987148e-02  1.05297611e-02  1.54967045e-02\n",
            "  1.50495922e-02 -1.12667079e-02 -3.57309541e-02  9.93193453e-03\n",
            " -8.22033316e-02  2.25784510e-02 -2.75818503e-03  1.21032571e-02\n",
            " -2.06556467e-02  1.19161187e-02  2.80961844e-02 -4.00095485e-02\n",
            " -2.12041399e-02  3.53066675e-02 -1.58496958e-03  3.45255070e-02\n",
            " -3.93790440e-02 -5.29321575e-03 -2.30785654e-02  2.81352246e-03\n",
            " -5.05286678e-03  2.72305924e-02  1.51399181e-02 -7.26619727e-02\n",
            "  2.59433909e-03 -6.51890815e-02  1.70580005e-02  4.46073315e-02\n",
            " -6.79048733e-02  5.34409937e-03 -3.70785533e-02  6.89142747e-03\n",
            " -4.06102863e-03 -1.96248012e-02 -4.31749364e-02  1.65460365e-02\n",
            "  4.52852136e-02  3.36689910e-02  8.53768824e-03 -1.34669635e-02\n",
            " -4.56623319e-02  2.00092981e-02 -1.03651929e-02  8.94962673e-03\n",
            " -1.57068214e-02 -9.23038166e-03  1.47758879e-03  7.53419797e-03\n",
            "  9.16235217e-03 -1.90755197e-03 -1.06416492e-02  1.62337770e-02\n",
            " -5.03727365e-03 -1.87518178e-02  1.90760861e-02 -1.69867363e-02\n",
            " -2.14791090e-02 -2.06826513e-02  4.49025805e-02 -1.98946051e-02\n",
            "  4.84246686e-02 -8.25876563e-03 -1.39282836e-02  2.65977180e-02\n",
            "  3.72031680e-02  1.88571747e-02  2.24904245e-02  5.81143551e-02\n",
            "  5.37578819e-02  1.15381242e-02  1.96424803e-03  4.75651030e-03\n",
            "  1.39548567e-03  9.89014523e-03  4.29724401e-02 -1.14801517e-03\n",
            " -2.98735599e-03 -2.64874753e-03  1.35089112e-02  4.92936456e-03\n",
            " -1.71662751e-02  2.70427757e-02  3.37340020e-02 -2.21517522e-02\n",
            " -1.24648884e-02  1.88216944e-02  1.23582884e-02  1.82591017e-02\n",
            "  2.47143903e-02  2.43236655e-02  3.40129706e-02  4.40147028e-03\n",
            "  2.93933002e-02  2.67478178e-02 -2.43733626e-03 -2.78230561e-03\n",
            " -2.91120169e-03 -6.11726393e-03 -6.40861725e-03 -7.06974110e-02\n",
            " -3.38341219e-02 -3.23859114e-02  1.20603828e-02  5.54353343e-03\n",
            "  2.83116558e-02 -3.68514905e-02 -3.91622524e-03  1.45686410e-02\n",
            "  1.40035465e-02 -4.74127476e-02 -4.62474976e-02 -4.85800963e-02\n",
            " -2.54222179e-02  1.65422526e-02  9.57187770e-03  3.74587449e-02\n",
            " -5.00290683e-03 -1.48231246e-02 -7.48527664e-03  2.17725495e-02\n",
            " -2.51523002e-02 -2.13405298e-02  3.40840390e-03 -1.03602214e-02\n",
            "  3.82821255e-02  1.26930918e-03 -6.12790423e-03 -2.37541444e-02\n",
            "  2.30696731e-02  2.68027047e-03  5.28362895e-03 -4.00111464e-02\n",
            "  9.86369232e-03 -7.42456768e-02 -5.44476554e-03 -5.21496561e-03\n",
            " -1.63480779e-02 -2.73955295e-02 -2.80447719e-02  4.76573634e-02\n",
            " -1.54292040e-02 -2.19722068e-02 -1.63173881e-02  5.93001307e-02\n",
            " -1.76263560e-02  1.82056842e-02  4.58239706e-02  3.51081301e-02\n",
            " -1.74000781e-02 -2.57495264e-02 -1.49988327e-02  8.04911344e-03\n",
            " -8.89307681e-04 -2.63831817e-02 -1.12483057e-02 -2.15255103e-02\n",
            "  2.86159493e-02 -3.66481974e-02  6.31424720e-02 -9.06223887e-03\n",
            " -9.46200245e-03 -1.50056287e-02 -1.50056287e-02 -1.50056287e-02\n",
            " -1.50056287e-02 -2.05431191e-02 -1.01043307e-01 -1.83147879e-02\n",
            "  4.93263736e-02 -4.37090690e-03  7.72856344e-03  3.29784721e-02\n",
            "  2.14167378e-03  2.30479394e-02 -4.85182528e-03 -1.51817426e-02\n",
            " -3.84581370e-03 -7.94794569e-04  2.34632755e-02 -3.20486269e-03\n",
            " -2.74069362e-02  3.19376728e-02  9.93130330e-05  2.29057341e-02\n",
            "  7.23846337e-03 -5.53326772e-03 -1.44663323e-02 -5.35178591e-02\n",
            " -5.89281796e-03  2.32500973e-02  8.67428476e-03 -1.94762811e-02\n",
            " -3.56360793e-02 -2.69539991e-02  6.52951636e-03 -1.55337073e-02\n",
            " -7.88072404e-03 -4.30570417e-02 -4.77529107e-03  3.92160337e-02\n",
            "  4.17596693e-03  2.91462045e-03  2.22299637e-02  2.11273666e-03\n",
            "  2.94144276e-02 -1.88536379e-02  9.28860240e-03 -4.19524200e-02\n",
            " -4.77194136e-02  5.77363980e-02  5.22810968e-02  2.54948879e-02\n",
            " -6.92238924e-04 -2.91578069e-02 -9.46362040e-03 -5.75748443e-02\n",
            "  1.54131097e-02  4.70535842e-02 -6.52808199e-02  7.06289012e-03\n",
            "  2.32033607e-02 -2.28465670e-02 -2.54667957e-02  1.25325259e-02\n",
            "  9.87660712e-03 -1.87659840e-03  4.12334304e-02  1.32045785e-02\n",
            " -2.99449942e-02  1.26041802e-02 -2.80410443e-02  1.06319425e-02\n",
            "  2.35166482e-02 -3.19420236e-02  6.62945954e-02 -1.52980129e-03\n",
            "  1.69556782e-02 -4.25522894e-02 -2.39291340e-02  2.43470261e-02\n",
            " -7.79343786e-05 -1.80169834e-02  7.42772161e-04  1.68669456e-02\n",
            " -1.65568945e-02 -6.97260311e-02  6.85045943e-02 -1.48596109e-02\n",
            " -2.16313113e-02  3.61756592e-02  4.27632293e-03  1.20760000e-02\n",
            "  1.13054241e-02 -1.60017158e-02  2.54492782e-02  8.80117316e-03\n",
            "  2.09435052e-02 -3.32403643e-02  2.49843748e-02 -2.71937569e-02\n",
            "  4.27655142e-02  9.73978219e-02  4.20622297e-02 -6.37784215e-02\n",
            "  2.71806495e-02 -2.96016756e-03  3.72163729e-04  4.53715948e-02\n",
            " -3.98520816e-02  4.61379745e-03  1.58593828e-02 -3.55543197e-03\n",
            "  4.64274610e-02 -6.83868163e-04 -8.17483369e-03 -9.81142658e-03\n",
            "  5.60054478e-02  3.67790180e-02 -5.65601677e-02 -1.63706781e-02\n",
            " -3.12668997e-02  1.21417372e-02  6.47717003e-04  3.23423758e-02\n",
            "  2.73909262e-03 -4.72783727e-02 -6.64970901e-02  4.93071447e-04\n",
            "  4.42943685e-02 -7.84799753e-03 -5.16751521e-02  1.41605838e-03\n",
            " -3.64236055e-02  6.77209891e-02  1.62122469e-02  3.80244516e-02\n",
            " -4.19919083e-02 -3.59207196e-02  5.24781966e-02  1.37555341e-02\n",
            " -7.11489985e-03  1.10476136e-02  3.16629194e-02 -8.11060724e-03\n",
            "  3.93252425e-02 -8.49425246e-03  2.78949281e-02  2.13046658e-02\n",
            " -9.98217801e-03 -4.74909503e-02  2.03073435e-02  3.08691122e-02\n",
            "  8.20797237e-03  8.88847034e-03 -6.34347330e-02 -1.94741065e-02\n",
            "  1.33293715e-02 -7.53302382e-02 -1.20116152e-02  4.87236813e-03\n",
            "  3.09734210e-02  3.67190506e-02  2.37148739e-02  2.15362777e-02\n",
            "  2.21653039e-02  4.32475207e-02 -8.58544691e-03 -2.98345607e-02\n",
            "  1.24891944e-02 -2.22045008e-02  2.84631715e-02  3.40590168e-02\n",
            " -7.81002369e-03 -1.06100412e-01 -8.34275655e-03  7.74050368e-03\n",
            "  1.27404027e-02 -3.57826479e-02 -2.91564406e-02  3.15116614e-02\n",
            "  4.24988725e-02 -1.64535471e-02  4.84253308e-02  2.89679726e-02\n",
            "  4.50735635e-03  1.15572729e-02  4.14564750e-02 -4.44084147e-02\n",
            "  9.74787312e-04  1.56482103e-02  1.69755102e-02 -9.13609371e-03\n",
            "  5.67773413e-03  5.74395582e-03  1.67551103e-03  1.82682159e-03\n",
            " -1.75377859e-02 -3.28637716e-02  3.38708140e-02  7.09099017e-02\n",
            "  1.01192451e-02  1.82329380e-02 -3.24961076e-03 -3.33882568e-02\n",
            " -1.36880849e-02  2.38740633e-03 -4.45293357e-02  5.41090791e-03\n",
            " -3.75798075e-02  7.79684666e-03  7.73030114e-02  1.67812804e-03\n",
            "  4.63661278e-02  8.47796420e-05  5.09523217e-02 -2.17246233e-03\n",
            " -3.95466706e-03 -4.27656376e-02  6.43083778e-02 -1.05297249e-02\n",
            "  3.21845327e-02  3.10280788e-02  2.87284383e-02  3.57781827e-02\n",
            " -1.02780641e-02  5.20134847e-02 -8.99426910e-03 -3.01648764e-02\n",
            "  2.76178782e-02  5.15328672e-02  4.25085496e-02 -4.31459052e-02\n",
            "  5.78052280e-03 -9.23156161e-03 -1.93498938e-02 -8.17193366e-03\n",
            "  2.21560470e-02 -1.44662409e-02  7.53095022e-03 -3.31568501e-02\n",
            " -5.92822090e-03  3.97428060e-02 -2.57751004e-02  6.72525427e-02\n",
            "  6.48841579e-02  4.96748883e-02  2.00422360e-02  2.99856520e-02\n",
            " -2.41740692e-03  7.85474538e-03  1.20627785e-02 -2.26453980e-02\n",
            " -7.40544264e-02  4.59835128e-02  2.35222076e-02  1.58666499e-02\n",
            "  3.81596267e-02 -3.31050145e-02  2.08869461e-03  2.08869461e-03\n",
            "  2.08869461e-03  2.08869461e-03 -4.83693787e-02  5.28431687e-02\n",
            " -2.11046296e-02  7.83224585e-03  2.44106791e-03  1.49202766e-02\n",
            "  3.96323000e-02 -1.13656649e-03 -2.17903979e-03  2.24384222e-02\n",
            "  4.36317001e-02 -8.73163971e-03]  - intercept :  0.623014260996392\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.30495823726529664\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:08,985]\u001b[0m Trial 156 finished with value: 0.33499208630015115 and parameters: {'count_threshold': 7, 'postag': True, 'voc_threshold': 5723}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.05393689 0.04983681 0.11228786 ... 0.         0.45516603 0.01561814]\n",
            " [0.04237755 0.06977561 0.05203809 ... 0.01197649 0.03754738 0.02753475]\n",
            " [0.         0.04138662 0.         ... 0.02794515 0.         0.04664462]\n",
            " ...\n",
            " [0.32185007 0.0430741  0.54283384 ... 0.17882545 0.46660881 0.00933923]\n",
            " [0.3866472  0.05685915 0.46867275 ... 0.22353182 0.42535358 0.00389134]\n",
            " [0.37094354 0.02161947 0.4824289  ... 0.         0.50125757 0.00778269]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 2.85775593e-02 -3.00551572e-02  3.07323517e-02  1.38437317e-02\n",
            "  6.22027724e-03  5.39721646e-03  1.98204991e-02  3.18132500e-02\n",
            " -5.66116055e-03 -1.70542701e-02 -1.66414925e-03  6.24194654e-03\n",
            " -6.69420486e-03  1.38621367e-02  2.47110916e-02  1.03396942e-02\n",
            " -6.08580361e-02  4.57740188e-02  1.02602376e-02 -8.51796870e-03\n",
            "  3.55271922e-02 -2.88905290e-02  1.82562224e-02 -2.62700445e-02\n",
            " -3.76264749e-02 -1.54552944e-02 -8.68104846e-02  3.23460834e-02\n",
            "  2.94002940e-02  1.63837136e-02 -2.80306778e-02 -6.44858700e-03\n",
            "  2.59410675e-02  2.04320172e-02 -1.52799449e-02 -3.50184370e-02\n",
            " -5.29806706e-02 -7.03535206e-03  4.74657800e-02 -5.27879003e-02\n",
            "  3.40005752e-02 -3.47375545e-03  7.12722874e-03  2.68625731e-02\n",
            "  4.18147504e-02  4.40938009e-03  8.22234857e-03 -2.61253791e-04\n",
            "  4.22770969e-03  7.12626362e-03 -3.34367724e-02 -1.39975078e-02\n",
            "  3.88584693e-03 -2.82026371e-02  4.22425319e-02 -3.02228093e-02\n",
            " -3.86215846e-02  2.57725648e-02 -1.80370990e-02 -5.98666102e-02\n",
            "  4.75147857e-02 -6.21183457e-02 -1.68945923e-02 -4.38666152e-02\n",
            "  3.17290718e-02  3.68528711e-02 -1.04843741e-01  4.94064914e-03\n",
            " -8.18739632e-03 -4.62199030e-02  1.64604977e-03  3.90830079e-02\n",
            " -1.87973228e-02 -6.99065151e-03  1.12362040e-02 -3.61351109e-02\n",
            "  1.57503634e-03 -9.29112564e-03 -2.71898308e-02  3.56535377e-02\n",
            "  6.53454173e-03  6.29384122e-02  3.26021460e-02  4.85894693e-02\n",
            "  6.68557894e-02 -3.49827608e-02 -4.25276682e-02  7.21942592e-03\n",
            "  3.78888433e-03  1.32561459e-02 -5.94845955e-03  2.94218279e-02\n",
            " -3.55654931e-02 -1.32670459e-02 -3.52686131e-02  7.01111489e-02\n",
            " -3.53141049e-02  1.37361806e-02 -1.61777025e-03 -6.39820217e-02\n",
            " -2.24191626e-02  2.66737435e-04 -1.57927422e-02  2.95999759e-02\n",
            " -6.36990544e-02  1.16897056e-03  2.95058284e-02 -2.85495008e-02\n",
            "  3.04181803e-02 -2.56438351e-02 -7.77966220e-02 -3.39360247e-02\n",
            "  2.10045532e-02 -4.98783105e-03 -9.27760901e-02  2.40445937e-02\n",
            "  2.97396039e-02  3.87482020e-02  1.75451159e-02 -3.23479517e-02\n",
            " -4.93948867e-02  5.89212567e-03  2.80009433e-03 -3.36941497e-03\n",
            "  5.77210930e-02  4.36280986e-02  8.91831431e-02  2.59859435e-02\n",
            "  5.61123969e-03 -6.13677734e-02  1.38777016e-02  3.41320248e-02\n",
            " -9.04534658e-02  1.88060092e-02  6.82553204e-02 -2.71855656e-02\n",
            "  3.27792543e-02 -5.34353357e-02 -6.73596756e-03  4.50457893e-02\n",
            "  6.33807578e-02  9.72483243e-03 -1.34368772e-02  1.83464152e-02\n",
            "  4.30121109e-02 -1.01455739e-01 -6.88611126e-02  6.17626143e-02\n",
            " -1.47141344e-01  1.00291248e-02  4.75384051e-02  1.13955690e-02\n",
            " -3.26720733e-02  3.50882879e-02  1.62082465e-02 -1.97023438e-02\n",
            " -4.86565708e-03 -5.73809437e-02  1.62441385e-02 -2.98213316e-03\n",
            " -4.50734282e-02 -2.14787985e-02  5.21194668e-02 -6.81167566e-03\n",
            "  5.82485480e-02 -8.33633419e-02  1.00279027e-02  7.54840994e-02\n",
            "  8.44427934e-02  3.88527856e-02 -5.20271469e-02 -4.44673448e-02\n",
            "  4.46187860e-02 -3.96789446e-02  2.51097109e-02  1.87659557e-02\n",
            " -8.98757596e-03  4.70852651e-02 -5.07702943e-02 -8.31859017e-02\n",
            "  2.42883530e-02  2.18840244e-02  3.28063733e-02  4.07324867e-02\n",
            " -3.75766950e-02  2.55393481e-02 -4.92217858e-02  2.32559121e-02\n",
            "  1.65342024e-02  2.42015875e-02 -2.06174314e-02  2.62022508e-02\n",
            " -5.08407165e-02 -2.46266549e-03 -9.23513423e-02  1.99771910e-02\n",
            "  4.38789773e-02  2.57240492e-02 -5.52631920e-02 -2.91953095e-02\n",
            " -2.31175025e-02 -4.45625149e-03  5.75583396e-03  3.37708947e-02\n",
            " -1.56079648e-02  2.54122803e-02  6.10148038e-02 -7.20971794e-02\n",
            " -1.05804700e-02  1.82722412e-03 -3.22342952e-02  4.45274921e-02\n",
            "  8.17590802e-05  2.82802719e-02  3.60133164e-02  1.63274224e-03\n",
            "  2.01180291e-02 -2.48186555e-03 -1.15643190e-02 -7.30598648e-02\n",
            "  1.69324997e-02 -1.11429848e-02 -4.51377394e-02  1.13406731e-02\n",
            " -8.41809025e-02  2.98372996e-02 -1.38137151e-01 -1.72918864e-03\n",
            "  1.34365368e-02  6.24933154e-02  2.01337695e-03 -6.56521035e-03\n",
            " -6.14201559e-02  1.57056681e-02 -4.99752327e-02 -3.30877596e-03\n",
            "  9.65718989e-03  9.28856503e-03 -1.11041137e-02 -2.50844650e-02\n",
            "  2.83601973e-02 -1.77841329e-02 -3.08105780e-02 -3.69045306e-02\n",
            "  9.22589797e-03  4.04417793e-03 -2.38154408e-02  4.91480221e-02\n",
            " -2.80506303e-02 -2.81530468e-02  1.57034722e-02  1.55434858e-02\n",
            " -3.15201900e-02  8.57710454e-06  4.14659560e-02  3.35349978e-02\n",
            "  6.34060092e-03  3.01972647e-02  4.00645187e-02 -3.58772680e-02\n",
            "  4.80258341e-03 -2.97950889e-03  1.44717868e-02  3.36947920e-02\n",
            "  3.67913595e-03 -9.45934872e-03 -2.65129119e-02  4.08994398e-02\n",
            "  7.34081766e-02  4.55622343e-02  2.55459784e-02 -1.29867110e-02\n",
            "  1.04396744e-02  1.86715164e-02 -3.23571980e-03 -1.16159064e-02\n",
            " -4.78589060e-02  2.30252328e-02  1.21586382e-02  3.13916730e-03\n",
            " -1.23372310e-02 -3.28125539e-02 -1.02265818e-02 -9.42421863e-03\n",
            " -2.33801009e-02 -1.09567171e-02 -1.02670648e-02  3.23012665e-02\n",
            "  1.40862337e-02 -2.36957648e-02  3.18712972e-02 -4.03681560e-02\n",
            "  1.96910097e-02  2.16655162e-02  4.20221328e-02 -1.05593389e-01\n",
            "  6.16808547e-02  4.42723441e-02 -1.15221109e-01  8.35775359e-03\n",
            " -2.84615098e-02 -1.44947701e-02 -3.87932373e-02  3.07143203e-02\n",
            " -2.19938437e-02 -3.18090574e-02  1.14831494e-02 -5.78387668e-02\n",
            " -2.84096622e-02 -1.13631572e-02  9.60539551e-03 -9.63767494e-03\n",
            " -3.92954849e-02 -4.87983403e-02 -3.16283950e-03  1.00071016e-02\n",
            "  2.64979163e-02 -1.52714188e-02 -1.68474753e-02  1.61628385e-02\n",
            "  3.12960809e-02 -3.20176722e-02 -6.42820042e-02 -1.62233765e-02\n",
            " -4.02595323e-02 -5.72934112e-02  1.64281324e-02  7.95623481e-03\n",
            " -4.13715449e-02 -5.59393796e-02  4.03940882e-02 -5.42008196e-02\n",
            "  2.98230723e-02 -2.86838642e-02 -6.99814306e-03 -1.68791359e-04\n",
            " -5.98141149e-02 -7.79351203e-03 -5.06585603e-02  1.55200933e-02\n",
            " -6.44565144e-02  3.04318201e-02 -1.48833898e-02 -4.02572957e-02\n",
            "  6.10375179e-02 -3.23427408e-02 -1.91469821e-02  7.88032532e-02\n",
            " -3.13660513e-02  4.71785418e-02  2.03576852e-02 -2.42541168e-02\n",
            " -2.99709567e-02 -4.97147176e-02  2.56259793e-02  3.60130063e-02\n",
            "  2.44015497e-03 -8.25548720e-02 -1.21156355e-02  2.46764992e-02\n",
            "  2.45690121e-02  4.58107417e-02 -1.19956036e-03  1.51478148e-02\n",
            "  3.04753855e-03 -6.83839445e-02  1.59024443e-02  3.20437452e-02\n",
            "  1.51290062e-02 -2.29269323e-03 -2.56383572e-02 -4.02348532e-03\n",
            " -8.64231261e-02  3.09672905e-02 -3.30709184e-02 -8.31004794e-02\n",
            " -7.56584383e-02  1.01934176e-03 -2.66024015e-02  5.16963380e-02\n",
            "  5.57355923e-02  6.65867119e-03  2.57464488e-02  5.44840184e-02\n",
            " -1.91796129e-02 -2.82784095e-02  2.05887247e-02  1.33702666e-02\n",
            "  6.08977486e-02  1.27848497e-02  2.55117195e-02 -5.33301491e-02\n",
            "  6.60460647e-02 -1.27048608e-02 -3.68556108e-02 -5.69459641e-02\n",
            "  2.52190422e-02 -2.64890242e-02  2.88105273e-02  3.65312070e-02\n",
            "  1.33907291e-02 -1.43547695e-02 -4.16429639e-02 -8.47987511e-02\n",
            " -4.51881296e-02  1.38362722e-02 -1.72618428e-02 -3.65543850e-02\n",
            "  2.91314192e-02 -2.71960806e-02  4.58829038e-02  1.09425876e-02\n",
            "  1.87992296e-02  1.04048961e-02 -4.81718508e-02 -1.72115523e-02\n",
            "  1.45875385e-02 -7.96408020e-04 -1.07929108e-02  2.48706893e-02\n",
            " -2.74694174e-03  1.91064075e-02  2.32262070e-02 -7.71352829e-02\n",
            " -6.77057605e-02  9.42733950e-03  2.45258901e-02  1.95266299e-02\n",
            " -2.91263814e-02  4.01111133e-02  3.09021229e-03 -2.76489449e-02\n",
            " -9.37073196e-03 -1.49200185e-02  1.69530816e-02  1.71784526e-03\n",
            " -2.95408312e-04 -2.17388696e-02 -2.63892826e-02 -5.24112365e-02\n",
            "  1.93658356e-02 -7.84156836e-03 -2.26534505e-02 -8.20841886e-03\n",
            " -2.49802829e-02 -4.55103803e-03  6.08591455e-02  4.60258457e-02\n",
            " -4.27412482e-02  9.51674126e-03  3.75711700e-02 -1.74004705e-02\n",
            "  4.02898635e-02  2.09667078e-02  7.25321547e-02  9.57302776e-03\n",
            "  1.41950430e-02 -6.06964121e-03 -3.33768469e-02 -2.83509425e-03\n",
            " -9.60100650e-02  1.01781951e-02  6.33808521e-03  3.00119168e-03\n",
            "  6.78112558e-02  4.33061569e-02  3.64404462e-02  3.58140092e-02\n",
            "  4.58307055e-02  1.72483924e-02 -4.20063976e-02 -2.41512364e-03]  - intercept :  0.8665241239669472\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.33499208630015115\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:09,579]\u001b[0m Trial 157 finished with value: 0.3735991248747409 and parameters: {'count_threshold': 7, 'postag': True, 'voc_threshold': 6168}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.00000000e+00 6.34315642e-02 0.00000000e+00 ... 2.80592629e-02\n",
            "  0.00000000e+00 9.54930590e-02]\n",
            " [7.34883642e-04 5.29821412e-02 1.35070862e-02 ... 1.31671897e-01\n",
            "  1.52494482e-01 2.68810787e-02]\n",
            " [7.02520456e-01 4.19383888e-02 8.87330973e-01 ... 0.00000000e+00\n",
            "  3.39117297e-01 0.00000000e+00]\n",
            " ...\n",
            " [0.00000000e+00 1.00646897e-01 0.00000000e+00 ... 3.10810297e-02\n",
            "  0.00000000e+00 6.32957858e-02]\n",
            " [0.00000000e+00 2.80108371e-02 1.18012187e-01 ... 5.15467578e-01\n",
            "  1.87606299e-01 5.69283623e-02]\n",
            " [6.00161440e-01 2.72240932e-02 8.54565221e-01 ... 0.00000000e+00\n",
            "  6.45485387e-02 0.00000000e+00]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 1.48386641e-02 -1.63094780e-03 -3.41903145e-02 -3.38298006e-02\n",
            " -2.29892559e-02  1.37723096e-03  7.68976618e-02  7.00575424e-03\n",
            "  8.46272648e-02 -1.96599055e-02 -5.36353539e-02 -1.17424519e-02\n",
            "  3.04882248e-02  4.12294704e-02  6.37600167e-03  5.25703607e-03\n",
            "  2.46452982e-02 -1.52207817e-02  2.36609791e-03  3.57231576e-02\n",
            " -1.13404877e-02 -1.74149953e-02 -1.41576910e-03  2.60147584e-02\n",
            " -2.96276370e-02 -1.72338226e-02  1.54358866e-02  1.37826207e-02\n",
            " -1.21459661e-02 -7.06274904e-02 -1.86575150e-03 -4.65476193e-02\n",
            " -2.68218949e-02 -3.07553796e-02  4.87145739e-02 -5.26955556e-02\n",
            " -5.92907173e-02 -1.17355203e-02 -4.41398673e-03 -1.43316046e-02\n",
            "  2.99948121e-02 -3.83975487e-02 -5.28225785e-02  3.52265432e-02\n",
            "  3.07971571e-03  6.10320615e-03 -3.91429995e-02 -1.93650209e-02\n",
            "  1.39154131e-02  7.41207248e-03 -2.42948671e-02  6.71438691e-04\n",
            " -5.17134698e-02  4.62595526e-03  4.84869922e-02 -1.67956050e-02\n",
            " -1.43779957e-01  2.61388696e-02 -2.50936358e-02 -6.74446982e-02\n",
            " -1.77924164e-02  6.65468699e-02  5.72748346e-02  2.13965428e-02\n",
            "  2.48100512e-02 -3.26505421e-03 -4.09426910e-02 -5.27482513e-02\n",
            " -1.19069781e-03 -1.71328210e-02 -6.97564916e-03  3.63694420e-02\n",
            "  9.95153678e-03  4.22477016e-02 -7.64295040e-02 -9.92056661e-02\n",
            "  8.87598054e-03  4.49097725e-03 -1.46815176e-03  4.04390203e-02\n",
            " -2.70111658e-02  1.07005352e-02 -9.65861014e-03 -1.91603249e-02\n",
            "  1.66138725e-03 -1.44221784e-02 -3.07276042e-02 -5.80084641e-02\n",
            "  2.57784524e-03  1.92404995e-02  2.35188611e-02 -1.03309683e-02\n",
            "  3.88226061e-02  3.03219477e-03 -6.49492103e-03  4.36115606e-02\n",
            " -6.61138478e-02 -1.53519368e-02 -6.65764751e-02 -4.40191575e-02\n",
            "  2.88853013e-02 -4.34623786e-02 -4.09942800e-02 -7.23760753e-03\n",
            "  3.06783438e-02 -1.94165635e-02  6.41467699e-02  3.50200049e-02\n",
            " -3.15793775e-02 -2.15568674e-02  3.32731881e-02  5.52333352e-02\n",
            "  1.21757960e-01 -3.10915634e-03  2.60498715e-02 -4.22774669e-02\n",
            " -5.49155294e-02 -6.60162961e-02  8.03893914e-03 -6.67428682e-02\n",
            "  2.09847665e-02  1.89812370e-02  3.43846979e-02  1.13099117e-02\n",
            " -2.89085894e-03  2.21833292e-02 -7.21351395e-02  2.97436631e-02\n",
            " -1.10993052e-01 -4.30564471e-03  4.22274364e-03  4.05373785e-02\n",
            " -3.65910088e-02 -3.12347365e-02  5.51404653e-02 -4.75666476e-03\n",
            " -1.10164957e-01 -2.30254051e-02 -3.76780089e-02  1.73845823e-02\n",
            "  2.18734435e-02 -1.08248054e-01  2.41948552e-03  1.89400241e-03\n",
            " -1.13725964e-02  1.08400573e-02 -3.84571612e-02  3.96749689e-02\n",
            " -3.20046571e-02  1.73016109e-02 -4.38164104e-02  2.63004115e-03\n",
            " -1.24978838e-02  5.24890150e-02 -2.31882597e-02  4.85062804e-02\n",
            "  4.36677720e-02 -1.61310214e-02  1.02267767e-02 -4.37098917e-02\n",
            "  6.59954064e-02 -2.15871979e-02 -2.16425765e-02  1.98751169e-02\n",
            "  7.69418880e-03 -4.63352513e-02  1.72521093e-03 -6.70727376e-02\n",
            " -4.09178201e-02  1.76801027e-02 -6.50458541e-04  2.42360513e-03\n",
            "  1.00222374e-02  1.24799359e-02 -4.20413115e-02  4.91449171e-02\n",
            "  2.76716999e-02 -6.25783162e-02  1.52749272e-02 -2.66346370e-02\n",
            "  5.38567302e-03 -1.95955761e-03 -9.92435292e-03 -4.76522832e-02\n",
            " -2.35203150e-02  2.52746837e-02  1.19200510e-02  4.38040744e-02\n",
            "  3.19445238e-02  3.66245291e-02 -4.43361450e-02 -1.23212490e-02\n",
            " -3.57317699e-02 -5.63521628e-02  3.04073374e-02  1.47821539e-02\n",
            "  4.14364825e-02  7.13994138e-03 -1.64198754e-02 -4.19565205e-02\n",
            " -1.91145505e-02  1.62673480e-02 -1.09441218e-03 -1.67000031e-02\n",
            " -1.81604618e-03 -1.23116467e-02  2.91227904e-02  2.60878845e-03\n",
            "  1.73541609e-02  2.46218447e-02 -8.60555010e-03  7.55039852e-02\n",
            "  1.53696770e-02 -2.44456791e-02 -3.94812222e-02  1.28898073e-02\n",
            "  1.28898073e-02  1.95140144e-03 -8.91188385e-02 -7.93665840e-03\n",
            " -3.76157167e-02  1.10067317e-02 -4.63787407e-03  3.21687770e-03\n",
            "  2.66768744e-02  3.81254636e-02 -4.28852755e-02  6.89682562e-02\n",
            "  3.77842824e-02  1.71711346e-02 -6.46414802e-02  4.90998959e-02\n",
            "  7.00020789e-02  4.14330182e-02  5.44050954e-02  3.37587761e-02\n",
            " -2.65765510e-02 -4.47238955e-03  3.82279914e-02  2.21219573e-02\n",
            "  2.63666379e-02 -4.98941672e-02 -2.15978545e-02  5.42495149e-02\n",
            "  4.09444402e-02  1.74678708e-02 -9.56006354e-03  4.74103214e-02\n",
            "  3.54019635e-02  6.93269100e-02  1.39592474e-02  3.24132358e-02\n",
            "  8.58557813e-02 -1.48226777e-02 -4.03213038e-03 -9.02118111e-03\n",
            " -2.85662196e-02 -2.95987497e-02 -1.85088398e-02 -5.34247246e-03\n",
            " -3.18128059e-02  2.20610884e-02  3.16146738e-02  9.07147905e-03\n",
            " -4.27917008e-02 -6.86300381e-03  2.28389204e-02  3.80522829e-02\n",
            "  5.74644605e-02  3.99966640e-03  2.24500250e-02 -5.69816548e-02\n",
            " -5.30773083e-03  5.66285302e-03 -2.98653112e-02 -2.32675435e-02\n",
            "  2.58660252e-02  6.14755366e-02 -2.19128968e-02 -3.11912666e-02\n",
            "  1.46097787e-02  1.15060403e-02 -3.35510579e-02 -8.95020739e-02\n",
            "  1.65516774e-02  2.71442410e-02 -4.42486040e-02 -1.99026049e-02\n",
            "  3.20449395e-03  1.07068832e-02 -9.24271608e-02 -2.36490571e-02\n",
            "  1.68175852e-03 -2.62922925e-02 -1.66457646e-03 -3.19080011e-02\n",
            " -4.50122674e-02 -9.72127674e-03 -4.72903913e-02 -3.03100553e-02\n",
            "  2.09690021e-02  2.64614517e-04  2.29326054e-02 -4.75707749e-03\n",
            "  3.13149664e-02 -2.04774771e-02 -7.40451958e-02  3.93016021e-04\n",
            " -3.16165552e-02 -3.15659016e-02 -4.21814032e-02 -2.90244187e-02\n",
            "  1.30800080e-02 -1.82635058e-02 -1.20954009e-02 -7.84710046e-02\n",
            "  5.94351240e-02 -4.80500494e-02  3.67251228e-02 -2.08318708e-02\n",
            "  1.63431451e-02 -3.24501952e-02 -1.06684271e-02 -3.63605399e-02\n",
            " -3.19854207e-02 -3.38120545e-02 -7.85651684e-03 -1.01149430e-02\n",
            " -2.84728260e-02  4.38852210e-03 -4.72889475e-02 -6.71909096e-03\n",
            "  1.02462280e-02  1.84466948e-02 -4.31656234e-02  8.34119917e-03\n",
            " -1.15432780e-02 -5.02274505e-02  1.83093852e-02 -2.74318068e-04\n",
            "  4.24268858e-03 -2.16117912e-02  1.23744607e-02 -2.24556005e-02\n",
            " -2.11265951e-02 -6.35103036e-02 -7.33665646e-03  5.21171897e-03\n",
            " -3.24086723e-02  1.18439183e-02  9.74599501e-05  4.50095826e-03\n",
            " -2.92155694e-04 -2.59852221e-02 -1.92172462e-02 -5.23956824e-02\n",
            " -1.34552470e-02  2.70642931e-02  5.65254372e-02  4.12938886e-02\n",
            " -2.36795036e-03 -2.18308316e-02 -2.99429201e-03 -1.14047600e-02\n",
            " -1.17555142e-02  1.05920319e-03 -3.88628271e-02 -5.50032568e-02\n",
            "  5.05295314e-02  1.32696514e-02 -1.45081626e-02  6.91603428e-03\n",
            " -1.87752270e-02  3.54798158e-02 -8.01143750e-03 -6.80157961e-03\n",
            "  1.91228894e-02  3.03869547e-02 -2.73438023e-02 -1.45483217e-02\n",
            "  6.12451862e-03 -5.88719121e-02 -8.52198710e-02 -3.09422971e-02\n",
            " -2.93490781e-02  1.44919463e-01 -3.52880495e-02 -2.72126886e-02\n",
            " -1.06888082e-02  3.64844165e-02 -4.40618261e-02 -7.12005284e-03\n",
            " -4.07785172e-03 -1.78893280e-02  1.53131512e-02 -4.53928267e-02\n",
            " -3.31480644e-02  2.73722607e-02 -1.09370372e-02 -8.48069788e-04\n",
            "  7.63369509e-02  1.14252481e-03 -4.42168957e-02  8.43711951e-03\n",
            " -9.79249811e-02 -2.82675330e-02  1.98238600e-02  2.73372630e-03\n",
            " -2.04705141e-02 -1.25407333e-02 -2.73372495e-02 -1.23049174e-02\n",
            " -3.92895090e-02 -1.70719731e-02  4.95657975e-02  1.13453187e-02\n",
            " -1.17566854e-02 -1.19225676e-02 -5.28864566e-03  4.95081374e-03\n",
            "  2.82254019e-02 -1.07218361e-04 -3.47894668e-02 -6.27772256e-03\n",
            "  5.74683960e-03 -5.88011739e-03 -1.73876291e-02 -3.46084251e-02\n",
            " -5.95859641e-02  9.97842588e-04 -1.50230632e-02 -2.85643261e-02\n",
            "  1.20436607e-02  2.74393863e-02 -1.60570149e-02 -1.17916644e-02\n",
            " -4.37825914e-02 -2.40449222e-03  3.45144694e-03 -1.46930209e-02\n",
            "  1.26848663e-02  3.61495036e-02  2.00204437e-02  1.27983660e-02\n",
            " -7.87590143e-02  2.30700899e-02 -2.28088867e-03 -4.72955275e-02\n",
            "  7.97735610e-03  1.62456264e-02  6.09566871e-03  1.48592894e-02\n",
            "  3.28062150e-02  1.17789203e-03 -9.95319695e-05 -2.12080591e-02\n",
            " -2.21078147e-02 -3.04245659e-02  2.21624722e-02 -2.62522142e-02\n",
            "  3.43235720e-02 -1.14263894e-02  2.85780840e-02 -3.37611194e-03\n",
            " -1.67860922e-02  2.12989315e-02  3.22382898e-02  1.59603230e-02\n",
            " -4.54101751e-02 -1.29286228e-02 -1.29286228e-02 -2.92116812e-02\n",
            " -1.03447095e-01  1.37150528e-02 -2.83417114e-02 -1.26164821e-02\n",
            " -3.23507423e-03 -6.01720938e-02 -1.09098108e-02  3.96406631e-02\n",
            "  2.72615461e-02  6.11074669e-03  7.17748816e-02  1.01314289e-02\n",
            " -3.06686649e-02  1.75028582e-02 -5.17202786e-03 -2.97662639e-02\n",
            "  1.14434291e-02  1.52928492e-02 -1.20738261e-02 -1.33126966e-02\n",
            " -2.41201536e-03  1.57874697e-02 -4.71320531e-02 -1.98746218e-02\n",
            "  1.27442487e-03 -1.48177894e-02  3.48361923e-02 -5.77966743e-03\n",
            " -3.19083426e-02  1.96447222e-02 -1.44214971e-02  7.48779779e-03\n",
            " -2.01996108e-02 -2.74102603e-02  3.94739582e-02 -9.72905665e-03]  - intercept :  1.0056869059391877\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.3735991248747409\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:10,157]\u001b[0m Trial 158 finished with value: -0.024344548550238345 and parameters: {'count_threshold': 7, 'postag': True, 'voc_threshold': 5799}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.29657205 0.17873939 0.35698789 ... 0.16639869 0.05180243 0.02319654]\n",
            " [0.4881493  0.0232387  0.71298951 ... 0.15320812 0.05620442 0.0074096 ]\n",
            " [0.         0.04284242 0.01309636 ... 0.07989797 0.42056005 0.096859  ]\n",
            " ...\n",
            " [0.72227385 0.01148544 0.85935483 ... 0.132427   0.03349728 0.        ]\n",
            " [0.0178463  0.03775624 0.05757698 ... 0.09210381 0.05676784 0.08179903]\n",
            " [0.47107997 0.31588169 0.28051382 ... 0.02280897 0.0209358  0.0046141 ]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 5.90708416e-03 -6.80122754e-03  1.33597844e-04  3.76298210e-02\n",
            "  8.99702023e-03 -2.06816929e-02 -2.54992193e-02 -1.93049780e-02\n",
            " -3.11282251e-03 -1.09080127e-02 -1.89960651e-02 -1.90181922e-02\n",
            "  5.16937390e-03 -1.95113548e-02 -2.48973369e-03 -2.62550192e-02\n",
            "  2.23837481e-02  8.31442128e-03 -5.33428575e-02 -2.13592896e-02\n",
            "  2.34992656e-02 -5.17636751e-02 -6.58029939e-03  1.51908962e-02\n",
            "  1.49231050e-02  1.10971925e-02 -1.76398522e-02 -9.32681803e-03\n",
            "  1.47257470e-02 -1.60629496e-02  2.08382450e-03  3.43339914e-02\n",
            "  4.54042219e-03 -1.77049006e-03  6.43755357e-03 -5.85934992e-03\n",
            " -2.48583900e-02 -6.36244616e-03 -2.38499593e-03 -7.46040141e-03\n",
            " -1.69588070e-02  6.24759546e-02  1.28154520e-02  1.33403125e-02\n",
            "  8.59973934e-03  6.47677079e-03 -1.14125146e-02  2.49431103e-02\n",
            "  4.03331416e-02 -5.06676862e-02  4.10753497e-02  1.04590383e-02\n",
            "  7.19296125e-03  2.16064043e-02 -1.42937398e-02 -4.39389168e-02\n",
            "  3.19935839e-02  5.30639513e-02 -3.30768713e-02 -5.43515459e-03\n",
            "  4.79635926e-02  1.30523025e-02 -7.44452261e-03 -4.87639691e-02\n",
            " -1.67391084e-02  4.50699997e-02  7.69445673e-04 -1.94752273e-02\n",
            "  3.72041150e-02  8.33696378e-03 -1.62685876e-02  2.30949456e-02\n",
            "  1.17779229e-02  2.77904929e-02 -3.03163836e-04 -2.06148535e-02\n",
            " -3.58698667e-02 -1.64046776e-02  6.31950452e-02  5.58000085e-04\n",
            " -1.81145988e-02  4.09596693e-02 -2.95640761e-02 -4.49603183e-02\n",
            "  1.03116256e-02  6.78396189e-03  5.05193238e-02  2.22268494e-02\n",
            "  1.38834588e-02  1.14442715e-02  1.08424095e-02 -6.03778041e-02\n",
            "  7.63501271e-03  1.16789435e-03 -9.66499414e-03  7.85455464e-03\n",
            " -1.18831883e-02 -6.67633302e-02 -2.46469202e-02 -5.63106152e-03\n",
            " -5.46679398e-02  1.48958293e-02  2.08211348e-03  1.35895048e-02\n",
            " -1.40607490e-02 -5.19092845e-02  2.82023803e-02  3.98604082e-02\n",
            "  2.67733913e-02 -1.10429471e-02 -1.20467435e-02 -7.32294529e-03\n",
            " -1.15416186e-01 -1.01769070e-02  7.17888412e-03  9.85092924e-03\n",
            " -5.54757364e-04 -1.88145807e-02 -2.05858968e-03  2.08040707e-02\n",
            " -3.22427534e-02  1.02882426e-02 -3.83774393e-02 -4.02335689e-02\n",
            "  8.81080285e-03 -2.00681338e-02  2.22477748e-02  3.12932679e-03\n",
            " -2.57042108e-02  8.72944710e-03  5.62238128e-02 -3.33221022e-02\n",
            "  2.40849098e-02 -1.87231184e-02  3.21221115e-02  2.56780161e-02\n",
            " -1.43404821e-02  7.06070199e-02 -1.74495035e-02  3.88090013e-03\n",
            "  3.88090013e-03  4.26086976e-02 -5.32872824e-02  1.23261818e-02\n",
            " -1.27371549e-02  9.76789472e-03  1.40393311e-03  2.51653528e-03\n",
            " -2.99834771e-02  4.19622972e-02 -2.41014022e-02  3.58465435e-02\n",
            "  4.86495734e-02  4.93459163e-02  6.13408905e-03  5.91852828e-02\n",
            " -4.79404185e-02  2.27267622e-02  2.10817679e-02  1.14130140e-02\n",
            " -2.44601471e-02  1.65922369e-02 -1.69752367e-03 -2.69214139e-02\n",
            "  4.67457274e-03 -2.10829930e-02 -3.96752460e-02 -1.38086392e-02\n",
            "  5.41056768e-03  2.88513835e-02  1.75833719e-02  7.07842686e-03\n",
            "  2.46449903e-02  6.28001584e-02  4.82963405e-02 -2.98596974e-02\n",
            " -3.95229112e-02 -6.22971987e-03  2.53127478e-02  3.48887553e-03\n",
            "  1.66517448e-03 -2.78545066e-02 -3.74380038e-04  2.69069349e-02\n",
            "  2.34437926e-02  2.11665635e-02 -9.62077903e-03 -3.62342204e-03\n",
            " -3.50558095e-03 -6.77317653e-03  2.33321222e-02 -1.01203847e-02\n",
            " -1.35988378e-02  2.17777321e-02 -2.29748558e-02 -1.42590976e-02\n",
            " -1.51044715e-03  1.77875610e-02  2.20419606e-02 -8.31825972e-03\n",
            "  6.54609827e-03  1.68335583e-02  4.90064552e-02  2.25095858e-02\n",
            " -1.12406400e-02 -1.04978633e-02  6.40005603e-02  5.20533993e-03\n",
            " -6.81147601e-02  1.88441892e-03 -2.25463603e-02  1.64402991e-02\n",
            "  1.73467171e-02 -4.74474223e-03 -1.94188887e-02 -8.97701136e-03\n",
            "  1.77669631e-02  8.73927015e-03 -1.71105330e-02 -1.96462273e-02\n",
            "  8.49057546e-03  1.71068661e-03 -4.69125839e-02  1.69461743e-02\n",
            "  2.47215650e-02  9.91148868e-03 -4.15783914e-03 -1.64190025e-02\n",
            " -1.18386516e-02 -2.30096265e-02  1.37413660e-03 -5.52017829e-03\n",
            "  4.00809004e-02  4.74370898e-03 -3.87674628e-02  3.32522275e-03\n",
            "  7.23683146e-03 -2.17677773e-02 -2.91412789e-02  3.53904827e-02\n",
            "  3.06610995e-02 -2.40029668e-03  1.49693127e-02  4.06983589e-05\n",
            "  1.84670090e-02  7.61743934e-03 -4.42681205e-03  2.33461048e-03\n",
            "  1.38952902e-03 -2.63262232e-02 -8.11957382e-03  5.89627375e-03\n",
            "  1.98308677e-03  1.48434102e-02 -9.81257109e-03 -2.78208637e-02\n",
            "  1.79787360e-02  2.10424878e-02  1.34607402e-03 -2.32832247e-02\n",
            " -2.93367262e-02  3.11411527e-02 -1.88668294e-02 -7.60429469e-03\n",
            " -9.83515467e-03 -1.43398857e-02 -2.21806989e-02  4.62745730e-02\n",
            "  2.26665152e-02 -6.64102999e-03 -4.74324365e-02 -3.26172266e-02\n",
            "  2.44764958e-02  2.46606938e-02  1.47675328e-02  5.07823358e-02\n",
            " -5.34685982e-03  1.84338188e-02 -2.37060886e-02 -1.48798431e-02\n",
            " -3.52758480e-03  5.23866722e-02  9.07844764e-03  1.25003281e-02\n",
            "  1.51176145e-02  3.54921374e-03 -1.55304768e-03 -2.08937066e-02\n",
            " -8.71901121e-03  2.92404261e-02  1.48018076e-02 -7.61859124e-02\n",
            " -1.69665404e-02 -1.11735636e-02  7.85191462e-03  2.93384768e-02\n",
            " -3.25533991e-02 -6.29429949e-03  2.89582867e-02 -2.71609949e-03\n",
            " -1.82887138e-02  1.66574060e-02  4.60711518e-03 -2.46402691e-02\n",
            "  6.92771415e-03 -8.73147714e-03 -2.74716184e-03 -4.25187277e-02\n",
            " -1.63316907e-02  4.57973361e-02  2.62326148e-02  1.50442687e-03\n",
            "  1.06371082e-02 -7.13880755e-03  6.72927201e-02  1.86783386e-03\n",
            "  2.36776277e-03 -3.07329624e-02  3.90671678e-03  2.81449067e-02\n",
            "  4.84368228e-02 -2.52545975e-02 -2.28763401e-02 -2.97719079e-03\n",
            " -3.54850200e-02 -2.82758578e-03 -6.13783917e-03  6.56755201e-02\n",
            "  1.98536772e-02 -2.83634894e-02  2.20070625e-02 -3.57039028e-02\n",
            "  5.88353693e-03 -4.49738127e-02 -2.93346243e-02 -1.87429262e-02\n",
            "  2.04010379e-02 -4.76085370e-02 -2.33055613e-02  2.39656773e-02\n",
            "  2.29451481e-02  4.85534840e-03  2.54047066e-02  2.56325659e-02\n",
            " -2.64340381e-02  1.86445136e-02  4.39590123e-03 -1.27839171e-02\n",
            " -6.79037902e-03  1.45135892e-02  2.97462259e-02  2.03452963e-02\n",
            " -4.88141717e-02 -1.76967374e-02 -1.25116667e-02  1.00938931e-02\n",
            " -4.41132338e-03 -4.66347739e-02 -1.47771551e-02 -4.98716924e-02\n",
            " -1.90758371e-02  3.64332670e-02  4.83585598e-03  5.17615859e-03\n",
            "  1.29627586e-03 -4.26578030e-02 -2.78927702e-02 -9.53344567e-03\n",
            " -2.08091567e-02 -4.21284071e-02  5.45311671e-02  3.95597876e-02\n",
            " -2.00579925e-02 -4.88240367e-02 -2.47100997e-03 -1.67807619e-02\n",
            " -1.16594812e-03 -1.21953403e-02 -2.63971705e-02  2.48024556e-02\n",
            " -1.35933821e-03  3.00481588e-04  3.00481588e-04 -2.74039238e-02\n",
            "  3.34747224e-03  2.10792552e-02 -2.37586682e-03 -5.38596411e-03\n",
            " -4.12973381e-02  1.57193997e-02 -3.82699454e-02 -2.22129912e-02\n",
            " -1.49797518e-02  5.36032939e-03 -1.17105542e-02  4.08045410e-02\n",
            " -2.97875124e-02 -1.30048361e-03  4.42085842e-04  5.15106220e-02\n",
            "  4.59191461e-02 -7.17483259e-03 -5.26454211e-02 -2.12199745e-02\n",
            " -1.15992158e-02 -2.70191726e-02 -1.02296442e-02  4.68818313e-02\n",
            " -5.54899837e-02 -4.22726575e-02 -2.39055513e-03  5.20450038e-02\n",
            "  9.86519630e-03 -1.43638850e-02 -4.13012118e-02 -2.22209812e-03\n",
            "  3.66048930e-02 -3.74014222e-02 -9.53882867e-03 -7.06262154e-02\n",
            " -8.96096701e-02  3.31549249e-02 -1.82303759e-02  8.17404272e-03\n",
            "  2.65389835e-03  4.12671437e-02 -2.95236647e-02  4.69798646e-02\n",
            " -4.49933199e-02 -4.03943568e-02  2.81296618e-03  2.19441388e-02\n",
            "  3.05956311e-02  8.74614547e-03 -1.11402039e-02  5.32625999e-02\n",
            " -3.17641936e-02  5.74516766e-02 -2.01156361e-02  2.76612240e-02\n",
            "  2.85822898e-02 -6.58850313e-03 -1.43829527e-02 -1.36459223e-02\n",
            " -7.19918620e-02  7.27240623e-03 -6.54697405e-02  8.19596128e-02\n",
            "  5.23430820e-02  6.96451569e-03 -4.60928150e-02 -1.13568878e-02\n",
            "  1.21859266e-02 -2.07987532e-03 -2.31911367e-02 -2.32658893e-02\n",
            " -1.94313533e-02 -1.60013587e-02 -1.44717959e-02  3.76370229e-02\n",
            " -3.45217514e-02  2.46450600e-02  3.47235952e-03  2.67701908e-03\n",
            " -2.71744736e-02 -2.00454214e-02  5.23951176e-03 -8.08166386e-03\n",
            " -8.35792021e-03 -3.18786186e-02  7.46329562e-03 -2.32969623e-02\n",
            " -1.37010383e-04 -1.99073264e-02  2.90452032e-03 -3.06628684e-02\n",
            " -1.61462923e-04 -3.87649425e-02 -1.22486035e-03  1.59827645e-02\n",
            " -2.14270711e-03 -5.39612429e-03  4.68376574e-02  6.53244309e-03]  - intercept :  0.6822090836282488\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.024344548550238345\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:10,724]\u001b[0m Trial 159 finished with value: -0.4262935409794812 and parameters: {'count_threshold': 7, 'postag': True, 'voc_threshold': 5361}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.00513798 0.01291153 0.0164268  ... 0.10675096 0.17716143 0.0624603 ]\n",
            " [0.01985113 0.00688328 0.07611085 ... 0.01807885 0.21934272 0.09340147]\n",
            " [0.         0.00401525 0.00552383 ... 0.00417783 1.02270233 0.07777288]\n",
            " ...\n",
            " [0.00558721 0.07742221 0.         ... 0.01044458 0.1535399  0.05731019]\n",
            " [0.09807544 0.06578991 0.14035068 ... 0.08743216 0.19916983 0.03609581]\n",
            " [0.26281816 0.0671982  0.3075934  ... 0.00929518 0.13647992 0.01601664]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-4.09526766e-02  4.27559602e-04 -2.65218237e-02  5.58765634e-03\n",
            " -8.06247163e-02 -1.13731769e-02 -5.93785737e-02  3.75613419e-03\n",
            " -2.19739250e-02  9.62504151e-05  3.94299733e-03 -8.26847455e-02\n",
            " -3.55177951e-02 -1.25146444e-02  1.45098347e-03 -2.14801646e-02\n",
            " -5.81216549e-02 -7.39796695e-02  2.27332101e-03 -3.88078016e-02\n",
            "  1.91567217e-02  1.38512947e-02  5.55073534e-03 -1.76709568e-02\n",
            " -1.06270291e-02 -4.99254067e-02 -6.06764913e-02 -1.29748786e-02\n",
            " -1.58265183e-02  1.22213861e-02 -7.06190613e-03 -4.39921676e-02\n",
            " -2.85150907e-02 -1.85083810e-02  1.32619376e-02 -1.04503914e-02\n",
            "  3.53090049e-02  1.69187512e-02  3.34308922e-02 -6.23084804e-03\n",
            "  3.37834778e-02 -9.67602356e-03  1.58952982e-02  1.59208399e-02\n",
            "  4.40115813e-03 -1.75225554e-02  2.46967966e-02  2.56053903e-02\n",
            "  2.75247092e-02  2.06010305e-02  1.25499322e-02  2.62726941e-03\n",
            " -2.38079540e-02  2.93356528e-02 -3.08839273e-02 -3.49705839e-02\n",
            "  2.16335917e-02  3.04014266e-02  1.37257962e-02  3.74627409e-03\n",
            "  1.33222274e-02  2.14055939e-02  1.33094166e-02 -6.98757204e-03\n",
            " -5.31282598e-03  1.11573794e-02  6.50693382e-03  1.29845333e-02\n",
            " -5.87259394e-02  7.01679145e-03 -1.56791081e-02  5.74975264e-03\n",
            " -9.03379468e-02  3.30715425e-03  5.19686872e-02  2.38639875e-02\n",
            " -2.51828026e-02 -2.75380955e-02  3.91853457e-02  3.27847315e-02\n",
            "  2.01844395e-02  1.11311030e-02  4.98389549e-02  2.18713040e-02\n",
            "  1.93741760e-02  4.08270292e-02  1.90562550e-02  3.91783780e-02\n",
            "  1.04147002e-02  6.42860653e-02 -3.32700833e-02  1.82936651e-02\n",
            "  2.52795338e-02  1.94775685e-02 -4.66539581e-02 -9.22646028e-03\n",
            "  3.06117999e-02  2.73773476e-02 -2.99464217e-02  4.35661254e-02\n",
            " -2.74614348e-02  2.57332998e-03 -1.39597301e-02  2.79049990e-03\n",
            " -3.33886278e-03  3.33124472e-02 -3.35787800e-02  5.77890729e-03\n",
            " -4.71200987e-03 -4.38311232e-02 -2.33260699e-02  1.47648065e-02\n",
            " -1.29224373e-02 -1.07145169e-02 -1.24440613e-02 -3.35646853e-02\n",
            " -8.94286184e-04  4.25453097e-05  2.56115735e-02 -1.55373568e-02\n",
            " -3.85152102e-02 -3.13455420e-03  1.94470691e-02  4.08371281e-02\n",
            " -3.50117292e-02  4.92865483e-02 -2.42115487e-02  1.78764005e-02\n",
            "  1.19501979e-02  3.20970894e-02 -5.04993249e-04  2.85227730e-02\n",
            "  2.53411070e-02  8.36666344e-03  2.04683506e-02  2.59263840e-02\n",
            " -2.53399577e-03 -1.29191586e-02  2.30144265e-02  2.06253910e-03\n",
            "  9.13734798e-03 -2.75736463e-02 -1.04622424e-03  1.10547281e-02\n",
            "  3.42971333e-02  3.37970990e-02 -8.44508580e-02  2.41962220e-02\n",
            "  2.57146036e-02 -3.19766944e-02  2.16840580e-02 -4.31976120e-02\n",
            "  4.32408189e-03 -1.94896111e-03  8.94032076e-05 -1.64337109e-02\n",
            "  1.84964584e-02 -1.00710570e-02  7.96829367e-03  2.06618935e-02\n",
            " -3.62904012e-02  1.12109113e-02  2.98086247e-02 -4.11350758e-02\n",
            " -1.23173790e-02  1.87241893e-02  5.36265540e-03  3.33137844e-02\n",
            "  3.45237943e-02  1.00281418e-02 -8.08667080e-03  5.82931994e-02\n",
            "  2.91026609e-02  1.14121511e-02 -4.99095662e-02  1.90648507e-02\n",
            " -2.34751170e-02 -9.41941913e-03  1.46627819e-02  2.66777113e-02\n",
            "  1.49180330e-02  3.62421192e-03  1.76924841e-02  1.29153691e-02\n",
            "  2.26069053e-02 -5.74695069e-03  1.69152789e-02 -1.59000176e-02\n",
            "  1.65483626e-02  3.38673485e-02  4.15218972e-02  2.67190614e-02\n",
            "  3.17566288e-02  2.67294076e-02  2.71660227e-02  3.96469458e-02\n",
            " -3.02162434e-02 -1.45644578e-02 -2.22988312e-02  2.68688527e-02\n",
            " -2.50573083e-03 -2.76532052e-02 -1.19386404e-02  2.87451067e-02\n",
            " -1.42776703e-02  2.39360216e-02 -1.12668413e-01  2.17246967e-03\n",
            "  1.93918133e-02  1.11212818e-02 -3.80037166e-02 -1.17251877e-02\n",
            "  1.19366735e-02 -3.23052938e-03 -1.60184528e-02  2.91051873e-03\n",
            "  1.31112296e-03 -2.88048159e-04  1.56023053e-02  8.48772502e-03\n",
            " -8.80424874e-03 -2.41962431e-02 -2.05617051e-03 -1.53584126e-02\n",
            "  1.39368376e-02  2.10740201e-02 -1.91608738e-02  1.98063216e-02\n",
            "  1.04792164e-02  1.22441156e-02 -2.50651651e-02  2.85129027e-03\n",
            "  3.24288008e-02 -3.64380205e-02  1.68033742e-03  1.58798115e-03\n",
            "  3.67932011e-03  2.28976416e-02  7.40941956e-03  1.22821177e-02\n",
            "  2.16864957e-02 -1.69690654e-02  1.31754575e-02  7.35507242e-04\n",
            "  1.72557282e-02  1.29278520e-02  9.55666589e-03  3.39236600e-02\n",
            "  3.89478520e-02  9.13329134e-03 -4.63742732e-03 -2.40476381e-02\n",
            "  4.06815097e-03 -1.91143184e-02  1.01839082e-02 -4.16262513e-02\n",
            " -1.44044660e-02 -1.17680243e-02  2.85787955e-03 -2.66808152e-02\n",
            "  1.54059252e-02  1.41028446e-02 -2.17269056e-02 -2.75862849e-02\n",
            " -7.23001504e-03 -1.59943899e-02 -1.51347320e-02 -1.95336751e-02\n",
            " -4.40944983e-02 -6.15555802e-03  4.24028807e-02  3.15040053e-02\n",
            "  8.24683797e-03 -5.57640907e-05 -2.39749332e-02 -4.39419404e-02\n",
            " -2.28873828e-02  3.62488014e-02  2.23334941e-02 -2.85064287e-02\n",
            "  1.65748353e-02  1.16813593e-02 -9.18373700e-02 -1.95101169e-02\n",
            " -4.87723070e-04  4.91510140e-03 -1.68691938e-02 -1.14224634e-02\n",
            "  2.78302112e-02  4.17144091e-03  1.48321502e-02  1.60748335e-02\n",
            " -3.06302715e-02  1.05114491e-02  3.66978317e-02  2.93353040e-02\n",
            " -1.18233489e-02  8.51179826e-03  3.29265631e-02 -1.24940202e-02\n",
            " -2.32032330e-02  2.40201995e-03  7.55834006e-04 -2.55994186e-02\n",
            " -6.13586180e-03  6.90994667e-03  2.72313310e-02  2.95135034e-03\n",
            " -5.62109900e-03  5.43190322e-03 -1.25883736e-02  4.62691972e-02\n",
            " -2.43362167e-02 -1.85474608e-02  5.38712196e-02 -3.40011534e-02\n",
            "  2.25393663e-02  3.94642948e-02 -2.96133097e-03 -2.62166911e-02\n",
            " -1.15859147e-02 -2.18329802e-02  1.21761969e-03 -2.32141010e-02\n",
            "  9.19428096e-03 -7.02957833e-03 -1.86161807e-02  3.05596865e-02\n",
            "  2.19386099e-02 -3.32931099e-02 -5.89686362e-02 -1.66514027e-04\n",
            " -7.04274794e-03 -1.42895794e-02  1.03458984e-02  1.47895306e-02\n",
            "  2.21725355e-02  1.16796542e-02 -2.05097882e-02  6.43012365e-02\n",
            "  6.22462732e-02 -3.46830745e-03 -3.72216695e-02 -6.01454227e-02\n",
            "  1.14810811e-02 -1.37561837e-02  1.50275616e-02  2.15324310e-02\n",
            "  9.99851231e-03  1.57285344e-02  1.59198074e-02 -4.91016124e-02\n",
            "  8.86800098e-02 -2.94135372e-02 -3.68542712e-02  2.79165070e-02\n",
            "  2.80416925e-02  4.50157760e-02  2.86913526e-02 -5.53046993e-03\n",
            " -3.10395198e-02  7.02215556e-03 -1.11884801e-02 -3.93996153e-03\n",
            " -7.53822552e-02 -8.79136072e-03 -3.42614572e-02 -7.11739069e-03\n",
            " -3.52645638e-03  2.39229791e-02 -2.39008016e-02  3.06680067e-02\n",
            " -5.73128463e-03 -4.13531421e-03  1.27583873e-02 -5.72517766e-02\n",
            "  6.81179802e-02 -5.39698629e-02 -1.17794892e-02 -7.83417264e-02\n",
            " -3.64864144e-02  1.58147210e-02 -1.69897511e-02 -3.91656066e-03\n",
            " -2.19948298e-02  2.02374597e-02  3.68145440e-02 -2.36537067e-02\n",
            " -3.97724467e-02  4.42121716e-02 -2.96645758e-02  1.35671099e-03\n",
            " -4.21666167e-02  5.58232363e-02  4.26205548e-02  1.05654384e-02\n",
            "  3.11147477e-02  1.85424640e-02  7.02215469e-04  8.47535887e-03\n",
            " -3.32660608e-02  1.04699483e-03  1.59325546e-02  1.58407434e-02\n",
            " -4.26013528e-02  3.91520334e-04 -2.57135685e-02 -1.41946463e-02\n",
            "  5.54373996e-03  1.79931958e-02  2.86537992e-02  2.68240512e-02\n",
            " -5.64388224e-02  2.42341121e-02 -6.78322956e-03 -9.29940199e-03\n",
            " -1.69194763e-03 -3.56240861e-02  1.20419411e-02 -1.42860064e-02\n",
            "  9.03726722e-04  1.72675974e-02  9.85482801e-03  5.05769855e-03\n",
            "  1.34789569e-02  5.13422117e-03  5.21290343e-02 -4.81763049e-02\n",
            " -1.52749916e-02  1.45796775e-02 -1.45615485e-02  2.65882097e-02\n",
            "  2.08712492e-02  1.39437671e-02 -2.55563165e-02  2.38704782e-02\n",
            "  2.29428286e-02  3.60891732e-02  2.16422096e-02 -8.97698371e-03\n",
            "  3.75274433e-02  5.31567243e-03  3.10243935e-03  5.96696257e-03\n",
            "  2.94152494e-02  1.29724399e-02 -4.38101483e-04 -1.77070255e-02\n",
            "  2.35972193e-02  2.59959750e-02 -2.94897886e-02  4.62051085e-03\n",
            "  1.41769427e-02 -1.86775844e-03  3.98148824e-03 -4.13683722e-02\n",
            "  1.80169607e-03  3.83142677e-02  1.72582560e-02  1.56663059e-02\n",
            "  9.21442689e-04 -2.88704290e-02  4.41500616e-03 -3.19061653e-02\n",
            "  1.26468342e-02  3.47350817e-02  3.20292996e-02 -5.06168403e-05\n",
            " -9.69436309e-03  2.00643391e-02  5.31312917e-02  1.74208377e-02\n",
            " -2.22014049e-02 -1.99074476e-04  1.11287813e-02 -9.03368982e-03\n",
            "  3.53500031e-02 -4.33376005e-03  1.26714082e-02  1.72404804e-02\n",
            "  9.98589822e-03  2.00790108e-02 -2.15264263e-02  2.03633026e-02\n",
            " -1.42185110e-02  3.29593822e-02  4.10495834e-03 -9.01787939e-04\n",
            "  1.44775606e-03 -2.64387527e-04  2.23692093e-02 -2.02493703e-02\n",
            " -1.45936927e-02  2.06460515e-02  1.10160806e-02  4.01368220e-02\n",
            " -4.57690758e-02  4.04081917e-02  2.54880807e-02  9.17409947e-02\n",
            "  1.35448937e-02 -4.71661842e-03]  - intercept :  0.5404975398861167\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.4262935409794812\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:11,339]\u001b[0m Trial 160 finished with value: 0.08110821445188969 and parameters: {'count_threshold': 7, 'postag': True, 'voc_threshold': 6151}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.         0.0550017  0.01524545 ... 0.0087004  0.         0.0599627 ]\n",
            " [0.03260445 0.12460945 0.05247984 ... 0.04072853 0.20865561 0.01628004]\n",
            " [0.07347554 0.06014834 0.07797617 ... 0.13640588 0.08599323 0.02695562]\n",
            " ...\n",
            " [0.37890547 0.08554418 0.41901075 ... 0.05599857 0.5749146  0.00553222]\n",
            " [0.04195839 0.08904881 0.06169669 ... 0.04799469 0.51626848 0.01640332]\n",
            " [0.52691233 0.02367365 0.89924405 ... 0.00174008 0.38036013 0.00140956]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-8.59946678e-03 -2.17262478e-03 -3.15831338e-02  4.16765918e-03\n",
            "  9.13756006e-03 -1.81316914e-02  4.17095360e-03 -2.17884644e-02\n",
            " -1.22903211e-02 -2.58597035e-04  1.12339133e-02 -2.84844802e-02\n",
            "  6.23684348e-03 -2.56506207e-02  1.03783989e-02  1.35291819e-02\n",
            " -2.37119031e-02 -9.21973793e-03 -5.48786021e-03  1.25871553e-02\n",
            "  5.35604044e-04 -1.68457846e-02  4.93756401e-02  5.33144574e-03\n",
            " -3.65519058e-02  1.00286090e-02 -2.37515309e-03  3.23742833e-02\n",
            " -5.56478549e-03  4.38926691e-02 -3.49885035e-02  2.18220620e-02\n",
            "  1.85971359e-02 -3.33176861e-02  2.90576691e-02 -1.20062690e-04\n",
            " -3.74645531e-03 -3.68330656e-02  3.74675939e-02  3.84719215e-02\n",
            " -3.04008381e-02 -3.29031742e-03  1.02153819e-02 -1.97741859e-02\n",
            " -5.94512650e-02 -1.28126558e-02  1.83935812e-02 -6.11523255e-03\n",
            " -8.78485646e-03  5.71484123e-03 -4.90347117e-03 -2.93109577e-02\n",
            " -1.03860145e-02  2.56375368e-02  3.64309372e-02  4.12550014e-03\n",
            "  4.81733004e-02  2.16040335e-03  1.08660063e-03 -2.01924573e-03\n",
            "  2.57923000e-02 -1.04909200e-02 -4.98707557e-04 -1.27912451e-02\n",
            " -5.47013248e-02 -1.66219289e-02  2.70877572e-02 -3.93723173e-02\n",
            " -6.32920313e-03  3.13652657e-03 -5.79635929e-05 -3.46118859e-02\n",
            " -1.72439247e-03  8.46063494e-03  4.33693088e-02  1.12298639e-02\n",
            "  1.37080853e-02 -2.28410813e-02  5.76095557e-03  9.50953437e-03\n",
            "  6.93119176e-03  4.40219901e-02 -4.38825046e-02  2.65072357e-02\n",
            "  1.25099364e-02  1.30857847e-02 -1.47818308e-02  2.69506711e-02\n",
            "  3.58097169e-03 -2.71209838e-02  1.06502229e-02  3.09256497e-02\n",
            "  3.09256497e-02  1.13232338e-02  7.56440867e-03  2.69612348e-02\n",
            " -1.14674053e-02  1.39542219e-02 -2.61682486e-02  1.48994869e-02\n",
            "  1.08280638e-03  8.86438862e-05  1.59647696e-02  6.15374929e-03\n",
            " -7.31766491e-03 -3.81619002e-02  1.19705371e-02 -1.23806103e-02\n",
            "  2.02516669e-02 -5.56150670e-03  4.63057835e-03 -9.47310947e-03\n",
            "  1.22225819e-02  7.25564925e-03 -1.23882791e-03  1.03346512e-02\n",
            "  1.38395313e-02  1.92126314e-02 -1.74660926e-02  4.35607838e-02\n",
            " -5.19461949e-03 -2.13566800e-02  5.81716303e-02 -4.87484314e-03\n",
            "  2.57123760e-02  1.35646360e-02 -2.89348990e-02 -7.68812677e-02\n",
            " -2.08529610e-02  5.91299903e-02 -2.06499449e-02 -1.67804214e-02\n",
            "  1.22114147e-03 -2.08022376e-02  1.94883293e-02  1.41987130e-02\n",
            "  4.95782271e-04  4.01164526e-02 -1.86558512e-02 -7.07848620e-03\n",
            " -4.47430474e-02 -2.71371963e-02 -1.39091397e-02 -1.58119741e-02\n",
            " -5.43596135e-03 -1.90474400e-02  3.93421836e-03 -5.60274464e-04\n",
            " -5.89578410e-02  5.74409685e-02 -3.00032181e-02  4.74458399e-02\n",
            " -7.29936834e-02  5.54011649e-02 -2.33308871e-02 -6.57532416e-02\n",
            "  1.76913717e-02  1.24743461e-02 -5.28497252e-02 -2.67637294e-02\n",
            "  3.18534284e-02  3.12908845e-02 -2.28516884e-03 -1.29667084e-02\n",
            " -1.43000111e-02 -3.09167461e-02 -4.67217988e-03  1.93797036e-02\n",
            " -6.83302557e-02 -1.14508513e-02 -8.22275899e-05 -1.15561779e-02\n",
            "  5.41151345e-03 -5.78630230e-03 -2.04972574e-02  1.98769235e-02\n",
            "  1.69705222e-02  3.00308491e-03  4.18794877e-02  6.73294426e-03\n",
            " -5.29750593e-02 -8.43204359e-02  3.91010899e-03 -3.88443797e-02\n",
            "  4.13582295e-02 -2.24825277e-02  4.57272270e-03 -2.15396246e-03\n",
            " -1.90708148e-02  6.24790168e-03  2.96387016e-04  1.84918285e-02\n",
            "  4.56759128e-03  2.05452124e-02 -3.06044003e-02 -3.55769452e-02\n",
            " -1.25220658e-02  1.06883249e-02 -4.52309196e-03  1.10086120e-02\n",
            " -3.15252389e-02  6.50355263e-02 -2.36120671e-02  2.15586073e-04\n",
            " -3.22063632e-02 -2.89122944e-02 -3.19057788e-02 -2.17047020e-02\n",
            " -7.51154812e-02 -3.32347599e-02  7.68296046e-04  1.70921644e-02\n",
            "  2.81431445e-02 -1.33599987e-02  3.85128651e-02 -4.93346448e-02\n",
            "  1.91974336e-02  3.69448120e-02 -5.78513610e-03 -4.78274440e-02\n",
            "  5.10192890e-02 -2.43014414e-02  2.18655614e-02 -3.32449594e-02\n",
            " -2.03451531e-02 -4.45292327e-02  1.30126021e-02  3.53484979e-02\n",
            " -4.76665559e-02 -1.44757160e-02  1.20241651e-02 -7.51254709e-02\n",
            " -1.88309136e-02 -9.69398567e-03  6.84638649e-03  4.30291324e-02\n",
            " -3.11080527e-02 -2.59484545e-03  2.68034149e-02 -2.33600548e-02\n",
            "  4.77403361e-02  5.34121247e-02 -7.39508428e-02 -3.21922354e-02\n",
            "  2.19831829e-02  2.46339912e-02 -4.87266455e-02 -7.11063942e-03\n",
            "  2.48430099e-02  8.48961798e-04 -1.15182445e-02 -1.30769576e-02\n",
            " -6.51427482e-04  1.31277002e-02 -2.07146036e-02 -2.09344738e-02\n",
            " -3.09136499e-03 -5.10828715e-03 -1.22179440e-02  5.57118707e-03\n",
            "  1.75221856e-03 -3.17907771e-02  3.45457143e-03  1.41220249e-02\n",
            " -6.14747941e-03 -3.10894507e-03 -2.48304795e-02  3.97577238e-02\n",
            " -1.74728292e-02 -2.38042072e-03  5.41604977e-03 -6.12830994e-03\n",
            " -1.37838177e-02 -1.06978095e-02 -2.32493389e-02 -3.62460451e-02\n",
            " -3.73459817e-03  8.74612388e-03 -2.49750153e-02  4.03026437e-02\n",
            " -3.25890696e-02  1.35343109e-02  2.89905400e-02 -6.26730701e-04\n",
            " -6.80152792e-03 -4.59130387e-03 -1.99491931e-02 -5.94576974e-02\n",
            " -1.65244552e-02 -1.07664326e-02  1.03784534e-02 -5.91921208e-02\n",
            "  4.06328053e-02  1.06373327e-04 -9.46346386e-03  1.06547256e-02\n",
            "  2.05328703e-02 -3.05147651e-02 -5.21127741e-03 -1.55024241e-02\n",
            "  1.13196501e-02  4.93300684e-02  1.69446611e-02  4.51913664e-02\n",
            "  2.14114409e-02 -6.63184627e-02 -1.51783649e-02  3.49795058e-03\n",
            "  7.34527596e-03 -8.26907398e-03 -3.67369168e-03 -1.37542384e-02\n",
            " -1.96542656e-02 -1.41858597e-03 -4.03603914e-02 -3.44149086e-02\n",
            "  2.31126137e-02  2.77044257e-02 -9.09676559e-03  2.44367709e-02\n",
            "  5.11603744e-02 -2.81174230e-02 -7.41962397e-03 -2.17884434e-02\n",
            "  3.18732935e-02  2.23303900e-02 -2.66374830e-02  4.82638843e-02\n",
            " -2.30459429e-03 -2.98060915e-04  4.32847960e-02  2.52513535e-02\n",
            " -1.55137603e-03  1.45003613e-02  3.49486494e-02  5.67868282e-02\n",
            " -3.22496378e-02 -2.36508937e-03 -7.47651687e-04  3.57035971e-02\n",
            " -7.11929316e-03 -8.05044284e-03 -8.05044284e-03 -2.99420922e-02\n",
            "  4.70681093e-02  8.14653123e-03 -4.28742843e-02  2.07734677e-02\n",
            "  9.09914625e-03  2.77480972e-02 -7.25616668e-03  4.48661006e-02\n",
            "  2.74577540e-03  1.80209804e-02  3.22516299e-03 -4.53919606e-02\n",
            "  2.15043589e-02  2.90699202e-02 -5.09157028e-03 -1.32208103e-02\n",
            " -9.98617228e-03  3.29120699e-02 -2.35189020e-02  6.83605666e-03\n",
            "  2.37799636e-03  1.87327817e-02  6.22169309e-02 -6.49323858e-04\n",
            "  6.94738514e-03  1.63047008e-02 -8.97644310e-03 -3.76772018e-03\n",
            "  4.02346462e-02 -1.46459673e-02  4.61295907e-02  3.14521944e-03\n",
            " -1.65209883e-02 -5.50861239e-02 -9.14866216e-03  2.32048917e-02\n",
            "  1.99121758e-02  3.02774632e-02  1.21365794e-02  2.26221435e-02\n",
            " -1.02605918e-02  2.51913160e-03  2.14033466e-03  1.48537823e-02\n",
            " -2.38131502e-02 -5.12742449e-02  2.07634077e-02 -2.57587175e-03\n",
            " -7.86538792e-03  5.69693387e-04  4.74794244e-02  5.01595938e-03\n",
            "  1.94137093e-02  7.98689854e-03 -7.01455957e-02  8.79005127e-02\n",
            " -9.87948105e-03  5.13280528e-02 -4.21735784e-02  4.41912198e-02\n",
            "  4.28051607e-02 -7.80501233e-02 -1.55691680e-02  2.84406570e-02\n",
            "  2.50901498e-02 -2.60551022e-02  2.86006847e-02  1.14429621e-02\n",
            " -3.01956568e-02  2.19821205e-02 -3.29742870e-02 -1.47022703e-02\n",
            "  3.05033995e-02  3.48766507e-02  1.61008022e-02 -8.55226690e-02\n",
            "  1.83250465e-02  5.52436173e-02 -4.71255026e-02  3.56240609e-03\n",
            " -2.98746093e-02  6.08217487e-02  8.68956460e-03 -4.14799023e-02\n",
            "  3.37882980e-02  2.85073010e-02 -6.56886350e-03 -5.69782541e-02\n",
            "  4.46272306e-03 -5.98055166e-03  1.61616047e-02  2.38050562e-02\n",
            " -3.15608731e-02  1.57577450e-02  3.97556805e-02 -4.68296168e-02\n",
            " -2.09812517e-03 -4.39862141e-02 -2.70816981e-02  1.97236881e-02\n",
            "  1.12310799e-02  1.85854968e-02  2.05350160e-02  4.22723827e-02\n",
            "  1.97039615e-02  2.24473253e-02 -1.54871084e-03  3.70719405e-02\n",
            " -8.47135109e-03  1.19089556e-02 -6.14601197e-03 -7.11794280e-03\n",
            "  1.53683253e-02  3.66604767e-02 -3.92349480e-02 -1.15018616e-02\n",
            "  1.27763136e-02  4.70465552e-02  1.73218261e-02  2.07043942e-02\n",
            "  2.77888036e-02  7.75803266e-03  8.39771332e-03 -9.56739783e-04\n",
            " -9.95262200e-03 -3.73375540e-02  6.32364882e-03  2.80881927e-02\n",
            "  2.01471307e-02 -4.32099212e-02 -1.35671853e-02 -1.06712681e-02\n",
            " -6.86223351e-03  1.49456363e-02 -4.33171148e-03 -2.67069084e-02\n",
            " -1.30384486e-02 -7.08710267e-03 -9.18102538e-02  1.33572034e-02\n",
            "  1.80754362e-02 -3.76181849e-02 -2.42348575e-02 -1.60283518e-02\n",
            "  5.39486639e-02 -3.34025677e-02  2.55697757e-02  4.15002764e-02\n",
            " -3.27356166e-03 -1.23693893e-03  3.62174194e-03  1.85574413e-02\n",
            " -3.06910042e-03 -2.26880761e-02  3.58113065e-02 -2.89633829e-03]  - intercept :  0.7061821122396664\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.08110821445188969\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:11,898]\u001b[0m Trial 161 finished with value: 0.1262520457619713 and parameters: {'count_threshold': 7, 'postag': True, 'voc_threshold': 5863}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.17944991 0.08289269 0.17785459 ... 0.30833412 0.0175376  0.01257444]\n",
            " [0.         0.00559972 0.         ... 0.         0.01380532 0.06434829]\n",
            " [0.         0.02812397 0.00128243 ... 0.         0.07486209 0.05921115]\n",
            " ...\n",
            " [0.         0.0526193  0.         ... 0.         0.0961347  0.06129272]\n",
            " [0.28932199 0.11089445 0.33020991 ... 0.54584307 0.01375265 0.01034126]\n",
            " [0.18075231 0.01950818 0.33373914 ... 0.98421075 0.25198164 0.01706443]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 7.70989928e-03 -1.64561116e-02  1.80474202e-02 -1.02153772e-02\n",
            " -2.97937925e-02  4.89397579e-03 -2.76702966e-02  8.23948673e-03\n",
            " -1.61885341e-02  1.42448061e-02  4.40286030e-03  6.34094063e-03\n",
            "  3.10092882e-03  2.37153703e-02 -2.36231341e-02 -2.82525078e-02\n",
            "  2.83571346e-03 -9.40753762e-03  3.30144170e-03  9.39398549e-03\n",
            "  1.16230595e-02  6.78821905e-03  1.39496660e-02  1.02621309e-03\n",
            "  3.67074951e-04  5.27135675e-02 -4.60777138e-03 -4.26960235e-02\n",
            "  2.68243780e-02  1.82224789e-02  2.53670043e-02 -1.33615553e-02\n",
            " -2.23440192e-02 -8.61519426e-03 -3.95884140e-02 -1.25990228e-03\n",
            " -4.45724651e-02 -5.63723479e-02 -4.87353908e-02 -1.10317390e-03\n",
            "  3.76537156e-03  1.56720765e-02  5.83102592e-03  3.00019578e-02\n",
            "  4.54670711e-02 -1.92764578e-02 -4.46870228e-03  1.83614417e-02\n",
            "  2.76683462e-02 -3.02979636e-03 -2.76564513e-02 -1.36018962e-02\n",
            "  1.49616560e-02 -9.20836593e-03  1.09473958e-02 -2.03923145e-03\n",
            "  8.40665758e-02 -2.23735846e-02  1.26732205e-02 -1.37369946e-02\n",
            " -1.20482703e-02  5.41095579e-03  1.53933492e-02  3.60146024e-02\n",
            " -2.88876500e-02 -1.25478153e-02 -1.09073883e-02  1.38314881e-02\n",
            "  1.49267762e-02  3.20971450e-02  9.68294459e-03  1.26719213e-02\n",
            "  1.49055808e-02  4.25075227e-03  3.63225320e-02  1.30334697e-02\n",
            " -2.18284460e-02 -7.58866479e-02  2.60502299e-02  1.62432101e-02\n",
            "  1.99546700e-03  3.77988441e-02 -2.82597249e-03 -8.06650126e-03\n",
            " -9.74118887e-04  4.67689887e-02  2.36552816e-02 -2.14804444e-02\n",
            " -1.39689907e-02 -1.84499287e-02 -1.83165494e-03  1.91570841e-02\n",
            "  1.82577917e-02  2.28482253e-02  3.15267441e-02  2.61239232e-02\n",
            "  1.48308058e-03 -8.51913982e-02  1.87601320e-02 -1.24512298e-02\n",
            " -7.26318848e-02  1.70998629e-02  6.28433054e-03  1.29237166e-02\n",
            " -1.84701965e-02  4.07255468e-02 -2.47803272e-02 -1.71893872e-02\n",
            " -7.52751164e-03  3.59799125e-02  4.02712379e-02  5.26773179e-02\n",
            " -1.08717612e-02 -3.90965217e-02 -3.70438578e-02  6.73130301e-04\n",
            "  2.92711355e-02  1.72760501e-02  2.81357373e-02  1.61472741e-02\n",
            "  3.64489582e-02 -1.72306157e-02 -8.18007965e-03  6.92909551e-03\n",
            "  2.67638670e-03 -5.00302967e-02  6.11556191e-03 -2.75187544e-03\n",
            " -2.82782339e-02  2.18354220e-02  2.49192309e-02  6.34969925e-02\n",
            " -4.71998867e-02 -3.73423410e-02  4.99979586e-03  6.75707031e-02\n",
            "  2.90914190e-02  1.53763682e-02  3.32089592e-02  7.72584842e-03\n",
            "  1.44853566e-02 -5.85477475e-04  2.26505428e-02  1.40913215e-02\n",
            " -5.68376154e-03  1.48374694e-02  9.53466160e-02  3.69721338e-02\n",
            " -1.38820306e-02  1.90028014e-02  2.85080710e-02  2.14333877e-02\n",
            "  2.53537595e-02  2.26085750e-02  2.04842112e-02  4.37665030e-02\n",
            "  9.34472099e-03  2.09755886e-02 -3.49985693e-02  1.61552541e-02\n",
            " -2.78046562e-02 -4.46875548e-03  1.57019759e-02  2.36370481e-03\n",
            "  4.84887889e-03  2.79773541e-02  1.49518691e-02  1.49518691e-02\n",
            "  1.58568586e-02 -3.82690760e-02  1.05727535e-03 -1.30594417e-02\n",
            " -4.39512140e-02 -1.69134161e-02  5.61003910e-03  6.97954540e-03\n",
            "  3.73033643e-03 -3.99908667e-02  1.90803420e-02  4.66655933e-03\n",
            " -1.19228271e-02  3.73329530e-03  1.77128673e-02  7.45203009e-03\n",
            " -3.23118160e-02  1.89464242e-02 -3.31700370e-02  4.16842451e-02\n",
            "  4.35702238e-02  1.45057803e-02  5.05032977e-02  4.62921054e-02\n",
            "  4.75154330e-03  7.46846661e-03  2.28879485e-02  2.43032720e-02\n",
            "  3.48346202e-02 -2.02378335e-03 -1.31127403e-02  4.74471890e-02\n",
            "  4.20800218e-04 -7.60391856e-02  8.28996988e-03 -2.17892023e-02\n",
            "  1.14637845e-02  9.81591677e-03 -5.24091496e-03  2.20041054e-02\n",
            "  2.09006008e-02  7.43984614e-03  6.10818566e-03  1.66657304e-02\n",
            " -1.98597316e-03  2.04747756e-02 -9.97170775e-03 -5.87814151e-03\n",
            " -1.71294557e-02  2.43900846e-02  3.33391561e-02  1.12601050e-02\n",
            "  1.04830353e-02  8.09299817e-04 -3.57538867e-02  1.69416787e-02\n",
            "  2.92522984e-02  4.53390787e-02  4.44877173e-03 -4.48698007e-02\n",
            " -8.40160547e-03 -4.19843160e-03  2.45172040e-03  1.71179632e-02\n",
            "  1.77690610e-02 -2.58939835e-02  1.78559128e-02  4.13892017e-02\n",
            "  5.36984531e-03 -1.27796269e-02  2.07166325e-02 -8.56666144e-02\n",
            "  2.72636278e-02  3.79852472e-02 -1.99380621e-03 -3.30850198e-02\n",
            " -5.91790242e-03 -8.23371228e-02  7.63944132e-03 -8.13303821e-02\n",
            "  1.72889081e-02 -5.16491498e-02 -1.00738143e-02 -7.67723932e-03\n",
            " -3.51492265e-02 -1.16874813e-02 -1.44637993e-04 -6.22678107e-02\n",
            "  1.04166249e-02 -1.42359034e-02 -1.22481784e-02 -7.27301027e-03\n",
            "  5.60466478e-03  1.06478894e-03 -5.09704932e-02  1.82892955e-02\n",
            " -3.85506410e-02  9.23038356e-03  1.55347608e-03 -8.43700976e-02\n",
            "  6.46576040e-02  3.19935575e-02 -2.62230403e-02 -1.55001159e-01\n",
            " -3.37923903e-02 -3.57612887e-02 -1.69434946e-02  3.29686033e-02\n",
            "  2.31358658e-02 -2.29882168e-02  2.00021633e-02  9.31966498e-03\n",
            " -3.87979607e-02 -4.49653116e-02  4.18286559e-03  1.33305112e-02\n",
            "  1.46397200e-03 -4.21337699e-02 -1.77717908e-02  5.48973922e-02\n",
            " -1.71247329e-02  2.56772035e-02 -3.71833384e-02 -6.79801487e-03\n",
            "  3.34880093e-02 -4.77163551e-02 -4.08039097e-02 -1.33836219e-03\n",
            "  9.27214838e-04  6.08573291e-03 -4.08755111e-02  5.12584917e-02\n",
            " -4.56182351e-02  3.38613633e-02  3.55768820e-02 -3.52433470e-03\n",
            "  4.85860087e-04  1.14235875e-02 -5.92288968e-02 -2.62040632e-03\n",
            "  1.27217811e-02  3.91660689e-02  1.73835233e-02 -4.26735022e-02\n",
            " -2.55217138e-02 -2.74083690e-02  3.79004634e-02 -7.30931532e-02\n",
            " -6.79432311e-02  3.06000234e-02  5.16000028e-02 -1.58138917e-02\n",
            " -7.68742533e-02 -5.54738955e-03 -1.97007835e-03 -1.84603132e-02\n",
            " -2.28642931e-02  8.41606880e-02 -1.15532658e-03 -2.89826163e-02\n",
            "  4.48035931e-02  6.30554840e-03  3.39706485e-02 -6.06532422e-04\n",
            "  1.82554920e-02  7.58339083e-03  1.71912975e-02 -7.09610088e-03\n",
            "  2.51058525e-02  3.84458181e-02 -2.97575494e-02 -2.27941769e-02\n",
            " -7.39637529e-02 -2.13306439e-02 -4.19342526e-02 -5.97004191e-02\n",
            " -3.43096776e-02  1.59138385e-02  8.78580981e-03  1.77866189e-02\n",
            "  3.85188286e-02 -9.07064880e-02 -8.15284117e-02  3.37892318e-02\n",
            "  1.44012493e-02 -1.08674324e-01  1.52419352e-02 -9.33433426e-02\n",
            " -6.76959491e-03  2.86759072e-02  1.35781573e-02  2.04968310e-02\n",
            " -1.91017207e-02  1.95063916e-02  8.88025863e-03  1.42687310e-02\n",
            "  1.42456715e-02 -4.05452885e-03  1.22582951e-02  4.00033678e-02\n",
            "  5.35802967e-03  3.32672525e-02 -6.35438762e-02 -2.73101562e-03\n",
            " -4.54784002e-03  5.80563110e-03  6.88586548e-02  3.32415773e-02\n",
            " -1.72239133e-02 -5.30781114e-03 -3.03890566e-02  7.71293372e-03\n",
            "  9.11997111e-03  2.90903563e-02 -5.81075686e-02  1.91691485e-02\n",
            "  4.67747650e-02 -9.77006971e-02 -7.09481382e-03  5.69342407e-02\n",
            "  2.02809662e-02  8.57536996e-02 -7.62450893e-02 -3.14409165e-02\n",
            "  2.20662868e-03 -3.25390602e-02 -4.94194599e-02  3.64898336e-03\n",
            " -1.73841968e-03  5.57634481e-02  1.69672625e-02 -7.36679184e-02\n",
            " -5.30735349e-02 -4.18611642e-02  1.36287688e-02 -7.05786680e-02\n",
            " -6.49452997e-03  3.30652650e-03  1.74105818e-02  3.14639529e-02\n",
            "  2.99092791e-02 -5.78787041e-03 -5.78787041e-03  1.89802792e-02\n",
            " -4.03344670e-02 -2.03649197e-02 -7.31377835e-03 -2.74682992e-02\n",
            "  7.61394699e-03  2.42484947e-02  5.21134295e-03  4.44523450e-02\n",
            "  7.54725834e-03  1.58360560e-02 -2.97603848e-02  8.56996540e-03\n",
            " -2.36118569e-02  3.27477664e-02 -8.89905490e-03 -2.05058691e-02\n",
            " -1.46388238e-02  2.33676687e-02  2.73056485e-02 -8.00180263e-02\n",
            "  1.82834087e-02 -2.71495693e-02  2.73104079e-02 -1.18119130e-01\n",
            "  2.04134600e-02 -6.38572901e-02 -7.61911299e-02  5.46649543e-02\n",
            " -2.53484024e-02 -1.98544635e-02  1.40718032e-02  3.03880213e-02\n",
            " -3.68736590e-02  1.84650082e-02 -4.07015789e-02  1.34168684e-02\n",
            "  6.37760857e-02  4.63862904e-02  5.68778684e-03  1.11547249e-02\n",
            "  3.58981170e-02  8.32526912e-03  3.85200426e-02  1.00172135e-02\n",
            " -3.41002343e-02 -7.67891801e-03  1.15471417e-02 -1.28066369e-02\n",
            "  3.17026679e-03  2.32089506e-02 -3.27533327e-02  1.68632180e-02\n",
            " -6.38470338e-03 -1.63322866e-03 -1.23420074e-02  3.19754492e-02\n",
            "  8.84740928e-02  1.94759549e-03 -1.61270669e-02 -3.28814251e-03\n",
            "  3.51079014e-02  5.31368380e-04 -9.76586187e-03  2.43704116e-02\n",
            " -2.71471872e-02 -2.81253123e-02  5.24798492e-02  2.88991754e-02\n",
            " -4.07014724e-03  3.18967317e-02 -8.15947137e-02 -1.83745262e-02\n",
            "  8.66355106e-03  2.40780388e-03]  - intercept :  0.7331405266377099\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.1262520457619713\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:12,462]\u001b[0m Trial 162 finished with value: 0.19786823064603346 and parameters: {'count_threshold': 7, 'postag': True, 'voc_threshold': 5177}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.         0.         0.         ... 0.03409048 0.         0.05959993]\n",
            " [0.         0.         0.03840995 ... 0.03181778 0.         0.09099914]\n",
            " [0.24945173 0.31869449 0.07942293 ... 0.02478345 0.         0.00641146]\n",
            " ...\n",
            " [0.04098711 0.07985373 0.05071468 ... 0.11765392 0.15170983 0.03219403]\n",
            " [0.         0.         0.00277546 ... 0.01168816 0.         0.06575186]\n",
            " [0.19327367 0.23937833 0.06561967 ... 0.02824677 0.         0.        ]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 2.21809207e-02  2.54070362e-02  1.88986869e-03 -7.38600701e-03\n",
            " -2.84384916e-02  3.23158921e-02  4.66793698e-02  4.43417974e-03\n",
            "  1.96295159e-02 -1.71084580e-02 -3.97255552e-03 -6.39092178e-03\n",
            "  1.81561991e-02  1.35869682e-02 -2.47978897e-03 -8.22638766e-03\n",
            "  3.21901962e-02  5.73888353e-03 -3.93797793e-02 -9.97799693e-03\n",
            "  2.53841402e-02  8.42771052e-03  1.87644282e-02 -1.27364068e-02\n",
            " -4.63179445e-02  1.32581311e-02 -3.03249311e-02 -4.14346932e-02\n",
            " -4.09387829e-02 -1.02501912e-02 -3.12750198e-02 -5.82183019e-02\n",
            " -1.02987233e-02  8.19171301e-03  2.34625583e-02  6.02537718e-02\n",
            "  9.30111756e-04 -5.78515329e-03 -8.81063532e-02  2.75885663e-02\n",
            " -3.60968324e-02  2.28781189e-02 -4.65985499e-02 -4.97323169e-02\n",
            " -4.67255507e-02 -1.72705925e-02  4.82819226e-02  3.78089006e-02\n",
            " -3.29611681e-02 -3.19648545e-02 -3.69958877e-02 -5.65373997e-02\n",
            "  2.35499057e-03 -2.10559861e-02 -5.20709517e-02  6.99565824e-02\n",
            " -2.46888963e-02  2.49963380e-02 -2.25468330e-02 -3.40170483e-02\n",
            " -6.16514759e-02 -3.31555137e-03 -3.31326377e-02  8.74631956e-02\n",
            " -1.26356010e-02  3.85651011e-02  7.67958163e-02 -3.45017990e-02\n",
            "  5.27901862e-02  4.98729448e-02  3.94934622e-02 -2.90397846e-02\n",
            "  3.49789488e-02 -3.99912640e-02  8.36864285e-02  2.61352215e-02\n",
            " -8.32123580e-02 -7.57511309e-02 -2.58928188e-02 -1.73150736e-02\n",
            "  6.61383719e-02  7.61403152e-02  4.61447215e-02 -2.79834410e-02\n",
            "  5.29794919e-03 -3.59305514e-02 -3.67316109e-03 -4.44997642e-02\n",
            " -3.43289009e-02 -1.58067438e-02  1.87213772e-02  2.73215837e-02\n",
            " -4.16177379e-03  3.04589029e-02  3.22468978e-02  2.88705496e-02\n",
            " -4.69316761e-02  1.56933618e-02 -4.31423464e-02  1.26176038e-02\n",
            " -8.13742056e-02  3.67456333e-02  1.31584014e-01  6.56039469e-02\n",
            " -1.73035762e-02 -3.31065390e-02  4.39778498e-02  2.81261057e-02\n",
            " -1.28109182e-02 -2.57099598e-02  1.86566716e-02 -1.74034657e-03\n",
            "  1.37264110e-02  6.72477200e-02 -4.50790090e-02  1.43705639e-02\n",
            " -1.13697373e-02  9.80402304e-03 -2.73289541e-02 -1.10445655e-01\n",
            " -3.09061605e-02 -5.18919432e-02  3.45630065e-02  1.03228635e-02\n",
            "  8.02867064e-02  6.40720816e-02  5.39662135e-02 -4.48845101e-02\n",
            "  1.15053112e-02  6.02499092e-02  9.14719651e-02  6.69009767e-02\n",
            " -1.39130444e-02  2.94850081e-02  9.26090418e-02 -7.96160607e-02\n",
            "  8.70110930e-02  3.78210817e-02 -2.83240038e-02 -2.78105022e-02\n",
            "  5.80355121e-04 -1.71721266e-02  3.83565986e-02 -6.53183369e-03\n",
            " -1.46406736e-03  1.02297924e-02 -3.62679658e-02  1.22892010e-02\n",
            " -1.83339455e-02  9.45661331e-03  5.52787524e-02 -1.31824897e-02\n",
            " -2.42468029e-02  2.83501916e-02 -8.06655450e-03 -6.66615850e-02\n",
            "  2.83315586e-04 -1.00569711e-02  2.06039884e-03  5.61132926e-02\n",
            " -4.77378951e-02 -8.41977053e-03 -5.71720740e-02 -3.77888531e-03\n",
            " -4.21339383e-02 -3.23542669e-03  9.23179532e-03  8.71963607e-02\n",
            " -5.33285341e-02  1.23277968e-02  1.23277968e-02  3.08938286e-02\n",
            " -5.61310457e-02  3.65712924e-03  9.34312533e-03 -7.24669640e-02\n",
            " -4.33952579e-02 -4.57436703e-02 -1.89212737e-02  4.05020362e-03\n",
            "  4.88593582e-02 -3.98653134e-02 -4.86145537e-02  1.83404113e-02\n",
            "  1.30206292e-03  3.93455794e-02 -6.74390181e-02 -1.12811631e-02\n",
            " -8.28551902e-03 -4.05687390e-02  1.34716579e-02 -4.68181755e-02\n",
            "  6.35397862e-02  8.03153341e-02 -2.09138344e-02  3.30688106e-02\n",
            " -7.10949694e-03 -4.70471531e-02  9.83574909e-02 -1.88503824e-02\n",
            "  6.55984800e-02  6.94150576e-02 -1.59914256e-03 -2.16004510e-02\n",
            " -8.03319089e-03 -5.41707096e-02  2.68217531e-02 -1.68676811e-02\n",
            "  3.60487851e-02  4.95601727e-03  6.79777446e-02 -2.11544834e-02\n",
            " -3.30855013e-02 -1.66603569e-02 -3.38423279e-02  8.54630427e-03\n",
            " -2.65651298e-02  2.13576637e-03 -1.73685261e-02  1.87160373e-03\n",
            " -1.25588064e-02 -4.68354244e-02  5.75957744e-02  6.44274898e-02\n",
            "  1.14809195e-02 -1.37959244e-02  4.22427076e-02 -2.22008121e-02\n",
            "  6.94472477e-02  3.87319915e-02  1.49165871e-02  1.71879956e-03\n",
            " -3.89077100e-03  2.08173716e-03  5.24841173e-03 -3.65843108e-02\n",
            "  4.99645651e-03  1.14261117e-03 -1.77053446e-02 -4.62424994e-03\n",
            "  5.62722476e-02 -5.99017049e-02  1.38730112e-02 -3.00801494e-02\n",
            " -7.11450979e-03  6.27730499e-03 -4.77531013e-02  8.02392063e-03\n",
            " -4.89343129e-03 -6.73798686e-02  1.37175823e-02  3.42624826e-02\n",
            "  7.99838276e-03 -3.26016296e-02  8.04832527e-03 -1.48726005e-02\n",
            "  1.27237389e-03  3.66793780e-02  1.47179317e-03  2.11300183e-02\n",
            " -2.64831336e-02  7.53492706e-04 -1.42299493e-02 -3.75943971e-02\n",
            " -1.04075727e-04 -2.29862434e-03  2.23564899e-02  1.82037347e-02\n",
            " -6.13629193e-02  3.85077336e-02 -2.67083395e-02 -7.14877619e-03\n",
            " -5.67050874e-03  7.25579283e-02 -2.50160340e-04  5.17990006e-03\n",
            "  4.18018859e-04  1.70046702e-02 -5.89130964e-03 -7.71109805e-02\n",
            " -1.29117826e-02  1.93712181e-02 -1.36332207e-02 -1.13453579e-02\n",
            " -4.79813099e-03  3.68979947e-02 -9.21674784e-02  1.99781769e-02\n",
            "  3.62161034e-02  1.77360861e-02 -4.83954859e-02 -2.26037400e-02\n",
            " -2.35893759e-02  5.36108094e-02 -6.16559103e-02  2.74711160e-02\n",
            " -1.99692386e-04  8.75040452e-03  8.50635387e-02 -4.86105441e-02\n",
            "  7.98419053e-03 -2.65912518e-03  7.55883276e-03  6.92287468e-02\n",
            " -3.83093575e-03 -3.14001784e-02 -5.38324552e-03  4.31820510e-02\n",
            " -2.87501320e-02  4.13540818e-02  1.37003028e-02 -5.81096790e-02\n",
            " -7.67158674e-02  4.66121761e-02 -1.19552076e-02  7.44702767e-02\n",
            " -2.85333095e-02 -4.31651865e-02  2.13424921e-03  2.14474615e-02\n",
            "  4.43147259e-03  6.33330520e-02 -6.64402549e-02 -1.10812699e-03\n",
            " -1.88608743e-02 -5.84480081e-03  3.86523627e-02  5.36172706e-02\n",
            " -9.37483337e-02 -1.40205510e-02 -4.71129161e-02  6.97778571e-03\n",
            "  2.92644842e-02  2.52581050e-02 -8.07344707e-02  1.40854406e-02\n",
            " -2.88641749e-02  1.60045866e-02  6.04389656e-02 -1.87474212e-02\n",
            " -2.53502297e-02 -3.73678544e-02 -2.11941959e-02  3.63682005e-02\n",
            " -4.79256788e-02  2.02385570e-03 -3.31763581e-02  4.46199875e-04\n",
            " -1.50385884e-02 -2.56503431e-02  4.36357184e-02 -1.10905990e-01\n",
            " -5.70830379e-02  4.27349352e-02 -3.09919199e-02 -3.06136691e-02\n",
            " -4.83470726e-03  8.68494908e-02  3.78944462e-02 -6.67355578e-03\n",
            "  8.76429490e-03 -3.07221520e-03  9.84448978e-02 -3.18554601e-02\n",
            "  2.37157968e-02 -5.06980995e-02 -2.59798149e-02 -7.01315527e-02\n",
            "  1.94534266e-04 -5.87674655e-02 -4.94879291e-02 -6.08025920e-02\n",
            " -7.27412085e-02  2.67914845e-02  2.75489957e-02  2.45735884e-02\n",
            "  1.38564072e-02 -5.57949394e-02 -2.27912148e-02 -2.24811722e-02\n",
            " -8.48684135e-03 -7.69546932e-03 -4.59742137e-03 -8.58768132e-03\n",
            " -5.89805670e-02  3.58775076e-02 -7.97697741e-03  5.20644240e-02\n",
            " -2.01683212e-02  6.90096485e-03 -4.34938953e-02  9.06516908e-02\n",
            " -3.63155926e-02 -6.19256994e-02  5.81492775e-03 -5.05314354e-03\n",
            " -5.87003032e-02  7.65408591e-03  3.92946312e-04  3.08945196e-02\n",
            " -6.39400624e-02  7.26675039e-02  7.26675039e-02  1.08150348e-02\n",
            " -1.14843676e-02 -7.70649306e-02 -3.66854017e-02  2.91734169e-02\n",
            " -4.54576239e-02  2.56958699e-02  4.93303962e-02 -1.47319933e-01\n",
            " -2.56710554e-02 -1.35315661e-02 -4.00161580e-03  1.48374629e-02\n",
            " -1.54755369e-02  2.56777221e-02 -8.85808100e-02  2.03574983e-02\n",
            "  1.45970565e-03 -4.63394603e-02  3.97386111e-02 -5.65872514e-02\n",
            "  1.84085038e-02 -5.20061809e-03  5.93514559e-03 -3.47404943e-02\n",
            "  2.87293350e-02 -1.23791965e-02 -2.99950746e-02 -6.10400978e-02\n",
            " -8.08337039e-04  8.27013367e-02  9.71189007e-03 -8.56915769e-02\n",
            "  3.55332483e-02  1.46953695e-02  1.82344598e-02  1.42187538e-02\n",
            " -1.48305732e-02  2.35053104e-02 -7.78479530e-04  3.51254789e-02\n",
            " -3.32359010e-02 -1.09789186e-01  4.43546925e-03 -4.60942589e-02\n",
            "  1.63534036e-02  3.03127781e-03 -2.14011562e-02  3.63232776e-02\n",
            " -7.91668788e-02 -4.31418261e-02  2.10119765e-02  1.55786090e-02\n",
            " -3.44724995e-03  4.61194049e-03  9.43684391e-03 -4.07094986e-02\n",
            "  1.52387269e-02  6.39716877e-02  1.12835671e-02 -8.40592553e-03]  - intercept :  0.7468577785934077\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.19786823064603346\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:13,043]\u001b[0m Trial 163 finished with value: -0.28581223952495366 and parameters: {'count_threshold': 7, 'postag': True, 'voc_threshold': 5503}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.38777566 0.02369643 0.49008393 ... 0.01947009 0.02740391 0.00932102]\n",
            " [0.         0.         0.         ... 0.07788037 0.10961563 0.13054319]\n",
            " [0.07789791 0.10136605 0.1031078  ... 0.10334505 0.08474649 0.02921456]\n",
            " ...\n",
            " [0.06272865 0.07199178 0.09102832 ... 0.04008549 0.05641981 0.04816782]\n",
            " [0.01460086 0.0323776  0.04090916 ... 0.1513441  0.26371208 0.06464546]\n",
            " [0.01475095 0.04701314 0.0106263  ... 0.0809253  0.13821183 0.08557171]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 1.98842981e-03 -2.16297199e-02  2.09838153e-03 -2.66474439e-02\n",
            "  1.54578660e-02 -1.03775552e-02 -1.58575320e-02 -6.14308063e-03\n",
            " -1.37748272e-03 -4.68805560e-04  6.12216894e-03  2.13243749e-03\n",
            "  1.75521406e-03 -2.64501928e-02 -1.20883942e-02 -2.53206974e-02\n",
            " -9.17567835e-03  6.21257514e-02  4.09365833e-03  6.08979756e-03\n",
            "  8.97970125e-03 -2.75496868e-02 -8.88089879e-03  1.52124401e-02\n",
            "  2.82389069e-03  7.97526205e-03 -8.00502186e-03 -1.42386091e-02\n",
            "  2.60364872e-02  2.81997541e-03 -1.44646715e-02 -5.09556156e-03\n",
            " -2.14319265e-02  1.16731507e-02  2.40811559e-02  1.03954750e-02\n",
            " -1.47985978e-03 -1.38053240e-03 -8.94887643e-03  8.02260671e-03\n",
            " -1.55252802e-02 -1.53034490e-02 -1.31119391e-02  1.45813728e-02\n",
            " -2.67647567e-02 -2.93672565e-03 -7.02881722e-03 -3.84490138e-02\n",
            "  6.74484153e-03 -1.79560939e-04 -3.70116799e-02 -5.29165902e-02\n",
            "  2.44462709e-02 -2.74950983e-02 -1.25329136e-02 -4.45023086e-02\n",
            "  2.41965439e-02  1.33554249e-03  2.34081455e-02  4.53878884e-02\n",
            " -1.11661767e-02  4.78372682e-03 -3.79406422e-02 -1.35271973e-02\n",
            "  1.81826834e-03 -2.35490217e-03  1.31100942e-02 -1.75493013e-02\n",
            " -6.95904037e-03  8.73667812e-03  5.85570162e-03 -3.56307008e-03\n",
            "  2.68225389e-02 -2.75800488e-02  1.16133676e-02 -2.28911104e-02\n",
            " -5.61106085e-02 -7.41525913e-05 -2.79007131e-02 -1.09271533e-02\n",
            "  2.68502896e-02  6.26031756e-03  1.40985671e-02 -4.54301876e-02\n",
            " -1.70292800e-02  4.45999759e-03 -1.19566253e-02  2.35291309e-03\n",
            "  2.89343319e-02  8.37486112e-03 -3.80402676e-02 -3.92815535e-02\n",
            " -3.43186038e-02 -3.43183171e-03 -2.42579875e-02  3.23909891e-02\n",
            "  1.61638072e-02 -4.50795449e-02  4.03226220e-02  1.36953212e-02\n",
            "  3.41821292e-02  5.39479583e-02 -2.19529357e-02 -1.99991773e-02\n",
            " -2.63778151e-02 -4.21230167e-02 -1.25187664e-02  3.88422857e-02\n",
            "  6.33577822e-03 -8.48478328e-04 -1.18987298e-02 -3.87943603e-03\n",
            "  1.69765745e-02  2.95420980e-02  3.02736691e-02 -3.92325431e-02\n",
            "  3.88204362e-02  1.37237068e-02 -2.39213022e-02  2.33141155e-02\n",
            " -4.95104936e-02  8.14397442e-03 -3.26161000e-02  2.47779332e-02\n",
            " -5.48025178e-03  1.47768341e-02  7.63378226e-02  3.44314060e-02\n",
            "  1.17634970e-02 -3.47958449e-03  4.43879408e-03 -1.07553927e-02\n",
            " -7.89018439e-03 -4.73277578e-02 -4.94335500e-02  1.97205912e-02\n",
            " -1.43132560e-04 -3.70200983e-02  5.27795955e-03  1.57721316e-02\n",
            " -4.42621540e-02 -2.11040136e-03 -4.63184869e-02 -3.34044233e-02\n",
            "  4.43527168e-02  1.47819894e-02 -2.50734235e-03 -9.59255194e-04\n",
            " -7.30199264e-03 -8.47328198e-03 -1.21843344e-02 -1.06185741e-02\n",
            " -1.20961077e-02 -1.56407853e-02  9.07086862e-03  7.35351734e-03\n",
            " -7.32058278e-04  1.65014180e-02  3.78699792e-02 -4.33573650e-02\n",
            " -2.30940807e-02  3.30040300e-03 -4.57099366e-02 -2.05422182e-02\n",
            "  2.48829948e-02  2.20381972e-02  8.01524277e-03  1.36313843e-02\n",
            "  1.04772061e-02  6.32587546e-02 -2.64331539e-02 -3.69673032e-03\n",
            " -3.89973555e-03 -4.26758915e-03 -7.61999387e-04 -2.02570962e-02\n",
            "  9.41298602e-04  2.22565096e-02  1.64739512e-02 -3.10313911e-03\n",
            "  1.84389163e-02 -5.90066940e-02 -1.07074270e-02 -2.36292643e-02\n",
            "  4.82390465e-02 -2.83650689e-02 -1.50313078e-03 -1.04664410e-02\n",
            " -4.05404669e-02 -3.00224970e-02  2.02297894e-02 -2.99283611e-02\n",
            "  3.09879091e-03 -2.01997394e-02 -1.96985863e-02 -1.75551038e-02\n",
            "  1.50807453e-02  2.72207880e-02  2.85995724e-02 -1.81658272e-02\n",
            " -1.17697423e-02  4.00572878e-02 -5.53858406e-02 -1.69524954e-02\n",
            "  3.10003151e-02  9.59513193e-03  1.79994088e-02  1.46972412e-03\n",
            " -1.46984412e-02 -3.93255033e-02  4.58329675e-02 -5.78573553e-03\n",
            " -2.85441760e-02 -3.30615845e-02 -1.56453426e-02 -8.02271036e-03\n",
            "  2.02915671e-02  1.68487364e-02  3.14483400e-02 -1.05693999e-02\n",
            " -1.36340552e-02  3.09816207e-02  3.34737550e-02  3.30071789e-03\n",
            "  3.71465877e-02  6.71925248e-03  2.88814369e-02 -4.36573162e-02\n",
            "  7.34941368e-04 -5.03705794e-04  9.43764103e-03 -2.90449041e-02\n",
            "  1.30069180e-02  1.30069180e-02  3.75346926e-02 -1.21675885e-03\n",
            " -1.59106865e-02 -3.54965935e-02 -2.74016470e-02 -2.47740224e-03\n",
            " -8.04442502e-03 -2.47309258e-02 -1.24428853e-02  7.13150674e-03\n",
            " -1.86974649e-02  7.74544251e-03 -5.04923828e-02 -4.96859665e-02\n",
            "  3.04810091e-03  1.86001749e-02  1.14524205e-02  4.04894578e-02\n",
            " -5.90648442e-02  6.42023598e-03  2.77102278e-02  1.25526613e-02\n",
            "  2.13164164e-02  1.08903358e-03 -3.94540936e-02 -5.86658559e-03\n",
            " -4.16524005e-03  1.95458117e-02  1.04904199e-02  8.72389178e-03\n",
            "  8.63988112e-04  8.57405203e-03 -3.55475613e-02 -2.11006852e-02\n",
            " -7.95159416e-03  2.13020172e-02 -1.36484509e-02 -1.11042084e-02\n",
            "  4.49730194e-02 -1.41837804e-02 -5.30399511e-04 -2.83943461e-02\n",
            " -7.65310763e-03  3.62177659e-02  2.19012703e-02 -9.53387448e-03\n",
            "  5.38203090e-02 -2.97279280e-02 -1.72399774e-02 -5.60659445e-02\n",
            "  4.20077800e-03  2.45314110e-03 -2.82169701e-02  7.48411681e-04\n",
            " -5.78428172e-02  3.16606007e-03 -1.43309807e-03  3.18244125e-02\n",
            "  1.54395925e-03  1.23862770e-02  1.47804912e-02 -3.64714270e-03\n",
            " -2.25451155e-02  3.49068418e-02 -1.15401450e-02  2.93540978e-02\n",
            "  6.38127638e-02 -2.57371513e-02  1.42669920e-02 -4.64111088e-02\n",
            "  3.51249667e-02 -9.82313055e-03  5.13543233e-02  3.76561536e-02\n",
            " -1.10457154e-02  4.00394033e-02 -3.62937751e-02  6.73247611e-04\n",
            "  7.60724505e-03  3.84696509e-02 -1.97989618e-03  1.00688990e-02\n",
            "  1.45740450e-02 -1.73137996e-02 -1.02250083e-02 -3.25361356e-02\n",
            " -6.69283151e-02 -7.12614099e-03  1.74227682e-02 -6.73863854e-03\n",
            " -1.07669773e-02 -3.29708538e-02 -1.70519608e-02  5.23074230e-03\n",
            "  2.26395752e-02 -1.33496977e-02  2.58267669e-04  2.01578959e-03\n",
            "  2.11272033e-02 -4.28541144e-03 -1.48095741e-02  5.19191262e-04\n",
            "  2.67069734e-02  5.62682127e-03 -6.32768861e-03  8.04972386e-02\n",
            " -3.19890422e-03  1.41017409e-03  2.32232053e-02  2.04490859e-02\n",
            " -1.00116176e-03 -3.07905051e-02 -2.33338550e-03  9.48262811e-03\n",
            "  1.53902014e-04  2.69660177e-02  7.76838534e-04 -1.73119518e-02\n",
            " -2.51891479e-02  3.47382456e-02 -4.52852510e-03  1.04363999e-02\n",
            " -1.97519391e-02  2.61984672e-02 -6.77694210e-03 -6.54548050e-02\n",
            " -2.25638581e-02  8.62447695e-03  1.11259846e-02 -3.42303290e-02\n",
            "  5.75420328e-03 -4.31882794e-03  5.80542293e-03 -5.13010328e-02\n",
            "  5.48514993e-03 -1.90558637e-02 -6.77867338e-03  6.49864072e-03\n",
            "  1.68385651e-02 -1.78354453e-02 -4.75780667e-02  4.36846315e-03\n",
            " -3.99855252e-03  3.61141232e-02 -2.41523798e-02 -1.01885155e-01\n",
            "  2.22297968e-02 -2.11309914e-02 -2.52089919e-02 -1.95510789e-02\n",
            " -1.47531321e-02  1.08303851e-02 -3.39943504e-02  2.77473178e-03\n",
            " -4.12811679e-02 -1.64639504e-02 -3.61278739e-02  2.57343275e-02\n",
            "  1.34949421e-02 -5.61544773e-02  2.11770663e-02 -2.81825562e-02\n",
            " -6.73825015e-02 -4.47059932e-02  3.13082990e-02 -1.00430288e-02\n",
            "  4.72426262e-02 -7.61239693e-03  9.62442885e-03  6.81006199e-03\n",
            "  1.87893210e-02  1.26839200e-02  3.81200970e-03  2.49660813e-03\n",
            " -5.09349505e-04  4.71098351e-02 -1.61403974e-02 -7.02848013e-02\n",
            "  5.35462611e-02  3.98261010e-03 -4.71534146e-03  5.26614897e-02\n",
            "  9.54178520e-03 -5.32950306e-02  1.84967865e-02  5.69094922e-02\n",
            "  1.48417807e-02  1.10828883e-02 -2.89329191e-02 -1.47261657e-03\n",
            " -2.46360762e-02  2.48367422e-02  3.32898786e-02  7.19083098e-03\n",
            " -4.66540186e-02 -3.19635830e-02  6.82802246e-02  1.67019310e-02\n",
            " -4.52281774e-02 -3.18769372e-02  2.98439947e-02 -3.64879885e-02\n",
            "  1.00672510e-02 -2.24863735e-02  3.98751692e-03 -2.85412890e-02\n",
            " -9.23095194e-03  2.24006895e-02  3.46605606e-02 -4.53822179e-05\n",
            " -1.71297499e-02  3.71865136e-02 -2.33888255e-02 -3.41970588e-03\n",
            "  5.67083946e-03 -3.78814937e-02 -3.72768550e-02  2.85696559e-02\n",
            "  1.09667093e-03  1.15963807e-02 -8.57606767e-03 -7.34924568e-04\n",
            "  6.55782125e-03 -2.36146493e-02  2.21323693e-02  4.05517657e-02\n",
            " -4.93575474e-03  1.32502536e-02 -1.84140007e-02 -9.66086555e-03\n",
            "  1.07749334e-01 -2.09960962e-03 -5.00321840e-03  3.81582369e-03\n",
            " -7.34012911e-03  3.00167429e-02  3.70645797e-02 -4.93351719e-02\n",
            " -3.37735807e-02 -9.68955757e-03 -9.68955757e-03  1.95661293e-02\n",
            "  6.96474274e-03 -1.51789227e-02 -4.66816849e-02 -2.77120573e-02\n",
            " -1.66010311e-02  3.72797330e-03]  - intercept :  0.7606395280644909\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.28581223952495366\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:13,623]\u001b[0m Trial 164 finished with value: -0.17977908407268767 and parameters: {'count_threshold': 8, 'postag': True, 'voc_threshold': 5762}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.03696703 0.04157784 0.09074914 ... 0.17905339 0.04460489 0.03238343]\n",
            " [0.         0.         0.07968563 ... 0.05780011 0.         0.05749664]\n",
            " [0.61908826 0.63943364 0.10196456 ... 0.00842918 0.29831525 0.00336803]\n",
            " ...\n",
            " [0.         0.         0.00481404 ... 0.05994085 0.         0.04604667]\n",
            " [0.05403871 0.04846404 0.09938296 ... 0.18435093 0.21902179 0.02908115]\n",
            " [0.41336427 0.48827045 0.02474489 ... 0.0192667  0.43620722 0.00769835]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-1.47719950e-02 -2.14588753e-02 -1.10192355e-02 -1.12558702e-02\n",
            "  2.17737746e-02 -4.79591284e-04 -2.66886260e-02  6.42127187e-02\n",
            "  1.75571405e-02 -2.43246224e-03  2.03608147e-02 -3.73200404e-04\n",
            " -1.53116541e-02 -4.82054601e-02 -2.47816200e-02  2.86434691e-02\n",
            " -5.65081307e-02  3.51608808e-03 -2.90352799e-02 -8.28640029e-03\n",
            "  5.68118437e-03  2.50790897e-02 -2.72595854e-02 -9.99107504e-02\n",
            " -2.60419501e-02  1.39136335e-02 -4.30610583e-02  1.06232553e-02\n",
            " -6.14748470e-02  5.45721983e-02 -3.06883704e-02  6.57856302e-02\n",
            "  4.76728661e-03 -2.14833742e-02 -3.88706327e-02  7.91127633e-03\n",
            "  1.20003638e-01 -4.29531623e-02 -9.42761680e-02 -1.09524395e-02\n",
            "  6.88014819e-03 -5.16660987e-02 -4.15158208e-03 -2.88153031e-03\n",
            " -1.30169426e-02  2.35095480e-02  8.93575552e-04 -1.15397714e-02\n",
            " -4.34200408e-03 -4.32091470e-02  1.47032509e-02 -4.40340136e-04\n",
            " -4.19683610e-02  1.45629320e-02 -1.39948360e-02  5.09482665e-02\n",
            " -2.64241022e-02 -3.58254346e-02  1.35141551e-01  2.85638409e-02\n",
            " -4.52169031e-03 -5.99472872e-02  1.60669297e-02  2.71137466e-02\n",
            " -4.03948724e-02  1.13512201e-01  5.05319801e-02 -1.55014002e-02\n",
            "  4.95603521e-02  8.25703031e-02  1.58041162e-02 -3.16410762e-02\n",
            " -2.01223742e-02 -3.72149798e-02 -6.83310859e-03 -5.42158518e-02\n",
            "  1.55041415e-02 -1.21749305e-02 -7.78076380e-04 -4.59492129e-03\n",
            " -3.01849439e-02  3.28659043e-02  2.27152882e-02  1.99922640e-02\n",
            "  5.00619425e-03 -2.94180898e-02 -5.54421729e-03 -8.16216050e-02\n",
            " -9.98857188e-03 -7.09388615e-02  3.39396129e-02  8.36561018e-02\n",
            " -7.91671797e-02 -6.32080298e-02 -4.72647334e-02 -2.74402960e-02\n",
            " -1.23563595e-02  4.20325399e-02  4.76109270e-02 -1.51381417e-02\n",
            " -7.30334473e-02  9.72940294e-02  4.15371774e-03 -3.85763362e-02\n",
            "  2.60440247e-02  6.63663722e-05  9.65637566e-03  1.16663106e-03\n",
            "  5.32254321e-03  3.02818482e-02  5.11014978e-02  4.12054418e-02\n",
            " -1.24762478e-02  4.17494265e-02  3.28680120e-02  5.13771224e-03\n",
            " -4.33605614e-02  6.57036077e-02  3.94995378e-02  4.91004962e-02\n",
            " -8.88569927e-03  2.10977205e-02 -5.19822118e-02  1.67301382e-02\n",
            "  6.55616610e-03  6.55616610e-03  6.55616610e-03 -5.87348219e-02\n",
            "  7.41124181e-03 -6.32931022e-02 -9.66360403e-02 -1.18259784e-02\n",
            " -1.61797003e-02  6.97080859e-02 -2.33403470e-02  2.38409577e-02\n",
            "  1.79923319e-02  1.73977688e-02  4.25613914e-03  3.69876674e-02\n",
            "  7.45170068e-02  1.34849721e-02 -1.32861492e-01 -5.44854315e-02\n",
            "  8.66603568e-02  3.56840496e-02 -2.77942894e-02  2.20250744e-02\n",
            "  2.20250744e-02 -1.32815920e-02  1.12034503e-01 -4.12166418e-02\n",
            " -3.10390902e-02 -4.16685940e-02  2.96065223e-02  5.03252456e-02\n",
            "  6.92068137e-02 -1.66730277e-02  2.94267435e-02  1.64170751e-02\n",
            " -7.17700409e-02  2.05336141e-02 -2.15822874e-02  2.08719950e-03\n",
            " -1.99683727e-02  6.35806810e-02  2.33260977e-02 -6.02691841e-02\n",
            " -6.34361922e-02  1.23424965e-02 -8.03136801e-03  2.38202443e-02\n",
            "  3.85434819e-02 -4.25193394e-02  1.17221771e-01 -4.16806520e-02\n",
            "  1.47739617e-01 -3.09875975e-02  8.55966329e-02  4.00292287e-02\n",
            " -1.25948991e-01 -6.39149419e-02 -1.73856003e-02 -1.33212805e-02\n",
            " -3.28888843e-02 -1.90566646e-02  8.38991697e-02 -6.62996509e-03\n",
            "  5.87026382e-02 -2.32237601e-02  4.71042716e-02  8.05630968e-03\n",
            "  4.51455802e-03 -2.51758051e-03 -1.17430814e-03 -6.45404958e-03\n",
            " -4.06083843e-02  4.29677181e-02  2.45257733e-02  2.14739325e-02\n",
            " -1.53748029e-02 -2.73047540e-02 -1.70575340e-03 -4.63395756e-02\n",
            "  2.74310036e-02  2.45486401e-02 -3.64498553e-02 -5.19481456e-02\n",
            "  7.45865724e-03 -1.16552632e-01 -1.41040753e-02 -7.42838303e-03\n",
            "  9.88719047e-03  1.62334840e-02  1.70236891e-02 -1.33698763e-02\n",
            " -2.18762282e-02 -2.27421240e-02  3.03673171e-02 -2.78519110e-02\n",
            "  1.31672142e-02 -9.29831187e-02  5.17919433e-02 -2.28411603e-02\n",
            " -2.87755454e-02 -8.67577511e-02 -2.23238586e-02 -2.92503062e-02\n",
            "  2.37758602e-02  1.06549444e-01 -1.77047501e-02 -2.42644139e-02\n",
            " -2.41859254e-02  1.86545038e-02 -1.33277750e-01 -2.59581384e-02\n",
            "  9.49912286e-04 -4.86928281e-02  2.43385631e-02  3.45985581e-02\n",
            " -2.79345706e-02 -2.89516995e-02 -1.38858423e-02 -1.54383551e-02\n",
            " -2.76427100e-02 -4.66315550e-03  2.43790105e-02 -3.17096034e-02\n",
            "  3.89945133e-02 -1.89385247e-02 -5.86355211e-02  9.33742548e-02\n",
            "  3.43482176e-02  5.29388794e-02 -4.45204346e-02  1.89650155e-02\n",
            "  1.47124128e-02 -4.73098531e-02  7.88742632e-02  2.85747603e-02\n",
            "  2.75159733e-02  1.07686047e-02  8.32353235e-02  1.07298335e-02\n",
            " -3.83424464e-02  8.15348999e-02 -4.10654513e-02 -1.79614039e-02\n",
            " -3.82205589e-02  3.09283477e-02  1.94922687e-02  4.82498752e-02\n",
            " -3.05617401e-02 -5.35703319e-02 -7.42083999e-03  5.23918262e-02\n",
            "  2.32936473e-02 -5.43855081e-02 -8.16609413e-02 -3.20766201e-02\n",
            " -6.92265665e-02 -1.05673820e-02 -3.99287594e-02  1.09674241e-01\n",
            " -1.64430055e-02  9.09537215e-03 -7.12462288e-02 -3.08111905e-02\n",
            " -1.63042590e-04 -3.13917153e-02  1.03820167e-02  2.11348439e-02\n",
            " -3.50766459e-02 -4.18829989e-02  4.47993816e-02  5.71780494e-02\n",
            " -1.57096439e-02  5.92479098e-02  1.69942985e-02  3.64174684e-02\n",
            "  4.96024485e-03 -1.02993967e-02 -4.87155875e-02  6.35619160e-02\n",
            "  9.85590728e-03  5.14171438e-02 -2.50968110e-02  3.90366093e-03\n",
            "  4.80100823e-02 -7.47809489e-02  2.29308556e-04  4.19094200e-02\n",
            "  3.56768419e-02  4.41681621e-02  1.48502570e-02 -1.89680531e-02\n",
            "  9.97733871e-02 -1.47269903e-02 -1.47269903e-02 -1.47269903e-02\n",
            " -8.53545078e-03  1.67858841e-02 -6.74023848e-02 -1.04857644e-01\n",
            "  3.93886075e-02  2.12952114e-02  7.38003513e-02  1.98845118e-02\n",
            "  2.21351610e-02  4.38949483e-02 -8.77304033e-03 -4.32318357e-02\n",
            "  1.55665385e-02  1.83411119e-02 -9.29069882e-02 -5.94759186e-02\n",
            " -1.45007037e-02  7.29426920e-02  7.37041784e-02  5.08742402e-03\n",
            "  1.63633977e-02  1.63633977e-02  2.96241966e-02  3.58686884e-02\n",
            " -3.29444858e-02  2.36217299e-02  3.11996285e-02  7.17866298e-02\n",
            "  7.72996459e-03 -4.63458925e-02  1.54061962e-02 -3.97874232e-02\n",
            " -3.05124875e-02 -9.88965426e-02  1.26243746e-02  1.08906009e-02\n",
            " -1.12072874e-02 -6.65633553e-02 -8.89094072e-02  6.73504401e-03\n",
            " -3.04900125e-02 -2.63850608e-02 -4.38570859e-03 -8.29721215e-02\n",
            "  3.67940929e-02  8.47733592e-03 -3.19918368e-02  1.75410966e-01\n",
            "  9.18794223e-03  3.17690034e-03 -1.77430752e-02  4.09316044e-03\n",
            "  2.26600263e-02 -6.35543231e-02 -1.34863740e-02  2.78088721e-02\n",
            "  3.07210811e-02 -8.97346324e-03  2.51475493e-02 -2.23390151e-02\n",
            "  6.23963101e-02  1.41206108e-01 -7.49458391e-02  2.70349561e-02\n",
            "  7.14001905e-02 -1.13149125e-02]  - intercept :  0.6438383392896734\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.17977908407268767\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:14,169]\u001b[0m Trial 165 finished with value: 0.01979482580838977 and parameters: {'count_threshold': 7, 'postag': True, 'voc_threshold': 6363}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.         0.         0.         ... 0.16203994 0.         0.10616778]\n",
            " [0.09644586 0.12450196 0.0409288  ... 0.17879674 0.19568525 0.0412337 ]\n",
            " [0.03752245 0.04562903 0.08692254 ... 0.15357078 0.11079961 0.05399494]\n",
            " ...\n",
            " [0.47498429 0.72805487 0.18693057 ... 0.22134049 0.25866247 0.02091614]\n",
            " [0.02344844 0.02819316 0.14493471 ... 0.25435203 0.05590818 0.05806467]\n",
            " [0.         0.         0.05598789 ... 0.21263157 0.09253991 0.07811086]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-1.32144297e-02  1.94699930e-02 -8.59929475e-03 -6.63974850e-02\n",
            "  1.69941114e-02  2.64529737e-02 -3.97918017e-03  2.32383924e-02\n",
            "  1.13477780e-02  5.37927008e-03  6.65661192e-03 -2.11856123e-02\n",
            "  1.50573369e-02 -8.45698066e-03  1.51940459e-02 -1.33518636e-01\n",
            "  2.07576000e-02 -7.30242698e-03  2.85556027e-02  1.67307686e-02\n",
            " -1.14364758e-02  3.43631817e-02  4.07706222e-02 -1.88279030e-02\n",
            " -4.63564033e-03  1.73360915e-02  1.23318836e-02  6.52729294e-03\n",
            "  2.14382483e-03  1.93102110e-02  2.61200376e-02 -1.60295179e-02\n",
            "  2.01595594e-02 -3.58552500e-02  2.80214337e-02  8.61057149e-03\n",
            " -2.32624722e-02  3.13705492e-02  3.18341288e-02  2.84162283e-02\n",
            "  1.56854182e-02 -8.50068944e-02  2.35518489e-02  2.73584319e-02\n",
            " -5.24351235e-03  8.54729151e-03 -8.59732223e-03  1.55538403e-02\n",
            "  2.86081391e-03  3.38831303e-02  2.01227816e-02 -8.92677049e-03\n",
            "  6.54052422e-03 -7.07333540e-02  6.35095854e-02  2.87568677e-04\n",
            "  1.09746870e-02 -4.29230200e-02 -1.40229589e-01 -7.55415336e-03\n",
            " -5.84479752e-03  7.63986235e-04  2.48260715e-02 -1.05394744e-02\n",
            "  7.28138815e-03 -6.19130957e-02  3.81101503e-04  4.68362230e-02\n",
            " -4.54118146e-02 -1.64893924e-04  3.94787504e-02 -1.74537580e-02\n",
            "  4.62039589e-02 -4.98209716e-03  2.02859394e-02 -5.32231530e-02\n",
            "  1.67150416e-02 -8.78160358e-02 -4.23090435e-02  1.41420084e-02\n",
            " -2.38130267e-02  1.52930811e-02 -1.52795836e-02 -4.26312002e-03\n",
            "  4.09443372e-02 -1.38428478e-02  2.48855044e-02  2.77179718e-02\n",
            "  1.33924798e-02 -3.15789691e-02  3.58465826e-02  2.71002514e-02\n",
            " -1.81481871e-02  2.16205052e-02  3.81909598e-02  2.58700044e-02\n",
            " -1.30227084e-02 -3.77458391e-02  2.13225158e-03  7.16715452e-03\n",
            " -3.62015320e-02  4.78529549e-02  2.80988611e-02 -2.31436513e-02\n",
            "  2.75145999e-02 -1.16257961e-02  1.31312697e-02  1.70277721e-02\n",
            " -1.04907915e-02 -4.33802455e-02  1.54365011e-02  4.91476385e-02\n",
            "  1.47198074e-02 -6.11052908e-02  2.17236163e-02 -4.42389760e-03\n",
            " -1.49024470e-02 -1.64489303e-03  3.03079132e-02 -1.83365224e-03\n",
            " -2.92447387e-02  8.71520131e-02  2.30332388e-02 -6.30233739e-03\n",
            "  8.97322767e-04 -2.22590478e-02 -6.17792366e-02  2.72533647e-03\n",
            "  6.95688061e-03 -3.37186610e-02  2.15235944e-02 -3.87388910e-02\n",
            " -1.77483657e-02  2.22248217e-02  4.82037640e-02 -2.05228712e-03\n",
            " -5.56280121e-02 -5.44713885e-02 -7.41060228e-04 -3.96353645e-02\n",
            " -2.71916760e-02  1.40466689e-02 -8.52511990e-02  7.11805320e-03\n",
            "  5.95379668e-02 -1.69221001e-02 -1.72574022e-02 -3.29580253e-02\n",
            "  7.18957372e-03  2.96606015e-02 -3.07213911e-02 -1.77764633e-02\n",
            "  1.40007438e-02  3.10792185e-02 -2.44711317e-03 -3.97132765e-02\n",
            "  9.97483582e-03  1.36857368e-02 -1.83127399e-02  1.17825400e-04\n",
            " -1.33361284e-02 -5.81574462e-02 -4.91940141e-03 -2.38585750e-02\n",
            "  4.30592627e-02  1.66553429e-02 -1.74826998e-02  9.11304748e-03\n",
            "  4.59174761e-02  1.66903237e-02 -6.06455524e-02 -1.00945091e-02\n",
            " -9.42044168e-02 -1.95233108e-02 -4.08259771e-03  8.28342621e-03\n",
            "  1.82952662e-02  2.29097087e-02  4.51998576e-02 -1.62317456e-02\n",
            " -2.21220834e-02  2.26232788e-02 -9.30228008e-02  6.63097548e-02\n",
            " -4.77347022e-03 -5.46647677e-02  3.63581489e-03 -6.61452957e-02\n",
            "  1.91823460e-02  1.91823460e-02 -2.05930201e-02 -2.45091951e-02\n",
            " -2.03806580e-02 -5.39775291e-02  7.64906955e-02  7.11849454e-03\n",
            "  2.37350782e-03 -3.39915101e-02 -8.65309864e-03  1.95374299e-02\n",
            "  4.19405302e-02 -8.56127969e-02 -5.39463118e-03 -3.46597088e-02\n",
            " -4.96909590e-02 -3.26292295e-02  2.93292322e-02  2.56615652e-03\n",
            "  5.31258090e-02 -1.01449792e-02  2.28137234e-02 -4.46251985e-02\n",
            " -6.93189942e-02  2.71535399e-03 -5.52991058e-04  1.46263001e-03\n",
            "  1.57774545e-02 -5.05432196e-02  4.84304831e-03  1.99413830e-02\n",
            "  2.55084194e-02  2.45798646e-03 -5.55846721e-02  1.73116692e-02\n",
            "  3.77355382e-02 -4.95784069e-02  9.59480084e-03 -1.47298359e-02\n",
            "  2.25712056e-02 -8.62885985e-03  1.87879497e-02  4.01464979e-02\n",
            " -6.19661264e-03  2.90401391e-02  3.68850523e-02  1.13436584e-02\n",
            "  1.78023209e-02  4.64859117e-03  6.37312890e-02 -1.24526148e-02\n",
            "  1.93562616e-02 -2.55703038e-03  2.41678610e-02  4.45356555e-03\n",
            "  9.14252953e-03  5.76176127e-02 -2.76565491e-02  1.12000882e-03\n",
            " -5.87661434e-03  1.14956321e-02  1.92674565e-02  2.00466379e-02\n",
            "  1.36092753e-02 -3.97562898e-02  1.49365386e-02  1.14266081e-02\n",
            " -2.01634042e-02 -6.61556767e-03 -7.33360446e-03  4.64170973e-02\n",
            "  1.36371749e-01 -2.24122090e-02  9.62110846e-03  1.69396486e-02\n",
            "  3.08674433e-03  2.24797707e-02 -4.06862387e-02  2.41637398e-02\n",
            " -1.28519621e-03 -3.45376302e-02 -4.27926318e-02 -1.12518670e-02\n",
            "  5.88523511e-03  1.21938661e-02 -4.64680169e-02 -1.60588220e-02\n",
            " -1.79868022e-02 -2.57109062e-02 -1.47923533e-02 -3.68160013e-02\n",
            " -2.94573949e-03 -5.95213872e-02 -1.21643871e-02  2.82604662e-02\n",
            " -2.44086940e-02 -1.61312020e-02 -6.32257490e-02  1.79741936e-02\n",
            "  5.27616780e-03 -9.48278752e-03 -7.27337178e-03  3.10925566e-02\n",
            "  1.27096994e-03  2.33443798e-02 -2.88889300e-02  2.39554927e-02\n",
            " -2.45689978e-02  8.74804564e-03 -8.38566925e-04 -4.36963483e-02\n",
            "  5.22884834e-02  5.12205532e-03  8.84890222e-03 -4.31424627e-02\n",
            " -2.97009540e-02  3.11104467e-02 -6.81303481e-03  1.21199721e-02\n",
            "  1.59620861e-03  2.56199584e-02 -2.84262517e-02 -2.41936966e-02\n",
            " -3.03079553e-02  3.81847285e-02 -2.68423672e-02  2.17903530e-02\n",
            " -9.01138480e-03  5.18861200e-02 -9.16687977e-02  4.49415611e-02\n",
            "  7.85003317e-03 -9.33481277e-04 -6.44214506e-02 -1.22793707e-02\n",
            "  3.39029268e-02  1.38087893e-01  3.35352319e-02  4.39432185e-02\n",
            "  1.91076112e-03 -1.14683947e-02 -7.36999491e-02 -4.69844582e-02\n",
            "  3.35769729e-03 -1.17542534e-02  4.69395156e-02  4.24666005e-02\n",
            " -4.73321954e-02  6.45749311e-02  6.71408218e-03  2.83676604e-02\n",
            "  3.26257977e-03  6.53278432e-03 -1.30814302e-03 -1.53260086e-02\n",
            "  3.66611528e-02  2.00462900e-02 -1.69565191e-02  1.12009240e-02\n",
            " -3.79635829e-02 -1.00223173e-02  6.08922501e-03  4.02487112e-02\n",
            "  2.18887321e-02 -4.71686661e-02  3.30291002e-02 -1.15207383e-02\n",
            " -4.86303034e-03  5.48988602e-04  4.49795231e-03 -3.24525665e-02\n",
            "  2.93151371e-04 -4.66325062e-02  4.41208786e-03  2.35362001e-02\n",
            " -2.89729195e-02 -9.48296412e-03 -4.28637370e-02 -1.48077324e-02\n",
            " -7.60246235e-03 -3.44791042e-03 -1.34420095e-02  2.42566933e-03\n",
            " -6.71293982e-03 -2.66923853e-02  2.23078340e-02 -5.78818747e-02\n",
            " -7.34014153e-03  2.86151771e-02  1.24725014e-02 -5.22204020e-02\n",
            " -1.28009017e-02  1.08431089e-02  6.30580195e-03 -1.76865370e-02\n",
            " -1.28253826e-02  1.26581153e-02  2.03899749e-02 -8.49411584e-03\n",
            " -2.43555107e-02 -2.36542750e-02 -2.70945161e-02  1.68899769e-02\n",
            "  1.68007147e-02  1.65713538e-02 -1.72479138e-02  1.75510788e-02\n",
            " -7.33603786e-02  3.59943950e-02  2.98076011e-02  1.86174429e-02\n",
            " -3.98281869e-03 -1.54976886e-02 -1.99124440e-02 -3.31350020e-02\n",
            " -3.37466817e-02 -3.79887744e-02 -8.77367265e-03 -5.72197586e-03\n",
            "  6.93662804e-03  1.65791797e-02  1.88253268e-03  6.12748918e-02\n",
            "  3.76933753e-02 -1.55192785e-02 -2.85015029e-02 -3.17460899e-02\n",
            "  1.90569657e-02  3.95753309e-02  7.03609120e-03 -6.21884136e-02\n",
            " -1.54633940e-02 -2.52557011e-02 -2.52557011e-02 -5.77725301e-02\n",
            " -2.28183329e-02  1.94608315e-02 -4.01723812e-02  6.49787125e-02\n",
            "  2.63390449e-02  1.06568484e-03 -1.71432203e-04 -6.15169623e-03\n",
            "  2.84035018e-02 -1.84253876e-02 -2.80526526e-02 -6.19740814e-02\n",
            " -4.55950412e-02  3.38319695e-02  2.18822781e-02  2.73676275e-02\n",
            " -3.47446291e-03  2.37087717e-04 -2.09329711e-02  2.03878018e-02\n",
            "  5.53053643e-02 -4.18442869e-02 -1.47584389e-02  2.53945620e-02\n",
            "  3.03388084e-03 -2.57708272e-02  5.54087126e-04  5.08523174e-02\n",
            "  1.04880220e-02  2.80775477e-02  2.97703675e-02  5.13393409e-02\n",
            "  1.43394327e-02  8.14497232e-02 -2.02245528e-02  4.33356206e-03\n",
            "  2.57818298e-03  1.46317387e-02  1.59570128e-02  1.03981648e-02\n",
            "  6.02313989e-03 -1.12029597e-02]  - intercept :  0.8452038661362193\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.01979482580838977\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:14,771]\u001b[0m Trial 166 finished with value: 0.10451397720188932 and parameters: {'count_threshold': 6, 'postag': True, 'voc_threshold': 6094}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.00420472 0.01583841 0.01722902 ... 0.         0.1158435  0.11969384]\n",
            " [0.         0.01693614 0.         ... 0.24997395 0.         0.06732033]\n",
            " [0.05369831 0.02333614 0.12146724 ... 0.         0.42729034 0.07436515]\n",
            " ...\n",
            " [0.03549695 0.02111238 0.0831845  ... 0.         0.1158435  0.07312141]\n",
            " [0.03733887 0.06854593 0.17244886 ... 0.         0.09088549 0.04304166]\n",
            " [0.         0.01714279 0.         ... 0.32535839 0.         0.08400372]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-3.82781683e-03 -8.81639932e-03  1.65973657e-02  1.42899723e-02\n",
            "  1.18064560e-02 -1.71813593e-03 -3.71127568e-03 -9.72572759e-03\n",
            "  1.54689803e-02  2.56820926e-03  1.40402835e-02  1.13160620e-03\n",
            "  5.91256691e-03  1.86726304e-02 -2.04693140e-02  1.84853291e-02\n",
            " -1.77989799e-02 -1.19583376e-02  4.96398866e-03 -4.41054465e-03\n",
            " -1.40814537e-02  3.39313238e-02 -2.79054193e-02  3.91554127e-03\n",
            "  1.45414135e-02  2.61249445e-02 -1.47466296e-02 -1.48996791e-02\n",
            " -2.97724549e-03  2.76529032e-03  1.41495768e-02 -6.16133982e-03\n",
            "  4.01676848e-02 -4.16389176e-03 -9.40550274e-03  1.95861496e-03\n",
            " -2.20728539e-02  3.20669529e-02  1.81001927e-02  3.50847513e-02\n",
            "  2.23809828e-02  1.08044133e-02  1.98021291e-02  3.02494517e-02\n",
            " -3.78207638e-03  2.75596164e-02  4.47678680e-02 -1.46788987e-02\n",
            "  1.20077118e-03  2.71012832e-02 -1.64687587e-02 -1.32895856e-02\n",
            " -3.07320881e-03  1.82832678e-02  1.87051898e-02 -3.39093194e-03\n",
            "  3.05164558e-02  1.16004532e-02  7.51262277e-03 -3.90304136e-03\n",
            "  6.60912651e-02 -3.64260602e-04  1.74184313e-02 -1.52049676e-02\n",
            " -7.22505438e-02 -4.22096958e-03  2.82757204e-02  1.42321858e-02\n",
            "  1.91893561e-03 -5.20375958e-02  6.95401937e-03  5.61651044e-02\n",
            " -2.54383832e-02  1.26898656e-02  3.17116098e-02 -1.13122779e-02\n",
            "  1.08964936e-02  3.95408052e-03 -2.20926611e-02  2.22607009e-03\n",
            " -3.47886305e-02  3.44989757e-02  2.36610476e-02 -1.90211546e-02\n",
            "  3.77237902e-02  1.43778262e-02 -2.35261048e-03  5.21569446e-02\n",
            "  1.35365157e-02  9.82969146e-03 -4.22495354e-02 -1.74863649e-02\n",
            " -1.83775600e-02 -2.23326485e-02 -1.60329197e-02  1.62618537e-02\n",
            "  1.93149606e-02  4.90226472e-03 -5.63859501e-04  2.14709991e-02\n",
            " -1.96053013e-02  1.09399775e-02  6.32199435e-02  2.92892185e-03\n",
            " -3.88502069e-03  3.79461195e-03  3.75389854e-03  1.70501884e-03\n",
            "  1.39543928e-02 -1.96136016e-02 -3.13059705e-02 -1.09788768e-03\n",
            " -2.48146401e-02  6.57733483e-02 -6.59766066e-03  7.50836228e-03\n",
            "  5.16731113e-03  7.44609026e-03 -1.01128819e-04  7.96474351e-03\n",
            " -9.99091222e-03 -1.10094236e-02 -7.16068904e-02  8.22913079e-02\n",
            "  4.58361424e-03  1.75557344e-02 -2.01682391e-03 -1.19335547e-02\n",
            "  1.42221052e-02 -2.42578091e-02  3.69770783e-02 -5.00310912e-02\n",
            " -6.47034872e-04  1.23057796e-02 -1.06289652e-02 -3.53993233e-02\n",
            " -8.24113498e-03 -9.87801294e-03 -2.55787306e-02  2.24797586e-02\n",
            " -3.87912561e-02  5.69728827e-03 -1.89366066e-02 -6.87327661e-03\n",
            "  4.91102144e-02 -1.07916997e-02 -1.78050282e-02  5.69329915e-02\n",
            " -7.18439963e-02  5.83439097e-02  6.62691287e-03  6.15067796e-03\n",
            "  1.37683422e-02 -2.06013801e-02  3.40500997e-03  4.54240597e-03\n",
            "  9.31851281e-03 -9.93614695e-03 -2.26251703e-02  2.68410198e-02\n",
            "  3.10093587e-02 -4.46418717e-03 -6.73181630e-02 -3.15424905e-04\n",
            " -1.93937300e-02  1.81854887e-03 -2.65403366e-02 -3.68312965e-02\n",
            "  7.03228707e-02  1.01380650e-02  1.62155467e-02 -1.07413549e-02\n",
            " -1.81764862e-02 -7.92652913e-02  1.21489396e-02  6.09549269e-02\n",
            "  4.02874060e-04  4.76718325e-03  4.17072609e-03  5.58172727e-03\n",
            "  8.73757951e-03 -1.87069480e-02 -3.98160629e-02  1.84038668e-03\n",
            " -2.77429276e-03 -9.01502615e-04 -1.12457312e-02  2.92565163e-02\n",
            " -7.15692490e-02  1.82074021e-02  1.02097062e-01  5.69164425e-02\n",
            " -1.41168430e-02  1.60221890e-02  1.54664669e-02 -9.29007186e-03\n",
            "  3.23045726e-03 -1.22826113e-02  3.18008488e-02  4.24663692e-02\n",
            "  1.90161640e-02  6.42491563e-03  4.53748461e-03  7.62541920e-03\n",
            "  1.87408380e-02 -3.91363502e-02  2.93749189e-02  1.04823557e-02\n",
            " -1.74186707e-02  1.11780444e-02 -2.86341302e-02 -3.71628117e-03\n",
            "  1.29126617e-04 -3.38391481e-03  3.31878212e-02  1.72091998e-02\n",
            " -1.11034084e-02  3.79029546e-02 -1.74213100e-02 -5.08103742e-03\n",
            " -3.32955713e-03 -1.73811759e-02 -1.94542569e-03 -3.67366140e-03\n",
            " -1.54041949e-02 -2.34010751e-03  9.78680680e-03  3.47179870e-02\n",
            " -2.09084628e-02  3.78971862e-04  6.97039580e-02 -1.06978854e-02\n",
            " -6.32721433e-03  3.23134957e-02  1.25633598e-03 -4.03589266e-03\n",
            "  1.22931135e-02  3.73269511e-02  3.42051998e-02  3.23124950e-02\n",
            "  6.28328166e-02  3.62859425e-02 -1.47585133e-02  4.13618321e-02\n",
            "  2.27305264e-02 -4.21691639e-02 -2.64523220e-02  1.36574515e-02\n",
            "  5.68877502e-03  2.55253007e-02 -4.39418697e-02  1.19099084e-02\n",
            "  4.93743078e-02 -1.15522164e-02  4.72494928e-02  5.12189297e-02\n",
            "  4.71472695e-03 -2.13208571e-02  3.33192877e-02  8.52413800e-03\n",
            " -1.28137312e-01 -1.02505716e-02 -4.37461928e-02  5.90484206e-02\n",
            " -5.74690859e-04 -7.65534471e-03  4.81338977e-03 -4.28722512e-03\n",
            " -5.01414173e-04  1.19963898e-02 -8.03439651e-03 -1.70833763e-02\n",
            " -1.49911362e-02  5.80308696e-02 -7.79877707e-03  1.79025511e-02\n",
            "  3.37900465e-02  1.39087990e-02  2.21697654e-02  3.21922121e-02\n",
            "  3.78021242e-02 -5.37850779e-02 -2.31703182e-02  1.92155280e-03\n",
            "  2.10978559e-02 -1.04872189e-02 -2.49568667e-03 -2.85201522e-02\n",
            "  2.39383271e-02  5.71765543e-03  2.10229482e-02 -1.12152909e-01\n",
            "  3.06184622e-02 -1.68629381e-03  2.96763845e-03  1.62578498e-02\n",
            "  1.52731945e-02  1.32674971e-02  4.44577062e-03 -1.04027710e-02\n",
            "  2.00350967e-02  1.92563381e-02  1.80202675e-02  4.94302380e-03\n",
            "  1.40388035e-02 -7.38033676e-03  8.42316790e-03  4.66569027e-03\n",
            "  2.28476864e-02  1.31787183e-03 -4.12234946e-04 -8.31987261e-03\n",
            "  1.61670175e-03 -2.39430257e-02 -9.59270675e-04 -9.40506533e-03\n",
            " -8.15372363e-04 -1.44083918e-02 -1.63696436e-02  1.70650675e-02\n",
            "  7.04823650e-03 -2.22831197e-03  1.44841029e-02  1.16566826e-02\n",
            " -2.28267899e-02  3.58862360e-02 -2.12789530e-03 -1.00918415e-04\n",
            "  2.04543748e-02 -2.60072065e-02  2.55513842e-02 -6.14385832e-03\n",
            "  9.92051698e-03 -1.81834671e-02 -1.23002122e-02  2.92498656e-02\n",
            " -4.37132130e-02  6.59921132e-02  7.47297555e-03 -2.65263441e-02\n",
            "  3.64207796e-02  2.27327181e-02 -1.06768587e-02  2.41956506e-02\n",
            "  7.94169497e-03 -4.21448502e-02 -7.16905859e-04  1.97663428e-02\n",
            " -4.15115710e-03  1.90608520e-02 -1.40148545e-02  1.72271016e-02\n",
            " -3.60874707e-02 -6.92994994e-03 -5.53688160e-03  1.26227936e-02\n",
            " -3.49669659e-04  1.40383894e-03  2.24539898e-02 -4.88594009e-02\n",
            "  5.92433244e-03  1.83340221e-03 -2.80995995e-02 -3.30788302e-02\n",
            "  1.13989180e-02 -1.26765440e-02  3.95807984e-04 -1.53040174e-02\n",
            "  8.44047645e-03  1.21198111e-03 -4.40192529e-02  4.13974564e-02\n",
            " -4.10503059e-03 -6.15175540e-03 -2.57898585e-02 -3.58555669e-02\n",
            " -6.52424794e-03 -1.03858496e-02  8.48888388e-03  2.87645585e-02\n",
            "  1.44686443e-02 -6.91573984e-03 -8.72253834e-03  2.62999383e-02\n",
            " -9.29114773e-03  3.91839989e-03  1.35402410e-03  2.78749870e-02\n",
            "  1.50827275e-02 -9.20208587e-03  3.24852101e-02  1.43469800e-02\n",
            "  1.69361185e-02  2.54809744e-02 -1.49189956e-02  2.70065779e-02\n",
            " -3.48294202e-02  5.31650199e-03 -1.10087051e-02  7.50944378e-03\n",
            " -2.18683329e-03 -2.73143927e-02 -6.89380231e-02  1.90972663e-02\n",
            "  1.90972663e-02  5.01327720e-02 -4.90300143e-03 -1.37739889e-02\n",
            " -5.87278210e-02  1.69760780e-02  6.35549609e-03  3.55461602e-02\n",
            " -2.68460303e-03 -1.02890354e-02  1.17223908e-02 -3.83067891e-03\n",
            " -2.70857555e-02  1.31781120e-02 -6.63028134e-03  1.67293368e-02\n",
            " -3.96374201e-02  5.17192392e-02 -2.35537164e-02  1.99239276e-02\n",
            " -1.04559084e-02 -6.63650502e-02  3.55375376e-02 -1.92209736e-02\n",
            "  9.96466521e-03 -4.10839850e-02 -7.10595757e-03  2.92028371e-02\n",
            " -1.14177708e-02  2.33944626e-02 -1.13611553e-02 -3.74203361e-02\n",
            " -1.79159810e-02  2.14116689e-02 -9.38849190e-03  2.40531639e-02\n",
            " -1.52124814e-02  2.44791377e-03  1.01249072e-02  3.13696230e-02\n",
            " -1.41815732e-02  2.97218654e-02 -1.46432051e-02  3.27468841e-02\n",
            "  9.52911832e-03 -2.59105655e-02  1.12514546e-02 -8.09476788e-03\n",
            "  2.52571729e-03 -6.74256557e-03  1.65506491e-02  5.94221829e-03\n",
            " -2.47716881e-02  2.11211490e-02 -3.38817682e-03  3.05238594e-03\n",
            " -9.86391881e-02 -3.28469586e-03  1.95165888e-02  7.61481026e-03\n",
            " -6.83388787e-03  1.91869387e-03 -1.79250824e-02 -3.50532230e-02\n",
            " -1.14397826e-02 -3.82066088e-03 -1.16661073e-02 -1.20948896e-03\n",
            " -7.01073430e-03  1.95712406e-02  8.26675626e-03 -1.05401090e-04\n",
            "  2.86387934e-02  2.34721949e-02  2.07272566e-02 -4.53154343e-02\n",
            "  1.83996503e-03  2.34250992e-03  1.36037774e-02 -1.18846833e-02\n",
            "  8.32838912e-03  3.36614157e-02 -1.69771829e-02 -4.42010509e-02\n",
            "  2.20629442e-02  6.58720637e-02  2.49648743e-02 -2.73869673e-02\n",
            "  3.30611430e-04  3.27313617e-02 -5.90141334e-03 -9.82516315e-03\n",
            " -1.45435913e-02  1.00336634e-03  9.33325885e-02  2.42566232e-02\n",
            "  1.58700188e-02  1.19708984e-02  1.59707277e-02 -1.24139336e-02\n",
            "  1.47821462e-02 -2.75458961e-03 -9.37810835e-03  1.17871557e-03\n",
            " -2.87269101e-03 -3.02546996e-02 -1.23879806e-02  2.61635491e-03\n",
            "  3.36366365e-02 -2.05258920e-02 -5.77122391e-03  1.17401190e-02\n",
            "  2.50405130e-02 -6.67214456e-03 -4.19315742e-02 -1.71905922e-02\n",
            " -1.23746847e-02 -1.86594074e-02  7.80161024e-03  3.32790026e-03\n",
            "  4.25599088e-02  1.57468091e-02 -2.69197462e-02 -1.91765171e-02\n",
            "  9.17573205e-03 -9.09822800e-03  1.26566010e-02 -2.40301210e-02\n",
            "  1.14253142e-02  2.79614339e-02 -7.07592836e-03  9.27245429e-03\n",
            "  3.09126018e-02  2.83971301e-02 -5.28257029e-03  2.55838844e-02\n",
            " -6.27071086e-02 -1.97744720e-02 -1.95367462e-02  1.57093803e-02\n",
            " -8.01841491e-03  5.42834636e-03  2.14498314e-02  5.22035153e-02\n",
            " -4.64854884e-02  4.17823190e-02  2.93904823e-02  1.31795935e-03\n",
            "  3.81139149e-02  2.70780241e-02 -4.52937545e-02 -1.07822968e-02\n",
            "  1.30044693e-03  3.21259101e-02 -2.50281596e-02  6.24164804e-03\n",
            " -3.62539002e-02  5.88135232e-02 -5.44016307e-03  7.05638855e-03\n",
            " -1.50988866e-02 -4.27568071e-04 -2.48278044e-02  5.59887074e-03\n",
            " -5.93183385e-02  2.61943783e-02 -1.72054004e-02  7.12721059e-02\n",
            "  2.40305991e-02  2.12138477e-02 -1.63168344e-03  1.25326880e-02\n",
            " -2.14393629e-02  4.04121613e-03  9.76731542e-03  1.92858300e-02\n",
            " -9.96233076e-03  6.20744388e-03  1.16312299e-02 -3.57988721e-02\n",
            "  1.34866377e-02 -3.27484293e-02  6.08381491e-02 -4.28710118e-02\n",
            " -4.06736402e-02  1.38601011e-03 -1.82760795e-02 -2.05348871e-02\n",
            "  2.30028909e-02  1.24787799e-02 -1.77712478e-02  6.22614536e-03]  - intercept :  0.4028089290611811\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.10451397720188932\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:15,357]\u001b[0m Trial 167 finished with value: -0.15002917247183292 and parameters: {'count_threshold': 7, 'postag': True, 'voc_threshold': 6571}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[6.31048093e-04 1.96320860e-01 2.04990466e-02 ... 1.41936334e-01\n",
            "  0.00000000e+00 5.66906293e-02]\n",
            " [0.00000000e+00 1.34671142e-02 0.00000000e+00 ... 1.50679182e-01\n",
            "  0.00000000e+00 6.45275760e-02]\n",
            " [7.13053930e-01 2.09931470e-02 6.14167721e-01 ... 0.00000000e+00\n",
            "  9.89519738e-01 0.00000000e+00]\n",
            " ...\n",
            " [2.94343519e-01 1.91308011e-01 4.12293742e-01 ... 0.00000000e+00\n",
            "  3.86345731e-01 1.19888334e-02]\n",
            " [6.22535689e-01 2.50783075e-01 7.70470973e-01 ... 1.59678376e-01\n",
            "  7.50128476e-01 6.48217120e-03]\n",
            " [2.88803150e-02 1.27122788e-02 3.07289923e-02 ... 4.43174065e-02\n",
            "  1.67692047e-01 4.35979515e-02]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-3.55374463e-02 -1.99370142e-04 -1.70297044e-02  4.57995597e-03\n",
            " -5.26899615e-03 -4.53289309e-03 -8.83907184e-03 -1.36983227e-02\n",
            " -6.53717551e-03 -1.49268608e-03 -3.87184822e-02 -2.30718676e-03\n",
            " -4.94958707e-02  4.35239819e-03  1.74818137e-03  1.54296811e-03\n",
            "  3.50698164e-02 -4.37283254e-02 -1.10234595e-02  2.35733781e-02\n",
            "  5.68487304e-04 -8.33668789e-03 -4.60352683e-02 -3.15833962e-02\n",
            "  4.35117404e-02 -4.62220073e-03  4.30683519e-03 -1.97342848e-02\n",
            " -4.41085924e-02  2.71118581e-02 -5.34608278e-04 -8.14420151e-03\n",
            " -1.45256910e-03 -1.82955183e-02 -1.50033601e-02 -1.58111524e-02\n",
            " -2.95207058e-03  1.89255441e-02  1.10620712e-02 -1.36781998e-02\n",
            "  2.83937759e-02  4.25389065e-02 -3.42144838e-02 -8.27590028e-03\n",
            "  5.01285615e-03 -3.38019494e-02  3.19362328e-03 -1.21421676e-02\n",
            "  3.08038233e-03  3.34396596e-02 -1.45383428e-02 -1.54544565e-02\n",
            " -1.53458054e-02  2.44071555e-02 -2.90945035e-03  1.52206318e-02\n",
            "  2.59427718e-03 -2.41865336e-02 -1.93721665e-02 -2.71503967e-02\n",
            " -1.74197592e-02  1.31483370e-02  4.37516531e-03 -9.35356039e-03\n",
            "  3.05952342e-02 -3.28655400e-02  3.03230866e-03 -2.35839278e-02\n",
            " -3.41065767e-02  6.18392170e-03  3.68065401e-02 -2.10610842e-02\n",
            " -1.89594490e-02 -2.04780665e-02 -2.19729821e-02 -1.70739085e-02\n",
            " -5.77443626e-03 -1.81039621e-03 -7.20009750e-02  2.01367566e-02\n",
            " -1.91831438e-02  4.75919927e-03  6.63317727e-03 -1.73594008e-02\n",
            " -1.42049354e-02 -2.45485728e-02  3.35175302e-02 -2.65282039e-02\n",
            " -3.32072745e-02 -2.44483219e-02 -2.71767232e-02  1.28963962e-02\n",
            " -1.44898768e-02  1.46200184e-02 -4.96736200e-03 -7.41182712e-03\n",
            " -1.36088041e-03 -1.47292952e-02  9.08865708e-04 -6.75919222e-03\n",
            "  9.76363207e-04  3.73332024e-02  1.51999584e-02  6.12470984e-03\n",
            " -1.43950517e-02  2.80983170e-02  1.19703009e-02  1.50174853e-02\n",
            " -1.88621906e-02 -1.13414394e-02  6.92389553e-03 -3.98802837e-02\n",
            "  1.51751895e-02 -2.63272156e-02 -2.01659778e-02 -3.79887457e-02\n",
            " -3.96038523e-02  1.81945876e-02 -3.37869307e-02  1.82234565e-02\n",
            " -5.36528765e-02 -4.94691007e-02 -4.53039194e-02  2.42405729e-02\n",
            " -6.34901651e-03 -4.93824748e-02  1.66327067e-03  3.68628802e-03\n",
            "  5.80680726e-03 -7.75771661e-03 -3.05763926e-02 -2.84652481e-02\n",
            " -1.22480584e-02 -2.36118910e-02 -3.85809812e-02 -2.20905351e-02\n",
            " -2.72586995e-02 -2.06257619e-02 -6.72060332e-02  4.90425393e-03\n",
            " -2.16024781e-02  2.50130316e-02  5.08165592e-02  4.19286118e-02\n",
            "  2.56932419e-02  8.49411872e-03  1.67117158e-03 -5.86474169e-02\n",
            " -3.91873951e-02  1.69164706e-02 -7.15287873e-04 -2.36872817e-02\n",
            "  5.31376305e-02 -3.89649704e-02 -5.89238558e-03  6.09581494e-03\n",
            "  1.45445908e-02  1.67909447e-02 -5.55985095e-02 -4.75687677e-02\n",
            "  6.22291401e-03 -3.53242514e-02 -2.49756303e-02 -2.12913187e-02\n",
            " -2.25716588e-02  4.45720687e-02  5.79250128e-03 -9.53815564e-03\n",
            "  2.22670340e-02  6.58090568e-03  2.57042835e-02  1.66682814e-02\n",
            " -5.64720438e-02 -3.78276496e-02 -4.90607300e-03 -3.50229547e-02\n",
            "  4.23343837e-02  1.32796534e-02 -3.50810258e-04 -5.33311056e-02\n",
            "  3.48437702e-02 -1.34487766e-02 -3.16200882e-03 -3.55958154e-02\n",
            "  4.09645268e-02 -4.38839323e-02 -7.47390143e-03  8.86978595e-03\n",
            " -8.90470056e-03  3.77721914e-02  1.65910634e-04 -3.42346024e-02\n",
            " -3.92591773e-02 -4.68308265e-03 -1.60006953e-02  1.52465209e-02\n",
            "  1.32820071e-02  2.70693740e-03 -1.48978219e-02  7.11520111e-03\n",
            " -2.64378214e-02  3.48451194e-02 -5.94627359e-03  2.94439876e-02\n",
            "  2.97485130e-02 -2.42002419e-02 -1.25635266e-02  2.67050514e-02\n",
            "  3.20042862e-02 -2.14110553e-02 -1.63547933e-02 -1.72530084e-02\n",
            " -2.03289235e-02  1.18050540e-02  1.79527740e-02  1.19375153e-03\n",
            " -8.35366759e-04 -5.06849573e-02 -1.32812854e-02 -1.05106095e-02\n",
            " -4.78773452e-03  2.49882586e-02  1.12117630e-02  2.39098559e-02\n",
            " -3.28674526e-02 -3.27422100e-02 -1.70727919e-02  6.33857057e-02\n",
            " -1.97247875e-02 -7.67934331e-02  7.45490729e-02  3.83435710e-02\n",
            "  1.76965038e-02 -4.85108815e-03 -2.74863907e-03  6.51555458e-04\n",
            " -2.63231580e-02 -3.90754350e-02 -2.22030474e-04  2.87771753e-02\n",
            " -6.55575260e-03  1.60390030e-04  2.43879764e-02 -2.56931757e-02\n",
            " -8.34842187e-03  4.55121533e-03  1.28152104e-03  3.58788825e-02\n",
            "  1.09715589e-02 -2.12666546e-02  1.89734776e-02  3.26031819e-02\n",
            "  3.12363501e-02  6.63986062e-03  2.59152076e-02  1.28140089e-02\n",
            " -1.79157883e-02  2.33863080e-02  1.79896284e-02  4.70167283e-02\n",
            " -1.76396630e-02  9.67548474e-04 -1.56346243e-03 -5.22728158e-03\n",
            " -2.45782520e-03 -1.03053628e-02 -2.03663742e-02  1.02207276e-02\n",
            " -3.29754533e-03 -1.55725012e-02 -1.77520468e-02 -8.80578731e-03\n",
            "  7.65860017e-03  1.79213780e-02  5.03694357e-02 -4.15834090e-02\n",
            " -2.80414902e-02 -3.75739088e-02 -3.59174479e-02  3.14327853e-03\n",
            "  1.43697994e-02  1.51966951e-02 -1.37749998e-02  3.11778934e-02\n",
            "  5.48784394e-02 -2.84735698e-02 -2.13944356e-02 -1.09386110e-02\n",
            "  4.93128381e-02 -1.39942899e-02  1.75794141e-02 -9.79959400e-03\n",
            " -7.42270079e-03 -2.66517965e-02  1.15644732e-02  1.90930307e-02\n",
            " -5.26593654e-03  5.95355858e-03  2.13255970e-02  3.60939520e-02\n",
            " -7.23230860e-02 -1.87429753e-03 -1.72452500e-02  5.00162365e-03\n",
            " -1.01138003e-02  5.00703064e-02  3.58871820e-03 -1.35469420e-02\n",
            " -3.32237309e-03 -5.41883722e-02  5.91234080e-02  4.44652487e-02\n",
            " -5.83199390e-02 -7.04565894e-03  1.41409000e-02  1.87609542e-02\n",
            " -5.71326544e-03 -6.78313555e-03 -2.86038708e-02 -2.14203078e-02\n",
            " -4.24271338e-02 -2.60881649e-02  9.86788823e-03  2.63001631e-02\n",
            "  2.82115886e-02 -2.09920268e-02  7.31359755e-04 -2.06184522e-02\n",
            " -1.07489897e-03 -3.64064319e-02 -8.13669459e-03 -2.07011614e-02\n",
            " -1.25917490e-02  4.74369075e-03  8.18051760e-03 -2.18913353e-03\n",
            "  1.04391644e-02  8.83520288e-03  1.97993550e-02 -8.17175391e-03\n",
            "  6.75833624e-02  1.60034230e-02 -2.10272211e-03 -1.90275785e-02\n",
            " -1.73257361e-02 -4.63783148e-02  1.26966697e-02  1.93380122e-02\n",
            " -2.79330509e-02  1.35328539e-02 -4.82892414e-02 -2.45283935e-02\n",
            " -2.89500652e-05  1.93592409e-02 -2.22157249e-02 -3.04999822e-02\n",
            "  3.48141937e-03  4.11971983e-03  2.36717629e-02 -2.86486544e-02\n",
            " -5.26050624e-02  1.83681105e-02  4.15418698e-03  3.82756965e-02\n",
            " -1.35109339e-01 -3.01558405e-02 -2.91012931e-02 -2.65194042e-02\n",
            " -8.14714771e-04 -8.88900433e-02 -8.82406981e-02 -6.94983725e-03\n",
            " -6.85230342e-02 -9.12510003e-02  1.68471148e-02 -3.55629328e-02\n",
            "  4.29878383e-02 -4.31735814e-03  4.98898230e-02  2.99214027e-02\n",
            " -1.02483956e-01 -7.43820616e-02 -2.18979702e-04  2.10212215e-02\n",
            "  2.19033448e-02  2.61694362e-02 -2.79559312e-02  7.72258675e-02\n",
            " -3.56141804e-02 -6.51588972e-02  1.76261001e-02  2.73385781e-02\n",
            "  2.04281527e-02 -1.04834726e-03  7.55420389e-03  9.95862853e-02\n",
            "  2.45269972e-02  9.90712284e-03 -3.38754719e-02 -1.52881123e-02\n",
            "  2.73776884e-02  6.15487716e-02  8.28253580e-02 -3.24263351e-04\n",
            " -5.98957501e-03  3.88443315e-02 -5.99895109e-02 -1.11414754e-02\n",
            "  1.71310766e-02 -4.11296384e-02 -4.97355340e-02  6.45180336e-02\n",
            "  1.21028789e-02  2.26891486e-02 -3.88433142e-02  8.88666747e-02\n",
            "  1.09959924e-02 -2.27360815e-03 -9.05902019e-03 -1.52810625e-03\n",
            "  1.26656343e-02 -1.12998021e-02  3.34825781e-02 -7.22459976e-03\n",
            "  1.07085575e-02  2.95645668e-02 -1.49605274e-03 -2.52786160e-02\n",
            " -3.01561967e-02 -2.78659551e-02  4.97171818e-02  1.96776528e-02\n",
            "  2.66478950e-02 -2.15514620e-02 -3.21634245e-02  2.25385189e-02\n",
            " -3.39193376e-02  3.59291430e-02  5.58223342e-02  8.21457727e-03\n",
            " -2.14006717e-02 -4.49088690e-03  1.45721391e-02 -2.99244138e-02\n",
            "  6.67424337e-02 -4.19509773e-02 -3.71837369e-03 -4.60627378e-02\n",
            "  6.92302820e-02  2.56815147e-02  1.30378311e-03 -3.61396491e-02\n",
            " -1.69679708e-02 -4.25672550e-02 -1.73628652e-02 -5.81662609e-03\n",
            "  4.14514790e-02  4.32347704e-03 -2.21366045e-03 -3.26339729e-02\n",
            " -8.15223022e-04 -9.71144664e-03  1.45449796e-03 -3.41176326e-02\n",
            " -1.16163524e-02  5.65814281e-02  7.01792081e-03  1.98745708e-02\n",
            "  4.09067477e-02 -1.69134942e-04]  - intercept :  0.7337891025838146\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.15002917247183292\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:15,948]\u001b[0m Trial 168 finished with value: -0.09771646774893049 and parameters: {'count_threshold': 7, 'postag': True, 'voc_threshold': 4943}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.         0.         0.         ... 0.04627717 0.13352073 0.06489401]\n",
            " [0.         0.01954728 0.02076938 ... 0.13914237 0.14328974 0.07693237]\n",
            " [0.29207901 0.07961154 0.4174612  ... 0.02429552 0.07009838 0.00828423]\n",
            " ...\n",
            " [0.36164862 0.01907797 0.47469216 ... 0.188579   0.17343394 0.02761783]\n",
            " [0.         0.         0.         ... 0.1483978  0.3349104  0.04569529]\n",
            " [0.03410269 0.02483408 0.14568816 ... 0.02045938 0.39480849 0.05033142]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 2.20999320e+08 -3.91728841e+08 -5.50881290e+08 -1.04739628e+08\n",
            "  1.13164026e+09 -2.05933923e+08 -1.00462590e+08  4.98418654e+07\n",
            " -2.22755145e+07  1.13653841e+08  2.34752603e+08 -5.78241061e+07\n",
            "  2.24486875e+08  1.36094405e+08  5.43145849e+08  2.76073379e+08\n",
            " -1.69594926e+08  1.22842519e+08 -6.67453171e+07 -6.89370245e+08\n",
            "  2.73156095e+08 -5.91009457e+08 -3.01175987e+08 -3.94855226e+08\n",
            "  4.41669614e+08 -6.66879745e+08 -6.30652797e+08 -1.71941557e+08\n",
            " -1.42122241e+07  1.90382954e+08 -8.48688275e+07  2.68528291e+07\n",
            "  9.93100539e+07  4.57758597e+08  1.36318983e+08 -3.05940172e+08\n",
            " -1.23350911e+08  1.01769022e+08  2.57921440e+08  4.81808033e+07\n",
            " -2.03931997e+08 -2.70005819e+08 -2.38307717e+08 -3.43371131e+07\n",
            "  3.69796036e+07 -2.99174637e+08  1.12439353e+08 -1.07881465e+08\n",
            " -2.06604273e+08 -3.28960002e+08 -2.01406141e+08  4.09976871e+07\n",
            "  2.18617388e+08  4.58299891e+08  2.62459767e+07 -1.13179055e+08\n",
            " -1.36170084e+08 -2.06797772e+08  2.14885554e+07  8.29934276e+08\n",
            "  1.22086559e+08  1.35168304e+07  2.51982216e+08 -1.74875169e+08\n",
            "  9.13939981e+07 -2.57271776e+08  2.02034235e+08  3.92986208e+07\n",
            " -6.60714064e+07  1.01508897e+08  2.35585695e+07 -7.67866802e+07\n",
            "  2.08822371e+07 -5.22906295e+08  1.83127219e+08  6.91404525e+07\n",
            " -2.94643071e+08 -2.30623380e+08  1.58395694e+08  1.89398683e+08\n",
            " -4.58076901e+07 -1.05292681e+08  9.58597378e+07  1.40718321e+08\n",
            "  1.57533757e+08 -9.04139356e+07  1.18884156e+08 -2.25606317e+08\n",
            " -9.57729323e+06  3.00363275e+07  2.65432820e+08 -4.28687619e+08\n",
            " -2.96284242e+06  1.94668662e+08 -9.57953452e+07 -3.45820612e+07\n",
            " -1.65334756e+08  1.05598924e+08  1.20538114e+08 -2.96683366e+08\n",
            " -1.41766204e+08  3.33091954e+07  2.37923933e+08 -2.78068278e+08\n",
            "  7.27861750e+07 -2.96106841e+08 -3.95858275e+07  3.91600753e+08\n",
            " -2.35484191e+08  5.25816633e+07 -8.95855048e+07 -1.66452604e+08\n",
            " -9.87193036e+07 -3.31543701e+08  5.27024717e+07 -6.01128460e+07\n",
            " -2.06098044e+08 -2.52148468e+08 -5.61968104e+07 -1.41509401e+08\n",
            " -7.01518027e+07  1.92274461e+08 -5.84176092e+07  5.27470911e+07\n",
            " -1.37307562e+08 -1.41508395e+08  3.91098516e+08  7.85269328e+07\n",
            " -1.31042654e+08 -3.68235794e+07  9.77425104e+07 -9.94439086e+07\n",
            "  4.66822621e+07  2.55053882e+08 -1.98264520e+08 -4.58982956e+08\n",
            "  3.44841589e+08  1.54551616e+08  4.70455569e+06 -5.49745387e+07\n",
            "  1.38291047e+08  2.15232436e+08  1.83397301e+08 -6.14337369e+08\n",
            " -1.90043818e+08  6.27824920e+06  1.40871068e+07  8.10084330e+07\n",
            "  5.53790673e+08  1.05491708e+08  1.94192915e+08 -1.44944072e+08\n",
            "  5.63153185e+08 -1.55287277e+08  2.38144701e+08  2.76863017e+08\n",
            "  8.42011221e+07 -2.37779958e+08 -1.52597101e+08  8.94475276e+07\n",
            "  2.28156465e+08  3.75432766e+07  1.42773280e+08 -1.77858955e+08\n",
            "  3.92690078e+08 -1.43364900e+08 -6.24346257e+07  1.52277441e+08\n",
            "  7.66298350e+07 -6.03436658e+07  1.18621483e+08 -3.06429334e+08\n",
            " -5.98798184e+07  1.45292011e+07 -9.52274804e+07 -1.20152169e+08\n",
            "  1.50396992e+08  3.08728772e+08 -7.68011380e+07  1.67191781e+07\n",
            " -4.36626278e+07  1.98643893e+08  2.62206008e+08  4.29672723e+08\n",
            "  1.61781445e+08  3.82085574e+07 -1.68458423e+08  1.51681835e+08\n",
            " -1.10141542e+08  2.24476490e+07 -7.45940482e+07  7.21744401e+07\n",
            " -1.44601600e+08 -5.08749744e+08  4.18950963e+07  1.15905415e+08\n",
            " -4.40893452e+08  8.63891946e+07  4.04311350e+08  2.03554408e+08\n",
            "  4.67818428e+07 -2.64292689e+08  3.85435442e+07 -1.90390052e+08\n",
            "  7.89691276e+07 -1.23608436e+07  1.62082344e+08  9.66583471e+07\n",
            " -1.00881796e+08 -1.28404847e+08 -1.30538767e+08  6.33291000e+07\n",
            " -7.05932248e+07 -9.81205977e+06 -9.81205977e+06 -1.12008426e+08\n",
            "  5.64410323e+06 -5.80727503e+07  1.36261299e+08  2.33449567e+08\n",
            "  3.98866744e+07  5.59963955e+07 -1.29720652e+08 -2.09597845e+07\n",
            "  7.12755574e+07  4.00907468e+07 -5.62041206e+07  1.45467284e+08\n",
            " -1.18137775e+08  2.70215182e+08 -3.08638269e+08 -1.51335875e+08\n",
            "  1.89350387e+07  1.58803423e+08 -1.02714245e+08  3.94881753e+08\n",
            " -4.98923044e+07  2.07239630e+08  7.89023690e+07 -1.27335952e+08\n",
            "  1.68888455e+08 -1.93782614e+08 -9.34617786e+07  3.67210051e+07\n",
            "  5.47303767e+07  2.68685320e+07  1.77413947e+08 -5.20004346e+08\n",
            "  1.20967430e+08  1.04799835e+08 -1.51990496e+08  2.36328164e+08\n",
            "  3.42412541e+08 -2.36546505e+07 -2.01830555e+07  2.84167781e+07\n",
            "  8.65422622e+07  1.30765234e+07  4.55959359e+07  4.42062249e+08\n",
            " -4.38741870e+07 -3.69029075e+07  1.93523878e+08 -6.56362367e+07\n",
            "  4.47213816e+08  1.07795844e+08 -1.24694645e+08  1.08117458e+08\n",
            "  8.37433435e+08  1.13086861e+08 -6.62686127e+08  5.71322774e+08\n",
            " -3.13039258e+07 -6.30906998e+07 -1.68317440e+08 -4.84676868e+07\n",
            " -4.36224011e+08  1.37946435e+08  1.08080932e+08  8.07542641e+07\n",
            " -4.09762048e+07  1.28403101e+08  1.38082685e+08  1.05704453e+08\n",
            "  1.40096888e+08 -8.84013775e+08 -7.22249448e+07 -3.23409832e+08\n",
            "  2.81821963e+08  1.12820877e+08  1.32711678e+08 -2.24453586e+08\n",
            " -3.23516105e+08 -1.80154314e+08 -6.88889094e+07 -9.79479020e+07\n",
            " -5.06567467e+07 -2.18158408e+08  2.97881332e+06  6.18603766e+06\n",
            " -1.73862059e+08  1.43914927e+08  3.31129839e+08  1.34758057e+08\n",
            "  3.19048617e+05 -3.79843637e+07 -8.33420969e+07  4.72908639e+08\n",
            " -9.76668914e+06  5.18083403e+08  6.58961161e+07 -2.23306423e+07\n",
            "  3.44403201e+08 -2.30220071e+08 -1.47475904e+07 -8.37193494e+07\n",
            " -2.07449696e+08  1.85192270e+08 -1.93033489e+08  9.84334253e+07\n",
            " -4.58024265e+07 -4.06764022e+07  2.24898667e+08  3.20876379e+07\n",
            " -4.41888695e+07 -5.63705014e+07 -9.02999709e+07  1.39837299e+08\n",
            "  3.34679207e+08  1.11757154e+08 -7.23908830e+07 -3.40003081e+08\n",
            "  1.67895310e+08 -2.87005031e+08 -1.42691034e+08 -1.01180758e+08\n",
            " -6.49992850e+06 -6.46858590e+07  8.92087305e+07  2.26773684e+07\n",
            "  1.32513182e+08 -9.82317610e+07 -2.92030841e+08 -8.56838339e+07\n",
            " -2.25179719e+07 -2.30352094e+08 -1.00466606e+08  3.04150678e+08\n",
            "  4.03879659e+07  1.96060542e+08  5.66569618e+07  2.23355773e+08\n",
            " -3.40237839e+07  2.70967548e+07 -1.13840086e+08  8.47727386e+07\n",
            " -2.86924311e+07 -1.37495891e+08  6.42612575e+07 -4.39180432e+08\n",
            "  3.88944947e+07  1.01027612e+08 -4.89569064e+08 -4.81204117e+07\n",
            " -4.85533707e+07 -5.70866806e+07 -4.98832198e+07 -3.15140298e+08\n",
            "  4.36715627e+07  9.79234366e+07  2.27824307e+08 -2.69083061e+07\n",
            " -5.21320013e+07  2.04513908e+08 -3.58505490e+07  1.42196564e+08\n",
            "  1.73301783e+08 -2.84331923e+08  5.86713279e+08 -1.11474657e+08\n",
            " -2.56683449e+08  1.97116933e+08 -2.13362538e+08  1.78237406e+08\n",
            "  4.00926086e+08  8.73698498e+07  2.14052495e+08 -1.23932738e+08\n",
            " -8.06296949e+07 -1.53790663e+08 -2.96408057e+08  3.47613703e+08\n",
            "  3.95131925e+07  2.00901320e+08 -4.52747202e+08 -9.23409660e+07\n",
            " -1.17273686e+08  5.46675182e+07 -2.32896089e+07 -2.69071734e+07\n",
            " -3.86312613e+07  3.53227237e+07  4.88613998e+07 -9.46237188e+08\n",
            " -2.63412366e+08 -6.77744993e+07 -5.89842770e+07  2.62646221e+07\n",
            "  5.11500487e+07 -1.86441282e+08  1.71704041e+08 -1.80436961e+08\n",
            "  5.31472119e+07 -8.16335336e+07  2.72911293e+07 -5.00007738e+07\n",
            "  5.03217152e+06 -6.22015605e+07  2.02029510e+07 -1.87158682e+08\n",
            " -3.69951423e+08  2.63938400e+08 -4.94412315e+08  1.01155213e+08\n",
            " -1.49694698e+08 -3.10058072e+08  1.67708253e+08  2.66934479e+08\n",
            " -6.06747299e+06 -6.00764931e+07  1.87558157e+08 -2.69018236e+06\n",
            "  1.77361289e+08 -1.07101555e+08 -1.47081580e+08  1.55795780e+08\n",
            "  1.68714611e+08 -1.15047393e+08 -9.92988184e+07  1.17709079e+08\n",
            "  1.17709079e+08  1.09631051e+08  3.28880715e+07  3.48469898e+07\n",
            "  1.57352423e+08  1.99432095e+08  2.22460174e+07  5.28647734e+08\n",
            " -5.64740362e+07  1.49636057e+08  1.15845200e+07 -2.43943051e+07]  - intercept :  -229289126.9213047\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.09771646774893049\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:16,561]\u001b[0m Trial 169 finished with value: 0.2602076653992519 and parameters: {'count_threshold': 6, 'postag': True, 'voc_threshold': 6841}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.         0.02699512 0.         ... 0.         0.         0.10260927]\n",
            " [0.02030715 0.06335067 0.03847518 ... 0.02690993 0.06880393 0.05571846]\n",
            " [0.0221569  0.06728762 0.09458813 ... 0.         0.         0.0988048 ]\n",
            " ...\n",
            " [0.62406422 0.06161721 0.65635039 ... 0.         0.6770722  0.        ]\n",
            " [0.52746596 0.03231893 0.66944466 ... 0.         0.6770722  0.00694341]\n",
            " [0.         0.01762947 0.04633448 ... 0.38647547 0.23653696 0.08134508]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 1.12246744e-02  3.53764336e-03  2.40481022e-02 -4.14292399e-03\n",
            "  6.31564511e-03  8.32608132e-03  3.65225378e-02 -7.35156982e-03\n",
            "  3.99255345e-03  1.36612202e-02  5.56928221e-03  2.06658694e-02\n",
            " -1.08015847e-02 -1.64458266e-02 -6.48720846e-03 -1.93714506e-03\n",
            "  1.64818327e-02  1.27715128e-02  1.25932427e-02  1.98178476e-02\n",
            "  7.95876525e-03 -2.89050181e-02 -1.40059577e-02 -1.51999974e-03\n",
            " -1.50147079e-02  3.48846958e-02 -1.77913290e-02 -1.47577832e-02\n",
            "  5.08145529e-02 -1.65670411e-03 -1.48218743e-02  6.98711590e-03\n",
            " -4.75119922e-02 -7.00078982e-03 -2.98577626e-04 -2.07740689e-02\n",
            "  2.16854354e-02  3.45945137e-04  6.15683024e-03 -4.73427377e-02\n",
            " -4.94773237e-02  7.17043989e-03  1.31348911e-02 -5.62510840e-02\n",
            "  1.03395584e-02  3.47142598e-02  2.21825120e-02 -4.58730272e-02\n",
            "  2.36919861e-02  5.26143940e-03 -6.78127873e-03  1.44042695e-02\n",
            "  8.45082603e-03 -4.81535797e-03 -4.67914045e-02 -2.09925955e-02\n",
            " -1.47264474e-02 -1.60569212e-02  7.25927331e-03  1.73840438e-02\n",
            "  9.05434431e-04 -6.36312809e-02  2.23064183e-02  1.56490085e-02\n",
            "  1.90552643e-02  2.95296889e-03 -4.05038420e-02  3.29868201e-02\n",
            " -3.83600359e-03 -1.21009966e-02  2.25831323e-02  1.12093534e-02\n",
            "  8.11055375e-03  5.66189393e-02 -5.92311298e-03  2.84231724e-02\n",
            " -1.72412609e-02 -5.84904966e-02  4.45172849e-03 -2.77562717e-02\n",
            "  3.90957618e-02 -1.63872312e-02  1.64180101e-02 -6.33250128e-02\n",
            "  1.58358464e-04 -2.58462409e-02  1.53349363e-02 -1.72413198e-03\n",
            "  2.49526482e-02 -2.49952453e-03  5.55781735e-02 -1.97436582e-02\n",
            "  1.98183740e-02 -2.76578276e-02 -1.15222887e-02 -2.56592175e-02\n",
            " -1.00984927e-02 -2.81718183e-03  2.33761541e-03  2.33761541e-03\n",
            "  1.28661030e-02  2.00783367e-02  1.63303793e-02  2.62461437e-02\n",
            " -4.19761504e-03  1.12774557e-03  1.36632667e-02  3.76709004e-02\n",
            "  1.69876443e-02  3.70244395e-02  1.71211193e-02  7.81192747e-03\n",
            "  3.38445344e-02  1.71571030e-02  9.07350551e-03 -3.99027786e-02\n",
            "  1.13758637e-02  3.70424675e-02  1.45228382e-02  6.05565016e-03\n",
            "  4.56966152e-03  6.53963874e-03  3.54677850e-02  4.00445882e-03\n",
            " -1.59314738e-02  2.29705412e-02  3.03780313e-02 -1.44204833e-02\n",
            "  2.29558792e-02  2.52540556e-02  1.54642732e-02  3.18427044e-03\n",
            "  5.08443634e-03  1.13058692e-02  1.10388699e-02 -4.50842745e-02\n",
            "  3.60113719e-02  3.27987638e-02 -2.63915800e-02  3.93615617e-03\n",
            " -5.98709669e-02  1.19972937e-02 -1.99439685e-02  2.34937531e-02\n",
            "  2.31792990e-02  1.61645078e-02  3.17919065e-03 -1.76288170e-04\n",
            " -7.48050423e-02 -3.51389914e-02 -6.54461957e-03  2.39387734e-02\n",
            " -1.00134845e-02 -4.01027694e-03 -2.68955162e-02  4.29736215e-02\n",
            "  7.42839504e-04 -6.18857467e-02 -6.06800987e-02  1.90693968e-02\n",
            " -2.48265649e-02  4.31798522e-03  1.53719883e-02 -3.72404501e-02\n",
            "  4.55996612e-03 -1.71672132e-02 -1.55200231e-02  3.24348210e-03\n",
            " -6.81366169e-02  7.06021766e-03 -1.43885522e-03 -1.06428640e-02\n",
            " -6.14075341e-03  2.36572320e-03  1.85434252e-02 -2.13399383e-02\n",
            "  1.17904400e-02  1.81412939e-02 -3.96968685e-03  1.31931368e-02\n",
            "  3.35602741e-03 -2.61683638e-02  1.40243280e-02 -5.47808759e-02\n",
            " -5.25267624e-03 -1.08198020e-02  1.97835469e-02  7.00013187e-02\n",
            " -3.19937310e-02 -2.37342912e-02 -4.75998256e-02 -1.72608671e-02\n",
            "  1.90144457e-02  7.80059963e-03 -4.28157409e-02 -3.86519911e-03\n",
            " -1.06590009e-02 -1.54705675e-02 -3.45507765e-03 -4.84732875e-03\n",
            " -1.78518620e-02 -2.68196129e-02 -7.69120800e-03 -3.07258797e-02\n",
            "  3.98623864e-02  1.07237447e-02  1.51365320e-02  1.96470934e-02\n",
            "  3.48800471e-02  1.41786457e-02  1.87164532e-02 -3.51174681e-03\n",
            " -2.27983384e-02  5.40986958e-02  4.74586908e-03 -2.20504374e-02\n",
            " -1.66445604e-02  2.76250106e-02 -4.45840050e-02 -3.44368002e-02\n",
            "  4.06120115e-03  6.70532309e-03  9.06884779e-03 -2.05144801e-02\n",
            " -4.50941990e-02  2.41837645e-02 -2.15894524e-02 -7.42437828e-03\n",
            "  1.75953678e-02  5.68880588e-02 -1.16195113e-02  4.85561042e-02\n",
            "  8.81762740e-02  3.11043574e-03 -9.79946651e-03 -3.02450718e-02\n",
            "  8.64254424e-03 -1.67453907e-03 -1.65192788e-02 -3.55627884e-03\n",
            " -2.78289496e-02 -6.52966552e-02  3.13908562e-02  1.09693240e-02\n",
            " -2.10605329e-03 -7.02908498e-03  1.22713892e-02 -8.91771543e-03\n",
            " -1.26589992e-02  1.40477898e-02 -2.15787551e-03 -1.66177136e-02\n",
            "  9.02044795e-03 -8.03641723e-03  2.48934491e-03  3.01717238e-02\n",
            "  8.51254017e-03 -2.97832452e-03 -1.32675370e-02 -5.52874966e-02\n",
            "  3.02532634e-03  3.67951405e-03 -2.66728046e-02  2.07235500e-03\n",
            "  1.96006444e-02  5.09093327e-02  1.62941920e-02  5.77701756e-04\n",
            " -1.56436033e-02  3.37531173e-03  6.67079766e-03  2.79684408e-03\n",
            " -9.47638146e-03  7.87145331e-04  1.64454382e-03  1.30747815e-02\n",
            " -2.78273127e-03 -1.35182110e-03  1.51579887e-02  4.12099373e-03\n",
            "  1.02785840e-02  1.00242270e-02 -1.44431511e-02 -2.51907541e-03\n",
            " -4.00724670e-03 -1.19443557e-02  3.61940916e-03  1.83687894e-02\n",
            "  1.00077224e-02  2.53436987e-02  5.99663087e-03  2.61816129e-02\n",
            "  8.96914121e-03  1.70295072e-02 -4.77945601e-03  4.16666476e-03\n",
            "  3.02069989e-03  2.72459117e-02  8.78901058e-03  6.57390922e-03\n",
            "  2.46597544e-02 -2.55134497e-03 -8.05039976e-03  3.27917084e-03\n",
            " -4.12863129e-02  3.13190051e-02 -1.02597083e-02 -4.21131934e-02\n",
            " -7.88533776e-03 -1.78547315e-02  1.60872780e-02 -4.96747254e-03\n",
            "  5.16643578e-03  2.30176766e-02 -5.14919029e-03 -1.18695245e-02\n",
            " -1.09249716e-02  1.89772372e-02  4.51812780e-02 -2.43579109e-02\n",
            "  1.74033112e-02  2.71102414e-02  3.24352256e-02  2.28218842e-02\n",
            " -1.17301808e-02 -1.31649917e-02 -1.00576580e-03 -4.92679687e-03\n",
            " -1.28101380e-02 -1.99974873e-02  1.94598949e-02  1.11943741e-02\n",
            " -1.95960203e-03 -7.71623247e-03  4.83390838e-02 -2.41068186e-03\n",
            "  3.08768657e-02 -2.15822491e-02  3.68226990e-03  2.03498376e-02\n",
            "  1.54097037e-03 -1.33017520e-02  2.61426580e-02  9.87547087e-03\n",
            " -3.74252387e-05  1.96052573e-02  6.52383947e-03  5.88253859e-02\n",
            " -2.69169610e-03  4.16591332e-03 -6.43244621e-02 -1.63809620e-02\n",
            "  1.71141138e-02  3.45953422e-03  5.93979981e-02 -1.20761206e-02\n",
            " -1.24082474e-03  7.66495932e-04 -1.34474576e-02 -7.28106273e-03\n",
            "  3.05242743e-02  1.33872181e-03  4.92806984e-02 -1.52031957e-02\n",
            "  1.48223513e-02 -2.77544690e-02  1.31023485e-02 -1.72293470e-03\n",
            "  1.73992639e-02  2.11010601e-02  9.52558803e-03  9.52558803e-03\n",
            " -1.06945897e-02  2.45557825e-02 -2.53394960e-03  4.01065484e-02\n",
            " -2.16731544e-02  3.41320361e-04  1.43424415e-02  7.42204225e-02\n",
            "  5.12478203e-02  2.61760977e-02  7.37770042e-03  1.86223183e-02\n",
            "  4.48355488e-02  3.85669024e-02  1.15679904e-02 -4.18581041e-02\n",
            "  2.06594321e-02  9.59548207e-03  2.49963752e-02  7.30162659e-03\n",
            " -2.98127511e-02 -7.30528998e-03  6.43254087e-02  3.74063683e-03\n",
            "  1.93964860e-02  4.32590431e-02  2.12231530e-02  1.07447895e-03\n",
            "  1.78718293e-02  1.74330530e-02  8.29383602e-03 -2.34208857e-02\n",
            " -2.11034164e-02  2.89176946e-02  2.42423282e-02  2.27557528e-02\n",
            "  3.50187079e-02  5.07789299e-02 -9.74373331e-03  3.03825736e-02\n",
            " -2.73181031e-02  1.65733823e-02 -1.57756939e-02  2.03978103e-02\n",
            "  1.68042223e-02  1.59429929e-02  2.55607907e-03 -4.86646693e-03\n",
            " -5.05138601e-02 -2.86947173e-02 -3.77547488e-03  9.74114562e-03\n",
            "  5.32396246e-02 -2.11527136e-03  1.60286133e-02  2.24879319e-02\n",
            " -4.09212323e-02 -2.54431914e-02  2.15387800e-02 -2.91125396e-02\n",
            " -7.90460075e-03 -2.18591620e-03  9.20383643e-03  2.25567583e-02\n",
            "  1.80146175e-02 -8.81322928e-03  2.75738722e-02 -1.21841255e-02\n",
            " -7.81531355e-03 -2.94167443e-03  4.81627365e-02 -3.14345548e-02\n",
            " -2.54672371e-02  6.96454912e-03  2.15212714e-02  3.68499821e-02\n",
            "  2.43627181e-02  1.74843095e-02  8.13364831e-03  3.14508693e-02\n",
            "  4.88099771e-03  1.25232438e-02  1.28578314e-02 -2.58168074e-02\n",
            " -6.17703774e-03  1.56065028e-02  3.33558486e-02  1.29115617e-01\n",
            " -1.76580120e-02  1.73874251e-02 -5.43475651e-03 -1.60714756e-03\n",
            "  1.04783350e-02  2.92961145e-02 -2.33799447e-03  2.45949233e-02\n",
            "  9.44070305e-03 -1.99925380e-03 -1.56863587e-02  7.00247220e-03\n",
            " -4.46704207e-03 -2.00732947e-02  5.51996243e-03  1.01285428e-03\n",
            "  1.26859484e-02  1.41002729e-02  1.85910960e-02  5.72236084e-03\n",
            "  2.27730330e-02  3.74609104e-02 -4.24445262e-02 -4.33402228e-03\n",
            " -2.11429554e-02  3.89548426e-02 -1.04583943e-03 -1.69872573e-02\n",
            "  4.19719483e-02  3.14060303e-02  1.04676478e-02 -2.58058814e-02\n",
            "  3.95518007e-02  1.01395133e-02 -5.14675576e-03  2.74052389e-02\n",
            "  1.39733489e-02  5.96672149e-02  2.50418802e-02  7.30096438e-03\n",
            "  8.13128565e-03  4.82236131e-02  5.18491191e-03  4.70761841e-02\n",
            "  7.60352514e-02 -6.15060261e-02 -6.64528127e-03 -3.75581402e-02\n",
            "  3.09791836e-02 -8.48927220e-03 -1.53457144e-02 -6.43451561e-03\n",
            " -3.30769110e-02 -2.70480566e-02  5.25256233e-03  1.26751452e-02\n",
            " -1.87337132e-02  2.44111236e-05 -1.12138715e-02  7.65213980e-03\n",
            " -3.83366061e-02  1.76071531e-02  2.19258197e-02  6.17011120e-03\n",
            " -2.44779047e-03 -1.26656582e-03 -7.96632072e-04 -1.17391880e-02\n",
            " -1.16069857e-02  5.85738561e-03 -2.41639938e-02 -2.35518305e-02\n",
            "  6.23752132e-03  3.15091670e-03 -1.25432478e-03 -1.90682077e-02\n",
            " -2.56887604e-03  8.93215805e-03  2.08548109e-02  4.69858571e-02\n",
            " -3.52692338e-02 -1.56254168e-02 -4.47614696e-02  2.79766040e-03]  - intercept :  0.34230921623752764\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.2602076653992519\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:17,153]\u001b[0m Trial 170 finished with value: 0.16870441585205448 and parameters: {'count_threshold': 7, 'postag': True, 'voc_threshold': 5683}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.34854701 0.46637951 0.03215689 ... 0.05495662 0.06595466 0.02053271]\n",
            " [0.17810906 0.09522398 0.03858827 ... 0.07327596 0.06595466 0.03059032]\n",
            " [0.26409775 0.30122197 0.22283647 ... 0.06479555 0.30723113 0.0219119 ]\n",
            " ...\n",
            " [0.04688386 0.07175511 0.15099505 ... 0.10560852 0.4123994  0.04041004]\n",
            " [0.00596547 0.05968074 0.03749597 ... 0.07327737 0.06595466 0.01854041]\n",
            " [0.10542812 0.03967666 0.06071047 ... 0.17091856 0.049466   0.03669418]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 7.80976382e-03 -4.59952762e-02 -2.64115664e-03  6.44116870e-03\n",
            "  2.42493151e-02  1.14420019e-01  6.43652641e-02  6.43576210e-04\n",
            " -3.71060636e-02  1.55014858e-02  3.14874668e-02  4.00781635e-02\n",
            " -2.36579972e-02  3.08977994e-04  5.77661628e-03 -2.59079187e-02\n",
            " -2.25406654e-02  1.28980260e-02 -3.41345654e-02  9.71976947e-03\n",
            " -1.50978279e-02  1.91089032e-02 -3.65816333e-02 -2.20754111e-03\n",
            "  1.04675464e-01  4.58262216e-02 -6.31026301e-02  8.82117468e-02\n",
            "  1.97231160e-02  5.90500632e-02 -3.22827577e-02  1.18379021e-01\n",
            "  2.56268182e-02 -2.47520268e-04 -5.92204885e-03  4.91351840e-02\n",
            " -1.25384082e-03 -1.59993664e-02 -1.15660195e-01  3.22282851e-02\n",
            " -1.03065345e-02 -2.53300669e-02 -3.78596484e-02 -4.52837511e-03\n",
            " -1.34058491e-02  2.10450115e-02  7.70358678e-02  5.52535754e-03\n",
            "  7.21005620e-03 -9.61244217e-02 -5.67203861e-02 -3.33674744e-02\n",
            "  6.18941284e-02  1.33669421e-02 -6.34478398e-02  2.76818265e-02\n",
            " -6.29105895e-02  2.24257913e-02  1.55753424e-03  1.67311619e-02\n",
            " -6.33796896e-03  3.93786118e-03  1.03210853e-02  3.37721628e-02\n",
            "  2.90970134e-02 -3.07349695e-02 -5.71553286e-03  9.84070779e-03\n",
            " -2.46935848e-04 -1.22455700e-02 -2.57371426e-02  1.47498340e-01\n",
            "  8.01021623e-02  1.43645276e-02 -8.08020546e-02  5.84923385e-02\n",
            " -5.24155896e-02 -1.25288942e-02 -7.63421468e-02 -2.95504394e-02\n",
            " -1.90767359e-02 -7.96343201e-03 -2.33468429e-02 -5.34815153e-02\n",
            "  4.63617581e-02 -7.42253049e-02 -4.69679238e-02  6.22201034e-02\n",
            " -3.54170061e-02 -1.83987117e-02 -2.52700668e-02  6.46216150e-03\n",
            "  2.38841249e-02 -3.53563263e-02 -1.41320601e-01  5.66342078e-02\n",
            "  6.15718108e-03  1.84380323e-02  5.51047672e-02  7.67289586e-02\n",
            "  1.00395914e-02 -4.69371160e-02  6.17235310e-02  2.08983172e-02\n",
            "  5.52951980e-02  8.39171038e-02  7.44683891e-02 -3.93161510e-02\n",
            " -1.03053863e-01 -9.21317345e-03  9.85532509e-03  1.26196026e-01\n",
            " -5.49148990e-02 -1.72549120e-03 -7.49001366e-02 -4.43441477e-02\n",
            " -3.98171016e-03  9.41267672e-03  4.25880853e-02 -2.32911881e-02\n",
            " -5.76019598e-02  2.24930718e-02 -3.80505267e-02  1.44152321e-03\n",
            " -1.69218081e-01  1.53743696e-01  7.51935869e-02 -7.80227711e-02\n",
            "  1.19105879e-02  3.43046539e-03  1.21107718e-02 -1.73983939e-02\n",
            " -1.16301548e-01 -6.42090036e-03  4.91502695e-02  2.43123230e-02\n",
            "  1.73021125e-01  5.97928593e-02 -7.01557213e-02 -5.43479638e-02\n",
            "  3.82409054e-02 -2.78667762e-02 -3.05932416e-02  3.03369399e-02\n",
            "  2.58253194e-02  1.99370178e-02  2.38847806e-02 -8.29155664e-03\n",
            " -1.40495663e-02  5.53996309e-02  5.17745876e-03 -6.49196705e-02\n",
            "  2.96476853e-02  1.66923387e-02 -6.37649481e-02 -1.16524677e-02\n",
            "  2.15282008e-02  1.72841679e-02  3.72009119e-02  6.63080549e-02\n",
            " -3.37544263e-03 -8.24248125e-02  2.31670472e-02  5.62655973e-02\n",
            " -1.09599525e-02  2.94281720e-04  6.21492704e-02 -5.67266674e-02\n",
            " -5.59831661e-02 -8.53710256e-03 -1.87287369e-03 -4.21520011e-02\n",
            " -3.54692557e-02  1.97349352e-02  4.96290783e-02 -4.63250879e-02\n",
            " -5.17309453e-02  4.92529599e-02 -6.37013305e-03  9.15540870e-03\n",
            " -2.19047038e-03 -7.81656074e-02  3.31463464e-02  1.48603021e-02\n",
            " -2.19869449e-03 -2.57713252e-02  1.31067013e-02 -3.56502713e-03\n",
            " -1.01086468e-01  1.77413275e-02  6.23432347e-03  1.07090376e-01\n",
            "  5.92142717e-03 -1.96128891e-03  1.45566301e-01 -2.01876066e-02\n",
            " -2.10214017e-02 -6.37519755e-02  2.07371273e-05 -3.95544096e-02\n",
            "  4.19860370e-02 -2.35681368e-02 -7.56023773e-02 -2.51662269e-02\n",
            "  3.13038396e-02 -2.18049908e-02 -8.22802805e-02 -5.93762153e-02\n",
            " -7.21969321e-02  1.10239562e-02 -9.79729608e-02 -3.51915023e-02\n",
            "  3.54956175e-02 -1.60174764e-02  2.88299531e-02  9.71031887e-02\n",
            "  8.38148180e-02 -1.10666590e-01  1.07702256e-02  2.97218647e-02\n",
            " -5.59254973e-02  1.05306297e-01  3.31900761e-02  4.14674116e-02\n",
            "  3.30333056e-02  8.37514318e-02 -2.71987458e-03 -2.41353640e-02\n",
            " -6.08673375e-02 -5.32462548e-03  1.01443248e-02  3.63394625e-02\n",
            " -3.89433107e-02 -6.57862243e-02  5.92302087e-03 -4.43874928e-03\n",
            " -1.07544180e-02 -1.44164707e-02 -4.85830587e-02 -1.05887883e-02\n",
            "  6.33024224e-03 -1.61685481e-02 -4.22103681e-02  1.16339271e-02\n",
            " -3.99442471e-03 -8.08694174e-02 -3.11384235e-02  5.84376673e-02\n",
            " -1.88389842e-02 -3.90585550e-02  1.72773162e-04  1.85290188e-02\n",
            "  6.92727216e-02 -8.71508978e-02  6.79807962e-02 -3.28057421e-02\n",
            "  3.67200882e-02  2.34395440e-02 -6.07722726e-02  1.12653210e-02\n",
            " -1.28291465e-03 -7.16292556e-03  5.83505304e-02 -1.32778477e-02\n",
            "  1.48542619e-02  1.62649348e-03  3.71371330e-02 -1.99274595e-02\n",
            " -4.06413977e-02 -6.65073994e-02 -2.29913128e-02  2.34789530e-02\n",
            " -6.28079598e-03 -2.71947877e-02  2.00355247e-03 -2.04264929e-02\n",
            "  3.11537672e-03  7.11080339e-03 -3.34710006e-02 -2.63754202e-02\n",
            " -8.86144956e-03 -1.79175801e-02  3.03472091e-03 -2.77493054e-02\n",
            " -5.45449344e-02  8.07040714e-05  6.43743733e-02 -2.19678587e-02\n",
            "  3.07045193e-02  3.18756724e-03 -8.61371452e-02  4.30169470e-02\n",
            "  2.41512300e-02 -4.75336826e-02 -1.28021740e-03  5.09882743e-02\n",
            "  4.88778491e-02 -5.77548456e-03  4.09342203e-02 -1.09086201e-02\n",
            "  2.58643904e-02 -4.11966028e-02  1.76699757e-02 -2.85466657e-02\n",
            " -3.72449571e-02  1.28947011e-02 -3.68313641e-02 -2.56394075e-02\n",
            "  8.17560623e-03 -6.48458375e-02  2.11091486e-02  4.66415059e-02\n",
            " -4.84725269e-02 -1.08196764e-02 -3.03359050e-02 -5.28474461e-02\n",
            " -2.82756687e-02 -1.54700744e-02 -4.02708513e-02  1.74642002e-02\n",
            " -4.07133701e-02 -2.18840943e-03  4.13221112e-02 -3.33061581e-03\n",
            "  1.62399936e-02  4.33162378e-02 -3.43604645e-02 -6.26041430e-03\n",
            "  7.33146879e-04  8.55358563e-02 -3.19354997e-02 -2.59239523e-02\n",
            " -2.46952333e-02  1.74006540e-02 -6.27488679e-03 -7.39874160e-02\n",
            " -1.62782792e-02  4.41638968e-03 -3.43514599e-03  8.34975423e-04\n",
            "  1.03198425e-02 -5.18407754e-02 -2.71603491e-02 -1.81919091e-02\n",
            "  2.73473447e-02  2.44600862e-02 -4.39072288e-03 -1.44192614e-02\n",
            "  3.81367262e-03 -9.36715688e-03 -1.33440714e-02  6.49428082e-02\n",
            "  2.28129353e-03  6.21640195e-02 -1.19044137e-02 -1.79797986e-02\n",
            "  2.13645198e-02  3.73164220e-02  2.56452327e-02 -3.74584962e-02\n",
            " -2.91001207e-02 -3.26038474e-02  1.54046671e-02  1.69240528e-01\n",
            " -3.88438575e-03 -3.09331203e-02  1.27894900e-02  3.62957832e-03\n",
            " -1.97565418e-02 -4.51275860e-02 -1.39152809e-02 -4.90364915e-03\n",
            "  2.59429088e-02  9.07618635e-03 -1.93327231e-02 -3.75146722e-02\n",
            "  1.98102449e-02 -4.03773499e-02 -1.03381099e-02  5.21739530e-02\n",
            " -7.58026384e-04  9.90919951e-03 -1.29663397e-02  8.84698613e-03\n",
            " -3.74297917e-02 -2.02583150e-02 -1.05806644e-04  3.74132324e-03\n",
            " -8.77601489e-02 -6.70655847e-03  4.44747139e-02  7.86822656e-02\n",
            "  2.51131575e-02  1.06703899e-02 -7.90896429e-03 -3.73957470e-02\n",
            " -1.91645065e-02 -3.22444611e-02 -1.35322380e-02 -2.91848012e-02\n",
            "  1.98347199e-02  4.10976393e-02 -5.36523112e-02 -4.84534306e-02\n",
            "  7.34917115e-03  3.37567177e-02  1.99462577e-02 -1.78987428e-02\n",
            " -1.81437595e-02 -7.24253889e-03  3.12166673e-02 -1.16305848e-02\n",
            " -5.15011496e-02 -3.12004811e-02 -1.71987415e-02 -9.52532981e-03\n",
            "  2.54219296e-02  1.59534638e-02  1.38442564e-02 -1.50647681e-02\n",
            "  5.63167930e-03  5.21341301e-03  1.41125651e-03  2.09432973e-02\n",
            "  1.22078315e-02 -3.55485637e-02 -1.99334794e-02 -2.60357682e-02\n",
            " -3.08617851e-02 -3.32843303e-02  2.57947689e-02 -4.36895432e-02\n",
            "  1.75322706e-03 -8.97832976e-02 -2.87856007e-02 -6.80921809e-02\n",
            "  2.43650891e-02  7.73997958e-02 -5.58274625e-02  1.58466923e-02\n",
            " -1.24180448e-02  1.98384850e-04 -9.75415481e-03  8.48030450e-03\n",
            " -7.64543717e-05 -1.99735865e-02 -7.09105634e-03 -6.36312745e-02\n",
            "  3.73009424e-02 -1.36066335e-02  1.57407520e-02  4.04374509e-02\n",
            "  3.86222620e-02  2.82811186e-03]  - intercept :  0.804457292151985\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.16870441585205448\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:17,760]\u001b[0m Trial 171 finished with value: -0.24546153720851957 and parameters: {'count_threshold': 4, 'postag': True, 'voc_threshold': 5959}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.04458491 0.04651102 0.03177824 ... 0.1820153  0.19689972 0.07018458]\n",
            " [0.01108888 0.         0.00948997 ... 0.         0.04988961 0.09220227]\n",
            " [0.234741   0.15280623 0.02802877 ... 0.14174258 0.06651947 0.01466331]\n",
            " ...\n",
            " [0.01670814 0.04290364 0.01501221 ... 0.22650149 0.10455204 0.04037489]\n",
            " [0.51419571 0.55347687 0.34750435 ... 0.14009227 0.08698132 0.00598443]\n",
            " [0.04426916 0.06203115 0.05270626 ... 0.20685617 0.15715378 0.07008832]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 0.00502319  0.00689615  0.00349532 ...  0.00258707 -0.00410706\n",
            " -0.00048315]  - intercept :  0.7991473479954908\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.24546153720851957\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:18,346]\u001b[0m Trial 172 finished with value: -0.1864483796472099 and parameters: {'count_threshold': 8, 'postag': True, 'voc_threshold': 6275}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.02793156 0.05078616 0.12696171 ... 0.27944994 0.1749785  0.05209836]\n",
            " [0.         0.         0.         ... 0.         0.         0.03861369]\n",
            " [0.0492775  0.03782234 0.07476045 ... 0.10204757 0.03894265 0.04599815]\n",
            " ...\n",
            " [0.         0.09309268 0.         ... 0.00339317 0.33498432 0.04027933]\n",
            " [0.45856834 0.0886607  0.77643167 ... 0.00212073 0.07378648 0.01106089]\n",
            " [0.31085111 0.1054447  0.37569343 ... 0.2112477  0.11132062 0.02706825]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-3.42684329e-02 -7.96540877e-02 -1.67656032e-02 -3.07885376e-03\n",
            " -1.64747997e-02 -3.71505560e-03  2.62230725e-02 -5.36666155e-02\n",
            " -3.74745232e-02 -4.30366815e-03 -3.70222626e-02 -4.39572180e-02\n",
            "  1.89530581e-02 -1.10181285e-01  7.38247890e-02 -6.96041289e-02\n",
            " -6.24623732e-02  3.19260438e-02 -6.55988072e-02 -8.91304952e-02\n",
            "  1.17396311e-02 -2.12495840e-02  1.86254122e-02 -2.91581115e-02\n",
            "  3.14412778e-03 -9.99362485e-02  8.69193600e-03  7.66451683e-02\n",
            " -3.53553043e-03 -1.18280197e-01 -6.19097719e-02  3.66046631e-02\n",
            "  6.26154594e-02  2.62812554e-02 -1.14313255e-02  1.11552218e-02\n",
            " -1.09887700e-01 -1.21144358e-01  4.52005003e-02  3.62034369e-03\n",
            " -3.66897067e-02 -9.58200182e-02  1.88392426e-02 -5.14129348e-02\n",
            "  1.83706899e-03  5.51367055e-02 -3.44667435e-02 -2.14894551e-02\n",
            " -4.57089179e-02  2.03701264e-02  1.10878719e-02  8.80707503e-02\n",
            "  3.10591173e-02 -2.29594620e-02 -2.30273616e-02 -8.96645233e-02\n",
            " -1.06385792e-02 -5.55727260e-03 -9.77400762e-02  6.88243204e-02\n",
            " -7.07673247e-03 -4.05064995e-02 -6.35339005e-02  2.55956462e-02\n",
            "  6.54655569e-03  8.55073959e-02 -4.66552912e-03 -3.07669980e-02\n",
            " -5.03986602e-02  1.42959517e-02  1.65026311e-02  4.45679920e-03\n",
            "  8.44097954e-02 -1.66325616e-02 -3.89870840e-02 -3.15014376e-02\n",
            "  2.82684199e-02 -3.93689117e-02  4.12031982e-02 -9.32039803e-03\n",
            " -2.03283103e-01 -6.47392139e-03  2.30443713e-02  4.74095943e-02\n",
            "  2.30402548e-03 -6.34650797e-02  5.26936389e-02  4.67179078e-02\n",
            "  3.64629849e-02 -9.13858453e-02  5.28160451e-02 -5.32348784e-02\n",
            " -5.32348784e-02 -1.70983895e-04  4.60167760e-02 -6.84381672e-02\n",
            " -4.97594731e-02  3.94307977e-02 -8.37785085e-02  6.98717360e-02\n",
            "  8.53101823e-02  3.83784517e-02  6.43255386e-03  1.03404119e-02\n",
            "  8.23926459e-03 -6.08084028e-03 -1.60901569e-02  3.18652169e-02\n",
            "  2.52876713e-02  1.09902147e-02 -1.55734181e-01  4.39326080e-02\n",
            "  3.96451976e-03 -7.07856277e-02 -2.02078188e-01  4.98508881e-02\n",
            "  2.31771724e-02  2.53666304e-02 -1.34907700e-01  3.24029596e-02\n",
            " -1.45334354e-01  9.25868196e-02 -5.29646897e-02 -3.34804847e-02\n",
            "  2.03574294e-02 -3.62086231e-02  4.06076489e-02  1.42991278e-02\n",
            "  2.53405321e-02  3.61054088e-02  3.18831796e-02  4.68400090e-02\n",
            "  4.70788406e-02 -3.68504470e-02 -2.17917777e-02  2.21877888e-02\n",
            "  2.73114031e-02 -3.25675373e-02 -1.57218012e-02  6.41508693e-02\n",
            " -3.23500643e-03  4.46244356e-02 -1.83274407e-01  4.53109335e-02\n",
            "  3.90518896e-02  6.94483582e-02  1.10813626e-02 -9.21357629e-02\n",
            "  3.66212386e-02  1.70872067e-02  7.90937972e-02 -3.92567445e-02\n",
            " -8.70079797e-03  7.54131120e-03  4.76292697e-02  2.73741884e-02\n",
            " -3.84933900e-02  2.56165603e-02  1.31219586e-02 -5.99293665e-02\n",
            " -1.81754612e-02  1.05040504e-03  6.30844779e-03 -1.40516246e-02\n",
            " -3.07882130e-02 -3.07519150e-03  3.59597197e-03  1.19311457e-02\n",
            "  6.25122074e-02  3.05038077e-02 -4.98997883e-02  2.81331826e-02\n",
            "  3.10320945e-03 -8.57855220e-02  3.08995482e-02  5.84329187e-02\n",
            "  2.19224416e-02 -4.97771438e-02 -8.33420768e-02  4.68302587e-02\n",
            "  4.24875201e-02  1.05595644e-01  2.49744996e-02  5.02223250e-02\n",
            "  2.18331704e-02 -1.40871312e-02 -1.40871312e-02 -1.40871312e-02\n",
            " -1.13486578e-03 -8.14408712e-06  5.23968064e-02 -1.03804273e-02\n",
            " -1.79496048e-02 -2.02480692e-01 -3.14599254e-03 -8.21785627e-03\n",
            "  1.76279823e-02 -2.57325040e-02 -2.39883325e-02  9.06492396e-03\n",
            " -8.41151906e-03  1.14871347e-02 -3.11494351e-03  3.33794155e-02\n",
            "  2.67022998e-02  6.30414311e-03 -7.50695543e-02  1.67626699e-02\n",
            " -4.21734919e-02 -1.83052199e-02 -5.59814177e-02 -3.76959748e-02\n",
            "  2.01425236e-02  1.77530937e-02  3.10680388e-02  1.10514026e-02\n",
            " -1.55845158e-02  1.32862465e-02 -3.90542965e-02 -1.32558231e-03\n",
            " -3.20272625e-02 -1.72378524e-02 -4.53838617e-02  2.45878624e-02\n",
            "  1.35173942e-02 -3.66130075e-02  2.66640979e-02  8.32265031e-02\n",
            "  9.46889620e-03  1.24444351e-02 -1.09599208e-02 -3.63567912e-02\n",
            " -1.33631708e-02  2.15112190e-02  6.18499467e-03  8.96942981e-03\n",
            "  3.88765241e-02  4.19841136e-02 -2.61931677e-02  2.23423798e-02\n",
            " -1.37051437e-02  8.69749303e-03  6.11837027e-02 -3.17173121e-02\n",
            " -1.37744659e-02  5.18505384e-02  2.44735459e-02 -1.18924454e-01\n",
            "  8.70147579e-03  7.39461992e-04  1.41208445e-02  1.52285302e-02\n",
            "  9.74101282e-05  5.41789602e-03  2.53591849e-02 -8.76669128e-02\n",
            "  8.89200518e-02  8.43939998e-02 -3.63206016e-02 -2.51765670e-02\n",
            "  5.07483014e-02  1.04175105e-02 -4.58500163e-02 -1.03057834e-01\n",
            "  7.80687396e-02  8.88978471e-02 -5.78328914e-02  4.18659054e-02\n",
            "  4.94705548e-03  5.94117967e-02  7.90486322e-03  6.37418310e-03\n",
            " -1.69242642e-02 -8.80414454e-02  1.37762785e-02 -4.09051487e-02\n",
            "  2.31202446e-02  3.37059950e-02 -1.15633981e-03  2.04836026e-03\n",
            "  4.72988959e-02  6.47776066e-03  2.50743292e-02  2.28748213e-02\n",
            " -1.12489445e-03  6.23473751e-02 -2.60611977e-02 -2.60611977e-02\n",
            "  2.90551279e-02 -1.36775151e-02 -6.19551566e-02 -1.93415634e-02\n",
            "  2.06845524e-02 -7.17278643e-02 -1.21946018e-02  1.82792678e-02\n",
            " -5.20704399e-02  1.11987513e-02  5.48780813e-02  2.41200858e-02\n",
            " -2.36837000e-02  3.20089446e-02  6.33361463e-02 -2.92456424e-02\n",
            " -1.11292701e-02  1.18730055e-02  1.78402761e-02 -9.11245586e-03\n",
            " -1.13543732e-01  2.67824696e-02  5.08020860e-02  6.10405578e-03\n",
            " -2.36991805e-02  1.10269462e-02  3.93876195e-02  1.36849596e-02\n",
            "  4.22770211e-02 -1.81983961e-02 -7.00086737e-02  1.17981506e-02\n",
            "  3.73295768e-02  4.82907314e-02  3.82328952e-03  1.21244798e-02\n",
            "  2.48066485e-03  5.31840017e-02 -5.58125383e-03  4.90802645e-02\n",
            " -9.15369294e-02 -9.50060128e-03  3.54059651e-02 -7.14913767e-04\n",
            " -4.08471264e-02 -8.90546935e-02 -8.72130203e-03  5.63368351e-04\n",
            "  3.58867977e-02 -1.95915068e-02  1.60094479e-02  5.80730429e-02\n",
            " -1.93643635e-03 -7.31337614e-03  1.16249112e-03  4.07547492e-02\n",
            " -1.54798753e-02 -5.55894781e-03  1.89701041e-02 -1.02277988e-03\n",
            " -2.61553607e-02 -3.61551518e-02  7.99552138e-03 -3.95900289e-02\n",
            " -6.12253860e-03  3.28260966e-02  7.29856436e-03 -5.88455558e-02\n",
            "  3.80826623e-02  1.31451996e-02  2.45122407e-02 -9.99654577e-03\n",
            "  3.06763138e-02  2.41049801e-02  6.19312773e-03 -6.58816607e-02\n",
            " -4.46436209e-03 -4.08359576e-02  2.44359685e-02  1.71491629e-02\n",
            " -1.26353206e-02  1.62967100e-03 -5.29514842e-04 -6.09575583e-02\n",
            "  1.01884617e-02 -2.88975845e-02 -1.05770928e-02 -1.12043654e-01\n",
            "  1.63187300e-02  5.02881862e-02  1.06005392e-02 -8.14796211e-02\n",
            "  2.47316961e-02  2.47316961e-02  2.47316961e-02  1.84837519e-02\n",
            " -1.03602784e-02 -1.30643747e-02 -3.68060441e-02  3.68870716e-02\n",
            " -3.37470622e-03 -1.25375424e-02]  - intercept :  0.8323481547665038\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.1864483796472099\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:18,935]\u001b[0m Trial 173 finished with value: 0.08305958855981649 and parameters: {'count_threshold': 7, 'postag': True, 'voc_threshold': 3465}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.29767217 0.01448456 0.52436353 ... 0.02773813 0.06520698 0.03796682]\n",
            " [0.06559766 0.09109359 0.12214987 ... 0.01929609 0.03130402 0.04552556]\n",
            " [0.         0.04236171 0.         ... 0.         0.0917952  0.08108933]\n",
            " ...\n",
            " [0.00658738 0.02905645 0.         ... 0.         0.01476961 0.11781187]\n",
            " [0.34167767 0.08206269 0.38967767 ... 0.00616267 0.07799825 0.01344826]\n",
            " [0.09102917 0.12104029 0.05002556 ... 0.         0.01033873 0.03389656]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-0.01298952  0.02282894  0.01228797 -0.01633185 -0.03714731 -0.01553236\n",
            " -0.03445143  0.00289676  0.00930062 -0.01751015 -0.01299495 -0.0372374\n",
            "  0.01519027 -0.018852    0.05884973  0.00163273  0.00687478  0.00925162\n",
            " -0.01775867  0.03854519  0.03983807  0.00014523  0.0177629  -0.01775938\n",
            "  0.02157515  0.06022894  0.0072982   0.01274548  0.00685543  0.00694598\n",
            "  0.0301564   0.05295435  0.05147885 -0.00644154  0.01923446 -0.01469314\n",
            "  0.00083454  0.01096738  0.03263398  0.01103164  0.01476657  0.007667\n",
            "  0.02115375 -0.02142709 -0.05128378  0.02207577 -0.00238122 -0.02863876\n",
            " -0.00870451 -0.00735312  0.01141132 -0.03814972  0.01952693  0.04970834\n",
            " -0.01795724 -0.03518326  0.03143807 -0.02536714 -0.08053088  0.04404485\n",
            "  0.00639412  0.04257221  0.01823273 -0.01108428 -0.00913713  0.02158018\n",
            "  0.02287624 -0.02323034 -0.04774656 -0.07689869 -0.08288655 -0.02902516\n",
            "  0.00270525  0.00083481 -0.00549965  0.04647602 -0.00502737 -0.02703052\n",
            " -0.00483555  0.06190451 -0.03120094 -0.0395304   0.00042325 -0.0285259\n",
            " -0.03177837 -0.02298175  0.03229997 -0.05513649 -0.01721176  0.04092832\n",
            " -0.02751759 -0.02031457 -0.04922197 -0.03027087  0.02874261  0.03735014\n",
            "  0.03007064 -0.03449525 -0.01764805  0.03006828 -0.10346209  0.03247747\n",
            " -0.01505485 -0.11150542 -0.04629208  0.03266542  0.01185302  0.02102278\n",
            " -0.00679739 -0.01737622  0.03224147  0.01210947 -0.05658286 -0.03451869\n",
            "  0.01908348  0.04471586 -0.01187469 -0.03859932 -0.01935738 -0.06503388\n",
            "  0.00127748 -0.00368845  0.01782198 -0.03222703  0.00478472 -0.04215269\n",
            " -0.03277284  0.02059207  0.02807124 -0.01980334  0.02037456  0.00284252\n",
            "  0.01316443 -0.00157843  0.00260049  0.01937848 -0.00749258 -0.04147194\n",
            "  0.05631246 -0.11393797  0.0174896   0.01374917  0.00554612  0.01409661\n",
            "  0.01043779  0.00180903  0.06856473 -0.01464048  0.00494201 -0.01838352\n",
            " -0.01938229 -0.04539671 -0.02527761  0.0149048   0.01133521  0.03413374\n",
            " -0.01607113  0.00229358 -0.03531898  0.02622376  0.03767747 -0.00112116\n",
            "  0.01235001 -0.04576032  0.03904242  0.0297084   0.04818198  0.02209648\n",
            "  0.01802604  0.00553811 -0.0153762  -0.06015766  0.04659518 -0.00604959\n",
            "  0.00029975  0.03445702 -0.00344578 -0.01485084 -0.06879169 -0.00463033\n",
            "  0.04520835 -0.00544766 -0.00144371 -0.00606997  0.01985189  0.02399825\n",
            " -0.03550775 -0.06873979  0.03451714 -0.00853881  0.01917036  0.01917036\n",
            " -0.05023657  0.03074174 -0.03115976 -0.04170757  0.04242552 -0.06266327\n",
            " -0.05695162  0.05113869 -0.00703285  0.03962892  0.02392141 -0.02410844\n",
            " -0.00119448 -0.01087608  0.00082958 -0.0031097   0.03748919  0.0109108\n",
            " -0.0147671   0.02079886  0.02463959 -0.01612829  0.0081084   0.00317816\n",
            " -0.00917805  0.01436299  0.03617855  0.00049036 -0.00032201 -0.01953549\n",
            " -0.06800236  0.02397171 -0.00128184 -0.04436753  0.01963288  0.00855934\n",
            " -0.01408561 -0.03490028 -0.01585749 -0.00072544  0.02860607  0.02007716\n",
            " -0.01051324  0.01903678 -0.0042935   0.00449871  0.01050758  0.01429559\n",
            "  0.0050873  -0.01949214 -0.00921298 -0.02247267  0.00871965 -0.0132254\n",
            " -0.02817581  0.00991728  0.01760891  0.03254237 -0.01731775  0.02466988\n",
            " -0.0141795   0.02400994 -0.0222693   0.00133811  0.04040391  0.01855414\n",
            "  0.01511291  0.00645065 -0.02796667 -0.01100281  0.00223977  0.01482151\n",
            " -0.01259811  0.01303838  0.01793057  0.00598845  0.06439595  0.05760635\n",
            "  0.00700559 -0.03503938 -0.03427087  0.00941351 -0.01390112  0.00356979\n",
            " -0.04613476 -0.043022   -0.00455249  0.02864998  0.00993835  0.01790854\n",
            " -0.01915605  0.0109805   0.00735969  0.00121379 -0.03078048 -0.043774\n",
            "  0.00700591  0.02380168 -0.03531994  0.00800929 -0.0009086   0.02352943\n",
            "  0.03617957 -0.01536111 -0.01890337 -0.00517658  0.08755092  0.0298149\n",
            "  0.06989342  0.04700198  0.01339722  0.08837478 -0.03037233 -0.02785739\n",
            " -0.05088331 -0.06597017 -0.00057444  0.02734021  0.00466055 -0.04541903\n",
            "  0.00213501 -0.01769187 -0.01193092 -0.00292128  0.0079394  -0.024426\n",
            " -0.03598264  0.01860729 -0.02845191 -0.03674886 -0.00250343 -0.0126831\n",
            " -0.02689342 -0.02435963  0.09407477 -0.00661987  0.04694732 -0.03643825\n",
            "  0.03637918  0.04090654 -0.01030682 -0.04227214  0.01568761  0.01075088\n",
            "  0.00087986 -0.01337257  0.10957685 -0.00544115  0.01080769  0.00076418\n",
            " -0.01489028  0.01338906  0.02971749  0.00645142  0.00169989  0.01744956\n",
            " -0.01531282 -0.11400655 -0.04499936 -0.04915516  0.00462401  0.01245904\n",
            " -0.08400021 -0.03267797 -0.00253328  0.0564289  -0.0118236  -0.01944686\n",
            " -0.01905688  0.04619858 -0.03200324 -0.00414883 -0.00441599  0.0419333\n",
            "  0.00876773  0.01108633  0.03671273  0.02925953 -0.03072975  0.05591812\n",
            "  0.00772525 -0.00521145 -0.04487578 -0.03294704  0.01420431 -0.0743675\n",
            "  0.02298778  0.05795858 -0.03931805 -0.09762496 -0.03189788  0.0884849\n",
            "  0.03159409 -0.02554363 -0.02365126 -0.05686413 -0.0302536   0.01867519\n",
            " -0.04587763  0.06561897  0.00770357 -0.0121545   0.00167708  0.03389548\n",
            "  0.02377763  0.04143103 -0.03609043 -0.03412341  0.05144963  0.00595684\n",
            "  0.06639327  0.01563091 -0.01360908 -0.0067397  -0.04924648  0.03347253\n",
            " -0.0220964  -0.01089883 -0.01947427  0.00302304 -0.00172422 -0.0418962\n",
            " -0.03584394 -0.04058311 -0.01310709  0.04395086  0.05242622  0.07720996\n",
            "  0.01356724  0.0181477   0.01273794  0.03574628  0.00616831  0.0889675\n",
            " -0.10890681 -0.00728484 -0.00728484 -0.09216869  0.02577252  0.02541153\n",
            " -0.00605411  0.00153644 -0.06493624  0.02890806 -0.07008277  0.07813896\n",
            "  0.02115619 -0.00807005 -0.09379095 -0.00978602  0.02085387 -0.03434719\n",
            "  0.04034701  0.01241046 -0.00236557 -0.01326479 -0.00476406 -0.03429219\n",
            " -0.02723424  0.04344226 -0.01415638 -0.00357213  0.04477703 -0.03805411\n",
            "  0.00312626  0.03779829  0.01529477 -0.0100834   0.017282   -0.01032123\n",
            " -0.02327367 -0.04419781  0.03672635  0.00139311 -0.01273037  0.00317715\n",
            "  0.02786266  0.01740533  0.02590076  0.00106206  0.01678037 -0.00426976]  - intercept :  0.7190578507892444\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.08305958855981649\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:19,513]\u001b[0m Trial 174 finished with value: 0.140356741402055 and parameters: {'count_threshold': 6, 'postag': True, 'voc_threshold': 9971}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.07315817 0.10513662 0.03313131 ... 0.10269469 0.04359004 0.03626092]\n",
            " [0.         0.         0.02958315 ... 0.4827294  0.02972048 0.11356814]\n",
            " [0.         0.         0.0275819  ... 0.0073724  0.35203272 0.04834635]\n",
            " ...\n",
            " [0.         0.         0.02922593 ... 0.0073724  0.03566458 0.04755022]\n",
            " [0.12391326 0.14487173 0.06247176 ... 0.01909465 0.22089473 0.04514754]\n",
            " [0.56233413 0.55450649 0.03181969 ... 0.2611988  0.01981365 0.01550194]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-3.87555458e-03  2.34946738e-02  2.37421429e-02 -5.35871278e-03\n",
            "  3.16311893e-02 -3.97137924e-02 -8.82900447e-03 -7.69915597e-02\n",
            "  1.74754739e-02  3.20326722e-02  1.47010673e-02 -1.77977679e-02\n",
            "  3.07542763e-02  4.48989429e-02 -7.29728060e-03  2.21182447e-03\n",
            " -1.41543267e-04  1.78379042e-02  2.40843691e-02 -5.41952269e-02\n",
            " -8.80589173e-03  2.69107936e-02 -5.39439243e-03  3.51829771e-02\n",
            " -2.19935676e-02 -7.56228301e-03 -4.87338437e-02  4.41592417e-02\n",
            "  1.04448638e-01  5.01825527e-03 -1.52064462e-02 -9.68614110e-03\n",
            " -2.26962524e-02  7.49504556e-04 -1.22884985e-02  2.30301140e-03\n",
            " -2.53424306e-02  1.36499369e-02 -1.20989990e-02 -6.59947115e-02\n",
            " -3.34248737e-02 -2.02030502e-02 -3.56596780e-03  2.94750467e-03\n",
            " -1.15332155e-02 -7.90278413e-04  8.37891223e-03  3.64365947e-03\n",
            "  3.65473052e-02  6.59669105e-03 -7.26490758e-03 -6.84994534e-03\n",
            "  3.23715831e-02  2.76575258e-02  3.36965656e-02 -4.26483510e-03\n",
            "  1.79712734e-03 -4.22591471e-02  2.69883146e-02 -2.23817603e-02\n",
            " -2.56202566e-02  1.14715129e-02 -3.38508965e-02 -1.75978931e-02\n",
            " -5.47640126e-03 -1.19181327e-02 -3.72222721e-02 -5.29167264e-02\n",
            " -4.48595997e-03  3.40395601e-02  1.87582872e-03  3.74166981e-02\n",
            "  1.00224270e-02 -4.22843907e-02  2.76209960e-02  2.47496242e-02\n",
            " -1.23017865e-02  2.90384807e-02 -1.32701929e-02 -1.97217877e-02\n",
            "  2.63404490e-02  6.86759090e-02  1.53957406e-02 -2.20042474e-02\n",
            " -9.96338395e-03 -5.99127829e-02  4.20193877e-03  2.76244995e-02\n",
            " -3.04853076e-02  1.73351469e-02 -1.78150627e-02  6.03592881e-02\n",
            "  3.16454059e-02 -5.63820542e-03 -1.32791882e-02 -4.18349686e-03\n",
            "  1.82534171e-02 -1.20603823e-02  1.23221961e-02 -2.16000250e-02\n",
            " -3.51392798e-02 -2.05857285e-02 -1.86439335e-02  2.80411062e-02\n",
            " -2.80899792e-02 -5.30699413e-02 -2.90868036e-02 -2.67538079e-02\n",
            " -6.58066149e-04 -2.03084255e-02  1.11281832e-03 -1.21916387e-02\n",
            " -1.26157778e-02  1.83866837e-02 -4.10979820e-02 -6.48132556e-02\n",
            "  1.54292888e-02 -9.85414577e-03 -3.25461461e-03 -2.94362433e-02\n",
            "  4.13349394e-02  1.50445965e-02 -5.22722276e-02 -1.61100353e-02\n",
            "  3.84011885e-03  8.35814977e-03 -4.10926562e-02 -1.99698525e-02\n",
            "  1.10305217e-02 -1.46852096e-02 -9.88506932e-03 -7.28721236e-03\n",
            " -1.03184321e-02 -2.00297286e-02 -1.94035819e-02 -3.98371089e-02\n",
            " -2.89903456e-02 -1.35051435e-02  4.88363124e-04 -7.62794864e-03\n",
            " -1.56565309e-02  2.11354267e-02 -5.48171607e-02  8.22545425e-03\n",
            "  1.47442802e-02  2.89068608e-02  3.31581609e-02 -4.02532884e-02\n",
            "  2.93585750e-03 -9.75049416e-03 -3.70428868e-02 -1.71300232e-02\n",
            " -1.15609030e-02  2.33965434e-02 -9.74603492e-03  3.52171885e-02\n",
            " -5.38514438e-02 -3.51106434e-02 -1.24798302e-02 -5.51962254e-03\n",
            " -1.23038441e-02 -9.36035710e-03 -2.78687943e-02  8.72351539e-03\n",
            "  5.26518994e-03  2.19954053e-02  9.77293913e-03 -7.52743249e-03\n",
            " -2.24212130e-02 -2.57242417e-02 -1.32314641e-02 -1.12722641e-02\n",
            "  3.41349866e-02  2.96984688e-02  2.93987128e-02 -2.36005926e-02\n",
            "  8.41307609e-03  1.58381496e-02 -4.21624442e-02 -1.83741005e-02\n",
            " -6.06449994e-03 -1.03608529e-02 -5.30976654e-03 -8.57562429e-03\n",
            " -9.91719004e-03 -2.39822702e-02 -1.36634739e-02 -5.74626906e-02\n",
            " -6.25333282e-02  3.53750151e-02  9.83925701e-03 -4.67747571e-03\n",
            " -7.52144520e-02 -4.09816096e-02 -1.44011487e-02 -7.86414547e-03\n",
            " -2.81644549e-03  2.93377658e-03 -4.04572788e-03 -6.45407888e-03\n",
            " -4.32605156e-02 -6.23118226e-03 -1.59512176e-02 -4.23239036e-03\n",
            "  4.27939882e-02  9.12069622e-03  4.33825500e-02  5.17003617e-03\n",
            " -1.95501671e-02  2.77436094e-03 -2.33760231e-02 -5.72821559e-02\n",
            "  6.17188941e-02 -3.02159234e-02  1.57421560e-02 -7.71853875e-03\n",
            " -1.27291937e-02 -1.59702561e-02 -3.63778736e-03 -5.94551592e-03\n",
            " -5.93839322e-03 -3.00931751e-02 -7.64366607e-02 -6.86100589e-03\n",
            " -5.92993419e-02 -3.22386839e-02 -4.13706008e-02 -2.47670753e-02\n",
            " -3.49630690e-03 -6.24149103e-02 -3.86152236e-02  3.14054556e-02\n",
            "  2.64192423e-02 -8.11100757e-03 -1.07795311e-02  2.79090354e-02\n",
            " -4.10910675e-02 -1.19461006e-02 -6.82530166e-03  8.01877081e-03\n",
            " -3.58500401e-02 -1.73579659e-02 -3.82935964e-02  4.34153342e-02\n",
            "  1.46107109e-02 -5.79549149e-02  1.23563934e-02 -1.21234326e-02\n",
            "  2.90959432e-02 -9.18827807e-03  3.66199789e-02  4.56587023e-02\n",
            " -5.57775836e-02 -2.40653808e-02  6.23036413e-03 -2.56308516e-03\n",
            " -2.56308516e-03 -3.43220289e-02 -4.27162114e-02 -5.90041412e-02\n",
            " -1.98328732e-04 -1.59922270e-02 -3.33078080e-02 -2.57523745e-02\n",
            "  4.17703745e-03 -3.91149774e-02 -5.27525668e-03  1.04846327e-03\n",
            "  3.49263077e-02 -1.71774976e-02 -3.97866067e-02 -3.97866067e-02\n",
            " -5.79172730e-05  5.00250607e-03  3.55744410e-02 -4.43465598e-02\n",
            "  5.56345160e-04 -2.11159123e-02  6.62465535e-03  4.07569420e-02\n",
            " -1.17710832e-02  2.51058253e-02 -6.73270198e-03 -3.80410971e-02\n",
            " -4.01236560e-02 -8.17921712e-02 -3.74227352e-02 -4.17879619e-02\n",
            " -9.80917939e-04 -8.85191149e-02 -4.74935800e-02  2.05111872e-02\n",
            " -7.72014777e-03  4.67718049e-03  7.26192838e-03 -6.96590515e-03\n",
            " -4.05614325e-02 -4.89570605e-03 -3.78721765e-02 -2.68744088e-02\n",
            "  1.26166847e-02 -4.04442463e-03 -5.62313718e-02  1.16895386e-02\n",
            " -1.59182694e-02 -6.54456312e-02  1.15595301e-02  6.45944884e-03\n",
            "  1.22259265e-02  1.48460845e-02 -4.51884750e-02 -4.02259161e-02\n",
            "  2.04699327e-03 -4.04783206e-02  2.60833569e-02  1.41979475e-02\n",
            " -2.72830996e-02  1.72036456e-02 -1.48754607e-02 -8.77061744e-04\n",
            "  1.87784644e-02  4.55797712e-02  1.87612419e-02  7.13130778e-05\n",
            " -5.62137088e-03 -2.44815885e-02 -3.72696802e-02  1.93674728e-02\n",
            "  1.01787558e-02  2.20471390e-02  2.64690343e-02 -1.24778989e-02\n",
            " -1.26801784e-04  3.78457623e-02  2.60040705e-02 -3.79660672e-03\n",
            "  2.84933706e-02 -2.72626219e-02  7.50926094e-03 -1.18272821e-02\n",
            "  3.91711255e-02 -1.80902190e-02 -8.13354896e-03 -7.45982933e-03\n",
            "  1.88056384e-04 -2.79286248e-03 -1.47104752e-02 -1.53572252e-02\n",
            "  2.38380540e-02  1.01393244e-02  3.85273475e-02  1.89823646e-02\n",
            " -8.77574188e-03 -3.48491894e-02  1.28496412e-02  2.37757226e-02\n",
            "  7.94149020e-03 -3.61928012e-02 -9.16076599e-03 -7.22198903e-02\n",
            " -3.22069094e-02 -2.30095359e-02 -1.24170266e-02  2.98052537e-02\n",
            "  3.42348088e-03  1.79808750e-02  2.34312183e-03 -6.84560299e-02\n",
            " -3.87620379e-04  3.96412319e-02  5.97102435e-03  5.14590049e-02\n",
            " -7.77556703e-02  5.55970504e-02 -4.03559050e-02 -9.67132810e-03\n",
            " -3.68475383e-02  9.35368084e-03  1.77301268e-02  9.49531599e-03\n",
            " -9.63963102e-03 -2.46499662e-02 -1.95196569e-02  7.96387071e-03\n",
            " -4.83459178e-03  1.04014360e-02 -4.26305680e-02 -2.24023330e-02\n",
            " -1.21247057e-02  5.72550456e-03  2.91150736e-02 -1.45351886e-02\n",
            "  6.78765705e-02  2.32159156e-02 -3.14384926e-02 -2.51977920e-02\n",
            "  1.59639922e-02  2.89588591e-02 -6.29080584e-02  4.54122948e-04\n",
            " -1.06756406e-02 -2.39712680e-02  3.41497828e-02  3.16954715e-02\n",
            "  5.02428784e-02  6.29990005e-03 -4.74122705e-02  7.13490866e-02\n",
            "  3.66832895e-05 -1.67168172e-03 -4.41104562e-03  4.07197174e-03\n",
            " -4.38825546e-03 -5.28023563e-02 -4.02999769e-02 -2.69576595e-02\n",
            " -1.05839392e-02  2.59779314e-02  4.44082642e-02  1.82087778e-02\n",
            "  1.55579909e-02  4.48305485e-02  6.30972848e-03 -2.72403444e-02\n",
            " -2.56161576e-02  3.45982747e-02 -3.51008915e-02  2.76413295e-03\n",
            "  2.33726056e-02  5.03726931e-02 -2.78198562e-02 -2.28926064e-02\n",
            " -3.50960852e-02 -6.62718952e-03 -5.81276063e-03  5.21059534e-03\n",
            " -2.43617821e-03  3.92247877e-02 -3.14193012e-02  1.59934832e-02\n",
            "  3.49575109e-02 -2.48880517e-02 -4.18984156e-02 -3.84887696e-02\n",
            " -5.17817948e-02 -2.54286975e-02 -1.98963627e-03  1.45136508e-02\n",
            " -2.46343675e-03 -4.29201943e-02  8.85260436e-04 -9.63947710e-03\n",
            "  8.01102603e-03  5.18269505e-02  3.20890742e-02  1.73727977e-02\n",
            " -8.69423638e-03 -2.15104921e-02 -1.78216271e-02 -3.02214336e-02\n",
            "  1.44424634e-02  1.96474367e-03  4.19045484e-02  1.74208186e-02\n",
            "  4.13934261e-03 -3.26081614e-02 -4.66745912e-03  4.74895909e-02\n",
            "  2.61130373e-02 -6.11571780e-02 -1.13370637e-02 -1.13370637e-02\n",
            "  5.75519707e-03  8.76458312e-04  3.41270607e-02  1.92509908e-02\n",
            "  1.17148576e-02  1.66904907e-05 -2.18571926e-02 -1.38223771e-02\n",
            " -1.57167508e-03  5.77367745e-03 -3.86940533e-02 -1.23808182e-01\n",
            " -1.15669771e-02  2.32368647e-02 -2.46629430e-02  9.41543269e-03\n",
            "  1.97789113e-03  5.59530938e-03 -4.82876220e-02 -1.27451377e-02\n",
            "  2.50039568e-02 -8.54831342e-03 -5.35475589e-03  2.35616918e-02\n",
            " -1.23317081e-02 -2.11887925e-02 -1.74273651e-02 -8.15988037e-03\n",
            " -2.01500053e-02 -6.91162229e-03  5.49236655e-02 -1.03256152e-02\n",
            "  1.02358039e-01  8.98341922e-03 -2.77273239e-02 -4.96408331e-02\n",
            "  9.49566478e-03 -1.17156759e-02  1.96045711e-02  5.81570711e-03\n",
            " -2.07125683e-02  1.08403017e-02 -3.94567852e-02  4.47106021e-03\n",
            " -1.96212652e-02 -8.22878284e-03 -1.88081517e-02 -2.00008822e-02\n",
            "  9.30561122e-04 -2.08031922e-02 -7.48411945e-03 -3.55972322e-03\n",
            " -1.60806543e-02  3.43315886e-02  3.58107266e-02  5.91052960e-03\n",
            "  3.72685627e-02  2.88877931e-02 -3.85135054e-03  1.73045083e-02\n",
            " -1.87474515e-03 -1.05686958e-02 -3.00817945e-02 -1.88681683e-03\n",
            " -3.84119296e-02 -1.83066409e-02  3.18685977e-02  9.20580888e-04\n",
            "  4.87218591e-02 -1.21538231e-02 -3.04235519e-02 -1.09968369e-02\n",
            "  6.44082890e-03 -3.58320185e-03  3.73543360e-02  9.86086690e-02\n",
            "  1.45174958e-02  6.72317960e-03 -1.96961164e-02 -3.99568524e-02\n",
            "  1.18086301e-03  2.47502288e-02  2.47502288e-02  1.47717789e-02\n",
            "  8.56124713e-03  3.80713626e-03  2.13451971e-02 -1.19749452e-02\n",
            "  9.85966363e-04 -1.19776977e-02  6.64578179e-03 -5.38561890e-03\n",
            "  1.49423281e-02  1.31890605e-03  3.90289820e-02  5.82853745e-02\n",
            "  3.89933324e-02  3.89933324e-02  3.31852205e-02 -1.17444009e-02\n",
            " -1.03591334e-02 -2.11209926e-02 -3.09110904e-02 -3.71569150e-02\n",
            " -2.09338726e-02 -4.54391160e-03  2.11681080e-02  3.59898526e-02\n",
            " -2.98631445e-02  3.07692348e-02 -3.04701015e-04 -1.35417442e-02\n",
            "  1.21770895e-02  3.58304251e-03 -3.12246695e-02 -2.39002336e-02\n",
            "  1.03463729e-02 -1.20084972e-01  2.44126629e-02  8.46432632e-03\n",
            "  4.68163416e-02 -2.59326692e-02 -1.42428707e-02 -2.35916292e-03]  - intercept :  0.9083525997451262\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.140356741402055\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:20,098]\u001b[0m Trial 175 finished with value: -0.2358296441156636 and parameters: {'count_threshold': 6, 'postag': False, 'voc_threshold': 9592}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.02039606 0.020195   0.05840921 ... 0.16884484 0.12205163 0.03326237]\n",
            " [0.12619424 0.020195   0.31553394 ... 0.1074485  0.098584   0.03018017]\n",
            " [0.09503299 0.07462922 0.10485517 ... 0.18909578 0.0193695  0.02217373]\n",
            " ...\n",
            " [0.71105122 0.06104517 0.93838191 ... 0.         0.05093014 0.        ]\n",
            " [0.02497245 0.03942353 0.12005322 ... 0.1023319  0.04980729 0.11431926]\n",
            " [0.         0.00721926 0.01632446 ... 0.16858797 0.061615   0.11087226]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 4.38420643e-03 -2.37694897e-03  4.83187579e-05  4.76986882e-03\n",
            " -4.57584684e-03 -1.50283963e-02 -9.48847015e-03  2.30487804e-02\n",
            "  3.29953611e-02  8.52794940e-03  2.31835506e-02 -1.63372792e-03\n",
            " -1.13020455e-02  2.18021084e-02  3.35407973e-02  3.55917868e-02\n",
            " -3.38594486e-02  5.08124315e-02 -2.18655893e-02  2.03265342e-02\n",
            " -7.22229032e-03  3.01807755e-02 -3.38243202e-02  4.56869796e-02\n",
            "  9.78160374e-03  8.61451235e-03  2.43691295e-02  4.38248032e-02\n",
            " -5.52094851e-02  5.07482462e-02  2.75479921e-02 -5.83638091e-03\n",
            "  1.51150696e-02 -3.04834388e-02  4.36459953e-03 -5.49832775e-03\n",
            "  7.68005654e-02  1.67238063e-02  2.37891520e-02  3.98069549e-02\n",
            "  1.82391863e-03 -7.99603906e-03 -1.04302046e-03 -1.60028975e-02\n",
            "  1.83181917e-02  7.46577191e-02 -4.44682307e-02 -3.73428894e-02\n",
            "  2.56742210e-02 -2.54432929e-02  6.93841405e-02 -4.91158966e-02\n",
            "  2.81536419e-02 -1.46009222e-02  2.70444791e-02 -1.10844109e-02\n",
            "  4.89198093e-02  5.03935881e-02 -3.95743888e-02 -5.36620689e-02\n",
            "  3.84335804e-02  4.52818007e-02  8.41535863e-04  3.57162573e-02\n",
            "  3.16192872e-03 -5.38309626e-02  7.19783612e-02 -1.07112358e-02\n",
            "  2.99206692e-02  2.44505873e-02 -3.17519402e-04  2.94824214e-02\n",
            " -8.79613613e-03  4.81808630e-02  1.24699000e-03 -6.99965434e-02\n",
            " -3.81413052e-02 -1.46814309e-02  8.89739897e-02  1.03275282e-02\n",
            "  3.17756107e-02 -2.48335663e-02  5.08944699e-03 -7.48613138e-04\n",
            "  3.63484639e-02 -3.70894541e-02  1.08409399e-03 -8.15843386e-03\n",
            " -3.31368060e-02  1.22548061e-01 -8.61454157e-02 -6.42330937e-02\n",
            " -2.85535660e-02 -1.47947572e-02 -7.94924565e-03  6.63137145e-02\n",
            "  1.73637827e-02 -5.25493873e-03 -5.46655347e-02 -3.09458081e-02\n",
            "  7.54091781e-02  3.55320476e-02  4.95827080e-04 -1.14564642e-02\n",
            "  1.77902559e-02 -4.03457750e-02  5.17582212e-02  3.69601848e-02\n",
            " -2.56659392e-02  1.73465910e-02 -1.05823433e-02 -2.30932121e-02\n",
            " -9.49353212e-02  3.22278682e-02 -6.29042630e-03 -1.24499952e-02\n",
            " -2.11889613e-02 -2.79093002e-02 -2.61974028e-02 -7.74106375e-02\n",
            " -2.03862468e-02 -2.23528266e-02 -2.75970577e-02 -3.28309811e-03\n",
            " -1.82238333e-03 -6.87834676e-02 -6.24149782e-02 -4.42814805e-02\n",
            "  9.26876054e-03 -6.80648926e-02  2.08282657e-02 -5.82850558e-02\n",
            "  5.99440567e-02  2.44571553e-02  1.46716451e-02 -7.60156478e-03\n",
            " -5.33042227e-02 -1.05866284e-02  1.65457468e-02 -3.80823313e-02\n",
            " -3.32499528e-02  8.07269573e-02 -1.73938762e-02 -2.47526646e-02\n",
            " -4.09609025e-02  2.21363683e-03 -5.13552847e-02 -3.51477637e-02\n",
            "  2.03758120e-02  2.10697566e-02 -4.78259899e-02 -8.21317565e-03\n",
            " -1.18759272e-02 -3.75710702e-02  1.50458893e-02 -2.41679088e-02\n",
            " -2.80840385e-02 -1.02377735e-01  1.31593258e-02  5.81709027e-02\n",
            " -9.57107208e-04  6.61571403e-02 -4.96047768e-02  1.65541094e-03\n",
            " -8.80022697e-03  6.36355808e-02 -4.85112715e-02  8.41096971e-02\n",
            "  1.26767391e-03  8.27690015e-02  2.30305306e-02  1.70828288e-02\n",
            " -3.91470750e-02 -3.91470750e-02  3.63832497e-02 -8.05331610e-03\n",
            "  1.91868621e-02  2.46050422e-02 -2.73587574e-02  3.77525994e-03\n",
            " -6.63457143e-02  3.35965056e-03 -6.24934882e-03  4.52895874e-02\n",
            "  2.27413312e-02  5.96236514e-02 -2.23159348e-02  4.51033919e-03\n",
            "  5.51174003e-02 -2.12483359e-03 -4.74709224e-03 -1.27843180e-02\n",
            " -7.36448253e-02  2.09324869e-03 -4.61325799e-02 -2.28175021e-03\n",
            " -7.16877506e-03  9.73120536e-03 -4.05834545e-02 -1.11514024e-02\n",
            " -1.82890046e-02 -2.36188896e-02  4.16790979e-02 -6.53577376e-02\n",
            " -5.10576614e-02  1.42862143e-02 -4.40972010e-04  1.27096481e-02\n",
            " -5.31458581e-03  1.94413907e-02  4.47648522e-02 -6.93258146e-02\n",
            " -5.46889707e-02 -5.88849180e-02  1.06070787e-03 -3.18679834e-02\n",
            "  2.62339773e-02  1.91674034e-02  3.48994576e-02 -2.09411647e-02\n",
            " -1.61778529e-03 -5.31819910e-03 -6.08434159e-02  1.29065415e-02\n",
            "  3.00041118e-02  5.22549297e-02 -5.68444730e-03  5.54348788e-02\n",
            "  1.80110389e-02 -5.47632254e-02  5.10214696e-03  2.97362391e-02\n",
            " -4.15489503e-02 -3.10929390e-02  2.51779552e-02  2.55931141e-02\n",
            " -4.67606661e-02 -6.89704873e-03  5.86540496e-02  8.32151976e-03\n",
            "  1.35601508e-02  8.16753363e-03 -3.10978687e-02 -2.03004774e-02\n",
            " -3.69164379e-02 -1.02285670e-02  1.22356679e-02 -4.24629688e-02\n",
            " -3.38273992e-02 -8.50848532e-02 -7.30327546e-02  3.33377999e-02\n",
            "  4.28819616e-02 -2.79058665e-02 -2.38626492e-02 -1.41961322e-02\n",
            "  2.87583139e-02 -5.81797596e-02 -2.65656902e-02 -1.55003753e-02\n",
            " -2.02563063e-02  1.47385102e-02 -5.14120649e-02 -4.36533966e-02\n",
            "  3.39563029e-02 -7.02885926e-02 -7.41298524e-02 -4.50873121e-02\n",
            " -1.14207486e-01  3.27908049e-02  1.56708571e-02 -7.36394799e-03\n",
            " -3.40245019e-03  2.67452853e-02 -4.92411525e-02  1.28615614e-03\n",
            " -3.11032565e-02 -2.69372039e-02 -5.14446937e-02  1.19401802e-03\n",
            "  2.45742771e-02  6.19981618e-02  2.71017414e-02  7.99441624e-03\n",
            "  5.13172318e-03 -1.48058304e-04  1.71901256e-02  1.49958656e-02\n",
            "  4.63416132e-03 -1.41689420e-02 -1.22144270e-02 -2.93257247e-03\n",
            "  4.35791947e-02  2.57216243e-02 -1.79727702e-03  1.55890909e-02\n",
            "  3.34981653e-02  1.56170523e-02 -9.23637325e-03  3.60755342e-02\n",
            "  4.48775399e-02 -9.24680278e-04  4.54936770e-02 -1.46522785e-02\n",
            "  4.73371092e-03 -1.43995751e-02  5.06939759e-02 -1.72299019e-03\n",
            " -2.39166951e-02  2.08825520e-03 -3.12184866e-02  1.93699438e-02\n",
            " -2.89403957e-02 -1.63376095e-02 -6.11148772e-02  1.27243420e-02\n",
            "  6.21506185e-03  2.75681106e-02  3.55891057e-03  3.19496656e-03\n",
            " -2.39837689e-02  4.99214752e-02  3.31688003e-03  4.28060502e-02\n",
            "  2.40232126e-02  1.05792527e-02 -3.89501119e-02  5.72823901e-03\n",
            "  1.89424958e-03 -2.43741645e-02  8.18165425e-03 -5.24448400e-03\n",
            " -4.92018731e-02 -2.54173990e-02  2.87788791e-02 -1.56045218e-03\n",
            "  1.10263510e-02  4.99989286e-02 -3.64382805e-02  5.63697992e-02\n",
            " -3.11821955e-02  8.40689203e-02  1.65212841e-02 -1.44904826e-02\n",
            " -2.72276094e-02  5.27284627e-02 -1.09342947e-02 -2.49488263e-02\n",
            "  3.04495913e-02  3.29443008e-02 -3.73358637e-02  1.18044188e-02\n",
            "  2.21315630e-02  3.51414011e-02 -7.13413449e-03 -4.77140536e-04\n",
            "  4.67752483e-02  1.80953883e-02  3.57432601e-02  4.34049274e-02\n",
            "  4.49995320e-02  1.88917159e-02 -1.71245831e-02  3.67059191e-02\n",
            "  1.58916583e-02 -1.76468064e-02 -4.76666457e-02  8.69717449e-02\n",
            " -8.49601948e-03  4.37506910e-02 -6.36542287e-03  6.17206439e-03\n",
            " -7.66585801e-04 -1.61755093e-02  4.07416939e-02  1.27762759e-02\n",
            " -2.92857976e-02 -4.52523215e-02 -2.70321016e-02 -1.94011189e-02\n",
            "  3.21643377e-02 -1.63957566e-02  9.34598485e-03 -1.30635264e-02\n",
            " -4.60222378e-03  2.13833258e-02  1.83085705e-02 -1.35111615e-02\n",
            "  3.52320989e-02  4.83419283e-02 -1.37341604e-02  5.93478712e-02\n",
            "  6.11641076e-02 -9.96158966e-03  1.91639074e-02 -1.42523192e-02\n",
            "  2.72537892e-03 -8.42864497e-02 -9.07046774e-03 -1.69367545e-02\n",
            " -1.82489726e-02  2.83492002e-02  1.07873069e-02 -1.92461021e-02\n",
            "  3.75029888e-02 -5.00961332e-03  1.65347588e-02  7.69189729e-04\n",
            "  2.06665288e-03  1.65370054e-02  4.11545391e-02  2.41689821e-03\n",
            " -1.51927209e-02  2.21081139e-02 -4.07550060e-02  2.05051278e-02\n",
            " -4.21586669e-02  1.94476923e-04 -3.58445311e-02 -6.35875647e-03\n",
            "  2.89645468e-02 -1.09428343e-02  1.32409654e-02  1.62734546e-02\n",
            " -7.19888441e-02 -3.05477635e-02  1.00296310e-01  1.28154394e-02\n",
            "  8.63893767e-03 -1.51592552e-02  3.09860333e-02 -1.37180852e-02\n",
            " -2.30347837e-02 -4.32504264e-03  5.64971524e-02  5.31921164e-03\n",
            " -6.51930134e-03 -3.63686572e-02  7.65301288e-03  2.18689019e-02\n",
            " -1.07474009e-02 -1.17796667e-02  2.18107799e-02  1.02459255e-02\n",
            "  1.39871961e-02  1.66196712e-02  2.84916938e-02 -1.48543007e-02\n",
            " -2.43393935e-02  4.46494288e-03  5.10133004e-02 -8.46804442e-02\n",
            "  7.47172941e-02  6.08044758e-02  6.55292979e-02  8.40464805e-03\n",
            " -2.95950850e-02 -3.17589763e-02 -3.17589763e-02  5.84751441e-03\n",
            "  5.61783625e-02  4.83820412e-03  1.52165878e-03 -7.79311955e-03\n",
            "  2.88946652e-03  2.60361299e-02 -4.49213574e-02 -9.37787117e-03\n",
            "  3.42684670e-02 -2.65337099e-02  5.86298163e-02  1.60177551e-02\n",
            "  3.92275878e-02 -9.89285402e-03  4.85399517e-03  1.08133839e-03\n",
            " -5.40344117e-02  9.16429133e-03  5.80937691e-02  9.08936418e-03\n",
            "  7.51421963e-03 -2.73305526e-02 -5.23677076e-03 -2.10996240e-02\n",
            "  1.13357308e-02  2.18577318e-02 -1.60466245e-03  5.02017863e-03\n",
            " -4.79799803e-02  1.08653960e-02 -3.78712422e-03  2.21962319e-02\n",
            " -4.15282733e-02 -1.51979830e-02  3.08248493e-02  1.86936145e-02\n",
            "  7.07856908e-03 -7.56559803e-03 -7.28044103e-03 -6.19536567e-02\n",
            " -3.34009709e-02 -1.19050850e-02 -3.86272351e-03 -2.07439370e-03\n",
            " -9.75508039e-03  2.74562621e-02  1.08745847e-02  1.43154507e-02\n",
            "  2.21104741e-02 -8.07930204e-03  5.36475252e-02  2.61892056e-02\n",
            "  1.01097489e-01  4.46672284e-02 -4.32311012e-02  2.60609132e-02\n",
            "  3.79088680e-02 -4.42467638e-02 -2.65705700e-02 -2.09554169e-03\n",
            "  2.88372107e-02  5.41736783e-02 -1.41954162e-02  1.53648190e-02\n",
            "  8.72102318e-03 -1.48946424e-02  3.08235983e-02  2.50594382e-04\n",
            "  1.01179778e-02 -3.87176913e-02 -7.41806486e-03 -2.96929470e-03\n",
            "  3.16662855e-02 -2.08065949e-02 -3.44645538e-03  3.51593348e-03\n",
            "  2.91271136e-02 -4.27937560e-03  4.22787753e-02  2.96415853e-02\n",
            "  8.40691416e-03 -5.44311070e-03 -5.61104442e-03  1.84510246e-03\n",
            "  9.29662022e-04  8.17464005e-03 -1.55532937e-02 -2.30937388e-03\n",
            "  3.65517857e-03  2.76250181e-02 -1.44428648e-02  3.60791756e-02\n",
            " -1.87198275e-02 -1.55307806e-02  9.09851509e-03  3.16682262e-02\n",
            " -1.66263180e-02 -1.61169376e-02  1.12975759e-02 -2.64331850e-02\n",
            "  3.16274354e-02  3.60109392e-02 -3.92323894e-02 -5.59778191e-03\n",
            " -1.87250627e-02  5.75134649e-02  3.43778769e-02  3.52072011e-02\n",
            " -1.31773674e-02  8.14210848e-03]  - intercept :  0.4936367437677829\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.2358296441156636\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:20,659]\u001b[0m Trial 176 finished with value: 0.03691043247121465 and parameters: {'count_threshold': 8, 'postag': False, 'voc_threshold': 1423}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.         0.02176488 0.66154468 ... 0.1783125  0.13742487 0.04826587]\n",
            " [0.04341177 0.05005792 0.06531083 ... 0.21761627 0.0903086  0.03723429]\n",
            " [0.75139263 0.98919835 0.00963082 ... 0.08305419 0.06773145 0.01478062]\n",
            " ...\n",
            " [0.06854108 0.12755804 0.02926164 ... 0.21339084 0.18002126 0.04700406]\n",
            " [0.00385795 0.01457798 0.03740988 ... 0.25623104 0.11288575 0.04813357]\n",
            " [0.56601456 0.75579279 0.07065692 ... 0.         0.         0.        ]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-2.54224649e-02 -4.78366924e-02 -4.02060384e-03  2.52536333e-02\n",
            " -2.44594503e-02 -1.57690449e-02 -5.97192635e-02  3.02112913e-02\n",
            " -2.83615353e-03 -2.88141197e-03 -2.43212183e-03 -1.60204805e-02\n",
            " -4.43003728e-02 -4.36645281e-02 -3.32645455e-02 -1.08078233e-02\n",
            " -1.87666421e-02  6.56334259e-02 -1.02338570e-01 -1.18572393e-03\n",
            " -2.94598682e-02  4.03474707e-02  1.60519851e-02 -3.22378071e-02\n",
            " -8.87730673e-03  3.04546135e-02 -7.23129659e-03  4.70109980e-02\n",
            " -1.62686353e-02  9.20217517e-03  5.29565131e-02 -3.85651537e-02\n",
            "  1.91394321e-03 -9.85449308e-03  7.12731709e-02 -4.19518332e-02\n",
            "  2.05356369e-02  9.52311860e-03 -5.76014090e-02 -2.36156191e-02\n",
            "  5.09976623e-02 -1.00729808e-02  1.47066917e-02 -6.00750471e-02\n",
            "  2.80594371e-02  9.97539017e-02 -9.50983084e-03 -5.47072301e-02\n",
            " -2.53780942e-02  1.44089203e-02 -1.26413637e-01  4.75596940e-02\n",
            " -4.79437366e-02  2.19727240e-02 -3.59206412e-02 -1.06318916e-02\n",
            " -1.30786216e-02 -4.69686144e-03  2.08560762e-02  1.08387224e-01\n",
            "  3.91468953e-02 -5.98044021e-02 -1.24225885e-01 -9.21574097e-03\n",
            " -7.06823681e-02 -3.43392846e-02 -1.46725246e-02 -1.09761721e-01\n",
            " -3.29308635e-02  3.70549406e-06 -5.51372420e-02 -1.67060526e-01\n",
            " -2.01770139e-02  5.48063370e-02 -1.12047625e-01 -3.46365872e-02\n",
            " -8.18594174e-02 -3.81288401e-02 -6.59058826e-02 -8.39543776e-02\n",
            " -3.17349599e-02 -3.53482677e-02 -3.76945874e-02 -1.44742204e-02\n",
            "  6.00226725e-02 -4.32021610e-03  7.02297446e-02 -1.72299272e-02\n",
            "  2.47082609e-02 -3.97944523e-02 -2.13000029e-02  3.35756798e-02\n",
            "  4.65742203e-02 -1.06212795e-01 -3.69483856e-02 -5.26594480e-02\n",
            " -3.93933053e-03 -2.08809615e-02 -6.13199851e-02 -6.13199851e-02\n",
            " -6.72476128e-03 -5.23557840e-02  4.59474840e-02 -1.09621228e-01\n",
            " -7.01803001e-03 -1.45234997e-02 -7.93754652e-03  7.18307698e-02\n",
            "  4.49717070e-02 -3.34204498e-02 -5.41353745e-02 -6.18461613e-02\n",
            " -9.52001344e-03  5.07257435e-02 -8.69301637e-02  1.23874669e-01\n",
            " -8.61883507e-02  4.75320675e-02 -2.31914316e-03 -1.24500139e-03\n",
            " -6.35457308e-02 -9.59803226e-02 -2.22205244e-02 -6.27804385e-02\n",
            "  2.00467852e-01 -3.94618410e-02 -1.48329528e-01 -6.93622360e-02\n",
            " -4.21395620e-02  4.49061033e-02  1.76406487e-02  5.96060849e-03\n",
            "  7.21651857e-03  7.93313377e-02 -1.29735125e-02 -5.29830030e-02\n",
            " -5.87905743e-02  2.50642993e-02  6.71567913e-02 -9.57003993e-03\n",
            " -4.70285991e-03 -6.37558326e-02 -6.66956748e-02 -8.31753709e-02\n",
            " -1.21109057e-01  7.60185310e-02  2.05535260e-02  7.78848796e-02\n",
            " -3.14096164e-02  3.60271964e-03 -1.15780229e-02 -5.74668411e-02\n",
            "  7.25607763e-03  1.05225857e-02  1.58021205e-01  2.86169798e-02\n",
            " -9.93240098e-02  2.57744032e-02 -9.50258151e-02  6.50950592e-02\n",
            " -7.17894390e-03 -2.67061849e-02  7.37454242e-02 -7.90351758e-03\n",
            "  3.62119387e-02  8.96691904e-03 -6.99105421e-02 -4.10712828e-02\n",
            "  2.72406936e-02 -9.67652755e-02  9.39768248e-02 -8.98209457e-02\n",
            "  1.78060719e-02  7.84263165e-02  4.79520605e-02  8.48878132e-02\n",
            "  5.01039841e-02 -6.16288953e-02 -1.36565300e-02 -1.64747033e-02\n",
            " -5.11120304e-02  6.15685936e-03  1.48718134e-02 -2.68734663e-02\n",
            " -2.64384659e-02 -4.43426958e-03  1.55478417e-02 -6.16742057e-02\n",
            " -4.59547497e-02 -1.41403467e-02 -1.82177653e-02 -5.31938236e-02\n",
            " -9.09979421e-02 -2.50798220e-02 -1.05155842e-02 -4.41079879e-02\n",
            "  5.43886737e-02 -4.93977097e-02 -4.39117936e-03 -6.21790539e-02\n",
            "  4.04371845e-02 -4.28591334e-02 -1.55863725e-02  8.49549060e-02\n",
            " -1.05721263e-02 -6.31340643e-02  8.02802553e-02  3.13411756e-02\n",
            " -1.83186493e-02  9.48124468e-03 -6.15822485e-02 -1.18811185e-02\n",
            "  2.19724847e-02 -5.96422205e-02 -4.03990635e-02 -3.35614931e-02\n",
            " -3.06068912e-02 -1.02754885e-01 -1.05970598e-02  6.27344487e-02\n",
            "  2.12647725e-02 -2.26890032e-02 -8.49815381e-02  1.45764339e-02\n",
            "  8.32764465e-02  1.24743972e-02  1.72652988e-02 -1.03917916e-01\n",
            " -8.12623537e-02  6.30918211e-02  6.48214146e-02 -1.65344711e-02\n",
            "  2.37469708e-02 -4.04543819e-02  1.72475542e-02  1.48367933e-02\n",
            "  4.57161973e-02 -9.11795049e-02  4.98031020e-02  5.14459258e-02\n",
            " -3.89603802e-02  3.55302260e-02  1.00812491e-02 -4.47522521e-02\n",
            "  1.65380938e-02 -3.26508613e-02  4.97690816e-02  1.14472007e-01\n",
            "  3.22275057e-02 -7.05066244e-02 -6.16418325e-02 -4.40949231e-02\n",
            " -7.88597653e-03 -1.18675424e-01 -1.84942491e-02  5.92202723e-03\n",
            " -6.30465117e-02 -4.30379406e-02  2.44621368e-02 -1.99932700e-02\n",
            " -1.31901929e-02  2.83393293e-02 -1.17932490e-02  4.68539954e-03\n",
            "  1.69800998e-02  5.61017260e-02  3.73433415e-02  2.14294656e-02\n",
            " -2.60158016e-02 -3.38557783e-02  5.80875212e-02  4.11673676e-02\n",
            "  1.24258491e-01  1.17650812e-01  2.53075966e-02  8.18736361e-02\n",
            "  6.80650112e-02 -6.87931718e-02 -6.87931718e-02  6.44118401e-02\n",
            "  1.04958095e-01 -1.25241933e-02 -2.77332651e-02 -6.20742399e-02\n",
            "  1.27394109e-01  1.10316666e-02 -1.74562734e-02  1.15574625e-01\n",
            " -1.43109190e-02  4.26430172e-02  2.80659167e-02  4.27873781e-02\n",
            "  4.34036595e-02  6.94663390e-02 -6.35295383e-02  6.51591878e-02\n",
            "  9.09871432e-02  3.47506557e-03  5.36642076e-02  4.02898945e-02\n",
            "  1.49089806e-02 -4.63741528e-02  1.11499135e-02  1.49322081e-01\n",
            " -5.57321319e-03 -5.30547367e-02 -6.13265524e-02  7.05081055e-02\n",
            "  1.32423939e-02  5.12498021e-02  6.66505604e-02 -6.97841855e-03\n",
            " -6.14946069e-03 -2.95218377e-02 -2.35820001e-02  5.88526474e-02\n",
            " -4.88375163e-02  1.26942577e-02 -7.54103490e-02 -2.47781696e-02\n",
            "  7.01193307e-03  1.57795835e-02 -1.06059523e-01 -1.46683509e-02\n",
            "  1.14317184e-01  6.05663338e-03  6.36726206e-02  7.13148041e-02\n",
            "  1.02470282e-01 -2.19099119e-03  7.19881228e-03  5.13771895e-02\n",
            "  5.29306220e-02  1.62689525e-01 -5.90237930e-03 -3.22317728e-03\n",
            "  1.09084921e-02 -1.25869832e-01 -1.29236083e-01 -1.42402358e-02\n",
            "  8.89793780e-02  9.17203926e-02  1.61616524e-01  6.52637818e-02\n",
            "  3.74948653e-02 -2.16620293e-02  2.07702327e-02  6.29299371e-02\n",
            "  1.22331447e-01  6.70863975e-02  2.92534507e-02  1.68579551e-02\n",
            "  7.06065405e-02  7.75303136e-02  5.70972217e-02  2.27834149e-02\n",
            "  5.66707142e-02 -2.65427401e-03]  - intercept :  0.636099689635657\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.03691043247121465\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:21,245]\u001b[0m Trial 177 finished with value: 0.047140698504429 and parameters: {'count_threshold': 10, 'postag': False, 'voc_threshold': 1970}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.58058684 0.64588253 0.0200043  ... 0.06410924 0.         0.00205731]\n",
            " [0.06982936 0.10633772 0.02774301 ... 0.00919955 0.         0.02045318]\n",
            " [0.20557099 0.30324636 0.01148834 ... 0.10650219 0.         0.00846383]\n",
            " ...\n",
            " [0.53062352 0.81246889 0.01579696 ... 0.13834672 0.         0.00714442]\n",
            " [0.02127715 0.10449678 0.03794885 ... 0.07227594 0.         0.0486647 ]\n",
            " [0.48952713 0.6798927  0.0464526  ... 0.06410924 0.         0.00205731]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 0.01725627  0.04950951  0.02001207 -0.00522052 -0.06434337  0.07073875\n",
            " -0.00701616 -0.02357517  0.08540547  0.01322099  0.11380512 -0.09455342\n",
            "  0.07170196 -0.06545657  0.06578876  0.010038    0.13918477 -0.00514698\n",
            "  0.03975589 -0.02506448 -0.04018108  0.01244843 -0.03964349  0.06088176\n",
            " -0.01202857  0.02249785 -0.07694865  0.02895804  0.04742771  0.04210362\n",
            " -0.03475515  0.02933197 -0.06127365  0.0728078   0.03758001 -0.02342254\n",
            " -0.09609424  0.03404807  0.03569411  0.01923298 -0.06192074  0.00408367\n",
            "  0.02337241  0.0577032  -0.01093496  0.0017933  -0.00712949 -0.0357206\n",
            "  0.04788643  0.15832482  0.186353    0.04136729  0.00623664  0.08912273\n",
            "  0.01083278 -0.14110636 -0.16130736  0.0271525   0.02665138 -0.00222268\n",
            "  0.02774072  0.09304886  0.12694296 -0.05422008  0.00637582 -0.02579267\n",
            " -0.00926439 -0.10951165 -0.01648415 -0.01944848  0.04603888 -0.07166538\n",
            "  0.08758686 -0.04012736  0.02236334  0.01640263  0.03978248  0.06754267\n",
            "  0.02604983  0.21837163 -0.04236921  0.04383096  0.10586973  0.14871999\n",
            "  0.20972985 -0.1033233   0.00919308 -0.01809565  0.02676018  0.15695268\n",
            "  0.03610758 -0.11936808  0.05412218  0.0482529  -0.17402509  0.03751875\n",
            "  0.18493342  0.11944382  0.17479286 -0.06842842  0.05661064 -0.09058099\n",
            " -0.0214806  -0.02383809  0.00490389  0.08820865 -0.09750488 -0.0346657\n",
            " -0.1757458   0.02986561  0.01962368  0.09387099  0.09891287  0.11410495\n",
            "  0.0325184   0.03631476 -0.08373384  0.12042691  0.12728404 -0.04112155\n",
            "  0.14119505 -0.01562484 -0.00250031 -0.05598831 -0.06621218 -0.04610083\n",
            "  0.0419654  -0.08655574  0.12549933 -0.0766577  -0.02161323  0.00508679\n",
            " -0.13870958 -0.13030938 -0.08994725 -0.04737051  0.04518132  0.07852549\n",
            "  0.02498751 -0.02702172 -0.03710622  0.03203373  0.02887827  0.00220809\n",
            "  0.00592661  0.00950196 -0.05120251 -0.05653064  0.007715    0.00198391\n",
            " -0.02542314 -0.0892301  -0.09989884  0.00609144  0.03211441  0.04057608\n",
            "  0.03579084  0.01400781 -0.02725752 -0.02313644  0.01597019 -0.01259103\n",
            "  0.12108378  0.02062444  0.12192895  0.03147079  0.00436134 -0.07574508\n",
            " -0.01245452  0.02424373  0.01949867 -0.06005881  0.02054915 -0.03523486\n",
            "  0.02120493  0.10280468  0.06867507 -0.01640901 -0.102471    0.06945924\n",
            "  0.08197147 -0.06966044 -0.11662207  0.03473552  0.10271772 -0.08272491\n",
            " -0.06023075  0.03882105 -0.04476804  0.11821081 -0.02680268  0.0604364\n",
            "  0.04986332 -0.08115571 -0.01579061 -0.03584863 -0.00729994 -0.03687263\n",
            " -0.02531722  0.04368015 -0.14671246 -0.17893953 -0.23954706 -0.04859785\n",
            " -0.0518009   0.00029719  0.00405231 -0.06545544  0.11627061 -0.00808943\n",
            " -0.02454647 -0.1014454  -0.03095916 -0.01325737  0.06741306 -0.03726979\n",
            " -0.00112101 -0.07082182  0.08880071 -0.11342298 -0.05403315 -0.01733867\n",
            "  0.04810169 -0.00031493  0.13543835  0.07829494 -0.09148062 -0.0512134\n",
            "  0.09628964  0.00453144  0.17563027 -0.16196921 -0.0186479  -0.06778344\n",
            " -0.05899521  0.28578335  0.13478969 -0.07224925  0.1472916   0.01892525\n",
            " -0.13360202 -0.05650393  0.07970042  0.11603504  0.01485376 -0.01207513\n",
            "  0.05003872 -0.10184771 -0.02165977  0.00905069 -0.04521434 -0.0151346\n",
            "  0.01788084  0.02421873 -0.17442633 -0.00244356  0.04199688  0.04866713\n",
            "  0.11893593  0.09054786  0.08669886 -0.07481051 -0.11989508 -0.00874768\n",
            "  0.04905444 -0.07742363 -0.04542747  0.01720052 -0.10013349 -0.03906594\n",
            " -0.03706731 -0.13940824 -0.0658234  -0.06603427  0.02059406  0.03112874\n",
            " -0.17436426  0.06049222 -0.12682499 -0.08033751  0.006272    0.04884738\n",
            "  0.07212044 -0.12863378  0.13054915 -0.06990805 -0.07035672  0.01048298\n",
            "  0.0079644   0.02066089 -0.00094034  0.00486001]  - intercept :  0.43825941876087454\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.047140698504429\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:21,818]\u001b[0m Trial 178 finished with value: 0.26125265422919447 and parameters: {'count_threshold': 9, 'postag': False, 'voc_threshold': 1662}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.         0.04521472 0.0500481  ... 0.1960464  0.11997001 0.09877447]\n",
            " [0.04119722 0.04987477 0.05602382 ... 0.15542642 0.1852633  0.05615671]\n",
            " [0.         0.01752926 0.         ... 0.21433614 0.11197201 0.07940447]\n",
            " ...\n",
            " [0.         0.00779078 0.         ... 0.21957197 0.13436641 0.08402665]\n",
            " [0.01269681 0.01526704 0.0152778  ... 0.2090353  0.23303247 0.05774857]\n",
            " [0.31072263 0.21500686 0.16007889 ... 0.131289   0.17191702 0.01026229]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-0.00052257  0.00376381  0.09839402  0.01359607 -0.0128366  -0.01269449\n",
            "  0.02050346 -0.04215335 -0.08720772 -0.01330068 -0.0559377   0.00236411\n",
            " -0.02882085  0.002354   -0.06945379  0.01099847 -0.02249339 -0.04208193\n",
            " -0.07873029 -0.04915402 -0.09228775  0.07611795  0.00895056 -0.01910624\n",
            " -0.02310019 -0.06271048  0.03357647  0.00061503  0.04497403  0.04443438\n",
            " -0.06002083 -0.01870622  0.06791994  0.07421228 -0.09806238 -0.08992234\n",
            " -0.07894709 -0.0049762  -0.00845441 -0.08734625 -0.03634989  0.13289779\n",
            "  0.02873049 -0.04378068  0.03425328 -0.04165073 -0.17969112  0.12944849\n",
            " -0.01266279 -0.00335544  0.0613034  -0.00071743 -0.05552222  0.01668732\n",
            " -0.05606134  0.07282093 -0.15472483 -0.12325665  0.05002071 -0.03651089\n",
            "  0.02234041 -0.04439998  0.02787598 -0.00528937  0.03171537 -0.05127747\n",
            "  0.02874824  0.03209648  0.05825767  0.15559033 -0.0139696  -0.12580121\n",
            "  0.03148871 -0.07331666  0.04494558  0.00977108  0.03068194  0.05739247\n",
            "  0.20051104 -0.05311557  0.00855401  0.05640718 -0.09623256  0.23013618\n",
            "  0.08954142 -0.07644609  0.05139543 -0.03700074 -0.09747605  0.0303716\n",
            " -0.01617018 -0.01871182 -0.10113293  0.04617222 -0.05842292  0.00158517\n",
            " -0.02619856 -0.02207954 -0.02123221  0.01499208  0.05651014  0.0739515\n",
            "  0.03400357  0.10621877  0.03352558  0.03541474  0.03070275  0.00276104\n",
            "  0.02910672 -0.10533635  0.12636529  0.06730258  0.01849555  0.00165949\n",
            " -0.1088127  -0.00265565  0.0240869  -0.07313926  0.03211736  0.0095905\n",
            "  0.01960217 -0.02211632 -0.02188555 -0.12118833  0.01641263  0.03328674\n",
            "  0.02513321 -0.01688412 -0.10948779 -0.01707417  0.01955933  0.02176906\n",
            "  0.01070131  0.01635638 -0.00604499 -0.00995949  0.07266487 -0.11700392\n",
            "  0.16646114 -0.05162005 -0.02581867  0.02327977  0.08428133  0.1868778\n",
            "  0.16663575 -0.05108776  0.05313708  0.03130087 -0.11733468  0.01513199\n",
            " -0.03565006  0.05121048  0.07009622  0.04344575 -0.04870363 -0.04061521\n",
            " -0.02723624 -0.12155847 -0.04932241 -0.03168451  0.01905274 -0.08282083\n",
            " -0.12745886 -0.02190365  0.00296093 -0.0838088  -0.09099164 -0.06753381\n",
            " -0.02921889  0.02443971 -0.01467527 -0.02082984  0.02859886 -0.02775967\n",
            " -0.11565775 -0.15656557 -0.10229542 -0.15749393 -0.04950669 -0.12081684\n",
            "  0.00608464 -0.14643929  0.02236224  0.0649303  -0.02903803 -0.07136736\n",
            " -0.0089971  -0.09047763  0.02972914 -0.04340298  0.08972098  0.06967603\n",
            " -0.06938305  0.00679996 -0.04639066 -0.12883621 -0.00106545 -0.07412928\n",
            " -0.15436443 -0.03923776 -0.038107    0.07411978  0.08072567 -0.14226024\n",
            "  0.00495756  0.03350184 -0.05510408  0.08084641 -0.09451885  0.00620042\n",
            "  0.03304286 -0.06621811 -0.02118607 -0.01104897  0.08019078 -0.01612629\n",
            "  0.04107462  0.01667323 -0.02028214 -0.03135345  0.02405336 -0.01588087\n",
            " -0.03398743 -0.1359875  -0.1910556   0.02247052 -0.00335792  0.13768069\n",
            "  0.07843542  0.01729709 -0.04004128  0.03693571 -0.04218897 -0.02238818\n",
            " -0.08158463  0.08713867 -0.02326331 -0.0263273  -0.00493639 -0.16371586\n",
            "  0.02578469  0.05028106 -0.13019921 -0.0344241   0.02645748  0.11483778\n",
            " -0.18519089  0.02256438 -0.01414247  0.11691914  0.02710123  0.09565968\n",
            " -0.20453549  0.0865474  -0.01692415  0.07944171  0.04517112 -0.14977924\n",
            "  0.00118033 -0.04846147 -0.0980494  -0.02286117 -0.09308798 -0.1425419\n",
            "  0.02219067  0.14548638  0.12248451 -0.07054456 -0.04823498  0.06941587\n",
            "  0.02332595 -0.06516665  0.02228891 -0.01252551  0.06515517  0.33390188\n",
            " -0.04283353 -0.14535353 -0.11848522 -0.01652664 -0.11162029  0.09532003\n",
            "  0.11892563 -0.11292977 -0.14805535  0.00606225 -0.01507287 -0.1219011\n",
            "  0.07974126 -0.12252464 -0.01581773  0.04853221 -0.0504358  -0.01609539\n",
            "  0.14222457  0.04675852 -0.05325954  0.02044052 -0.05711159 -0.00208216]  - intercept :  0.9688818560931213\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.26125265422919447\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:22,412]\u001b[0m Trial 179 finished with value: 0.1796061589523668 and parameters: {'count_threshold': 5, 'postag': False, 'voc_threshold': 5362}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[7.57699091e-02 1.56316390e-02 4.71274921e-02 ... 0.00000000e+00\n",
            "  4.61138296e-01 1.03572626e-01]\n",
            " [0.00000000e+00 1.28166769e-02 4.05301730e-02 ... 8.08883883e-02\n",
            "  2.39952073e-01 4.82595831e-02]\n",
            " [2.53448118e-02 7.59905747e-02 3.53225660e-02 ... 2.74801080e-02\n",
            "  2.66528344e-01 5.69710443e-02]\n",
            " ...\n",
            " [3.81579489e-04 9.38386461e-02 1.37864258e-01 ... 0.00000000e+00\n",
            "  1.35588863e-01 7.72916676e-02]\n",
            " [7.09507336e-02 8.85175027e-02 1.30558015e-01 ... 2.61906582e-02\n",
            "  3.14893764e-01 5.53544034e-02]\n",
            " [2.56618932e-01 1.39289465e-01 3.66638290e-01 ... 2.01526816e-01\n",
            "  2.45484256e-01 2.84471915e-02]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-4.10353014e-03  7.30921343e-03  9.75204261e-03 -5.60138158e-03\n",
            " -4.86482356e-03  7.08214960e-03 -5.54915713e-03  1.24863698e-02\n",
            "  5.14598892e-03  7.17370283e-04 -3.79774529e-03 -2.28382338e-02\n",
            "  1.47505187e-02 -8.95657781e-03 -1.13983629e-02  1.03138982e-02\n",
            " -1.36036134e-02  1.05468661e-02 -1.05260864e-04  8.64848002e-03\n",
            " -1.02830843e-02  1.62994350e-03  2.22252515e-02 -3.90867080e-03\n",
            " -1.48625037e-03 -1.76668527e-04 -1.92171987e-02 -1.93337222e-02\n",
            "  1.65064592e-02  2.29181278e-02 -4.58329042e-03  1.09628035e-02\n",
            "  4.34954881e-03 -3.75757529e-02  4.50315726e-03  2.27376160e-02\n",
            "  1.72127631e-02 -4.02399158e-02 -8.05140531e-03 -8.86131917e-03\n",
            " -1.78167680e-02 -8.62947693e-03  1.13029433e-02  4.14998253e-02\n",
            "  1.11222450e-02  7.02636313e-03 -2.34902892e-02 -2.59774178e-04\n",
            "  8.91100587e-03  6.49906469e-03 -1.42026929e-02  1.02165237e-02\n",
            "  7.78448734e-03  3.16730349e-02 -6.56222443e-03 -5.72094134e-04\n",
            " -1.39054774e-02 -1.82966484e-02 -2.97562108e-03  2.48527293e-03\n",
            " -4.56724322e-02 -1.91406711e-02  3.43253030e-02 -9.62364049e-03\n",
            "  7.17808953e-04  4.95068631e-03 -2.66288870e-02 -4.22609373e-03\n",
            "  1.18863277e-02  1.81280785e-02  1.46959540e-02  3.00686554e-02\n",
            " -7.90953585e-03 -3.14264548e-02  6.78941811e-03  8.83883054e-03\n",
            " -1.90371403e-03  4.14969310e-02  3.75978966e-02  2.39124447e-03\n",
            " -2.38921162e-03  3.02155837e-03 -1.83188553e-03  8.93790585e-03\n",
            " -4.60525747e-03  4.90067050e-03 -1.95284405e-03  5.91254869e-03\n",
            " -2.02767715e-02  4.95423656e-02  3.27647771e-02 -3.94055085e-03\n",
            "  3.39634569e-03  5.52277221e-04 -2.88038759e-02  1.58813769e-02\n",
            " -8.14988532e-03  7.35374358e-03 -2.91575849e-02  5.89334179e-03\n",
            "  3.40192700e-02 -1.64923758e-02 -3.64740713e-02  2.27083342e-02\n",
            " -4.09304817e-02  4.28430380e-03  7.37542562e-03  2.58295048e-02\n",
            " -2.11791814e-02 -7.96087509e-03  2.27450631e-02  5.96510805e-03\n",
            " -2.15675916e-02 -8.34098101e-03  4.99741348e-05 -8.65897574e-03\n",
            "  1.27520054e-02 -3.98874704e-03  1.21162609e-02 -4.65706660e-03\n",
            "  1.16778779e-02  5.54034273e-02 -5.52062396e-03 -5.08882648e-04\n",
            " -1.62028125e-02  1.79690760e-03  3.99741881e-03  1.08688050e-02\n",
            "  4.77429947e-03 -8.28823678e-03 -1.91028000e-02  1.12147656e-02\n",
            " -5.27243270e-03  5.85241441e-03  5.85241441e-03  6.51787127e-03\n",
            "  8.48940547e-03  8.07188843e-03 -7.27917958e-03  3.92403621e-02\n",
            " -1.10780688e-02  3.41845628e-02 -1.68431773e-02  2.95659486e-02\n",
            "  1.01914271e-02  2.80469583e-02 -5.39671820e-03  1.07800231e-02\n",
            " -2.12342553e-02  1.11079565e-02 -1.94779670e-02 -1.75677244e-02\n",
            "  2.38547256e-03 -7.47055935e-03 -8.31214310e-03 -3.58166974e-04\n",
            " -2.11730507e-02 -1.81558819e-02 -2.17109177e-02 -3.15191761e-03\n",
            "  1.80741367e-02 -1.77502316e-02 -3.05877273e-03 -1.66752738e-02\n",
            " -1.28664283e-02 -7.85289405e-03  4.78405758e-03  6.57050077e-03\n",
            "  3.70339322e-03 -1.17072075e-02  1.29739392e-02  1.41155717e-02\n",
            "  2.31211604e-02 -1.48436265e-02 -1.25031641e-02 -1.34740309e-02\n",
            "  2.36778851e-02  3.69369162e-02 -1.90153920e-02 -4.78616516e-03\n",
            "  1.37679566e-02  6.15897660e-03 -4.02424855e-04  1.86419500e-02\n",
            "  3.03422994e-02 -8.15430029e-03  6.55129587e-03 -1.55396925e-02\n",
            "  1.29137478e-02 -1.42904941e-02  1.87032454e-02 -3.19395915e-02\n",
            " -1.07890906e-04 -5.50767505e-03 -1.61992185e-02  9.88648676e-03\n",
            " -1.42645888e-02 -1.61511488e-02 -1.78624181e-04  1.23977825e-02\n",
            " -3.86070949e-03 -1.04665449e-02 -8.74933325e-03  2.66014616e-02\n",
            " -1.41768383e-02 -2.75570190e-02 -1.34040855e-04  2.26220444e-02\n",
            " -2.51995898e-02  4.26521841e-03  1.15967896e-02 -2.69583377e-02\n",
            "  8.81749592e-03  7.72638781e-03 -4.12208921e-03 -2.97423463e-02\n",
            "  3.45327395e-03  1.85766111e-02 -1.79595303e-02 -1.46179987e-04\n",
            " -1.85211670e-03  1.90469134e-02 -4.75486161e-02 -3.07886265e-02\n",
            " -1.00152698e-02 -1.15436453e-02 -1.50643878e-02 -1.00190997e-02\n",
            " -4.27114566e-03 -1.54890109e-02 -2.91036875e-02 -3.09922429e-03\n",
            " -1.19017602e-02 -2.93191818e-04  4.32919440e-02 -2.49935640e-02\n",
            " -1.65171833e-03  2.97957988e-02 -1.64163839e-02 -8.03190091e-03\n",
            " -5.23481927e-03  2.92872443e-02 -1.90169360e-02 -1.66988177e-02\n",
            "  7.88629632e-03 -9.93950548e-03 -2.88269245e-03 -1.25204686e-02\n",
            " -2.12017032e-02 -9.97646551e-03  1.54014477e-02 -2.46459972e-02\n",
            " -1.18074229e-04  2.34907476e-03  2.31643152e-02  1.55100582e-02\n",
            " -8.50682951e-03 -7.06552825e-03  3.43017981e-03 -7.05049562e-03\n",
            " -1.11747880e-02  7.03257052e-03  2.26803665e-02  2.17910776e-02\n",
            "  3.43197799e-02 -1.71207170e-02 -3.37679308e-03 -1.98636727e-03\n",
            " -9.93198273e-04 -6.09848021e-03 -1.27781744e-02 -2.95631325e-03\n",
            "  2.14088655e-02  3.38782460e-02  2.83333309e-02  3.54035973e-03\n",
            "  3.70357092e-02  3.71065277e-03  2.44581530e-02  5.85402046e-03\n",
            "  3.92191030e-03  2.03577287e-03 -1.83640704e-02  2.64520430e-02\n",
            "  8.71905125e-03 -1.57071657e-02 -1.69156708e-02 -1.35552864e-02\n",
            "  2.31503628e-02 -1.40154504e-02 -1.70598559e-02 -4.06087396e-03\n",
            "  1.24037690e-02 -2.83027867e-02 -2.65736802e-02  3.54055453e-02\n",
            " -1.96536884e-02  1.03638605e-02 -2.09276261e-02  3.78834617e-03\n",
            " -9.83281333e-03  1.23202879e-02 -1.61713690e-02  5.73109333e-03\n",
            " -1.51690320e-02 -5.23990389e-03  3.59215028e-02 -7.30486947e-03\n",
            " -6.93817805e-03  5.89515442e-03 -1.31747377e-02 -8.12656464e-03\n",
            "  9.71632097e-04 -7.25165421e-04  3.87573585e-03 -1.62721321e-04\n",
            " -1.20577173e-03 -2.63621190e-02 -9.15272349e-04  9.33575658e-03\n",
            "  6.53271291e-03 -1.32471384e-02  4.39481109e-05 -1.94107834e-02\n",
            " -4.80676379e-03  1.81495536e-02 -6.75892127e-03  4.04103097e-03\n",
            " -2.06569377e-03 -3.38362109e-03 -4.57319014e-03 -7.54527981e-03\n",
            "  4.55875725e-03  3.05384138e-02  1.64839215e-02 -3.12898575e-03\n",
            "  2.65730932e-02 -1.25513733e-02  2.11567996e-02 -3.09406528e-02\n",
            " -1.29718542e-02 -1.74553197e-02 -3.18889606e-02  1.45122746e-03\n",
            " -8.38180702e-03 -7.98087543e-03  9.22019052e-03 -2.88423852e-02\n",
            "  3.63841840e-04  1.05086854e-03 -1.23765920e-02  2.41182887e-03\n",
            "  8.01446604e-03 -2.11284267e-02  2.04031212e-02 -4.14179127e-03\n",
            " -9.99160547e-03 -8.36824146e-03  2.38694841e-02  1.59310861e-02\n",
            " -4.48539852e-03 -2.36299752e-02 -2.26752635e-02 -3.64931103e-03\n",
            " -9.42801683e-03 -1.18780834e-02  2.92234470e-03 -8.32770931e-03\n",
            " -2.16317219e-03  4.65954586e-02 -9.40754929e-03 -1.33060314e-02\n",
            "  5.11048583e-03  1.78056533e-03  5.38292751e-02 -6.05802872e-03\n",
            "  1.77651977e-03  4.70393705e-02  1.65661959e-02 -2.68704670e-02\n",
            "  7.13588557e-03  5.22045489e-03 -1.44475517e-02  1.00911982e-02\n",
            " -6.22275824e-03  2.33203475e-02 -1.09097491e-02  6.21300033e-03\n",
            " -1.99204373e-02  1.00444104e-02  1.84669600e-02 -1.83807495e-02\n",
            " -2.33038667e-03 -6.18188498e-03  2.32275916e-02  7.00886584e-03\n",
            " -1.80414666e-02 -1.21583582e-02  3.92118197e-03  5.30194422e-02\n",
            " -1.17710011e-02 -4.95290244e-04  3.12554028e-02  8.59226355e-03\n",
            " -6.32788346e-03  6.08285311e-03  6.75954246e-03 -1.64518246e-02\n",
            " -1.56740350e-02 -7.25314738e-03  2.24199685e-03  4.85704638e-03\n",
            " -8.33172833e-03 -1.37248790e-02 -1.37248790e-02  9.26902571e-03\n",
            "  8.16989012e-03  8.95824403e-04  1.84840194e-03 -3.02584820e-03\n",
            " -2.10459168e-03 -1.52227779e-02  4.42891418e-03 -9.26467920e-03\n",
            "  9.60712384e-03 -2.94547440e-03  2.03744124e-03 -1.54085333e-02\n",
            " -1.44640738e-02  8.25663102e-04  7.42577730e-03  9.39102904e-03\n",
            " -3.64108607e-03  2.34918748e-03 -1.51384622e-02  7.39049333e-03\n",
            "  9.52856862e-04 -2.21797856e-03  1.12210265e-02 -1.33588356e-02\n",
            " -2.38466001e-02 -1.49186559e-03  1.39994381e-02  7.10151195e-03\n",
            "  6.75766615e-03  9.98439510e-03 -4.99477375e-03  6.15157958e-03\n",
            "  3.03024148e-02 -1.16618379e-02  3.05610654e-03  2.92907531e-02\n",
            " -1.71293044e-02 -5.44682108e-03  1.98951808e-02  3.98183206e-03\n",
            " -7.02041694e-03  1.27845351e-04  2.71417910e-02  7.19284940e-03\n",
            " -2.61342503e-02 -1.69486663e-02  4.67410120e-03 -2.28886357e-02\n",
            " -2.07745633e-02  1.29390961e-03  7.89622694e-03  1.09643392e-02\n",
            "  7.62766714e-03 -2.24649440e-02  4.69716894e-03  1.01523765e-02\n",
            "  6.81221664e-03 -9.22973353e-03  6.16544576e-03  8.21514951e-03\n",
            "  2.61749010e-02 -4.90227388e-03  1.94844438e-03  3.51400968e-03\n",
            " -5.23389357e-03 -1.60533076e-02 -1.52220020e-02 -2.44357050e-02\n",
            " -3.72656417e-02  4.79554279e-03  1.87224536e-02 -2.74937493e-03\n",
            "  9.32765937e-03 -9.01318051e-03 -5.66176314e-03  3.82014198e-02\n",
            " -8.00620418e-03 -9.57626227e-03  1.17708680e-02  1.54131600e-03\n",
            " -7.02584670e-03 -2.28337528e-02  2.86198625e-02 -6.89853527e-03\n",
            "  7.64436182e-03  2.74752687e-03 -5.02342465e-04 -8.31529652e-03\n",
            " -1.60326155e-02  5.33399548e-03  1.39036990e-02 -5.64836190e-04\n",
            "  1.58709217e-02 -4.34979431e-03 -1.97222546e-02 -2.44465496e-02\n",
            " -3.13635896e-02 -2.58157422e-02  3.08017870e-03  6.56412887e-03\n",
            "  2.35154316e-03 -3.11569572e-02  1.99028335e-02 -1.07937717e-02\n",
            "  1.21308581e-02 -3.64650941e-02  2.50214988e-02 -1.02947058e-02\n",
            " -2.44835710e-02 -2.73665236e-02 -3.28009283e-02 -5.77454266e-03\n",
            "  1.07562829e-02 -3.41793298e-04  3.20146035e-03 -5.61462030e-04\n",
            " -6.42341909e-03  1.38243048e-02  7.96426366e-05 -2.44995902e-02\n",
            " -5.89661471e-03  7.53568084e-03 -1.06102121e-02 -1.49106726e-03\n",
            "  7.96308897e-03  3.96986370e-03  1.29531249e-02  3.00642481e-02\n",
            " -1.77169072e-02  1.62352273e-02  6.61035089e-03  1.03000423e-03\n",
            "  7.90921217e-03  6.44758748e-03  6.46707180e-03 -4.17646776e-02\n",
            "  2.83621537e-02  2.36336404e-03  3.49126655e-03 -3.21063318e-03\n",
            "  4.15587242e-03  4.15587242e-03  1.47309477e-02 -2.18544878e-02\n",
            " -5.12113194e-03  3.41923126e-03  9.71396540e-04 -7.16354182e-03\n",
            " -5.83180632e-03 -1.38591922e-02 -5.24735478e-03  1.99508139e-03\n",
            "  2.15000529e-02 -3.53760156e-02 -3.85769825e-03  1.39418456e-02\n",
            " -2.56491566e-02  4.97444650e-03 -2.62036448e-02 -1.37339269e-02\n",
            " -1.25596481e-02 -3.38754511e-03  3.83862647e-03 -8.97270581e-03\n",
            " -1.41406113e-03  3.48291791e-03  1.20216833e-02  1.19990038e-02\n",
            " -8.77947608e-03 -1.69326853e-03 -1.20692394e-02 -7.16563335e-03\n",
            "  5.25999799e-03 -7.35446545e-03  1.50754012e-02  3.77902353e-02\n",
            " -1.41739966e-02  2.07943146e-02 -3.07522667e-02  2.54341723e-02\n",
            " -2.84057148e-03 -1.69367791e-02 -1.03612326e-02  7.89771864e-03\n",
            "  3.07646131e-02 -6.27935589e-04 -6.41710321e-03 -3.85967602e-03\n",
            " -5.17210380e-03 -1.56744957e-02  2.07688766e-02 -1.76566705e-02\n",
            "  7.16459111e-03  3.70702181e-02 -7.10392045e-03 -3.01059404e-03\n",
            " -1.46362687e-02  1.43842535e-02 -2.53758391e-02 -2.92585065e-03\n",
            "  2.79532118e-03 -4.14103984e-03  1.78217762e-03 -1.24824461e-02\n",
            " -1.08281686e-02  8.85714604e-04  1.28694908e-02 -1.00046809e-02\n",
            " -7.13420867e-03  5.38752883e-03 -2.95745843e-03 -1.65017701e-02\n",
            " -8.11232890e-03 -6.27285422e-03  1.90537526e-02 -2.06697888e-02\n",
            " -4.52937127e-03  1.10761376e-02 -2.98294773e-03  4.83502015e-03\n",
            "  2.67626461e-02 -9.91058084e-03 -3.61307488e-02  1.33712635e-02\n",
            " -4.57895502e-02 -1.29302601e-02  4.21095074e-03 -6.82517653e-03\n",
            "  1.37902445e-02 -5.11448120e-03 -1.55354713e-02 -4.86175198e-03\n",
            " -4.13934525e-03 -3.44115649e-02 -1.40818846e-04 -1.64519906e-02\n",
            " -1.31331127e-02 -2.31218631e-03 -1.75229500e-03 -9.15099306e-03\n",
            "  3.21792781e-02  1.58158929e-02 -8.82663700e-03  6.43223461e-04\n",
            "  5.74015693e-03  2.20881939e-03 -5.54163293e-03 -3.99207112e-03\n",
            "  3.85990742e-02 -1.82740187e-02 -2.11332277e-02  6.54513967e-03\n",
            "  2.35313972e-03  3.71814997e-03  1.13250417e-03  2.10204048e-02\n",
            " -5.65612037e-03  1.26920337e-02 -1.65245595e-02  7.21166979e-03\n",
            "  7.43759228e-03  1.73784979e-02  9.00286007e-03 -1.83851360e-02\n",
            "  2.28487091e-02  3.38789445e-03  2.77603662e-03  5.86937003e-03\n",
            "  1.28547920e-02  2.53793312e-02 -1.11705027e-02 -1.31521839e-02\n",
            " -1.35831892e-02  5.88288144e-03 -1.44087511e-03 -2.00030644e-03\n",
            " -4.98963064e-03 -6.76515253e-03  5.21490027e-03  2.75635128e-02\n",
            "  2.93733074e-02  5.24548827e-03 -4.60931629e-03 -3.94888671e-04\n",
            "  6.11937013e-03  1.80640967e-02  5.76574986e-03  1.37781325e-02\n",
            "  1.67717607e-02  4.57624487e-04  1.50675947e-02  6.77522644e-03\n",
            " -4.17091789e-03 -4.13717243e-03  7.15057362e-03  1.43564353e-02\n",
            "  1.91760315e-02 -5.30499083e-03 -8.97213293e-03  1.19528851e-03\n",
            " -1.06819701e-02 -2.39485795e-02 -9.06778059e-03 -1.26841263e-02\n",
            " -7.87185586e-03 -6.60814231e-03  2.74513339e-02  3.78937774e-03\n",
            "  4.13000855e-02 -1.25304323e-02  8.79924551e-03 -1.01991473e-02\n",
            "  1.47974590e-02  1.05616252e-03 -2.33524471e-03 -7.55000397e-04\n",
            " -4.98433399e-03 -1.06606546e-02 -1.27254484e-02 -3.22156318e-03\n",
            "  2.09092256e-02 -2.13477064e-03 -1.29689671e-02 -9.18387497e-03\n",
            " -1.50267787e-02 -2.61602198e-02  1.03480612e-02  1.16221101e-02\n",
            "  4.19807101e-03  8.67020895e-03  2.91143579e-03 -2.29465953e-03\n",
            "  1.21301413e-02 -1.37990157e-02 -3.78030942e-03  1.78977307e-03\n",
            " -4.58460840e-03 -9.16893532e-03 -9.19897427e-03  2.26629068e-02\n",
            "  4.11541387e-02 -4.73304903e-03  4.88753766e-03  2.42358177e-02\n",
            "  8.33450268e-03 -1.54759446e-02 -1.54552430e-02 -6.10494335e-03\n",
            " -4.66225854e-03 -1.76501487e-02  1.40086131e-02 -2.68182472e-02\n",
            "  7.59930995e-03  1.51377672e-02  1.91261118e-02  2.75349506e-03\n",
            "  2.60693870e-03  7.86445756e-03  7.18586821e-03  8.82840793e-03\n",
            " -2.50872067e-02 -4.67734710e-02  2.79403639e-02 -1.73329927e-02\n",
            " -4.85789670e-04  1.09164949e-04  9.89597253e-03 -6.58813873e-03\n",
            " -6.26659890e-03  3.33670201e-02 -2.80747903e-03  2.63498019e-03\n",
            " -1.47597130e-02  1.85728696e-03 -2.45645621e-03 -6.79311562e-03\n",
            "  1.58384564e-02 -1.15853407e-03 -2.75955716e-03  8.33076103e-03\n",
            " -6.35844711e-03  3.13081123e-02 -2.10111952e-03  9.79991112e-03\n",
            "  2.59880827e-02 -2.77731419e-02 -2.33236421e-02  4.43200723e-03\n",
            "  3.55087670e-03  1.87406689e-03  1.32750888e-02  6.98484455e-03\n",
            "  3.03524313e-03 -1.35801133e-02 -3.61757086e-03 -8.29538737e-03\n",
            "  1.19481415e-02  2.81089464e-02 -1.55835595e-02 -7.56571785e-03\n",
            " -6.09138917e-03 -3.39339729e-02 -1.13678092e-02 -2.08742279e-02\n",
            "  1.30235129e-02  1.90326278e-02  4.63226922e-03 -7.31941220e-03\n",
            "  6.65399436e-03  4.39062023e-03  2.49724597e-02  2.49864685e-03\n",
            "  8.12103868e-04 -6.84137226e-03  1.21510211e-02  7.86754436e-03\n",
            " -8.29344459e-03 -1.14157814e-02 -2.26241213e-03 -7.38325025e-03\n",
            " -1.21444903e-02 -1.21444903e-02  5.64937386e-03  1.05662026e-02\n",
            "  8.48036262e-03 -3.34651606e-03 -4.10685095e-03 -2.33182799e-03\n",
            " -9.36650946e-03 -3.23309838e-03]  - intercept :  0.5653284913425913\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.1796061589523668\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:22,969]\u001b[0m Trial 180 finished with value: -0.3173199955270526 and parameters: {'count_threshold': 7, 'postag': False, 'voc_threshold': 9383}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.30384345 0.05314733 0.43383537 ... 0.02725226 0.02999304 0.01437831]\n",
            " [0.         0.         0.         ... 0.22907256 0.         0.05324523]\n",
            " [0.12859675 0.06315892 0.02959032 ... 0.1637975  0.11417613 0.0515373 ]\n",
            " ...\n",
            " [0.54303887 0.06830248 0.79229964 ... 0.02271022 0.01874565 0.0065126 ]\n",
            " [0.         0.         0.06956923 ... 0.15471342 0.22392022 0.08177455]\n",
            " [0.         0.02415115 0.03975384 ... 0.22101917 0.16310876 0.08480397]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-0.01259671  0.01240359  0.00487671 -0.00870545  0.00476895 -0.02964622\n",
            " -0.0026821   0.00309924  0.00898176  0.0032505   0.0062356  -0.00441223\n",
            "  0.00508127 -0.02657947 -0.00176593 -0.03472461  0.02408199 -0.02837966\n",
            "  0.02072958 -0.01956329  0.02144435 -0.01633961  0.01641527  0.07409353\n",
            "  0.02106514 -0.02642504 -0.00628272  0.0020385  -0.02855579  0.03622891\n",
            "  0.03634436  0.04934909 -0.00403852 -0.0249662   0.03355447  0.01109732\n",
            " -0.02139431 -0.00302784 -0.00365117 -0.01059397 -0.01940181  0.0252999\n",
            "  0.04174981  0.00667049  0.03643848 -0.01398008  0.00935359 -0.05023971\n",
            " -0.02785814  0.01747702 -0.01310635 -0.07220266 -0.03017327  0.03791549\n",
            " -0.01204313  0.03097805 -0.02285364 -0.02617105 -0.01646083  0.04704887\n",
            " -0.00420302  0.08393904  0.03973229  0.0191521  -0.0848365  -0.04450468\n",
            "  0.01170945  0.02540473  0.00853007 -0.0023827  -0.00544363  0.02077532\n",
            "  0.01749791 -0.05364363  0.01613863  0.02264869 -0.00898962 -0.02241307\n",
            " -0.01576608  0.01037108  0.06851649  0.05745227 -0.04070699  0.02418625\n",
            " -0.0444398  -0.06397673  0.00460162  0.00347533 -0.03657861 -0.00736563\n",
            "  0.03288234  0.02889123 -0.00518711  0.03053389 -0.04353226  0.06331157\n",
            "  0.0021432   0.02137892  0.03571229  0.02450481 -0.02320905  0.03417325\n",
            "  0.03466937  0.03545545  0.01584932  0.01125319  0.02067613  0.11783779\n",
            "  0.06986023 -0.06450501 -0.00698108 -0.00151085  0.03923472  0.01251616\n",
            " -0.07067446 -0.02465368  0.04227212  0.00919509  0.05244657  0.01615198\n",
            " -0.01975126  0.07095899  0.01667468  0.05506875 -0.02657036 -0.0411628\n",
            "  0.07747116  0.07961687  0.02928867 -0.03415802  0.03216087  0.00693851\n",
            " -0.01914036 -0.02181948  0.00372312  0.04422424  0.02743728  0.02733594\n",
            "  0.02068586  0.02681173 -0.01500266  0.02882683  0.02952519  0.00575342\n",
            " -0.02453838  0.04963061  0.00868193  0.02402129 -0.00578137 -0.04366761\n",
            "  0.00851177 -0.04245845  0.00949541  0.02274312 -0.02634078 -0.00501123\n",
            "  0.02797719  0.07465611  0.06735082  0.02638366  0.05491107  0.00690734\n",
            "  0.07544817  0.04309773  0.02873777 -0.00353395 -0.03053367 -0.04683187\n",
            " -0.00676553 -0.00882479 -0.03582834  0.01105903  0.03417496  0.02413408\n",
            "  0.0376407  -0.00871081 -0.00871081 -0.04516491  0.01932152 -0.0037347\n",
            "  0.00135789  0.02048725 -0.01520008  0.00607044  0.0661871   0.01893024\n",
            "  0.05868189  0.00209482  0.07831941  0.03611349  0.04544641  0.0025507\n",
            "  0.01274962 -0.02647897  0.00036451  0.05234665  0.04239583  0.02607125\n",
            "  0.03581459 -0.00433662  0.00580058 -0.04570076  0.00420202  0.00052821\n",
            "  0.01799864 -0.02723875  0.0300616   0.02647452  0.03534501  0.00574657\n",
            "  0.03340339  0.00401998  0.02300382  0.00559076 -0.046231   -0.01503254\n",
            "  0.00682089  0.01049803  0.05963036 -0.03563553  0.06792106 -0.01789167\n",
            "  0.05454814 -0.02392606 -0.02701858  0.00520368 -0.007325   -0.00164487\n",
            "  0.06906506 -0.02093683  0.10423002  0.04326393 -0.03294263  0.04964773\n",
            "  0.04598826  0.00333624  0.02677868  0.00907339 -0.03946375 -0.02938126\n",
            " -0.03035014  0.05721815  0.01187989 -0.00451607  0.00938832  0.00141307\n",
            " -0.02590123 -0.02129889  0.00892081 -0.01707236  0.00050132  0.00481854\n",
            "  0.02162757  0.00412379  0.00718938  0.00514182 -0.00673446 -0.00355725\n",
            " -0.00424472  0.02632211  0.02815147 -0.01543872 -0.00322989  0.00255883\n",
            " -0.00954695 -0.01774845  0.03311985  0.04424641 -0.03983018 -0.00116545\n",
            "  0.02425875 -0.00053479  0.00990249 -0.04452384  0.01620493 -0.02076537\n",
            " -0.01233029 -0.01342274  0.06238849 -0.02633864 -0.02672624  0.01303309\n",
            "  0.05750565  0.03280018  0.08129227  0.02475462 -0.01250469 -0.01463352\n",
            " -0.02621737 -0.00374533 -0.04779223 -0.04682426 -0.04872588  0.02816221\n",
            " -0.00396356  0.04138279 -0.00220572  0.02536551  0.02560415 -0.0371353\n",
            "  0.01619681 -0.01112419  0.03287274  0.00874635  0.03055284 -0.01945963\n",
            "  0.04690591  0.00353637 -0.05499792 -0.04177153 -0.00660265  0.03420499\n",
            "  0.01112985 -0.00758972  0.00309374 -0.05392643 -0.00473371  0.01303922\n",
            " -0.04606203 -0.04655806 -0.01351361 -0.02427153 -0.019265    0.02068974\n",
            "  0.05044531 -0.03352167  0.00551666 -0.02812792  0.03695543 -0.03982058\n",
            "  0.00557732  0.00151129  0.00646626  0.05380945  0.01959219 -0.00284598\n",
            "  0.00658659 -0.02206656  0.08006204  0.00148277 -0.00067713  0.02574696\n",
            "  0.04086936 -0.01954465  0.01548543  0.01362154  0.01656396 -0.00122326\n",
            " -0.0342266  -0.00059895  0.06208551 -0.02279105 -0.02041205 -0.00730966\n",
            "  0.02196264 -0.01884441  0.01857406  0.02715101  0.03347349  0.0030126\n",
            "  0.03016681  0.0851082  -0.01899292 -0.02264606  0.01620852  0.02415402\n",
            " -0.01212627 -0.03275852 -0.0261102   0.01554541  0.01068459 -0.04795554\n",
            "  0.0655904  -0.00430349 -0.01114608 -0.0288107   0.02043415  0.02181158\n",
            "  0.04805957  0.09027497  0.05631817  0.01914763 -0.08598037 -0.03283074\n",
            " -0.01226564 -0.04238235  0.00031612  0.03203589  0.06149928  0.01797446\n",
            " -0.02923281 -0.04195623 -0.03603836 -0.01439556  0.03981746  0.02689343\n",
            "  0.00953513 -0.01203998 -0.04471389 -0.01615106 -0.0319548   0.02250408\n",
            "  0.01478128 -0.01269167  0.01189455 -0.00408994  0.00408086 -0.02996507\n",
            " -0.10864673 -0.0407285   0.08617815  0.00951093 -0.04671675  0.04762596\n",
            " -0.02231248  0.0194628   0.02990385  0.0016517   0.02962122  0.02962122\n",
            "  0.02595967  0.02380386 -0.02232732 -0.02498555 -0.01971848 -0.06835789\n",
            "  0.03895021  0.07271427 -0.06825637  0.03911985  0.05999568 -0.01715948\n",
            "  0.02661693 -0.02234878  0.0409674  -0.05138199 -0.12524352 -0.07570363\n",
            " -0.08735572 -0.00257867 -0.00031362  0.01840699 -0.00109452  0.00024853\n",
            "  0.03372393 -0.04313548  0.01359015 -0.03004265 -0.02667781 -0.05324025\n",
            " -0.01080241  0.05602254 -0.03414475  0.02945043  0.02000339 -0.02443474\n",
            "  0.0028809   0.00121166  0.0238764   0.01677029 -0.06623104  0.0539905\n",
            " -0.02014267 -0.04277277  0.0046889  -0.05772531  0.04987362  0.06732459\n",
            "  0.09018928  0.00401493  0.03175713  0.01151411  0.01345343  0.00462588\n",
            "  0.06148689 -0.07357837  0.02925142 -0.03913697 -0.09293832  0.04744872\n",
            "  0.02511779 -0.0453675   0.00806221 -0.05672142  0.03326617  0.00507258]  - intercept :  0.3458433186111836\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.3173199955270526\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:23,526]\u001b[0m Trial 181 finished with value: 0.05646672829354299 and parameters: {'count_threshold': 7, 'postag': False, 'voc_threshold': 5558}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.         0.05609027 0.01226701 ... 0.05345885 0.10648927 0.06079363]\n",
            " [0.35168572 0.04665327 0.51036141 ... 0.33399258 0.         0.        ]\n",
            " [0.39093321 0.01937266 0.5537232  ... 0.67257414 0.00417959 0.00778861]\n",
            " ...\n",
            " [0.17616763 0.10124957 0.26061483 ... 0.19663329 0.26465448 0.01022255]\n",
            " [0.10903488 0.02541088 0.20865086 ... 0.22901129 0.21318644 0.01280785]\n",
            " [0.02032542 0.02822324 0.05563814 ... 0.05889646 0.10155747 0.04886106]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 4.11028999e-02 -3.58625481e-02  3.94249536e-02 -6.74591292e-03\n",
            "  6.40840565e-02 -2.23353527e-02  7.49040878e-02 -1.96622913e-02\n",
            " -1.28209626e-02  3.67227547e-02  3.69552382e-02  5.31701580e-02\n",
            "  2.36940843e-02  5.10979899e-02  3.02822775e-02 -4.52012695e-02\n",
            "  2.75017674e-02 -2.35611857e-02 -6.72734605e-02 -5.92453305e-02\n",
            "  1.11300045e-01 -1.44374493e-02 -8.72273986e-03 -3.57951469e-02\n",
            " -3.06576698e-02  8.24216056e-02 -2.67875101e-03  8.97692470e-02\n",
            "  8.47599457e-03 -1.21085764e-02  2.72242153e-02 -1.29584489e-02\n",
            " -5.21599732e-02 -4.87920721e-02  1.29766933e-02 -2.54432311e-02\n",
            "  3.93913121e-02  1.87633855e-02  1.13261324e-02 -2.64565654e-02\n",
            "  4.66372370e-03 -3.13167869e-02  4.79059214e-02  4.50388600e-02\n",
            "  5.18963495e-02  5.14520938e-02 -6.66179936e-02 -1.02237676e-02\n",
            " -3.76741118e-02  3.56602791e-02 -3.47155257e-03 -3.87530119e-02\n",
            "  9.20003848e-02 -9.75569076e-02  1.49249130e-02 -4.06188388e-02\n",
            " -6.21813776e-02  2.22645307e-02 -5.32910478e-02  1.40406704e-02\n",
            "  7.27771347e-02  5.09088349e-02  1.31418132e-02 -9.11532600e-02\n",
            " -4.57747047e-02 -6.61110793e-03  3.38879163e-02 -3.69093708e-02\n",
            "  4.82545137e-02 -6.93929738e-02  2.39571744e-03  7.87760471e-02\n",
            " -1.33092983e-02  2.75065888e-02  4.41346375e-02  4.19284867e-02\n",
            " -4.08160979e-02 -9.64389254e-03  1.81718586e-03 -2.15614601e-02\n",
            "  2.62646593e-02  2.30940848e-02  9.67875445e-03  3.90830287e-03\n",
            " -8.34315624e-03  4.83097713e-02 -1.77812934e-02 -1.16123101e-02\n",
            "  7.68823927e-03 -3.04107689e-02 -3.55603282e-02  2.49328148e-02\n",
            "  5.86954497e-02  1.78409103e-02 -1.28871919e-02  6.58032853e-02\n",
            " -7.30649487e-03  3.61012656e-02  1.08444221e-02  4.32638624e-02\n",
            " -3.16163479e-02  6.62570842e-02  4.86028261e-02 -4.16576328e-02\n",
            "  3.65601411e-02  2.23482460e-02 -1.48669071e-02  3.41381302e-02\n",
            " -8.58324296e-03  8.71076204e-02 -5.17665106e-02 -6.05672440e-02\n",
            " -5.37775566e-03 -5.15367210e-03  1.08489820e-02  2.81901797e-02\n",
            " -4.35087339e-02  5.51565682e-02  7.03342974e-02 -3.66892944e-02\n",
            "  2.60496051e-02  9.51666866e-03 -4.29359985e-02 -1.87882662e-05\n",
            " -2.63611726e-02  9.17139506e-03  3.67435796e-02 -2.85394399e-02\n",
            "  4.55461405e-03  1.55790262e-03 -1.74516503e-02  6.61746912e-02\n",
            " -8.61184030e-02  9.17003461e-02 -7.65545717e-02 -1.45357985e-03\n",
            " -3.27528272e-02 -7.50367698e-02 -4.28375054e-05 -5.92892329e-02\n",
            "  2.54591622e-02  3.34474292e-02  3.18778417e-04 -2.05207461e-02\n",
            " -8.57556464e-02 -4.22023081e-02 -2.97212556e-02 -5.46128831e-02\n",
            "  6.41442130e-02  7.72593289e-03  4.62258159e-02 -4.00426385e-02\n",
            "  3.71012441e-03 -7.15512884e-02 -3.71642798e-02  8.14627428e-03\n",
            " -1.84693478e-02  6.79811922e-03  2.76930891e-03 -3.31525786e-02\n",
            " -3.78867319e-03 -2.41018274e-02 -1.04007579e-02 -1.89940324e-02\n",
            " -7.96716946e-02  1.22186596e-02  1.91465968e-02  2.37257545e-02\n",
            "  7.97885296e-02  1.08660204e-02  5.77158093e-02  5.35927988e-02\n",
            " -1.46001782e-03 -8.83327760e-03  1.97245467e-02  1.16258000e-02\n",
            "  4.77397535e-02  2.33575113e-03 -5.22799245e-03 -7.46403877e-02\n",
            " -1.99896147e-02 -2.94008087e-02 -6.53254614e-04  6.66293918e-04\n",
            " -4.09436384e-02 -1.34480268e-02 -2.23261358e-02 -4.80597799e-02\n",
            "  1.03515670e-01 -3.43231009e-02 -2.77613604e-02 -1.04751294e-01\n",
            " -5.25605008e-02 -2.13381047e-02 -1.29426582e-01  1.39401115e-02\n",
            " -1.27230553e-02  9.14470817e-02 -3.54007489e-02  1.52576325e-02\n",
            " -3.08880587e-02 -8.72849886e-02 -7.40291895e-02  2.59929685e-02\n",
            "  2.78265580e-02 -3.58415976e-02  1.07195147e-02  3.95225233e-02\n",
            " -1.76866127e-02 -9.07399459e-03 -2.30112148e-03 -2.27356259e-02\n",
            " -2.05555853e-02  4.83038013e-02  4.34010096e-02 -3.55773542e-02\n",
            "  5.78184720e-02 -1.15116962e-01  6.43599624e-03  1.71924248e-02\n",
            "  2.39005352e-02 -2.39745291e-02  5.00206611e-02 -4.13364916e-02\n",
            " -5.55441145e-02 -3.10475062e-02  1.06678201e-02 -6.16753623e-02\n",
            " -4.62735289e-02 -2.88522389e-03  1.22775170e-02 -3.71381563e-02\n",
            " -1.10607360e-02 -3.62455039e-02 -8.29371486e-03 -2.38747520e-03\n",
            " -3.65645285e-02  1.89844738e-02 -8.48480081e-04  3.02940757e-03\n",
            "  1.47882630e-02 -3.77726532e-02 -1.92057709e-02  2.23486205e-02\n",
            "  1.21325092e-02 -8.45429141e-02 -2.93711279e-02 -1.67444311e-02\n",
            " -8.36246655e-02 -8.78755408e-05  4.45806632e-02 -4.88409247e-03\n",
            " -3.59345943e-02 -1.92732963e-02 -4.03148227e-02  4.10063945e-02\n",
            " -4.36394100e-02  8.81730092e-02 -1.94870193e-02 -2.68381809e-03\n",
            "  3.48423860e-02 -2.74820963e-02 -5.86780689e-02 -8.33208150e-02\n",
            "  4.16483059e-02 -5.58589718e-02 -1.07519457e-02 -4.52172430e-02\n",
            " -3.24467088e-02 -2.87929617e-02 -1.89133843e-02 -3.10445833e-02\n",
            "  5.56162318e-02  4.34912191e-02  6.05134278e-02 -1.00405060e-02\n",
            "  1.07281122e-02  4.13337879e-03  2.73261780e-02  4.71413329e-02\n",
            " -2.90631952e-02  7.92009897e-03 -1.64549922e-03 -4.24710848e-02\n",
            " -3.89266294e-02 -6.34283168e-02 -8.50789186e-02 -2.86383944e-02\n",
            "  4.59056173e-02  2.04050327e-02 -8.03429077e-03 -1.78198772e-02\n",
            " -4.77843527e-02 -3.64173011e-02 -1.89910779e-02 -4.85774589e-02\n",
            "  6.30504073e-03 -2.07010600e-02 -7.93507290e-03 -1.03763417e-01\n",
            " -6.16772094e-03  2.81020237e-02 -6.03502407e-03 -4.05949610e-02\n",
            "  2.50513079e-02 -5.92382110e-02 -2.74586039e-02 -4.40388382e-02\n",
            "  2.33011435e-02 -4.36563469e-02 -3.66924095e-02 -7.61298146e-03\n",
            " -7.58651280e-02 -6.30902707e-02 -4.05708173e-02 -6.59629714e-02\n",
            "  8.42606708e-04 -4.17358433e-02  2.80081491e-02 -3.08878204e-02\n",
            " -2.07698610e-02  9.68804900e-03  4.26156114e-02 -2.72234512e-03\n",
            " -5.18518680e-02  2.96120762e-02 -6.29167422e-02  1.28761175e-02\n",
            " -4.55073271e-02  1.96777057e-02  3.23989142e-02 -1.63862641e-03\n",
            "  4.56782843e-02 -6.60966137e-02 -4.49664582e-02  4.09299339e-02\n",
            "  2.14740550e-02 -2.11578542e-02 -3.23937674e-02  1.39450945e-02\n",
            "  4.67614239e-02 -8.08563024e-02  1.05158804e-04 -1.47914381e-02\n",
            "  3.84718324e-02 -5.99029003e-02  1.42365245e-02 -1.01130475e-02\n",
            " -2.99863615e-02 -4.00199207e-02  6.77261055e-02 -2.98465756e-03\n",
            " -1.81531095e-02 -2.22595600e-02 -1.94999669e-02  7.77966733e-03\n",
            "  2.28262891e-02 -2.49340989e-02 -1.64863219e-02  9.53272340e-03\n",
            " -2.87161783e-02  2.25709603e-02 -3.71273040e-02  3.37366143e-02\n",
            " -6.21950635e-02 -1.08388475e-01  3.51809703e-03 -7.39170185e-02\n",
            "  1.21706571e-02 -7.68577156e-02  2.90406795e-02 -2.52106574e-02\n",
            "  5.37317206e-02 -7.54598661e-02  1.38904636e-03  2.68783117e-03\n",
            "  2.82698167e-02 -3.44551064e-02  3.55223160e-02  1.75177469e-03\n",
            "  7.05292629e-02 -1.77175204e-02  2.58589132e-04 -2.19059638e-02\n",
            "  2.81707884e-02  9.78576283e-03  1.08258824e-02  4.54156579e-02\n",
            "  2.75417828e-02 -2.30016265e-02 -2.31131554e-02  4.65431597e-02\n",
            "  4.26977036e-02  2.72658774e-02  1.09774724e-02 -6.37675969e-02\n",
            " -7.72596029e-02  1.59560812e-02  3.63375931e-02 -2.36395546e-02\n",
            " -9.12826572e-03  4.22826472e-02  1.43938797e-03  3.40184795e-02\n",
            "  6.03641985e-02  4.95227361e-02  1.00658098e-02 -6.72230039e-02\n",
            " -9.04007140e-02  3.08726892e-02  3.28807514e-02 -2.14460097e-02\n",
            " -6.03256395e-02  2.99034484e-02 -2.23633203e-02 -2.97271391e-02\n",
            " -6.38792598e-02  3.56674491e-03  5.63737500e-02 -5.03825997e-02\n",
            " -5.48855162e-02 -3.93045527e-02 -2.86890234e-02 -7.56145391e-02\n",
            " -2.88655648e-02  3.65674874e-02  5.30853278e-02  6.11820654e-02\n",
            " -3.59138398e-02  3.08305000e-02  2.30338024e-02  3.01808258e-02\n",
            "  4.09951264e-02  8.25768748e-02  1.15179893e-01 -6.68671026e-02\n",
            "  5.84862161e-02  9.06246101e-04 -1.95463598e-02 -1.04703068e-02\n",
            "  3.31518792e-02 -9.60434950e-02  1.23551448e-02  2.11168834e-02\n",
            "  1.05495770e-01 -7.02264854e-02  1.18966122e-01 -8.26847863e-02\n",
            "  4.24308548e-02  3.04026678e-02 -4.96967912e-02 -4.07991508e-02\n",
            " -2.37090945e-02  7.38839287e-02 -7.76595264e-02  4.58611799e-02\n",
            "  1.84407350e-02  1.47427708e-02 -2.13585615e-02 -6.55502363e-03]  - intercept :  0.7827804874069392\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.05646672829354299\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:24,110]\u001b[0m Trial 182 finished with value: 0.30340058213188004 and parameters: {'count_threshold': 10, 'postag': True, 'voc_threshold': 9816}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.05126996 0.05074278 0.09460816 ... 0.13952113 0.03225794 0.02035246]\n",
            " [0.06422452 0.00943215 0.3360236  ... 0.10357667 0.         0.0555664 ]\n",
            " [0.02759556 0.05588908 0.04906055 ... 0.13854583 0.1267839  0.02746454]\n",
            " ...\n",
            " [0.53072079 0.02726472 0.02378947 ... 0.11486808 0.         0.00323466]\n",
            " [0.02251048 0.05998443 0.06439862 ... 0.10662029 0.09999059 0.02713972]\n",
            " [0.         0.06066219 0.06058167 ... 0.03388414 0.03619196 0.02039024]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-3.16924774e-02  2.16463818e-03  5.49549539e-02  1.02600712e-02\n",
            "  4.53460433e-02 -2.10776112e-02 -3.99929933e-02 -5.16567284e-02\n",
            " -1.02668931e-01  5.79755299e-05 -3.10406870e-02 -1.36124482e-01\n",
            " -1.10019597e-02 -3.72550118e-02 -9.02260519e-02 -1.53809055e-02\n",
            "  9.27676372e-02 -4.45556034e-03  7.61418372e-02 -1.51872577e-02\n",
            " -5.69330710e-02  1.59956579e-02  6.15549118e-02  8.92818359e-02\n",
            " -3.85928548e-02  2.03692463e-02  2.75244357e-02  4.27679044e-02\n",
            " -3.61364709e-02  1.62488395e-01  8.02736511e-02 -2.22769451e-02\n",
            " -1.83970123e-02 -1.00943904e-01  2.33173340e-02  2.57999878e-02\n",
            " -5.25722880e-02 -1.00751361e-01  1.67522410e-03 -1.38264433e-01\n",
            "  4.41569240e-02  3.53596277e-02  3.53445899e-02  1.02442527e-01\n",
            " -6.31288814e-02 -1.30529915e-02 -9.13466125e-04  2.85370238e-02\n",
            "  4.73289786e-02  9.14200053e-02  1.04250027e-01  1.40869755e-02\n",
            " -9.06483410e-02  2.49296559e-02  8.45261010e-02 -6.70128132e-02\n",
            " -1.00048244e-01 -1.03491504e-01 -1.10248802e-01  9.34250705e-02\n",
            "  2.57538539e-03  1.30321438e-01 -1.95969708e-01  4.68754516e-02\n",
            "  3.79116423e-02  1.48786801e-01  3.77399441e-02  7.84313909e-02\n",
            "  6.37476638e-02 -2.27985788e-02 -5.85229101e-02  1.09421503e-02\n",
            " -1.66320396e-02 -1.14417754e-01 -2.54665839e-01 -2.71121439e-03\n",
            " -3.39833500e-02  1.08059338e-01 -6.06552154e-02 -2.92300377e-02\n",
            "  1.84598553e-01 -8.82724250e-04 -6.90282342e-02  1.65152598e-01\n",
            " -1.51516226e-03  7.92821630e-02  1.96957562e-01 -3.62705360e-03\n",
            "  9.89605798e-02  1.15822416e-01 -1.04653856e-01  1.40418367e-02\n",
            " -1.61180264e-02 -1.52335931e-02 -1.62149495e-01 -4.57004353e-02\n",
            " -8.23655620e-02 -1.98378860e-02  1.85473467e-01  5.79056629e-02\n",
            "  3.35215651e-02  5.46922523e-02  1.53793778e-01  6.09615044e-02\n",
            "  6.43634545e-02  4.63265918e-02 -7.92517402e-03 -6.57537591e-02\n",
            "  2.06381954e-02  1.56459231e-02  1.19172954e-01 -5.10509687e-02\n",
            "  8.32552004e-02 -6.69063584e-02 -1.30408782e-01 -2.58653258e-02\n",
            " -8.76672221e-02  1.09377167e-01  5.75826408e-02 -2.96739664e-02\n",
            "  8.09707611e-02 -4.35653774e-03  3.48586792e-02 -5.76928565e-02\n",
            " -5.76928565e-02 -5.28083978e-02  1.05889671e-02 -1.22217243e-01\n",
            " -3.71466588e-02 -1.58466754e-03  4.21447543e-02 -9.26833140e-03\n",
            " -2.37148716e-02 -9.38749747e-02 -6.72685411e-03 -1.39145708e-01\n",
            " -1.05326700e-01 -6.76935910e-03  1.42505512e-01  2.12474479e-02\n",
            "  1.58542985e-02  1.93475878e-03  2.35838589e-02  5.22137715e-02\n",
            "  1.04635291e-02  1.33738826e-01  6.40622005e-02  7.47641798e-02\n",
            "  5.53347250e-02 -5.42599666e-02 -4.87038172e-02 -1.91220346e-02\n",
            "  5.91187441e-03  4.76364502e-03  3.95855830e-02  1.05365730e-01\n",
            " -6.35841936e-03 -2.41193048e-02  1.07100575e-01 -6.54167609e-02\n",
            "  1.01011909e-01  5.06818630e-02 -2.88482687e-02  5.83380671e-02\n",
            "  1.39808864e-01  4.93115854e-02  1.64235110e-02  5.70335764e-02\n",
            "  8.98516845e-02  1.19885173e-01  1.69536025e-03  1.23494543e-01\n",
            "  1.42855943e-01 -4.94363087e-02  4.15007898e-03  2.77992818e-03\n",
            "  1.24591296e-02  1.02987999e-01 -2.13373207e-02 -2.21138324e-01\n",
            " -1.11384483e-01  5.31977071e-02  8.00037129e-02 -4.55469376e-03\n",
            "  1.40469995e-02  1.54238785e-02 -6.96090410e-02 -4.02040750e-02\n",
            " -4.17538383e-02 -2.53765760e-02  1.00456701e-01  8.75713424e-02\n",
            "  1.08673576e-02  1.68577168e-01 -1.29290706e-02  7.58194553e-02\n",
            "  4.81308631e-02  2.20176453e-03  1.22753221e-01 -6.13539110e-02\n",
            " -8.36712500e-02  1.46598397e-04 -1.15184268e-02  1.32674670e-01\n",
            " -1.05228300e-01  3.94505660e-02  7.25813241e-04  4.86393986e-02\n",
            "  1.55014732e-02  9.35279534e-02  5.55653505e-02 -5.17576754e-02\n",
            " -8.55404933e-02  6.14826252e-02 -6.33924748e-03 -6.49064852e-02\n",
            " -8.88420216e-02 -1.35596005e-02  1.00108174e-01 -1.25206956e-02\n",
            " -7.20822947e-02 -1.90244756e-02  1.29174339e-01 -2.64546233e-02\n",
            "  2.70761897e-02  1.57934002e-01 -9.34083544e-02  6.28911282e-02\n",
            "  1.46738405e-01  1.48427904e-01  3.62662648e-03  1.67744856e-01\n",
            " -1.95878865e-02  2.25340468e-02 -5.22009141e-02  8.59581726e-04\n",
            "  2.88811907e-02  2.47716138e-02  3.98019176e-02 -1.26777927e-01\n",
            "  4.18469169e-02  2.85973684e-02 -1.31489182e-01 -4.08452450e-02\n",
            " -1.67114365e-01 -2.60124435e-02  1.10563240e-01 -6.49388634e-02\n",
            " -1.05988288e-01  1.66957350e-01  7.09127231e-02  6.89532573e-02\n",
            "  5.16196619e-02 -5.84930348e-02  2.56773127e-03  1.95920687e-02\n",
            "  1.71870511e-02 -5.08816574e-02  2.97049543e-02  9.40230655e-02\n",
            "  1.32724857e-01 -5.12441165e-02  1.83556239e-01 -5.63180427e-02\n",
            "  3.71665015e-02  8.90714918e-02  8.90714918e-02 -1.10183477e-01\n",
            " -1.12810375e-01 -4.94518645e-02  1.40472704e-01  6.96762042e-02\n",
            "  9.80226433e-02  1.26657673e-02  8.33672264e-02  1.24114899e-01\n",
            " -4.80868840e-02 -7.27999471e-03  3.17293408e-02 -2.35788632e-03\n",
            "  1.25600514e-01 -3.54571284e-03  4.36701020e-02  1.23080909e-02]  - intercept :  0.3029091512337615\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.30340058213188004\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:24,668]\u001b[0m Trial 183 finished with value: 0.12721575238616672 and parameters: {'count_threshold': 10, 'postag': True, 'voc_threshold': 9756}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.         0.0097222  0.         ... 0.         0.31757046 0.06657948]\n",
            " [0.43238326 0.02117109 0.57340354 ... 1.12501981 0.00956034 0.00701927]\n",
            " [0.03562248 0.09011782 0.07788011 ... 0.12715418 0.03824138 0.01138516]\n",
            " ...\n",
            " [0.03250346 0.09182788 0.02031655 ... 0.1340117  0.21216116 0.02632763]\n",
            " [0.         0.07330242 0.         ... 0.         0.04780172 0.06805951]\n",
            " [0.00988153 0.05297025 0.02200286 ... 0.04238473 0.19040161 0.04041835]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-1.82019821e-02 -3.37617596e-03 -2.63106518e-02 -2.19007837e-03\n",
            "  1.05757672e-02  9.77202193e-03 -3.72547795e-02 -4.31449521e-02\n",
            " -4.74335077e-03  6.66644302e-03 -2.02770150e-02 -3.96145856e-02\n",
            "  5.29215810e-02 -3.75273607e-02 -4.02782732e-02  3.71065948e-02\n",
            " -3.73058120e-02 -3.34814451e-02 -4.33762550e-02  3.57650794e-02\n",
            "  1.87042259e-02 -9.50782371e-02 -2.89973946e-02 -3.52729138e-02\n",
            "  5.69269932e-02 -1.82618499e-02  2.54538406e-02 -1.22888583e-02\n",
            "  8.97905888e-04 -1.31502487e-02 -1.18922612e-01  8.31757086e-02\n",
            " -5.54824241e-02 -2.34988992e-02  1.12246241e-02 -4.97074141e-02\n",
            "  5.20397840e-02  4.94242752e-02 -2.31451083e-02 -4.50980692e-02\n",
            " -4.21688773e-02  3.79768660e-02 -6.89425816e-02  6.43284616e-02\n",
            "  1.91986268e-03  2.74098243e-02  7.12626499e-02  1.61689092e-02\n",
            "  1.72224556e-02 -8.61619623e-02  4.03806397e-02 -1.66298562e-02\n",
            " -1.66464143e-02 -4.07167055e-03 -8.08549468e-02  5.30441817e-02\n",
            " -6.56056697e-02 -2.15311272e-02  7.46922257e-02  5.37954235e-02\n",
            "  2.86525852e-02 -7.23533806e-02  3.97320762e-02  8.39233867e-03\n",
            "  6.09325196e-03  5.46924631e-03  9.00802166e-02  3.29778341e-02\n",
            " -1.47451958e-01 -3.03161264e-03 -8.10652119e-02 -2.80555989e-02\n",
            "  6.72277202e-02 -1.31640619e-02  9.97769793e-03 -3.23582871e-02\n",
            "  4.77305808e-02  2.12579917e-02 -7.17671634e-02 -4.38941708e-02\n",
            " -1.45754833e-01  5.25811555e-02 -5.59120927e-02  8.59313621e-02\n",
            "  1.14102601e-01 -1.48372360e-03 -3.57507950e-02 -1.04439857e-01\n",
            "  2.66383746e-02  3.22724530e-02 -1.52731027e-01  2.75823370e-02\n",
            " -3.98936516e-02  4.08726050e-02 -1.44483146e-02  5.46369216e-02\n",
            " -6.85024732e-02 -7.27295335e-02 -1.11242629e-02 -6.96965619e-02\n",
            "  3.57889404e-02  2.31166546e-02 -1.00271978e-01 -6.83076864e-02\n",
            "  4.59987692e-02  5.87919860e-02  1.69738596e-02  8.18583400e-02\n",
            " -1.34691365e-02 -9.52768146e-02 -3.35240342e-03 -3.70736183e-03\n",
            " -7.02164791e-02  1.49726657e-02 -1.02448727e-01 -2.83599833e-03\n",
            " -1.48725432e-02 -8.72594662e-02 -7.33909421e-02  1.02763114e-02\n",
            "  2.12396799e-02  1.05905252e-01 -8.58901128e-02 -3.62911310e-02\n",
            "  6.52906496e-03  1.17503167e-02  4.13480148e-02  3.74921474e-02\n",
            " -3.03276110e-02 -7.50966041e-02 -6.50103229e-02 -1.64618304e-01\n",
            "  2.83151089e-02 -1.62277711e-02  4.89051795e-03  1.46706660e-02\n",
            " -7.52132694e-02 -6.01017417e-02  8.25901050e-02  1.76434395e-01\n",
            " -2.23320127e-02  2.70486908e-02  9.93602922e-02 -1.75573433e-03\n",
            " -7.05316290e-02 -8.46854469e-03  7.02181445e-03 -1.53650402e-02\n",
            "  3.84702553e-04 -1.05959724e-02  2.01600104e-02 -4.99575208e-02\n",
            "  5.10416587e-02 -7.97860893e-03 -1.35640949e-02  6.48236566e-04\n",
            "  2.85380541e-03 -4.41774979e-02 -4.62063243e-02  1.56544635e-02\n",
            " -5.91492232e-02 -1.47428758e-02 -1.80103512e-02  4.98149994e-03\n",
            " -2.36199136e-02 -2.85984149e-03  4.89517785e-03 -3.80073151e-02\n",
            " -1.88901522e-02 -1.47690043e-02  3.54221538e-02  5.35425900e-02\n",
            "  5.53239789e-03  9.14916981e-02 -1.37981271e-04 -2.58492801e-02\n",
            " -7.84705671e-03 -1.31431272e-01  8.24316611e-02 -9.64573348e-02\n",
            "  1.78951054e-02 -9.43904562e-04 -9.43744700e-02  9.51895257e-02\n",
            " -3.96847641e-02 -8.19076784e-02 -2.95928382e-02 -4.08276485e-02\n",
            "  1.38843607e-02 -4.87467144e-03 -1.52042712e-02  4.01945858e-02\n",
            " -4.04613047e-02  5.99492924e-02 -8.03716442e-02  9.48999329e-02\n",
            " -1.95921365e-01 -2.74689268e-02  2.32068977e-02 -8.54703623e-02\n",
            "  3.59604106e-02  1.38998426e-02  2.62876653e-02 -5.17439658e-03\n",
            "  3.31583369e-02 -3.41671189e-02  4.86960659e-04 -2.62018860e-02\n",
            " -6.24774863e-02 -7.30693529e-03 -7.08155021e-03 -5.84003783e-02\n",
            " -4.79576020e-02 -6.67658462e-02  2.07079045e-02 -2.42495359e-02\n",
            " -1.12461238e-01 -3.96762469e-02  4.81234321e-02 -2.60355184e-02\n",
            " -3.95125955e-02  1.86565950e-02  1.79733724e-02  1.08339114e-01\n",
            "  1.18953855e-01 -2.49391767e-02  1.15061880e-01 -1.08142801e-02\n",
            "  5.46805506e-02 -4.59777806e-02 -1.59199838e-02  1.99120216e-02\n",
            " -2.88683688e-02 -5.24842668e-02 -2.33099211e-02  1.42904646e-02\n",
            " -3.05504916e-02 -2.07040076e-02 -1.48174157e-02 -2.44190908e-02\n",
            "  5.86264502e-03 -1.64505009e-01 -2.09413418e-02  3.04474391e-03\n",
            "  2.49124340e-02  3.22269428e-02 -3.80713971e-02  1.92636034e-02\n",
            "  2.15026700e-02 -4.73075088e-02 -5.85724234e-02 -2.44729727e-02\n",
            "  5.09058728e-02 -1.40740250e-02 -2.90996512e-02  3.04687784e-02\n",
            " -2.03899568e-02  2.22039241e-02 -4.63982341e-03  1.27033796e-02\n",
            " -1.13238916e-01  5.15164118e-02 -3.69017162e-02 -8.54354450e-02\n",
            " -3.52027017e-03 -3.31493032e-02  3.77534217e-02 -8.00613358e-02\n",
            " -6.70405783e-02 -3.13593573e-02 -5.75349431e-03  4.73626566e-02\n",
            " -8.67342880e-02  1.95239376e-02 -4.37471533e-02 -5.03025829e-02\n",
            "  2.67874501e-02 -1.92128890e-02 -1.45726269e-01  7.26006162e-02\n",
            " -8.25940194e-02 -1.28754984e-01 -1.03395973e-01  3.00558015e-04\n",
            " -3.77323772e-02 -6.05127493e-03  1.10299889e-01 -6.56197502e-02\n",
            "  1.16343194e-01  3.38346215e-03  1.48846024e-01 -9.01378642e-02\n",
            " -1.56697663e-01  4.49634658e-03]  - intercept :  0.9697247642469387\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.12721575238616672\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:25,234]\u001b[0m Trial 184 finished with value: -0.23820544917729863 and parameters: {'count_threshold': 10, 'postag': True, 'voc_threshold': 2572}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.00000000e+00 1.30126131e-02 0.00000000e+00 ... 0.00000000e+00\n",
            "  3.32077419e-01 3.89321576e-02]\n",
            " [3.98962044e-05 1.59043049e-02 3.88374486e-02 ... 0.00000000e+00\n",
            "  3.19148445e-01 3.18961303e-02]\n",
            " [0.00000000e+00 1.53362940e-02 0.00000000e+00 ... 0.00000000e+00\n",
            "  1.56620372e-01 4.76463267e-02]\n",
            " ...\n",
            " [1.72133370e-02 5.60724809e-02 4.46621852e-02 ... 1.30805467e-01\n",
            "  1.90847776e-01 2.02663960e-02]\n",
            " [3.43945594e-03 6.96917472e-02 2.48786814e-02 ... 4.14280667e-02\n",
            "  1.62991339e-01 2.97042838e-02]\n",
            " [4.48832300e-05 4.64880574e-02 4.36921296e-02 ... 0.00000000e+00\n",
            "  1.27853365e-01 2.08880139e-02]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-4.29023839e-02 -2.03375761e-02 -1.15574479e-01 -2.59083131e-02\n",
            "  6.72790160e-02 -1.14264080e-02 -9.21917129e-02  8.64720003e-03\n",
            "  3.59177340e-02 -6.74242584e-02 -3.45843067e-02 -3.26049882e-02\n",
            "  6.76440228e-02  1.14311229e-02 -1.54490569e-02 -2.29636438e-02\n",
            " -5.18299446e-02  3.19806387e-02 -1.09099033e-02  8.24175555e-03\n",
            "  7.88159267e-03  2.76545760e-02 -6.66221775e-02 -7.42249449e-02\n",
            " -6.69531399e-04  1.01330620e-01  4.00850530e-02 -5.53127783e-02\n",
            " -7.85809533e-02  1.22790111e-02  6.81550013e-02 -5.00795684e-02\n",
            "  5.76519276e-02 -3.22504881e-02 -2.31178382e-02  4.26722964e-02\n",
            "  3.40094387e-02 -1.05221649e-02  1.69180788e-02 -6.20219310e-02\n",
            "  1.06679592e-01 -3.09240006e-02  9.81163132e-02  1.94963118e-02\n",
            " -6.73452574e-02 -3.32313943e-03 -4.09578950e-02  5.39333314e-02\n",
            " -1.38147241e-01  1.12757586e-01  9.49996202e-02  4.16245179e-02\n",
            "  3.04227894e-02 -1.74782773e-01  3.03446523e-02  1.23481874e-01\n",
            "  1.17456426e-01 -1.97756860e-02  7.91632224e-02 -2.06907975e-02\n",
            "  4.32763904e-02  7.59060392e-02  1.13259966e-01  8.49032072e-02\n",
            " -6.96070882e-02  1.27665974e-01 -4.03438499e-02  3.19070525e-02\n",
            " -1.00744392e-01  2.60164399e-02 -9.28345858e-03  5.89755598e-03\n",
            " -2.14096000e-01 -2.68617938e-02 -4.89352000e-03  7.25422939e-02\n",
            " -6.70446429e-02 -1.90311753e-03 -8.43158806e-02  1.24111271e-02\n",
            " -1.20121610e-01 -7.66067069e-02  6.77509945e-02 -9.55866174e-04\n",
            " -9.06914707e-02 -9.10901662e-02  2.01597857e-02 -2.46287996e-02\n",
            "  3.01439044e-02  3.12976878e-02 -7.77508114e-02  5.40507603e-02\n",
            "  1.05265686e-01 -5.75627276e-02  2.38292723e-02  1.49066580e-01\n",
            "  7.25042904e-03  6.55367215e-02  1.08224426e-01 -5.85727947e-02\n",
            " -8.49824872e-02  6.29769916e-02  1.02233010e-01  1.89298791e-02\n",
            " -7.49647489e-02 -4.46353481e-02  7.45947081e-02  1.64586349e-01\n",
            " -1.37966968e-01 -1.28311112e-01  2.32636157e-02  9.73212481e-02\n",
            "  2.63926238e-02  3.54528416e-02  6.82106233e-02 -1.28860091e-03\n",
            "  3.55579812e-02 -8.12801661e-02  3.00870205e-02  1.66000621e-02\n",
            "  3.71978880e-02  1.00371576e-01  3.90753455e-02  1.09902166e-01\n",
            "  1.64427620e-01  8.16781351e-02  1.51627493e-02  1.07613839e-01\n",
            "  9.13341713e-03 -1.30502128e-02 -2.77439326e-02 -5.72692834e-02\n",
            "  9.34714364e-02 -1.14856464e-03  2.68362972e-02 -1.63241676e-01\n",
            "  2.93319294e-02 -8.12725481e-02  1.18824184e-01  5.30658843e-02\n",
            " -1.32953852e-02  3.73668908e-02 -1.46227506e-02 -1.15815327e-02\n",
            "  3.08288785e-02 -4.99222743e-02  4.31615344e-02 -7.22516240e-02\n",
            " -5.47375303e-02  3.61130985e-02 -5.34489609e-02 -1.87392960e-02\n",
            " -8.50693463e-02  9.62468710e-04 -9.07572609e-02  5.17837307e-02\n",
            " -1.57324580e-01 -3.38394307e-02 -5.46835107e-02 -1.05545613e-01\n",
            " -9.23838041e-02 -3.29085019e-02 -3.09372421e-02 -9.16050872e-02\n",
            " -7.85483665e-02 -9.72629205e-03  5.49236138e-02 -7.38898805e-02\n",
            " -3.23974314e-02 -5.60874790e-02 -5.05718903e-02  6.23581259e-02\n",
            "  2.88179050e-02 -2.45693944e-03  5.78889920e-03  5.59519714e-02\n",
            " -1.28225956e-02 -7.61991939e-02  1.58402435e-02 -1.63160480e-01\n",
            "  1.19370064e-01 -1.02955846e-01 -4.34417517e-02 -5.82480510e-02\n",
            "  4.20961392e-02 -1.63798306e-02  2.69243718e-02 -8.06402113e-02\n",
            "  4.77108809e-02  2.07391919e-02  2.72145988e-02  2.85559297e-02\n",
            " -5.85612453e-02 -8.36531892e-03  7.41900323e-02 -3.34252035e-03\n",
            "  1.00452041e-01  9.74509063e-02 -4.80038021e-03  3.54306982e-04\n",
            " -1.45957451e-01 -1.22444622e-01 -4.18150275e-03  5.79414075e-02\n",
            " -1.34176053e-01 -1.02007690e-01  1.96911536e-02  1.29312486e-01\n",
            " -1.43103712e-01  2.72640144e-02 -1.54496832e-01 -5.58812773e-02\n",
            "  5.99594981e-02  7.41260072e-03  5.39863433e-03 -4.03737428e-01\n",
            " -1.38007528e-03 -2.39070637e-01  4.21470316e-02 -1.04695215e-03\n",
            " -1.68755316e-01 -6.03676966e-02  3.40903224e-02  2.37151121e-02\n",
            " -1.85188093e-02  4.41299550e-02 -1.33807299e-01 -9.69916235e-02\n",
            " -7.63484977e-02  6.62813885e-02  1.01638133e-01 -8.17513998e-02\n",
            "  1.06297720e-01 -4.26611904e-02 -3.13515244e-02  1.07596209e-01\n",
            "  1.56137317e-02  1.45983795e-01  7.75650769e-02 -1.45136324e-02\n",
            " -8.19509448e-02 -8.53587475e-02 -3.57423616e-01 -1.53445608e-01\n",
            " -1.24931856e-01  1.12614078e-01 -1.32365207e-01 -1.20591386e-01\n",
            " -9.72980458e-02  6.70022041e-02 -3.75023447e-02  2.38976316e-02\n",
            " -6.79881421e-02  3.34577715e-02  1.18875277e-02  1.08157342e-01\n",
            " -2.06244208e-01 -2.94828256e-02 -1.16355227e-01  5.91075473e-02\n",
            " -1.20826944e-02  3.88307505e-02  4.17752528e-02  1.01649612e-02\n",
            " -8.74155622e-02  1.46568592e-01  1.00068312e-01 -1.63795746e-01\n",
            "  1.43173593e-01 -9.11991455e-02  1.13502003e-01 -1.50447994e-01\n",
            "  4.39592566e-02  1.80464598e-03 -6.81009185e-02 -1.27551633e-02\n",
            "  4.75030738e-02  5.67721753e-02 -6.08094869e-02 -1.48630008e-01\n",
            " -3.91417050e-03  6.51775701e-02  2.15448902e-01  6.89383306e-02\n",
            " -2.29982903e-02 -3.05336567e-02]  - intercept :  0.9182552224899647\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.23820544917729863\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:25,813]\u001b[0m Trial 185 finished with value: -0.02035453338834466 and parameters: {'count_threshold': 10, 'postag': True, 'voc_threshold': 8915}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.06972511 0.04566812 0.0505932  ... 0.03328595 0.06212527 0.04208245]\n",
            " [0.08644005 0.08471107 0.15374376 ... 0.07308823 0.12553237 0.0229231 ]\n",
            " [0.         0.03918143 0.         ... 0.         0.07010572 0.03608721]\n",
            " ...\n",
            " [0.16908188 0.02645528 0.08394761 ... 0.04234321 0.1129586  0.02788978]\n",
            " [0.04383357 0.11002535 0.01541623 ... 0.1365524  0.20189485 0.03527741]\n",
            " [0.07464328 0.04990279 0.10175705 ... 0.         0.04673715 0.02487317]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-9.54083194e-03 -1.93211629e-02  4.74316678e-02  2.85971818e-02\n",
            " -2.08890672e-02  2.51118202e-02  3.25916930e-02  1.90318155e-02\n",
            " -5.14633717e-03 -6.28289523e-03  1.01489641e-02 -5.39635721e-03\n",
            "  5.06750089e-02 -6.21486508e-02  3.53605171e-03 -2.73699583e-02\n",
            "  1.71971716e-02 -1.15015490e-02  6.12433957e-03  1.30380838e-02\n",
            " -4.82707920e-02 -5.22595537e-02  4.36871654e-02 -2.23515953e-02\n",
            "  2.01062047e-02 -1.39239575e-02 -1.21204239e-02  8.43069205e-02\n",
            "  3.87277177e-02  2.47418851e-02 -3.23757398e-02  1.85266297e-02\n",
            " -9.62561831e-03 -5.41237899e-03 -2.51602618e-03  1.53540996e-02\n",
            " -9.16455829e-03  4.36259658e-03 -3.05245903e-02 -2.66906263e-02\n",
            " -3.72361910e-02 -3.58984241e-02 -3.06162989e-02 -1.70594217e-02\n",
            "  5.40751098e-02 -1.63331596e-02  3.05614547e-02 -1.08542698e-02\n",
            " -4.47576282e-02 -1.71625492e-02 -2.95887324e-02 -1.51906155e-02\n",
            " -1.82413049e-02  1.47291892e-02 -4.86269002e-02 -6.79554978e-03\n",
            " -2.01689345e-02  1.76446682e-02  5.94705453e-02  4.78396959e-02\n",
            "  8.70834996e-02  7.46329241e-02 -5.35237093e-02 -4.09334744e-02\n",
            "  1.06550462e-02  3.11347179e-02  3.68326775e-02  5.21531354e-02\n",
            "  3.22986554e-02 -1.49639119e-02 -4.43692534e-02 -3.32780090e-02\n",
            "  2.31219910e-02  1.47897075e-02 -2.33715398e-02 -2.91265882e-02\n",
            "  3.63144753e-02  2.71199632e-02  7.13548343e-03 -4.09178369e-02\n",
            " -3.87820535e-02 -5.78308116e-02 -1.06997868e-01 -8.12670445e-03\n",
            "  3.01217439e-02 -7.10612808e-03 -1.76159656e-02 -1.46162173e-02\n",
            "  3.50887728e-02 -3.97663104e-02  2.47039327e-02  1.55408765e-02\n",
            " -4.67255038e-02  1.55843288e-02  4.39298733e-02  1.51206848e-02\n",
            " -3.81340144e-02  1.02045607e-02  7.50156398e-02  5.04556699e-03\n",
            "  1.24405905e-02 -6.61896548e-02 -9.46058488e-02  7.80766292e-02\n",
            " -4.63618776e-02 -3.94991851e-03 -3.17524930e-02 -4.30082105e-03\n",
            "  3.57477021e-02 -3.74701964e-02  2.59163958e-02  1.11118448e-02\n",
            "  2.56668971e-02 -5.47444997e-02  3.71668336e-02  4.11396351e-02\n",
            " -1.22890746e-02  2.37510215e-02  1.14290973e-01  3.91873566e-02\n",
            "  1.74562239e-02  4.76305128e-02 -9.13402217e-03 -4.44347424e-02\n",
            " -2.49029040e-02 -2.06179784e-03  1.85921548e-02 -2.16911800e-02\n",
            "  5.66723313e-02 -2.90043582e-02 -2.27202338e-02  4.79227137e-03\n",
            " -1.50180093e-02 -1.52252893e-02 -2.66646375e-02  9.16029694e-03\n",
            "  2.13125543e-02  4.42346302e-02  6.82653021e-02  6.92347593e-03\n",
            " -8.01395453e-03  1.16979481e-02  4.64406930e-02 -4.31968342e-02\n",
            " -1.53767816e-02  2.98965012e-02  3.11466689e-02  1.04859367e-04\n",
            "  1.92600399e-02  2.30005211e-02  3.08114327e-02  5.45875347e-03\n",
            " -6.39024805e-03 -8.70974194e-03  4.81617680e-03  1.84533019e-03\n",
            "  6.08132481e-02  2.70629657e-02  1.46344903e-02  3.59792972e-02\n",
            "  3.92207231e-02  9.53852737e-02  9.06635327e-02  3.53164215e-02\n",
            " -2.03925380e-02  2.57697738e-02 -7.33801409e-02 -6.87918133e-02\n",
            "  3.70006458e-02  8.11928256e-02 -3.62763901e-02  1.60506852e-02\n",
            " -9.07354349e-02  4.77293592e-02 -2.40756061e-02 -2.24015341e-02\n",
            "  3.90273440e-02  2.47745641e-02 -7.36918161e-02  7.29387746e-02\n",
            "  1.38356055e-02  3.98408512e-02  6.04228072e-02  4.90621727e-02\n",
            " -1.15418420e-01 -1.45884167e-03  6.70398074e-04 -2.20389305e-02\n",
            "  4.87615795e-02  1.78533862e-02 -9.52453824e-02 -7.83629962e-02\n",
            "  7.10804340e-02 -4.43181205e-02  4.19695670e-02  3.78155448e-02\n",
            " -5.53394185e-02 -1.56071440e-04 -5.88268600e-02  5.63276162e-02\n",
            "  2.06534465e-02 -2.17500256e-01  2.24949422e-03  1.17789350e-02\n",
            " -1.40966801e-01 -1.28748365e-01 -8.38037981e-02  1.80110703e-02\n",
            " -9.91323989e-03 -2.24155560e-02  1.29136379e-02  1.36789416e-02\n",
            "  7.34014274e-02  9.35120004e-02  1.63933205e-05 -7.21340236e-02\n",
            "  8.42753008e-02 -4.25893733e-03  8.21928501e-02  2.67796553e-02\n",
            "  6.30842054e-02 -2.51298970e-02  3.98150362e-02  7.78068867e-03\n",
            "  7.58067396e-02 -3.36624056e-02 -1.05924528e-01 -7.20855676e-02\n",
            " -7.23386272e-02  2.30846698e-02  1.61674202e-02  5.65436693e-02\n",
            "  1.10617011e-02  2.58880637e-02 -1.59281459e-01 -1.16416603e-02\n",
            " -1.14705498e-01 -1.77929863e-02  4.68009088e-02 -6.17244373e-02\n",
            " -6.13241485e-02 -3.64359315e-02 -3.99836182e-02  5.26557722e-02\n",
            " -5.26082763e-02  8.34293958e-03 -9.57710892e-04 -1.49819342e-02\n",
            " -4.09561820e-02  7.32483974e-02 -4.18765864e-02 -4.43331630e-02\n",
            " -3.13441146e-02 -4.55801383e-02  6.34050027e-02  1.84768737e-02\n",
            " -6.20274605e-03 -1.10357827e-02 -7.54855762e-02  6.10358532e-02\n",
            " -5.79172841e-03 -9.92713929e-04 -3.85128363e-02 -1.34956785e-01\n",
            "  1.52982606e-02 -6.99046715e-02  2.22335566e-02  2.23079490e-02\n",
            "  5.15223763e-02 -4.02445710e-02  1.00406940e-01 -5.71794504e-02\n",
            "  1.73402010e-02  4.86240455e-02 -1.31725808e-01  4.29741185e-02\n",
            " -1.10064194e-01  6.23744713e-02 -1.80692794e-01 -6.63403746e-03]  - intercept :  0.7579169706559367\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.02035453338834466\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:26,321]\u001b[0m Trial 186 finished with value: 0.04649878942654413 and parameters: {'count_threshold': 10, 'postag': True, 'voc_threshold': 10000}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.69801848 0.6115595  0.32648683 ... 0.18344482 0.08081096 0.00613775]\n",
            " [0.         0.01100358 0.09427249 ... 0.12064968 0.18846696 0.02027012]\n",
            " [0.23565138 0.2567809  0.05045263 ... 0.14680148 0.03688368 0.00954506]\n",
            " ...\n",
            " [0.51189013 0.65196589 0.02276138 ... 0.11333621 0.02074707 0.00150698]\n",
            " [0.00627793 0.00293429 0.04854643 ... 0.09331256 0.19090595 0.03204465]\n",
            " [0.         0.         0.03488541 ... 0.03417576 0.1244824  0.06630107]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 0.00840838  0.02564514 -0.12412827  0.01858118  0.03996151 -0.00478899\n",
            "  0.06501002  0.0461284   0.11759425 -0.01392446 -0.00520489 -0.25158687\n",
            " -0.13243184  0.2587381  -0.03865134 -0.13100244 -0.06209709 -0.11697804\n",
            "  0.02565569 -0.08310859 -0.06628672  0.15497076 -0.16186661 -0.20175446\n",
            "  0.18640072  0.11710301 -0.18040895 -0.0893139  -0.13778473  0.01011292\n",
            "  0.11277181 -0.08332741 -0.59307491  0.04517297  0.07069568 -0.2735446\n",
            "  0.08702911 -0.38112337 -0.03196018  0.22126844 -0.04569222 -0.22730127\n",
            "  0.17485313 -0.01100336 -0.10071384 -0.0762888  -0.13250926 -0.04452589\n",
            " -0.05590726  0.00210005  0.12539998  0.08657977  0.53681827 -0.15286981\n",
            " -0.15928128 -0.02423318  0.15132462 -0.09737168 -0.39664964 -0.25051428\n",
            "  0.10892868 -0.06725709  0.05738669  0.04517483  0.49669028  0.11795756\n",
            "  0.10677155 -0.41065944  0.05917927 -0.39552204  0.08259212  0.13649459\n",
            " -0.17651383 -0.38824212  0.2514162  -0.37527076 -0.23032758 -0.11228769\n",
            "  0.22612504 -0.31914148 -0.11633709 -0.02657712 -0.12470509  0.40098575\n",
            " -0.05432841 -0.46917575 -0.04786242  0.12900299 -0.33656919 -0.19707679\n",
            "  0.03310008 -0.04514699 -0.38412136 -0.49671019 -0.23579946 -0.11590346\n",
            "  0.16690574 -0.15576641 -0.19144204  0.21058215  0.06651749  0.00790194\n",
            " -0.28577017 -0.03164648  0.4125531  -0.29605627  0.10971531  0.19694464\n",
            " -0.00837405 -0.26094135  0.38458288  0.10922142 -0.4888119  -0.14545707\n",
            " -0.0305494  -0.19017468  0.17003055  0.29808343  0.20867042  0.0465415\n",
            "  0.13975243 -0.55575932  0.2856399   0.2169545   0.20522    -0.14799626\n",
            "  0.08529186 -0.12359426 -0.39471211  0.23654691 -0.50586513  0.23443618\n",
            " -0.16644987  0.01571303  0.0134898  -0.13348761 -0.04876403  0.1373254\n",
            " -0.41121928  0.13091925 -0.27493506  0.05915598  0.07740787  0.2197486\n",
            " -0.89000297 -0.06129206  0.07352828 -0.66557116 -0.09773039  0.01836326\n",
            " -0.05199312  0.01755414  0.06494356  0.24055505  0.14262168  0.10239917\n",
            " -0.11512036  0.14648265  0.05897086  0.03506122  0.17574167 -0.22304597\n",
            "  0.13895598  0.02249786 -0.00222326  0.05714607 -0.01547106 -0.00778196\n",
            "  0.0217072   0.16993153  0.22556688 -0.19327213 -0.07591575 -0.07478832\n",
            "  0.36240085  0.09385972 -0.03943134  0.03165685 -0.0431097   0.04483929\n",
            " -0.02028888 -0.59142758  0.05454789 -0.28499383 -0.4890967  -0.3307179\n",
            "  0.01259288 -0.17172355 -0.13708431 -0.89862967 -0.32780803 -0.16827944\n",
            " -0.0345643  -0.24867486 -0.23216537 -0.16221844  0.03746951 -0.03081972\n",
            "  0.05302684 -0.10982343 -0.31750683  0.25482272 -0.38939267  0.11454012\n",
            "  0.19092706  0.04768835 -0.09340322 -0.07431061 -0.1118134  -0.66156522\n",
            "  0.11304436 -0.28103851 -0.14278994  0.03683173 -0.19738274 -0.1094212\n",
            " -0.51476467 -0.15221799 -0.07791582  0.00855337 -0.01809489  0.11169506\n",
            " -0.34702872 -0.59706771 -0.44196012  0.15404082  0.24683411 -0.43420065\n",
            " -0.03057498 -0.14461544  0.38109271  0.09605884  0.15000945  0.06754882\n",
            " -0.00590106 -0.06106595 -0.22473888  0.04794207 -0.11537348 -0.18127192\n",
            " -0.04036557 -0.51433159 -0.6155964   0.02072716 -0.04119136 -0.04030817\n",
            " -0.37543091 -0.01020486 -0.08638648  0.20370004 -0.81216191 -0.59160913\n",
            " -0.34195626 -0.13313986 -0.32170057  0.0906147  -0.42331621 -0.56875263\n",
            " -0.53482947 -0.07726578 -0.27033654  0.00731729  0.13445338 -0.37802312\n",
            " -0.28699183 -0.35930809  0.1210471  -0.16047899  0.07063245 -0.44940889\n",
            " -0.36537906  0.14534217  0.35053199 -0.65681262 -0.51306686 -0.49715782\n",
            "  0.10580898 -0.21319121 -0.33241097  0.16213855 -0.00849953  0.24055421\n",
            "  0.07982111 -0.27180919 -0.11637827 -0.00909086 -0.17207546  0.27422481\n",
            "  0.22262216 -0.59573419  0.16721829 -0.62039997 -0.45605516 -0.35688792\n",
            " -0.09531098 -0.09867242 -0.27442619 -0.11137278]  - intercept :  3.3320442037440396\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.04649878942654413\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:26,902]\u001b[0m Trial 187 finished with value: -0.2438368506038 and parameters: {'count_threshold': 9, 'postag': True, 'voc_threshold': 9535}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.00407554 0.04898534 0.0501131  ... 0.01137084 0.13687978 0.04539859]\n",
            " [0.05990254 0.08774181 0.0829876  ... 0.07798871 0.20682392 0.0448048 ]\n",
            " [0.35311328 0.17094374 0.45975794 ... 0.16481005 0.13199803 0.00710742]\n",
            " ...\n",
            " [0.         0.02648901 0.         ... 0.         0.13940613 0.05313938]\n",
            " [0.         0.00236885 0.         ... 0.         0.13276774 0.10894602]\n",
            " [0.53194481 0.0188194  0.68054624 ... 0.02384162 0.02655355 0.00464367]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 2.24011037e-02  3.62831005e-03 -8.29463471e-04 -4.03963208e-02\n",
            "  2.93871297e-02 -5.20581478e-02  3.59692827e-02 -2.37805610e-02\n",
            "  4.65002592e-02 -8.35662722e-02 -9.63441914e-03  1.09302831e-01\n",
            "  6.05363519e-02 -3.46012029e-03 -1.55108374e-02 -8.17755863e-02\n",
            "  5.57197880e-04  1.61515571e-02 -5.65204988e-02  1.73074752e-02\n",
            " -3.96814959e-02 -3.30104452e-02 -8.34866884e-02 -8.65348387e-02\n",
            "  2.66596660e-02  3.12061654e-03 -2.04843768e-02  4.15668053e-02\n",
            "  8.18761905e-02 -1.87572766e-02  3.70308381e-02 -4.29694840e-02\n",
            "  7.14495743e-02 -3.10596855e-02  4.95440362e-02  3.40118069e-02\n",
            " -8.74158086e-03  4.38638557e-02 -8.14288209e-02  4.68777986e-02\n",
            " -9.45293598e-03  3.97792844e-02  6.21635679e-02 -1.41217674e-02\n",
            "  2.16794438e-02  4.95946082e-02 -5.35699108e-02  1.09295265e-01\n",
            "  9.90083032e-03  8.00441689e-02 -2.20819494e-02  4.17886070e-02\n",
            " -1.89032600e-03 -1.14689047e-01 -8.41338852e-03 -4.68615870e-02\n",
            "  2.27169086e-02 -7.12506611e-02  1.62806465e-02  1.15236103e-01\n",
            "  7.59731034e-02  5.90929360e-02  9.67036101e-02  1.21749270e-01\n",
            "  1.45314649e-01  6.16511805e-02  4.47876785e-02 -6.61093955e-02\n",
            " -7.65092126e-02  1.44450032e-02  4.61400401e-02  4.32749257e-02\n",
            "  5.75210221e-02  4.86561135e-02  1.07390977e-01  1.83694297e-02\n",
            "  1.02175163e-02  7.50600280e-03 -5.72575870e-02  2.16235665e-01\n",
            " -7.51977183e-02  1.42925337e-02  1.23424538e-02  3.54852841e-02\n",
            "  6.79608682e-02  4.15287948e-02 -7.74685534e-02  7.04017716e-02\n",
            " -1.98239685e-04 -1.72828093e-02  1.37661304e-01 -2.69379028e-02\n",
            "  6.59169557e-02  3.26406593e-02  1.89156564e-02 -5.21042220e-03\n",
            " -1.72482653e-02 -1.46503490e-02  1.06382321e-01 -6.21089993e-02\n",
            " -3.31414316e-03  3.24952794e-02  1.28604611e-01  1.99295438e-02\n",
            "  1.25739686e-01 -1.58545190e-02  6.29244647e-02  8.50070198e-03\n",
            "  3.31630860e-02 -3.21443353e-02  3.55887080e-02  1.13758942e-01\n",
            " -1.23930479e-01 -1.39543670e-02  3.02271978e-02 -2.61881749e-02\n",
            " -5.99925546e-02  2.42920414e-01  5.40167945e-02  4.71199720e-02\n",
            "  6.90405383e-02  3.57418003e-02 -9.99615777e-03 -5.11230775e-03\n",
            " -7.14473838e-02  1.04646485e-02  8.67428652e-02 -6.50787519e-02\n",
            "  1.75593351e-02 -4.77037322e-02 -2.95028789e-02 -1.70761455e-01\n",
            " -9.42873240e-03  3.84476156e-02 -2.28058440e-02  2.75645642e-02\n",
            "  5.00495099e-02  1.94798933e-01 -7.27317197e-03 -5.16687714e-03\n",
            "  7.53211428e-02  4.37225126e-02 -5.13189378e-02 -5.40793136e-02\n",
            "  1.24438702e-01 -6.46634393e-02 -4.91196827e-02 -3.56877997e-02\n",
            " -9.63413841e-02 -3.09612183e-02  8.02147622e-02  7.63827798e-02\n",
            " -2.54807858e-02  1.04300231e-01 -2.38932598e-02  5.43281800e-02\n",
            "  8.32319602e-02 -3.52456198e-02  5.18854496e-02 -9.49746628e-03\n",
            " -3.59956561e-03 -5.53020951e-02  2.62052873e-02 -8.59363458e-02\n",
            " -6.16587158e-02  1.74756726e-02  8.69523204e-02  7.27961345e-03\n",
            "  2.73881043e-02 -3.82159164e-02 -2.47589838e-03 -3.75423724e-02\n",
            "  2.28750794e-02 -1.00762036e-01 -2.76049497e-02 -1.36919034e-01\n",
            " -3.18523812e-02  1.52449150e-02  3.90344395e-02 -1.30958390e-01\n",
            " -2.66904519e-01 -7.98643744e-02  7.23411914e-02 -4.23841172e-02\n",
            "  1.70965839e-02  2.96679084e-02  5.30130387e-03  5.88644312e-02\n",
            " -1.34610806e-02  2.95210058e-02 -3.38496221e-02 -4.65118913e-02\n",
            "  3.70112302e-02 -4.37416554e-02  4.59227304e-02 -4.10301278e-02\n",
            " -6.84380996e-03 -5.81443323e-02  2.38294070e-02  2.91311047e-02\n",
            "  4.04061192e-02  7.61762444e-04  5.82661126e-02  4.30642756e-02\n",
            "  8.78811480e-02 -1.13078941e-01 -1.69851593e-02 -1.67677743e-01\n",
            " -6.81923084e-02  1.61459546e-02 -5.71352023e-03 -1.10620754e-01\n",
            "  2.52127264e-02 -2.56852751e-02  1.06032578e-02  4.84585504e-03\n",
            "  2.35173775e-02  5.39304521e-02  1.09775073e-01 -2.32088749e-02\n",
            " -5.74628104e-02 -6.12833513e-02  6.51303299e-02  8.77753773e-02\n",
            " -6.84980618e-02  3.43743118e-02  3.74105176e-02 -2.25049408e-02\n",
            "  7.73554161e-03  6.70125582e-02 -9.38395662e-03  7.58605896e-02\n",
            "  2.73129573e-02 -5.42046240e-02 -1.44711637e-01  9.61182125e-02\n",
            "  2.43449288e-02  3.37480723e-02 -2.19208036e-02 -3.39310092e-02\n",
            " -2.09437421e-02 -4.38988667e-02  1.40135506e-03 -5.50244320e-02\n",
            " -9.29683680e-02 -4.55765798e-02 -5.86036889e-03 -4.88973870e-02\n",
            "  2.61353390e-03  8.91322732e-02 -5.24942341e-02  4.36176495e-02\n",
            " -1.57027892e-01  8.65923935e-02 -1.86477636e-01 -1.17237419e-01\n",
            "  2.45405768e-02 -2.13727615e-02  4.27450397e-02  5.94589191e-02\n",
            " -7.64809500e-02  1.04472937e-01  9.76743863e-02  4.53392780e-02\n",
            "  1.03391885e-01  1.76369827e-02  5.43280319e-02 -5.12797959e-02\n",
            "  1.99167118e-03  1.10757530e-01 -6.10402671e-02  7.36485238e-02\n",
            " -3.96502235e-02  1.35344937e-02 -1.37078586e-02 -6.11835563e-02\n",
            "  1.03416586e-02  2.55216236e-02 -4.44144006e-02 -6.22052785e-03\n",
            " -4.18997076e-02 -7.38976596e-04 -4.70359960e-02 -2.50579556e-02\n",
            " -9.35856254e-02 -1.24247222e-02  8.83125431e-03 -1.91887137e-02\n",
            " -7.22221068e-02 -8.35114083e-02 -6.72444762e-02 -2.30086540e-02\n",
            " -6.40367327e-02 -3.59199790e-03  5.07709449e-02 -6.70141812e-02\n",
            "  6.37864733e-03 -4.26579267e-02  2.75800945e-02 -6.78574167e-02\n",
            "  4.42839276e-03  2.88336659e-02 -1.04186837e-01 -5.56954352e-03\n",
            "  3.01457082e-02 -4.45742470e-03  5.38031455e-03 -1.00738739e-01\n",
            " -1.11293229e-02 -6.10318814e-03  5.66697506e-02  1.61632090e-01\n",
            " -1.93323307e-02  2.78478395e-02 -3.51817995e-02 -5.71091656e-02\n",
            "  8.05046698e-02 -2.13955606e-02  6.54024248e-02 -9.48530819e-03]  - intercept :  0.38266076475149013\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.2438368506038\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:27,467]\u001b[0m Trial 188 finished with value: -0.2882646208497773 and parameters: {'count_threshold': 10, 'postag': False, 'voc_threshold': 9830}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.08421121 0.05437703 0.14642183 ... 0.1796432  0.08320799 0.020041  ]\n",
            " [0.02982459 0.02551799 0.0503666  ... 0.11792929 0.04207647 0.02454781]\n",
            " [0.         0.00693964 0.02502487 ... 0.11619729 0.01418474 0.02876799]\n",
            " ...\n",
            " [0.15819331 0.03685294 0.21773147 ... 0.01510495 0.37949277 0.00688852]\n",
            " [0.12979078 0.05157536 0.19386042 ... 0.22276758 0.10260349 0.00982631]\n",
            " [0.576708   0.00818094 0.6968028  ... 0.00906297 0.50465766 0.00413311]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-4.16386284e-02 -2.56546789e-03 -3.58899309e-02  5.15148779e-02\n",
            "  7.28875852e-02  3.37662612e-03 -1.08870262e-02  3.76516179e-02\n",
            "  1.93175802e-02 -1.86128993e-02  2.32179739e-02 -4.53176659e-03\n",
            "  3.00254567e-02 -1.77199638e-02 -1.05659196e-01  1.62181903e-03\n",
            " -2.00365138e-02  5.84039509e-02 -2.64692911e-03 -2.54839835e-02\n",
            " -2.47681010e-02  7.23206669e-03  6.84764049e-02  1.22299799e-02\n",
            "  7.44785533e-03  6.33926396e-02 -1.01957138e-02 -1.01344271e-01\n",
            " -4.45845956e-02  8.20337308e-02 -3.05211530e-02 -1.35520063e-01\n",
            " -3.74241409e-02 -2.90662638e-02 -4.49918744e-02  8.18651865e-02\n",
            "  5.40959960e-02 -1.15938976e-02  5.90411828e-02  1.09884676e-01\n",
            "  4.71179855e-03 -4.84541055e-03  1.34624356e-01 -1.27590578e-02\n",
            "  9.72344955e-02  2.61108958e-02  1.56656892e-02 -1.02405391e-01\n",
            "  2.81524640e-02 -5.31384279e-02  1.31173525e-02 -1.23362478e-03\n",
            "  6.57050657e-02 -3.14233915e-02 -2.58297224e-02  6.74848137e-02\n",
            "  2.97453536e-02 -1.13724547e-02  1.25239301e-02  4.89173206e-02\n",
            "  8.37606216e-03  2.13078780e-02 -3.36059031e-02 -8.40646417e-02\n",
            "  3.87132085e-02  2.43390252e-03 -1.43380070e-01 -7.25823897e-02\n",
            " -5.87129891e-02 -3.41190494e-03  5.91468155e-02 -1.25616564e-01\n",
            "  2.66169484e-02  1.18676939e-02  7.24949448e-02 -6.71813709e-03\n",
            " -5.23037007e-02 -3.82600381e-02  3.88140490e-02  1.91521385e-02\n",
            " -6.63242975e-03 -2.95114219e-02 -9.06981879e-03  5.71962218e-02\n",
            " -3.42448744e-02 -9.21886313e-02  1.43448960e-02  4.02847507e-02\n",
            " -2.32964397e-02  3.46444595e-02  6.61080470e-02  4.12417789e-02\n",
            " -2.21425727e-02  1.09169900e-01  2.69855949e-04 -9.06721066e-02\n",
            " -7.49144539e-02 -4.77573028e-02 -3.50349059e-02 -4.08916989e-02\n",
            " -1.06474260e-01 -2.63555028e-02  4.02946535e-02  9.98806433e-02\n",
            "  9.98806433e-02  3.90037449e-02  8.52342679e-02 -2.78540311e-02\n",
            " -2.87198943e-02  3.69663062e-02  1.24231725e-02 -3.41178918e-02\n",
            " -6.13351519e-02  7.03953623e-02  9.51750311e-02  4.40516627e-02\n",
            "  1.20822553e-01 -4.33444470e-02  7.79676234e-02 -2.27789215e-02\n",
            "  6.49605097e-02  3.52752839e-02 -1.67626177e-02 -1.68792630e-03\n",
            " -2.30380553e-02  1.20234234e-01  5.08556065e-02  2.36265098e-02\n",
            "  4.88493270e-02  3.38566465e-02  8.15432741e-02  2.73749611e-02\n",
            "  1.37063089e-02  2.54752802e-02 -9.24329869e-02  4.20939825e-02\n",
            " -4.34277595e-02  7.80357390e-04 -6.99329238e-03  3.84205342e-02\n",
            "  2.06652958e-02 -1.00135783e-01  1.31297531e-01 -4.78384516e-02\n",
            "  7.94391747e-02 -1.41533418e-02  8.24741341e-03  9.70408102e-02\n",
            " -1.81250605e-02 -1.13760297e-02 -6.36330772e-02 -4.74040257e-02\n",
            "  7.10136182e-03 -2.67112053e-03 -1.01179458e-02 -2.68048124e-02\n",
            "  3.33833874e-02 -2.71945427e-02  1.16867515e-02 -1.45961395e-02\n",
            "  6.75951217e-02  5.26758500e-02 -5.00592231e-02  9.34022589e-03\n",
            "  3.77250131e-02 -2.27717397e-02 -4.75082349e-02 -1.28116855e-01\n",
            " -9.92308905e-03 -1.19876795e-02  5.27722500e-02  2.17286136e-03\n",
            " -9.64434195e-03 -1.84621751e-02  9.82599953e-03 -2.40852573e-04\n",
            "  2.89774570e-02 -6.24287744e-05  4.29355751e-02  5.21646771e-02\n",
            " -6.64705119e-04  3.09444472e-03  5.86730482e-02  7.10979900e-03\n",
            "  9.94270589e-02  1.88616755e-02 -6.12759057e-03 -4.06396028e-02\n",
            "  4.63073817e-02  1.16315325e-01  4.87673343e-02  5.21647988e-02\n",
            "  5.98049380e-02  6.77315367e-02  4.96226942e-02 -2.28675673e-02\n",
            " -1.65736924e-02  1.82465303e-01 -5.99932308e-02  1.04779284e-01\n",
            " -7.46347639e-02  2.54511579e-02  2.19951172e-02  2.46754218e-02\n",
            "  3.45915065e-02  7.25822064e-02  3.74101013e-02 -7.16695035e-02\n",
            " -5.82519682e-02  6.50406348e-03 -4.91829287e-02  1.08134359e-02\n",
            "  5.94888843e-02 -5.89799031e-02  4.12747151e-02 -3.65838405e-02\n",
            "  1.22832030e-02  2.98661573e-02  5.97955718e-02 -5.27065574e-02\n",
            "  3.69107596e-02  1.90310727e-01 -9.48235295e-02  8.38975465e-02\n",
            "  5.14634580e-02 -5.23397153e-02  8.72660430e-02 -2.12871252e-02\n",
            "  4.78432828e-02 -6.72686883e-02  4.51426954e-02 -8.55195026e-03\n",
            "  1.55855109e-01  1.45444684e-02 -6.99108979e-03  9.60163342e-02\n",
            " -4.21823723e-02 -4.40191049e-02  8.38380959e-02 -4.80106791e-02\n",
            " -1.93259044e-02 -2.21198324e-03 -3.55334391e-02  1.70242006e-02\n",
            "  8.79280496e-02  7.45924183e-02  1.42660604e-01 -1.79294738e-02\n",
            "  2.93293205e-02  1.61189403e-02  6.31376275e-02  1.77220184e-02\n",
            " -3.93349954e-02  4.46299207e-02 -1.42883924e-02 -9.62391929e-02\n",
            "  3.39529245e-02  3.39529245e-02 -5.84001532e-02  3.82344934e-02\n",
            "  3.91178574e-03  5.63701476e-02  3.36987493e-03  8.01906456e-02\n",
            " -8.91411130e-02 -4.70921766e-03 -7.66667041e-02  7.41246373e-02\n",
            " -2.06414931e-02  5.95332100e-02 -3.08947632e-02  6.98690531e-02\n",
            " -2.47231506e-03  2.49915469e-02  4.83019292e-02  3.17840148e-02\n",
            " -6.12925094e-02 -5.75674815e-02 -8.65476056e-03  1.03503756e-02\n",
            "  1.85370604e-01  8.37014415e-02  1.27064546e-02  3.38785097e-02\n",
            " -2.65872682e-02  5.10099097e-03  5.21862736e-02  1.64622828e-02\n",
            "  3.64320437e-02 -4.83202142e-03  9.28498477e-02  1.09810592e-01\n",
            " -3.23639817e-03 -1.20880079e-02 -6.12167416e-02 -2.87800799e-02\n",
            " -1.37183947e-02 -2.41204671e-02  7.03432018e-02 -5.07345202e-02\n",
            "  3.14250430e-02 -9.67224263e-02  1.29686269e-02 -5.13978373e-02\n",
            " -6.94920512e-03  9.81305288e-03]  - intercept :  0.2943809453099598\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.2882646208497773\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:28,030]\u001b[0m Trial 189 finished with value: -0.020271637639386338 and parameters: {'count_threshold': 7, 'postag': False, 'voc_threshold': 9664}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.         0.02178369 0.02217249 ... 0.         0.03784178 0.0395918 ]\n",
            " [0.08411387 0.0901398  0.15193167 ... 0.17747916 0.23038137 0.02926873]\n",
            " [0.         0.11845464 0.         ... 0.         0.02838133 0.07438696]\n",
            " ...\n",
            " [0.         0.01788504 0.         ... 0.         0.02027238 0.13254528]\n",
            " [0.60130097 0.01799371 0.92338216 ... 1.04219337 0.02149528 0.01762687]\n",
            " [0.         0.03398733 0.0089059  ... 0.         0.32087393 0.09396517]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-5.80920721e-03 -1.24337895e-02 -1.78098895e-02  2.96037188e-02\n",
            "  4.39046707e-02 -5.81212053e-03 -7.55541562e-03 -1.40939529e-02\n",
            " -2.02666575e-02  1.56871633e-02 -2.81905055e-02 -9.12850863e-03\n",
            "  1.10123998e-04  3.72489827e-02 -6.88484137e-03  9.82532771e-03\n",
            " -1.48298178e-02 -3.95795131e-02  1.59903414e-02  6.68012932e-03\n",
            "  2.63963786e-02  2.15788962e-02 -6.31873304e-02 -1.62271887e-03\n",
            " -5.91959937e-03 -1.10081472e-02  4.86803451e-02 -1.95352351e-02\n",
            " -6.94385511e-02 -3.65054470e-02 -2.08528493e-02  7.53995015e-03\n",
            "  3.77404255e-03 -8.38409478e-03 -5.72802817e-02  3.83945856e-02\n",
            " -4.52527691e-02  3.12307125e-02  1.05469341e-02 -1.07207204e-02\n",
            "  2.40088209e-02 -4.82686090e-02  1.77344983e-03 -6.40607151e-03\n",
            " -5.05225174e-02 -6.36018103e-03  1.97770961e-02  3.08890221e-02\n",
            " -2.44636417e-02 -4.84224304e-02 -2.26872333e-02  5.35743500e-03\n",
            " -1.63177762e-02  6.81726297e-02  7.35712355e-02  2.12031237e-02\n",
            " -5.20698910e-02  4.04950128e-02 -1.71251086e-02 -3.06103071e-04\n",
            "  4.51243781e-02 -5.48441276e-04  1.26481109e-01  4.26640066e-02\n",
            " -1.97762126e-04 -3.70691215e-02 -7.64059389e-04 -1.38409621e-02\n",
            " -5.20298353e-02 -7.85136089e-02 -1.82008720e-02 -3.74753982e-02\n",
            " -1.61252657e-02  2.96869766e-02  1.35648141e-02 -1.06758569e-02\n",
            "  1.93701293e-02 -9.10035356e-03  1.31841484e-02  3.13357998e-02\n",
            " -6.49275452e-03  4.31270464e-02 -2.12294067e-02  3.97560221e-03\n",
            " -4.74639965e-02  2.08837729e-03 -1.51938348e-04 -3.05594560e-03\n",
            " -3.59601997e-02  2.14256382e-02 -3.20456394e-03  8.65865165e-03\n",
            "  1.10026528e-02 -6.30896917e-02  1.23444494e-03 -4.26358108e-02\n",
            "  8.61699095e-03 -1.18553618e-02  1.26891898e-02  1.42566590e-02\n",
            "  2.88236584e-02 -6.40989846e-02 -1.80856183e-02 -9.83620799e-03\n",
            "  3.06637455e-02 -2.60477726e-03 -2.20330780e-02 -4.05175287e-02\n",
            " -6.26594892e-02 -2.12508526e-02 -7.46065417e-03  5.16991336e-03\n",
            " -3.10584870e-02  3.21382785e-02  5.03242249e-02  7.20815175e-03\n",
            "  1.04598866e-02 -7.72856610e-03 -4.16305848e-03  5.48522708e-02\n",
            " -3.54617970e-02 -1.71001589e-02 -2.74749688e-02 -1.40921158e-02\n",
            " -5.13359890e-03  2.64546979e-04  1.53291857e-02  2.09067356e-02\n",
            "  2.25827825e-02  4.95113011e-02 -5.24610500e-02  4.39883975e-02\n",
            "  4.04502130e-02 -1.09275497e-03 -3.12583067e-03 -5.95214047e-02\n",
            " -7.70775641e-03  5.53481264e-03 -1.74704559e-02  1.52176116e-02\n",
            "  6.50856357e-02  6.65921574e-02  2.10209984e-02  1.10413798e-02\n",
            "  2.95628668e-02 -3.80603102e-02  4.83760657e-02 -5.62073114e-02\n",
            "  3.18647404e-02  2.39816504e-04  1.98087295e-02  1.92289573e-02\n",
            "  5.85131672e-02 -6.14895500e-02 -6.72332397e-02  8.33378033e-03\n",
            "  1.37706610e-02 -1.87679296e-02  3.53009219e-02 -1.29793547e-02\n",
            "  4.94473002e-03 -1.05860663e-02 -5.45544758e-03  1.03036355e-02\n",
            " -1.02094825e-01 -4.41976977e-02 -8.33119150e-03 -1.19561319e-02\n",
            "  3.82901337e-02  3.06332803e-02  3.21310308e-02  5.48967053e-04\n",
            " -4.57360688e-02  1.13256041e-02  1.98671916e-03  1.02059162e-02\n",
            " -1.24625163e-02 -2.21821409e-02 -1.24154517e-03  5.37669652e-03\n",
            " -1.42977579e-02  1.26517762e-02 -2.15755889e-03 -1.94530690e-02\n",
            " -4.14919144e-03 -1.61572775e-02 -4.95518000e-03 -5.58670440e-02\n",
            " -1.49195372e-02  1.07175998e-02  1.07175998e-02  9.48633620e-03\n",
            " -2.54935237e-02 -5.35037720e-03  4.57748403e-02  3.20242556e-02\n",
            " -2.12070864e-02 -8.35852574e-03  1.26242383e-02 -2.92522055e-02\n",
            "  2.05729299e-02 -2.41978521e-02 -6.13865438e-02 -3.64991702e-03\n",
            "  4.42099143e-02 -5.38214014e-02  1.18338334e-02 -7.07387387e-03\n",
            "  1.23488538e-02  5.13805829e-03 -1.47000281e-02  2.52708454e-03\n",
            "  1.74680222e-02  1.87532813e-02 -2.82366614e-02  2.14785179e-02\n",
            " -2.20786537e-02 -5.93404655e-02 -6.42660359e-03 -5.70716129e-02\n",
            " -3.41171458e-03 -2.06531734e-02  1.11365581e-02 -3.35652097e-02\n",
            "  1.57308159e-02 -2.85011967e-02 -3.46144503e-02  2.95747502e-03\n",
            " -1.84280801e-03  3.98872448e-02  2.48050306e-02  1.53003056e-03\n",
            "  2.97336046e-02 -8.98756266e-03 -7.11136538e-02  8.44301866e-03\n",
            " -3.24657986e-02  7.00803382e-03 -1.93916947e-02  5.06315312e-03\n",
            "  6.86403964e-03 -7.33115312e-02 -1.17169031e-01  1.36663491e-02\n",
            " -1.07816038e-02 -1.84384672e-02  8.49290171e-03 -2.05815915e-03\n",
            " -2.15935615e-02  2.57537819e-02 -3.22010029e-02 -3.79784604e-02\n",
            " -6.76477679e-03 -1.37129845e-01  4.26464933e-02 -5.02101186e-02\n",
            " -1.58984881e-02  2.22796438e-02  3.99470063e-02 -4.38124751e-04\n",
            " -1.42807427e-02 -2.78280768e-03 -2.79354356e-02  7.36290491e-03\n",
            "  1.05892757e-01  9.25937311e-03 -5.78435360e-02 -1.17022894e-02\n",
            " -1.62348028e-02  4.07480933e-02 -5.01830434e-02 -3.21423759e-02\n",
            " -1.37627756e-01  6.56884702e-02 -1.09179818e-01  1.11678716e-02\n",
            "  7.88604162e-03  4.33845784e-02 -1.51139769e-02  7.21823741e-02\n",
            "  2.67133914e-02 -2.53649175e-03 -9.77464768e-03 -4.40495281e-02\n",
            "  6.61623639e-02  5.23142911e-02 -1.52837787e-02 -1.07892930e-01\n",
            "  3.92964092e-02  9.34149687e-02  2.19941441e-02 -6.74499773e-02\n",
            "  4.72507033e-02  6.52644059e-02 -1.75251565e-02 -2.87704254e-02\n",
            "  1.45608403e-02  5.62540494e-02 -3.90523650e-02  2.85694478e-03\n",
            " -1.58502325e-02 -6.58960236e-03 -8.52662382e-02 -6.80302123e-03\n",
            "  1.02812607e-01  1.03217071e-01  3.37681601e-02  4.65814941e-02\n",
            " -2.27776828e-02  5.57148461e-03  4.87664194e-02 -3.51515086e-02\n",
            " -6.80569499e-02  2.24893315e-02  2.62681231e-02  3.53577740e-02\n",
            "  4.40079763e-03  6.35119550e-02  2.85361682e-02 -3.70929742e-02\n",
            " -8.51067990e-03  5.07326780e-02  1.47392055e-02 -4.63779291e-03\n",
            " -1.79197877e-02  6.65182959e-02  4.41022099e-04  4.01998876e-02\n",
            "  2.17485324e-02  4.90674327e-03 -4.80307086e-03  5.62817366e-03\n",
            "  4.55904004e-02  7.74799968e-03 -1.00138564e-02  1.88214174e-02\n",
            "  4.97741538e-02 -6.81380085e-02  8.07135470e-02  1.97500451e-02\n",
            "  1.12558759e-01  4.19389844e-02 -2.67200689e-02  4.34774175e-02\n",
            " -1.55112146e-02 -1.22750327e-02  1.03616162e-01  4.12378662e-02\n",
            " -1.81125829e-03  2.02796590e-02  8.43516550e-02 -1.06315462e-02\n",
            " -8.09844338e-02  2.87064192e-02  4.27055049e-03  2.25415045e-02\n",
            "  1.45382904e-02  3.16959599e-02 -1.53574896e-01 -4.58770696e-02\n",
            "  4.04158573e-02 -1.48067780e-02 -2.92427750e-04  5.09135335e-02\n",
            "  2.03813062e-02 -2.02721935e-03  7.61665264e-03  1.32897361e-02\n",
            "  5.30094002e-02 -3.60780317e-02 -5.49397392e-03 -8.85544881e-02\n",
            " -4.15437610e-02 -5.61544269e-02  8.78677063e-02  5.06725210e-02\n",
            "  5.26772251e-02  4.37464243e-02  1.01626294e-02  5.87449634e-03\n",
            "  6.47106421e-02  9.59443714e-03 -4.14580273e-03  9.00179910e-02\n",
            "  6.64910351e-02 -1.16423875e-02 -1.41940079e-02  1.79836274e-02\n",
            "  1.38045173e-02 -1.44615922e-03  1.42291653e-02 -1.98217286e-02\n",
            " -2.96738898e-02 -9.09939853e-03 -1.52541671e-02 -7.55678145e-02\n",
            "  5.15282967e-02 -2.78457725e-02 -1.79774968e-02  5.01922904e-02\n",
            " -9.37007471e-03 -8.85019093e-02  3.02917789e-02 -3.25359143e-02\n",
            " -7.86561714e-02 -7.76478179e-02 -3.42265550e-02 -8.13058504e-02\n",
            "  3.93365912e-02 -3.34994358e-03  1.14792903e-02  8.91092121e-02\n",
            " -2.98227372e-02  3.89509309e-02 -1.36022124e-02 -3.79034661e-03\n",
            "  5.55912443e-02 -5.20736226e-02  4.00560968e-02  1.66753186e-02\n",
            "  4.99510905e-03 -2.41456014e-02  7.98950763e-03 -3.45915563e-02\n",
            "  1.78779660e-02  4.84904766e-02  4.84904766e-02  8.54185363e-02\n",
            "  2.77206780e-02  2.82235868e-02  9.52133358e-02  3.19899079e-02\n",
            " -4.98655421e-03  4.73645861e-02 -3.29856291e-02  3.79874334e-02\n",
            " -1.03566871e-01  1.13952496e-02  1.09465339e-01 -4.13623170e-02\n",
            "  2.20718082e-02  3.49960888e-02 -1.26651116e-03 -2.29828360e-02\n",
            " -8.24214332e-02  1.43676656e-01  1.63587253e-03  3.63920354e-02\n",
            "  5.91450617e-02 -6.38230838e-03 -1.66780100e-02  7.36496953e-03\n",
            "  7.08463714e-02  1.35670172e-02  7.59857576e-02  2.24582755e-02\n",
            " -1.77286426e-02 -2.66573823e-02 -4.02199722e-02  3.90131118e-02\n",
            " -3.45610985e-02  3.31019682e-02  1.48319665e-02 -3.12391974e-02\n",
            " -1.71725552e-02 -8.66645483e-02 -4.98666272e-02  1.26070917e-01\n",
            " -2.40589610e-02  2.43823845e-02  3.83003918e-02 -1.12101601e-03]  - intercept :  0.5284798315456466\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.020271637639386338\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:28,628]\u001b[0m Trial 190 finished with value: 0.26432975833576744 and parameters: {'count_threshold': 4, 'postag': True, 'voc_threshold': 1247}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.06445207 0.26952251 0.06128376 ... 0.         0.17781066 0.04802159]\n",
            " [0.04911621 0.12094344 0.09587066 ... 0.25619187 0.40340084 0.05857323]\n",
            " [0.02500229 0.02966552 0.03112483 ... 0.19007784 0.35618394 0.09714619]\n",
            " ...\n",
            " [0.37951049 0.23649311 0.33719338 ... 0.         0.15536542 0.01478521]\n",
            " [0.10417982 0.04809414 0.10898552 ... 0.07483647 0.23803335 0.07180528]\n",
            " [0.26955925 0.01684227 0.34967303 ... 0.         0.04647968 0.00697   ]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-0.0012437  -0.00077355 -0.00360105 ...  0.00576058 -0.00104326\n",
            "  0.00250066]  - intercept :  0.6614377102553788\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.26432975833576744\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:29,211]\u001b[0m Trial 191 finished with value: 0.09407342854168536 and parameters: {'count_threshold': 10, 'postag': False, 'voc_threshold': 6178}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.         0.01271933 0.01735205 ... 0.78381686 0.         0.05186315]\n",
            " [0.10059996 0.17555217 0.03364231 ... 0.15192474 0.06902112 0.02384163]\n",
            " [0.02960986 0.02092593 0.02403893 ... 0.28560673 0.13702975 0.03007121]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.26586829 0.06013755 0.03005736]\n",
            " [0.         0.02425003 0.040657   ... 0.24307958 0.         0.04597823]\n",
            " [0.         0.         0.00524668 ... 0.42766759 0.05530295 0.02857009]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-9.07362037e-03 -4.07593450e-02  1.10955805e-02  7.17636363e-03\n",
            " -8.07556147e-02 -3.88259509e-02 -2.80868759e-02  9.50357776e-03\n",
            " -2.57527944e-02 -6.39617363e-02  1.28779179e-02 -6.62125822e-02\n",
            " -1.28691980e-02 -5.73645731e-02 -7.20113633e-02 -3.91462632e-02\n",
            "  1.02978373e-02 -7.55505137e-02  4.06861628e-02  7.63152905e-03\n",
            "  5.99158624e-02 -3.67137472e-02 -2.65115709e-02 -1.08078725e-02\n",
            "  3.42960940e-02  1.87205601e-02 -4.88659914e-02  1.39869003e-02\n",
            " -1.37701816e-01 -2.63326489e-02 -6.38663225e-02 -4.66204972e-02\n",
            "  7.37027359e-02  8.40012785e-02  1.45023724e-02 -1.62696934e-01\n",
            " -5.13724718e-02  8.43243452e-03  7.17571484e-02 -9.57371754e-02\n",
            "  4.63827056e-03  2.36748155e-02 -9.68514456e-02  2.15589762e-03\n",
            "  1.02043427e-01 -9.24252402e-03  2.90802358e-02 -1.11614638e-01\n",
            " -2.33879952e-02 -6.14452036e-02 -3.29836055e-02 -4.91840938e-02\n",
            " -4.70603476e-02  4.69651117e-04 -8.29524862e-02 -9.35002199e-02\n",
            " -3.19293636e-02  3.68243163e-02 -5.46813310e-02  1.87903437e-02\n",
            " -8.41031935e-02  1.70518813e-01 -1.65553819e-01  2.66741550e-02\n",
            "  3.99343528e-04 -6.03270929e-02  1.44119169e-02 -5.17727473e-02\n",
            "  1.60752439e-03  1.78588180e-01 -6.26082906e-02 -4.79971889e-02\n",
            " -9.63536045e-02 -8.51503980e-02 -2.83440502e-02 -1.15322878e-01\n",
            "  5.46894971e-02  7.05744034e-02 -1.26798927e-01 -1.15729716e-01\n",
            "  9.07055885e-02 -1.17211800e-01  3.59927357e-02 -7.12702336e-02\n",
            " -6.31360744e-02 -1.02499292e-01  8.35164334e-03  3.37574024e-03\n",
            "  3.04138499e-02  1.85732591e-02  4.18606994e-02 -1.05851821e-01\n",
            "  7.00249799e-02 -3.40227644e-02 -5.10792295e-04 -1.40879707e-01\n",
            " -1.05443335e-01 -4.24732722e-03 -1.63560830e-02 -1.68586439e-01\n",
            " -6.43017110e-02  1.29094634e-02 -2.32929058e-02  6.61958995e-02\n",
            " -5.81853663e-03 -1.71606536e-01  1.02661740e-01 -7.23416057e-02\n",
            "  1.18959504e-01 -8.29642756e-02  1.14265939e-01  4.91418281e-02\n",
            "  3.94438967e-02  1.07833747e-01  6.21822763e-02  9.95049445e-02\n",
            "  3.54195231e-02  2.39134837e-02  1.35568338e-02 -6.14953336e-02\n",
            "  6.19332007e-02 -2.03781382e-01 -1.72078347e-01  6.40654735e-02\n",
            " -1.02942769e-01  6.91462626e-02 -9.60522092e-02 -2.17032624e-02\n",
            "  2.85840072e-02  8.19745818e-02  7.11196607e-02  4.34052037e-03\n",
            "  5.98977501e-02  3.76119089e-02  4.58230976e-02  3.71243494e-02\n",
            "  6.17819941e-02  7.14071379e-02  3.98929863e-02  8.69580640e-02\n",
            " -1.48789324e-02  2.36375890e-02  3.28971284e-02 -1.68917629e-02\n",
            "  2.81703321e-02  2.54596791e-02  5.72176691e-02  8.33408720e-03\n",
            " -5.40251167e-03  8.60855661e-03  5.43014789e-02  1.65915019e-02\n",
            " -6.02837038e-02 -1.72913176e-02  3.25592389e-02  2.01938430e-02\n",
            "  5.36219414e-02  1.19532705e-02 -8.92230570e-02 -4.25746845e-02\n",
            " -5.39829878e-02  1.63324511e-02 -1.53647969e-02 -2.58311164e-02\n",
            "  2.49826833e-02  3.46718364e-02 -1.65859492e-02 -1.34377676e-02\n",
            "  1.02288804e-01 -2.25657430e-02  9.32792332e-02 -4.60313573e-03\n",
            "  2.26944008e-02  5.35065599e-02  1.18330553e-01  6.94225310e-02\n",
            " -4.42672213e-02 -7.02112081e-02 -1.12981135e-01 -3.85748589e-03\n",
            "  4.14103183e-02  1.00652061e-01  3.71723377e-02  1.79316015e-02\n",
            " -5.55977177e-03  2.19721331e-03  6.50407004e-02 -1.15432923e-01\n",
            " -4.76184598e-02 -3.57122308e-02 -1.81410115e-03  5.52366388e-02\n",
            " -2.60604241e-02 -4.34722129e-02  8.53779136e-02 -2.05825660e-01\n",
            " -1.03353867e-01 -1.21855653e-01  1.82110543e-02  1.06644413e-01\n",
            "  8.67203937e-02 -6.39213864e-02  3.32255339e-02  2.37010656e-02\n",
            " -3.49060531e-02 -1.60643555e-02 -1.47047085e-04  5.42371909e-03\n",
            "  2.21746295e-01 -3.82005721e-02 -1.33515303e-02 -3.43704570e-02\n",
            "  2.31169445e-02  6.55688074e-02  2.35159625e-02 -5.63966035e-02\n",
            " -1.23124801e-02  8.38252509e-02 -4.41910653e-02 -1.59295972e-02\n",
            " -7.83725616e-03 -1.03602766e-01  1.86381004e-02 -1.49071596e-01\n",
            "  4.59566587e-02 -6.56526913e-02  2.71146705e-02 -1.00831349e-01\n",
            " -1.50777495e-01 -4.18803466e-02 -5.17301336e-02  9.66663114e-03\n",
            "  4.81073555e-02  6.27649078e-02 -1.11515603e-01 -1.42999496e-03\n",
            "  4.81318973e-02 -9.60956401e-03 -3.65973696e-02  4.00635364e-02\n",
            "  1.26992054e-01  4.21737096e-02 -3.09800953e-02 -9.97543434e-02\n",
            "  6.99900119e-02 -9.04184025e-03 -2.51654453e-02 -4.98505909e-03\n",
            " -2.13981793e-02  4.45128288e-02  2.03882869e-02 -8.82433624e-02\n",
            "  2.86573229e-02  5.78643144e-02  3.02157312e-02  2.13221551e-03\n",
            " -4.53474271e-02 -9.92611946e-02 -1.55643109e-01 -9.36364128e-03\n",
            "  1.30713760e-01  8.74489195e-03  6.64636956e-02  1.38361871e-02\n",
            " -1.73851927e-02 -7.17220919e-02  1.00255441e-01 -3.68148632e-02\n",
            "  2.76352635e-02  4.31842725e-02 -7.29558869e-02 -3.61826264e-03\n",
            " -6.07908170e-02 -5.06271654e-02  5.71668151e-02 -5.40526139e-02\n",
            " -9.63206187e-02  1.54877785e-01 -3.68299046e-02 -2.14017983e-02\n",
            "  1.21293500e-01 -4.80095644e-04]  - intercept :  0.883432029490103\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.09407342854168536\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:29,746]\u001b[0m Trial 192 finished with value: -0.04803044210441669 and parameters: {'count_threshold': 5, 'postag': True, 'voc_threshold': 6007}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.35370407 0.61502331 0.06997562 ... 0.00311403 0.03681124 0.01359692]\n",
            " [0.         0.         0.02425645 ... 0.06848529 0.26311984 0.07495394]\n",
            " [0.03092167 0.03337866 0.11257905 ... 0.21015831 0.15717935 0.06913112]\n",
            " ...\n",
            " [0.04089928 0.1072759  0.01088046 ... 0.07486596 0.08034869 0.13016337]\n",
            " [0.44477295 0.72606233 0.0291947  ... 0.04866934 0.01538798 0.00543877]\n",
            " [0.03090242 0.02941395 0.16641364 ... 0.28357464 0.13395566 0.05194416]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-9.99407233e-03  3.92385170e-03 -2.60106611e-03 -1.29829892e-02\n",
            "  1.41469932e-03 -1.83679585e-03 -2.84496832e-03  1.56516307e-02\n",
            "  1.50708130e-02  3.93196982e-02  2.05459159e-02  1.38071147e-02\n",
            " -6.62839363e-04  6.06226388e-03 -2.25692994e-03 -1.26458574e-02\n",
            "  2.02857889e-03 -1.25925808e-02  2.52390024e-02 -4.17552383e-04\n",
            "  1.60558537e-02  2.18847256e-02 -7.00878736e-03  8.27760871e-03\n",
            " -1.06102589e-02  9.79707658e-03  2.90900377e-02 -4.99762397e-02\n",
            "  4.10210626e-03  1.08956469e-02  9.78548847e-03  2.16033643e-02\n",
            "  2.09587243e-02  2.41593655e-02 -2.68374375e-02  1.21971966e-02\n",
            "  6.07064852e-04  5.72712134e-03  2.55212198e-02 -3.71765614e-02\n",
            "  3.12613068e-02  1.15217462e-02  3.42985964e-02 -4.61501156e-04\n",
            " -1.55448156e-02  3.09198025e-03  7.07841956e-03 -4.05333264e-03\n",
            " -3.16571675e-02  6.86940183e-03 -8.91187204e-03  3.40226416e-02\n",
            "  3.66153370e-02 -1.44057030e-02  2.84270726e-02  2.30801518e-02\n",
            " -7.34624296e-03  1.11545817e-02 -2.50129064e-02 -2.39280744e-02\n",
            "  2.15556615e-02 -1.55273595e-02  3.98300523e-02 -1.65933774e-02\n",
            "  7.94043133e-03  5.20140177e-03 -7.14711690e-04  2.19580097e-02\n",
            " -2.95165333e-03  9.06810695e-03  6.14618525e-02 -9.62525452e-03\n",
            "  1.89888070e-02 -4.33770080e-03  3.36917722e-02 -3.01285821e-02\n",
            "  3.02845761e-02  1.76120477e-03  1.94745768e-02 -2.02487542e-02\n",
            "  2.14277097e-02 -3.67579751e-02 -1.46105515e-03 -4.41992638e-03\n",
            "  5.09997519e-03  9.72066376e-03  3.80745444e-02  1.92729789e-02\n",
            "  1.20550840e-02  2.76716737e-02  1.11398956e-02  2.48946851e-02\n",
            "  4.02119172e-03 -4.98400555e-02 -1.17750534e-02  5.02977362e-03\n",
            "  3.31204829e-02  2.40479435e-02 -2.20344253e-02 -5.69095390e-02\n",
            "  1.91465261e-02  3.35584819e-02 -4.20436418e-02 -2.90999171e-02\n",
            " -9.07269419e-03  3.61983627e-03  7.95740194e-02  9.72089716e-03\n",
            "  1.89802202e-02  7.88429770e-03 -4.94608919e-02 -4.65257412e-02\n",
            " -7.65722831e-03  2.12558295e-02 -1.48444482e-02  3.60949362e-02\n",
            " -2.83782234e-02 -6.80620353e-03  1.06914475e-02 -1.95346991e-03\n",
            "  5.19281109e-02 -4.07666682e-03  1.55720218e-02 -2.40696682e-02\n",
            "  4.13450599e-03 -3.87713025e-02 -3.58287615e-03  2.32529411e-02\n",
            " -4.19436823e-02 -5.50992275e-02 -1.60711587e-02 -1.56033257e-02\n",
            "  1.31902690e-02  6.98062326e-03  1.41351850e-02  1.31146075e-02\n",
            "  1.60643675e-02  1.60643675e-02 -8.11302871e-04  3.10875823e-02\n",
            " -3.64507627e-02 -2.51639100e-02 -2.95183405e-02  5.65209766e-02\n",
            "  5.52049242e-02 -2.30196218e-02  2.40894097e-02 -2.71291939e-03\n",
            " -3.22114986e-02  2.11637617e-02 -7.70515351e-03  1.78166601e-03\n",
            " -1.80112711e-02  5.45667864e-03 -3.20345953e-02 -1.17267049e-02\n",
            " -3.90103783e-02  1.22481149e-02  7.62799343e-03  6.01212564e-03\n",
            " -6.47693745e-03 -6.26147440e-02  3.15757389e-02  4.65617563e-03\n",
            " -4.18594210e-02 -1.07949227e-02 -1.00355260e-02 -3.66735238e-02\n",
            " -6.14280164e-03  9.73682823e-03 -4.36051399e-03  3.74054131e-03\n",
            "  1.38617153e-02  2.04290129e-02 -2.27114131e-02  1.51226710e-03\n",
            " -1.15420666e-03  2.36119865e-02  1.02393994e-02 -2.31157576e-02\n",
            " -1.32712275e-02  3.75721503e-02 -2.34698778e-03 -2.33756793e-02\n",
            "  2.87229734e-02  1.38892019e-02 -2.16102242e-03  2.78894305e-03\n",
            " -2.78636793e-03  2.32206295e-02 -3.66997316e-03  2.28899079e-02\n",
            " -1.48472370e-02  1.39205768e-02  4.28481163e-03  1.54624706e-02\n",
            "  1.79058837e-02  6.59380799e-02 -2.19422242e-02 -3.04952451e-02\n",
            "  3.24170474e-02  7.48693955e-03  2.81295590e-04 -7.27916673e-03\n",
            " -3.78333811e-02  3.70305319e-03 -1.25340832e-02 -4.99677366e-02\n",
            "  6.51572143e-03 -3.86291108e-03 -8.57279700e-03 -1.72273507e-02\n",
            "  9.26047766e-03  3.01052166e-02 -2.60676169e-02 -7.90433252e-03\n",
            " -9.19630660e-03 -8.44134077e-03  1.16215524e-02 -3.06383695e-02\n",
            "  6.72449479e-02 -3.17456841e-02 -2.23010255e-02  2.05634661e-02\n",
            "  3.13330127e-02 -1.66049549e-02 -1.78064120e-02 -9.29303254e-04\n",
            "  1.01965123e-02  2.38664228e-02 -5.13711009e-03 -2.54732882e-02\n",
            "  1.18757463e-02 -3.21270246e-02  4.05017445e-02 -5.60519512e-04\n",
            " -2.17914575e-02 -1.17389525e-03  1.91199664e-02  2.91045938e-02\n",
            " -3.08994961e-02 -2.47572863e-02 -1.22151802e-02 -1.31731173e-02\n",
            "  4.86248092e-04 -9.41060535e-03  2.16765107e-02 -4.97931014e-02\n",
            "  8.84215045e-03 -1.51024776e-02  6.01815871e-03  2.66734573e-03\n",
            " -1.46301143e-02 -2.35944722e-02 -1.84523450e-02  8.93091181e-03\n",
            "  1.78900769e-02 -2.12418460e-02  2.04211523e-02  1.99015587e-02\n",
            " -1.93945600e-02 -2.52946865e-02  3.33599914e-02  3.29736508e-03\n",
            "  2.97747817e-03  1.34005269e-02 -4.18610217e-03  5.55494290e-03\n",
            "  5.55494290e-03  5.55494290e-03  5.55494290e-03  4.74685570e-02\n",
            " -1.09380677e-02 -1.33032341e-02  3.57140552e-02 -9.94003963e-03\n",
            " -7.34348494e-03 -2.89155246e-02 -9.59279078e-03 -1.47003097e-02\n",
            "  8.10288752e-03 -1.81842453e-02 -2.11995537e-02  4.78970026e-03\n",
            " -5.76994762e-03 -2.01624308e-02 -3.14998688e-02 -1.30297451e-02\n",
            " -7.76835054e-03 -1.22011277e-03  6.04763894e-03  1.34242183e-02\n",
            "  3.95775011e-02  2.45195027e-02  4.85840479e-02 -2.49329393e-02\n",
            " -3.02493939e-03  1.99203061e-02 -1.14037621e-02  3.22763795e-02\n",
            "  2.08343390e-02 -2.27305670e-02 -4.24461575e-02  1.12674435e-02\n",
            "  1.14118785e-02  7.63177567e-03  6.78502946e-04 -2.15344498e-02\n",
            "  2.06919209e-02  6.51272359e-03 -5.81310076e-03 -3.50020886e-02\n",
            "  3.11627918e-02 -8.92766578e-03 -3.89960602e-03 -2.03388891e-02\n",
            " -1.40837167e-03 -2.57864930e-02 -1.22806157e-02  3.78336389e-02\n",
            " -1.92609801e-02 -1.07597729e-02 -1.15153434e-02  1.43199848e-02\n",
            " -6.31872277e-03  1.17681648e-02  1.27382090e-02  8.63951660e-03\n",
            "  1.88639913e-02 -1.47499512e-02 -1.61099668e-02 -3.46959143e-03\n",
            " -9.04422940e-05  2.08223080e-02 -1.55942549e-02  3.59439139e-02\n",
            "  2.76074914e-02 -5.86558323e-03  1.46900972e-02 -4.28374427e-02\n",
            "  2.69949909e-02  8.57961834e-03  2.93572198e-02 -4.17250200e-02\n",
            " -2.82745709e-03  8.06412595e-03  3.27941975e-03  2.81038339e-03\n",
            "  6.26426447e-03 -9.90912121e-03 -4.25993239e-03 -9.51544988e-03\n",
            " -1.68899096e-04 -3.91543418e-03  3.05734265e-03  5.65775501e-02\n",
            "  3.69276374e-02 -1.37936428e-04  2.69709318e-02 -2.16326488e-02\n",
            " -1.06493373e-02  7.11772617e-03  7.98313089e-03 -2.24779072e-02\n",
            " -3.46570903e-02 -6.14295543e-03 -1.08396650e-02  2.90730608e-03\n",
            " -2.22325333e-02 -1.68472841e-02 -9.88421982e-03 -1.97386220e-02\n",
            " -6.23161436e-02 -2.96900159e-02 -1.26145870e-02 -3.18553487e-02\n",
            "  3.40998447e-02  2.11849740e-02 -2.17319546e-02  1.68220619e-04\n",
            " -1.79769194e-02 -9.03890888e-03  3.02051223e-03 -1.77161375e-02\n",
            " -1.40419357e-02  4.16827742e-04 -3.37714701e-03 -8.98014251e-04\n",
            " -5.33731004e-03  5.21619230e-03  2.61749021e-03 -4.38181345e-03\n",
            " -5.74155364e-03 -1.63590747e-02 -1.80055338e-02 -3.66654161e-03\n",
            "  7.04876171e-03  7.02707216e-03 -1.57811122e-02  3.41572407e-03\n",
            "  2.39371482e-02  1.13636111e-02  2.57190614e-02  1.44515493e-02\n",
            "  9.43011109e-03  2.42129510e-02 -1.81840512e-02  1.54885565e-03\n",
            " -2.31957002e-02 -2.31551624e-02 -4.73296141e-04  4.45401729e-03\n",
            "  7.44277691e-03 -1.52166078e-02 -1.13641266e-02  3.66398595e-02\n",
            " -6.67410109e-03 -2.05108782e-03  1.34442093e-02  2.83291533e-03\n",
            "  2.77811910e-02 -1.22878824e-02  2.39169694e-02 -4.36511602e-03\n",
            "  4.38294794e-03  1.07125949e-04  1.93964015e-02 -2.63108507e-02\n",
            "  7.56259569e-03 -1.33463058e-02  5.70086800e-03  3.67908764e-03\n",
            " -2.28514600e-02  3.39993968e-02  1.70738167e-02  1.80153902e-02\n",
            " -1.51723037e-02  3.43724800e-02 -3.31947402e-03 -2.80197960e-02\n",
            "  1.95279771e-03 -8.51813133e-03  1.27187555e-02  6.38115130e-03\n",
            "  1.07182404e-02  3.36915889e-02 -1.19660709e-02 -1.02726851e-02\n",
            " -2.22141647e-02  2.74131537e-02 -1.26681510e-02  7.49591691e-03\n",
            "  2.32636393e-02  5.65022904e-04 -3.90021990e-02  1.81163947e-02\n",
            "  5.35712304e-03  2.25168855e-02  4.44187470e-02 -5.86745811e-03\n",
            " -2.52613822e-03 -2.48258808e-02  5.41304404e-03  1.66173951e-02\n",
            "  1.78606154e-04  2.58041913e-02 -8.64862526e-04  1.43932629e-02\n",
            "  1.80682877e-02  1.32124036e-02  2.46282198e-02 -1.74032108e-03\n",
            "  7.25566436e-03 -1.68752027e-02 -6.00817758e-02  1.38491882e-02\n",
            " -4.82028705e-03 -2.71648319e-02 -5.27118839e-02 -4.55933272e-03\n",
            "  1.40377471e-02  5.05193026e-04 -5.47252077e-02 -1.78298668e-02\n",
            "  1.07996767e-02 -1.52876081e-02 -6.99193254e-03 -2.43633364e-02\n",
            "  1.39703154e-02 -7.22839273e-03  2.09096100e-02 -4.73394955e-02\n",
            " -2.84019620e-02  2.29983205e-02  3.65889028e-02 -2.73914602e-02\n",
            "  1.80046522e-02  1.27330260e-03 -1.63265018e-02 -2.18244289e-02\n",
            "  2.61634587e-03  6.66905277e-03  1.18519565e-02  1.31633772e-03\n",
            "  1.83116558e-02  1.87751764e-02 -1.18198337e-02 -2.95189326e-02\n",
            "  6.81374935e-03 -1.57916476e-02 -1.66553304e-02 -8.88333953e-03\n",
            "  7.27312094e-03  2.67848430e-02 -1.32219805e-02 -2.97357446e-03\n",
            " -6.93858097e-03 -6.93858097e-03  4.92378699e-03  3.60126470e-02\n",
            "  1.40079774e-02  5.49129393e-03 -1.15344817e-02 -2.49039930e-03\n",
            "  2.97318890e-02  1.11194495e-02  3.12372764e-02  1.49005063e-03\n",
            " -2.79266437e-02 -3.51716266e-02 -3.09089686e-03 -1.40038004e-02\n",
            "  1.60860915e-02 -5.99692889e-03 -2.97262952e-02  1.30333240e-02\n",
            " -2.37822326e-02 -2.04861581e-02 -2.76383314e-02  6.55273996e-03\n",
            " -5.88387702e-02 -8.58943421e-03  2.95632888e-02  1.54223286e-03\n",
            " -3.26136187e-02 -3.93145085e-02  3.74785047e-02 -2.17833658e-03\n",
            " -5.47924433e-03 -3.87640666e-03  1.66417020e-02 -2.15677546e-02\n",
            " -1.46581708e-02  1.82025224e-02 -4.11924498e-03  8.78733112e-03\n",
            " -4.28700660e-03 -8.67531308e-03 -3.75586068e-03  1.15981388e-02\n",
            " -1.24476277e-02  1.25740593e-03 -6.53417412e-02 -1.82694868e-02\n",
            "  8.73202153e-04 -2.61049098e-02  1.63839592e-02 -2.03573965e-02\n",
            "  1.52438027e-02 -1.59932268e-02  5.36260082e-02 -1.01922525e-02\n",
            "  1.29982892e-02  3.34039921e-03  3.81227161e-02  1.42535581e-02\n",
            "  6.52650987e-03  4.34114229e-02 -2.87688490e-02  4.56836609e-03\n",
            "  5.45737054e-02  1.11107407e-02  3.67519135e-02  1.82893557e-02\n",
            " -1.92961169e-02 -2.18322503e-02  1.73983696e-02 -2.38723370e-02\n",
            "  2.29964685e-02 -5.94812505e-03 -6.30947074e-03  3.78237955e-03\n",
            " -1.04718696e-02  1.79785801e-02 -2.51700288e-02 -3.71258300e-02\n",
            " -1.24034733e-02 -3.52605318e-03  1.21030694e-02 -2.47756526e-03\n",
            "  5.39990462e-02  3.28403895e-03  1.05833889e-02  1.67570986e-02\n",
            " -9.53852266e-04  1.06244339e-02 -5.71044680e-03 -8.33525232e-03\n",
            " -5.79700534e-03 -1.46561318e-02 -1.82803715e-02  7.74712869e-03\n",
            "  1.43499715e-02 -2.64314704e-02  2.95303359e-02  1.50920797e-02\n",
            " -3.26813716e-02 -2.17017858e-02  7.11444523e-03  5.85063304e-03\n",
            "  4.87259551e-03 -4.08682076e-02 -3.47160651e-04  1.76417583e-02\n",
            " -1.08019583e-02 -2.32020401e-02  3.12542513e-03 -2.51965151e-02\n",
            "  8.48996385e-03 -6.25232835e-03 -8.69973817e-04  5.69642333e-03\n",
            " -1.38634698e-02 -1.73402635e-02 -6.65673682e-03  2.04619972e-02\n",
            "  8.56806387e-03 -1.13312044e-02  5.31068040e-03 -3.28037066e-03\n",
            " -1.34580519e-02 -1.91257702e-02  4.07473736e-02  4.10400388e-03\n",
            "  5.45863506e-03  5.26033373e-03  3.42253016e-02 -1.80171215e-02\n",
            " -1.80171215e-02 -1.80171215e-02 -1.80171215e-02 -2.88530747e-03\n",
            "  1.26244452e-02 -3.36438027e-02  1.21265130e-02 -9.91550128e-03\n",
            "  1.78970034e-02 -7.55027869e-03 -1.00348517e-02 -1.14666713e-02\n",
            "  3.32232122e-03 -1.98857729e-02 -2.95757831e-03  1.77838510e-02\n",
            "  1.12929436e-02 -2.34874874e-02 -3.12274120e-02 -1.28395262e-03\n",
            "  2.48485768e-02  6.28867042e-03 -4.41333232e-02  1.59849464e-02\n",
            "  2.22496855e-03 -1.11876643e-02  1.74364198e-03  5.79715077e-03\n",
            " -1.45244533e-03  1.27629085e-02 -3.06139212e-02  1.66735027e-02\n",
            " -1.82601720e-03  1.97617635e-03  1.98570964e-02  4.70565582e-02\n",
            "  1.34742962e-02  2.26539150e-02 -1.66432861e-02  2.75686105e-02\n",
            " -4.36581789e-04 -2.32857281e-02  3.13021375e-02 -5.56084760e-03\n",
            "  4.50183832e-02  1.71598881e-02 -2.06078429e-02 -2.99305233e-02\n",
            "  8.13180219e-03 -1.17667921e-02 -2.87536265e-02  5.00085433e-02\n",
            " -1.49998182e-02  1.79808746e-02  9.11133943e-03  2.70993392e-02\n",
            " -1.51221646e-02 -9.90391385e-04  1.27182212e-02 -2.70754703e-03\n",
            " -9.67993097e-03 -1.56872434e-02 -3.01798138e-02 -2.07199135e-02\n",
            "  8.78701159e-03  1.83551237e-02  2.79560947e-02  3.43384968e-02\n",
            "  8.25056437e-03  5.23345382e-02 -1.99760071e-02 -3.32512846e-02\n",
            "  2.91745667e-02  1.75187090e-02  1.37587788e-02 -3.57665783e-03\n",
            "  3.27653882e-02  2.43149890e-02  7.46974451e-03  5.69149836e-03\n",
            " -2.89019221e-02  7.71284414e-03  2.85828863e-05  1.42826524e-02\n",
            " -7.20402015e-03 -2.35568591e-02  3.90460027e-02 -4.29690925e-03\n",
            "  5.36211935e-02  4.77791502e-03  2.20339904e-02 -1.40426943e-02\n",
            " -3.81546615e-03 -2.81485145e-03  1.66291527e-02  2.22647468e-02\n",
            " -7.22992336e-03 -3.98040059e-03 -4.79981336e-03  3.90773762e-02\n",
            " -1.13547708e-03 -2.63303975e-03 -1.74965567e-02 -2.04606210e-02\n",
            " -4.23487577e-02 -2.40616385e-03 -1.73645459e-02 -5.36537159e-02\n",
            "  1.60397062e-02 -4.99840009e-03 -3.82118185e-02  2.70118617e-03]  - intercept :  0.4867929727650515\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.04803044210441669\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:30,295]\u001b[0m Trial 193 finished with value: 0.16127216414059062 and parameters: {'count_threshold': 8, 'postag': False, 'voc_threshold': 6917}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.303251   0.34201484 0.04404389 ... 0.         0.43977911 0.01090846]\n",
            " [0.0963337  0.13677343 0.10777273 ... 0.02438165 0.37043192 0.0217286 ]\n",
            " [0.52914109 0.61517791 0.03576899 ... 0.02133395 0.3316271  0.0072723 ]\n",
            " ...\n",
            " [0.13705932 0.14500344 0.05632228 ... 0.09823826 0.282756   0.01508762]\n",
            " [0.         0.02488968 0.0495626  ... 0.         0.         0.09043815]\n",
            " [0.35422967 0.51331823 0.15514766 ... 0.06933338 0.56882949 0.0072723 ]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-0.03143831 -0.09896646 -0.02433243  0.014801   -0.03967822  0.01640904\n",
            "  0.05763639 -0.01272253 -0.01165577 -0.02670473 -0.05526421 -0.00147729\n",
            " -0.00154634 -0.04064124 -0.0555904   0.02750005 -0.00440354  0.03917719\n",
            " -0.02255811 -0.01609672 -0.03901177  0.00178511  0.01561566  0.01292654\n",
            "  0.06042557  0.06278365 -0.02305718 -0.04369793  0.03910316 -0.0302034\n",
            "  0.01908889  0.00434717 -0.03234297 -0.04100002 -0.00508882 -0.01700125\n",
            "  0.00116115 -0.03255167  0.02384012  0.05336714  0.0204952  -0.0080079\n",
            " -0.04026857 -0.03077478 -0.0075733   0.00084942  0.07340624 -0.0759809\n",
            " -0.01284506 -0.00973312 -0.01260342 -0.02989137  0.0145846  -0.07005919\n",
            "  0.02922671  0.00942207  0.00504942  0.01154196  0.02424146 -0.00720394\n",
            "  0.00715639 -0.03122164  0.02899413 -0.04048629 -0.05028756 -0.01196995\n",
            "  0.01784089 -0.01456805  0.13083605 -0.01040333 -0.01171092 -0.07421266\n",
            "  0.00233616 -0.05412468 -0.00672732 -0.07204623 -0.0187889   0.00509397\n",
            "  0.02869656  0.00697492 -0.07229527 -0.01604996  0.00261623  0.02332276\n",
            "  0.03528293 -0.07533735 -0.02484763  0.06086377  0.0234799   0.02936221\n",
            " -0.05468835 -0.03245935  0.04446428 -0.00263739  0.01921986 -0.03461429\n",
            "  0.01321917  0.00729893 -0.01253918 -0.0266754   0.02388814 -0.0724722\n",
            "  0.00308152  0.0415389   0.14406224  0.01340478  0.05834597  0.05328192\n",
            " -0.04366178 -0.00248649 -0.05729575  0.01535054  0.01278057 -0.02293993\n",
            " -0.0259453   0.02788555  0.02875859 -0.11642211 -0.01007871 -0.06665171\n",
            "  0.00904351  0.08211217  0.11087925 -0.00167977  0.00367488  0.05993458\n",
            "  0.00872822  0.01634748  0.04444519 -0.03185296 -0.03130291  0.02073866\n",
            "  0.06257434  0.00222887 -0.04573744  0.04419362  0.03919037  0.02497106\n",
            "  0.05500376  0.05495565  0.05736781 -0.01149951 -0.0630944  -0.00318489\n",
            " -0.00176001  0.03829106  0.02266146  0.00240067  0.02107681 -0.03162055\n",
            "  0.01984806  0.00252529  0.03174602 -0.065565    0.04542713 -0.00585424\n",
            "  0.05740315  0.00029554  0.06771501  0.05356813 -0.07607811  0.00919344\n",
            "  0.02633635  0.03508669 -0.00393985  0.0272504   0.00845952 -0.05980641\n",
            " -0.04105713 -0.0365403  -0.05010229 -0.00954913 -0.07361396  0.11831701\n",
            "  0.02984893  0.02638641  0.0609212   0.03277998 -0.00781415 -0.04279431\n",
            " -0.00505186 -0.00917489  0.01936047  0.02167381  0.0708984  -0.0593009\n",
            "  0.03711604  0.04171121 -0.0325497  -0.01552791 -0.06397808  0.00166092\n",
            " -0.06159382 -0.00308652  0.00531297 -0.03825446  0.02708877 -0.01542242\n",
            " -0.01919094  0.06514765  0.02186401  0.05258699 -0.03118104 -0.00619376\n",
            "  0.02263743 -0.0040261  -0.0251454   0.00534631  0.01838261  0.00861062\n",
            " -0.01559347  0.00117722 -0.04002009 -0.00056416 -0.02802753  0.01722043\n",
            "  0.00072429  0.01406749  0.00223802  0.0129206  -0.06091144  0.00616214\n",
            " -0.02272214  0.02604005 -0.04028725  0.01699457 -0.03832009 -0.0617594\n",
            " -0.02605375 -0.05352889 -0.09133412 -0.02905174  0.05013746  0.02654415\n",
            " -0.02859691 -0.05877109  0.04605534 -0.02346335 -0.04474415  0.0046722\n",
            "  0.00432031 -0.01099266 -0.03951032  0.02117202  0.00964774  0.0055634\n",
            " -0.01800241  0.02545952 -0.0014079  -0.06340631  0.01879054  0.01849144\n",
            "  0.01599463 -0.06538762 -0.05248548  0.01007847 -0.01506034  0.0057954\n",
            " -0.04475259 -0.00841612  0.01821293 -0.08168436 -0.0008275   0.00046714\n",
            "  0.06728559 -0.03589803 -0.00551086  0.01678538 -0.02597877 -0.06014746\n",
            " -0.10931158  0.00182448 -0.03000393 -0.00492006  0.01885674  0.03664581\n",
            " -0.05473542  0.01503451 -0.0182761  -0.03740614 -0.02738167  0.02096043\n",
            "  0.02360599 -0.02031962  0.00831852 -0.05753308 -0.00795279 -0.03558416\n",
            " -0.00073281  0.00260228  0.01937926 -0.03921918 -0.03374075  0.01591528\n",
            " -0.05604887 -0.03149029 -0.00236853 -0.02968518  0.09088381 -0.03394695\n",
            " -0.04550829 -0.02908719 -0.02337002  0.0180942  -0.05087432 -0.01580596\n",
            " -0.06383326  0.00095334  0.0094206  -0.04509364  0.01538998 -0.05670129\n",
            " -0.00994715  0.01596782 -0.01709715 -0.03528592  0.03650085  0.04798053\n",
            " -0.09344022  0.01871322 -0.03790058 -0.03387061 -0.00055413 -0.07668278\n",
            "  0.0049949   0.03295978  0.0109711  -0.04959789 -0.00474121  0.00083913\n",
            "  0.02506711 -0.00104213  0.00275295 -0.02031452 -0.05309042 -0.02953359\n",
            "  0.04694395 -0.0262323   0.00084053 -0.01833938 -0.02433547  0.05869073\n",
            " -0.04015687 -0.0491364   0.01572797  0.00761134  0.04490825 -0.0535054\n",
            "  0.02044439 -0.00398869 -0.01572529 -0.02876301  0.02040377 -0.04256785\n",
            " -0.10204361  0.00805364  0.00027398  0.05853476 -0.03606182  0.04827437\n",
            " -0.0145102   0.00732167  0.06276449 -0.08262078 -0.11814344 -0.01298081\n",
            "  0.00315902  0.0037431   0.02037636  0.00432441 -0.02666305 -0.01935121\n",
            " -0.00503058 -0.00354443  0.04877787  0.06395674 -0.02093424  0.01818351\n",
            "  0.06641519 -0.13225773 -0.01397506  0.02433071 -0.00886525 -0.01075162\n",
            " -0.03592042 -0.02086243 -0.04786838 -0.00313169]  - intercept :  0.9617758210183465\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.16127216414059062\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:30,865]\u001b[0m Trial 194 finished with value: -0.14789929680293268 and parameters: {'count_threshold': 7, 'postag': False, 'voc_threshold': 9389}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.61385739 0.05810709 0.78285499 ... 2.31853999 1.10868019 0.        ]\n",
            " [0.16398565 0.02855805 0.14653983 ... 0.62871841 0.30135826 0.01077659]\n",
            " [0.07841937 0.03608542 0.15627614 ... 0.         0.         0.02875112]\n",
            " ...\n",
            " [0.43789121 0.01821397 0.61918858 ... 2.31853999 1.10868019 0.        ]\n",
            " [0.4539924  0.0222833  0.61234162 ... 2.31853999 1.10868019 0.        ]\n",
            " [0.03072556 0.09692981 0.06408165 ... 0.12089337 0.11649905 0.0294444 ]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 2.15553474e-02 -1.03668322e-03  4.22024618e-02 -1.92191891e-02\n",
            " -1.74187480e-02  1.29889456e-01 -1.92233806e-02 -4.04212336e-03\n",
            "  1.59446548e-02 -1.01722979e-02  4.65429900e-03 -2.69131179e-02\n",
            "  7.76151676e-02  1.45829015e-02 -2.29557450e-02 -4.65992094e-03\n",
            "  1.18936516e-02 -2.56707348e-02 -1.09171782e-02 -1.84998001e-02\n",
            " -8.33621142e-03 -3.12398553e-02  9.74810251e-03 -2.69362673e-02\n",
            " -2.44818727e-02  3.51566514e-03 -3.77060439e-02 -1.91544481e-02\n",
            "  2.76857562e-02 -6.35427747e-04  6.19581947e-02  4.94344834e-04\n",
            " -9.24363860e-02 -3.87809743e-02  1.38784455e-02 -1.43821109e-02\n",
            " -4.44824091e-02 -1.89739641e-02 -5.18847276e-02 -3.85806407e-02\n",
            " -2.49138861e-02 -6.00573597e-03 -9.06409453e-03  1.09385978e-02\n",
            "  2.79204560e-02 -6.33429288e-02 -3.31830523e-02  2.48791011e-02\n",
            " -2.11460313e-02 -9.74592346e-03  9.74862492e-03 -1.40310583e-02\n",
            " -2.96911639e-02  1.30532844e-02  4.34913610e-02  2.68204911e-03\n",
            "  8.96512039e-03 -3.80402234e-02  2.91041671e-02  5.02488425e-02\n",
            " -4.62069570e-02 -1.44908461e-02  1.21694961e-02 -5.78363779e-02\n",
            "  2.21782128e-02 -1.72027020e-02 -3.34092187e-02 -5.43239579e-02\n",
            " -7.52696474e-02 -2.99316759e-02  1.32898918e-02  2.33415279e-02\n",
            " -5.63629897e-02  1.76730241e-02  2.47605377e-02 -3.31836130e-02\n",
            " -1.02590968e-02 -1.53847823e-02 -4.19445869e-02 -2.63371636e-03\n",
            "  4.87281057e-02  1.98642348e-02  3.49278268e-02  1.60298796e-02\n",
            " -2.42143072e-02 -7.74919689e-03 -7.22195001e-03 -4.45202659e-02\n",
            " -3.57017786e-02  5.01645011e-02  3.92951188e-02  4.54025054e-02\n",
            " -7.34502687e-04 -5.77534517e-03 -4.88585623e-02 -3.03971431e-02\n",
            " -9.21296860e-03  3.80251094e-02 -4.31013369e-02  1.13503637e-02\n",
            " -1.94000194e-02  1.73948133e-02 -3.48963779e-02  5.89596445e-04\n",
            "  2.91920229e-02 -1.11799997e-02 -1.51170675e-02 -1.09379540e-02\n",
            "  1.99380637e-02  5.93616122e-02 -6.05760011e-02 -2.41913097e-02\n",
            "  1.67818879e-02 -3.47974664e-02 -3.47974664e-02 -1.52977800e-02\n",
            " -1.28661098e-03 -1.63691844e-02 -1.75616488e-02 -1.26985896e-02\n",
            " -4.05015631e-02 -2.92877173e-02 -2.38513781e-02 -2.05193920e-02\n",
            "  2.85050297e-02 -4.13494474e-02  6.76637509e-03  2.19160068e-02\n",
            " -2.66126151e-02  6.08785302e-03 -6.47079315e-02 -3.59200689e-02\n",
            " -7.50451280e-04 -6.54954000e-02  1.63545369e-02 -6.98651460e-02\n",
            " -1.64620871e-03  5.25589061e-02 -1.21096025e-02 -6.51985290e-02\n",
            "  6.87072532e-03 -1.80660611e-02 -3.60127942e-02 -4.45958995e-02\n",
            " -3.86392398e-02 -8.93526756e-03 -1.03707196e-01  1.31082613e-02\n",
            " -4.06929874e-02 -2.65858508e-02  1.29367655e-02  1.12674894e-02\n",
            "  2.40005778e-02  1.20721464e-02 -2.70843928e-02 -7.94571277e-03\n",
            "  1.31934670e-02 -3.57366081e-02 -9.99144214e-03 -2.91042152e-02\n",
            " -6.56695502e-03 -8.71560113e-02 -1.25757411e-02  7.33988576e-03\n",
            " -2.60919397e-02 -3.54534064e-02  1.66439224e-02 -1.74556519e-02\n",
            " -5.39425676e-02 -3.91278283e-02 -7.84750109e-04 -2.85453557e-02\n",
            "  2.83363241e-02 -4.30772194e-02  1.72711583e-02 -1.89971627e-04\n",
            " -3.33451033e-02 -4.67602443e-02 -2.67879666e-02 -4.43126510e-02\n",
            "  1.60368203e-02  4.64060382e-02  3.07381048e-02 -3.99140452e-02\n",
            " -1.50763884e-03 -4.21504716e-03  6.72777863e-05  6.72777863e-05\n",
            "  5.36578668e-03  2.60368283e-02 -6.51825409e-02  4.77497217e-03\n",
            " -7.80524605e-03  1.11369821e-02 -1.60242571e-02 -5.75113098e-02\n",
            " -8.55184359e-03 -1.63410010e-02 -3.34025825e-02  2.02480101e-03\n",
            "  3.53363061e-03 -3.91138732e-02  6.07924866e-02 -6.69405488e-02\n",
            " -7.25836437e-02  8.46159790e-03 -6.83552196e-02 -6.51893479e-03\n",
            " -3.20694458e-03 -2.68120014e-02  1.24786170e-02  2.57045066e-02\n",
            " -1.06491588e-02 -4.24573128e-02 -1.59721945e-02 -1.83103000e-02\n",
            " -4.32293807e-03 -5.45606958e-03  4.21485402e-02  2.17862802e-02\n",
            "  2.90965172e-02 -8.62167142e-02 -5.41437097e-02 -3.51922112e-02\n",
            "  7.80589654e-03 -4.25076791e-02  3.06656980e-02 -2.21873596e-02\n",
            " -3.91630266e-02 -7.64582786e-02  1.63000560e-02  1.21959975e-02\n",
            " -9.07877485e-02 -3.29835670e-02  2.75922763e-02 -4.32120005e-02\n",
            " -2.76061464e-02  2.56221026e-02  2.90620353e-03 -2.89726953e-03\n",
            " -2.70067722e-02 -8.89605337e-02  1.10469588e-02 -1.06515705e-02\n",
            " -8.81872629e-03 -1.13106280e-02  4.47859453e-02  1.89288319e-02\n",
            " -1.09157835e-02 -6.70804120e-03 -2.67907193e-03 -1.35465104e-04\n",
            " -6.27062340e-03  5.50150785e-03  1.61444077e-02 -1.00248841e-02\n",
            " -3.45745970e-02  3.84665216e-03 -1.09796455e-03 -1.84782130e-02\n",
            " -2.13259660e-02 -4.38878489e-02  4.54679762e-03 -7.31523596e-03\n",
            " -3.71058675e-02  3.09730313e-03 -2.65794916e-02 -1.69573869e-03\n",
            "  3.88760040e-04 -1.00790494e-02 -9.75993533e-03 -1.64635003e-02\n",
            " -2.70017745e-02 -1.61635236e-02 -9.75127208e-03 -2.32427367e-02\n",
            " -9.81898484e-03  3.04100918e-02 -1.11474841e-02  3.04694804e-02\n",
            " -9.70613213e-03 -5.20524310e-02  6.10484263e-03  3.70202291e-04\n",
            " -2.35598765e-02 -4.18943484e-02 -3.00159346e-02 -3.92697357e-02\n",
            " -1.21605134e-02 -2.44098652e-02 -7.49994035e-03 -7.92953032e-03\n",
            " -2.86483065e-02  4.19069241e-02 -4.15289499e-02 -2.28468812e-02\n",
            "  1.12988057e-02 -9.31087274e-04 -4.13904137e-03  8.66930538e-03\n",
            " -2.21247718e-02 -4.16463605e-03  7.45796709e-03 -7.85905902e-03\n",
            "  4.14745110e-02 -1.05971495e-03 -1.01475350e-02 -7.28000828e-03\n",
            "  1.64538937e-02  1.05460534e-02 -2.66013751e-02  1.44158497e-02\n",
            "  3.87300224e-02 -2.39298071e-02 -2.90835914e-02 -1.31080855e-02\n",
            " -4.09006718e-02 -4.09615577e-02 -1.56506345e-02 -1.89136553e-02\n",
            "  1.53373860e-02 -3.12439063e-02  7.10998471e-02 -1.50743907e-02\n",
            " -4.44932133e-02 -2.57379020e-02 -1.18120230e-02 -4.47136454e-03\n",
            "  4.72803716e-03 -1.07363595e-02 -1.82234834e-02  2.84946379e-04\n",
            "  7.11618711e-04 -9.48392363e-03  6.62124567e-03 -1.13393320e-02\n",
            "  6.04394800e-03  4.04798910e-02  4.48776299e-03  9.39150299e-03\n",
            "  1.66882222e-02  2.93420441e-02  3.04539390e-02 -5.40787320e-02\n",
            " -3.15190842e-02 -1.75450550e-02  2.22737364e-02 -3.16287254e-02\n",
            "  1.26759630e-02 -2.75031160e-02 -2.51701612e-02  6.44597189e-03\n",
            " -4.98424666e-04  3.90465815e-02  2.65234108e-02 -7.45532845e-03\n",
            " -5.92895543e-03  1.56945536e-02 -1.68563190e-02 -5.65224212e-02\n",
            " -8.06276701e-03  3.20469360e-02 -1.46960719e-02 -1.46960719e-02\n",
            " -2.57516608e-02  8.55167288e-05  6.18903951e-02 -1.03379721e-02\n",
            " -6.40025546e-03 -2.57629995e-02 -2.23530884e-02 -7.48880424e-03\n",
            " -1.93899572e-02  3.65736642e-02 -1.39420442e-02 -9.82903462e-03\n",
            "  3.93766378e-02 -5.88046965e-04  1.79413614e-02 -2.49905528e-02\n",
            "  1.68538380e-03 -1.61453875e-02 -7.49559052e-02 -1.72830349e-02\n",
            "  1.41181595e-02  1.72742655e-02  4.40866803e-02  4.01481322e-02\n",
            " -2.80830991e-02  3.27075932e-02 -7.32433583e-03 -4.96816872e-02\n",
            " -3.42507947e-03 -3.18160624e-02 -8.18336078e-03 -1.60210399e-02\n",
            "  1.87151741e-02 -2.20013694e-02  1.89640810e-02 -2.90792247e-02\n",
            " -1.96474706e-02  4.31884245e-03  3.01401966e-03 -3.73193423e-02\n",
            "  4.74681496e-03  1.34024391e-02 -2.45938110e-02 -1.87263517e-02\n",
            " -2.29936887e-02  2.06516602e-02  2.97810850e-02  3.27133715e-02\n",
            "  7.13308515e-03 -1.49073547e-02 -1.50624777e-02  1.14802126e-02\n",
            " -3.95732425e-03 -5.40153376e-02  2.83596919e-03 -1.04743644e-03\n",
            " -1.90968958e-02 -6.25405608e-03 -6.02623420e-02  2.61588664e-02\n",
            "  1.66574608e-02  1.32757164e-02 -3.74820715e-02 -8.39004459e-03\n",
            " -2.25112766e-02  6.12725100e-02  3.32985431e-02 -3.35227444e-02\n",
            " -3.40249868e-02  4.79401994e-03 -1.69541676e-03  1.02003658e-02\n",
            "  1.02003658e-02  1.32791058e-02 -1.15373509e-03 -1.46720947e-02\n",
            " -2.58993797e-02  3.22082907e-02  9.69347261e-03 -2.03373587e-02\n",
            "  3.57066634e-02 -1.07368249e-02  9.68913008e-03 -7.74075824e-03\n",
            "  8.39680441e-03  1.65321237e-02 -1.40647403e-03  4.23563599e-02\n",
            " -1.03927814e-02 -1.61176532e-02 -5.02369324e-03  1.56314131e-03\n",
            "  1.13166646e-02 -4.21036042e-03 -5.21215222e-02  9.12863813e-03\n",
            "  3.02007051e-02  1.54866898e-02 -3.34086545e-02  2.92392513e-02\n",
            " -2.66683514e-02 -1.02660445e-02 -3.65623492e-02  6.01070989e-02\n",
            "  2.70137130e-02  1.14385525e-02 -1.43078215e-02 -1.26189972e-02\n",
            " -1.86617831e-02  3.45582946e-03 -2.33326293e-02 -2.87610887e-03\n",
            "  7.78691292e-03  1.46455858e-03 -2.93813544e-02  3.21076189e-02\n",
            " -7.42645207e-03 -5.43368203e-02  8.08873696e-02  4.02673797e-02\n",
            " -1.64326571e-02 -7.08657841e-03  2.58477169e-02  2.92432288e-02\n",
            " -1.71410840e-02  1.57951276e-02 -9.73674735e-03  1.36231739e-02\n",
            " -8.82719744e-03  7.17504096e-03 -1.69989296e-02  2.47519779e-02\n",
            "  2.08296801e-02 -6.03203139e-03]  - intercept :  1.0808274530924686\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.14789929680293268\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:31,390]\u001b[0m Trial 195 finished with value: -0.25974461180913877 and parameters: {'count_threshold': 10, 'postag': True, 'voc_threshold': 1493}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.0469388  0.06130747 0.04596237 ... 0.24097535 0.04222923 0.02488367]\n",
            " [0.70411303 0.83324201 0.0349626  ... 0.         0.55859392 0.        ]\n",
            " [0.2176838  0.27986245 0.0667341  ... 0.07538337 0.66608016 0.01692547]\n",
            " ...\n",
            " [0.         0.03408596 0.04044987 ... 0.34671868 0.01279063 0.04942967]\n",
            " [0.26954004 0.391801   0.03627101 ... 0.08615242 0.71754074 0.02125801]\n",
            " [0.03624421 0.02725    0.05293235 ... 0.18092009 0.06334385 0.02556347]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 3.55592740e-01  2.63559604e-01 -7.03149857e-03  9.60380023e-02\n",
            "  1.31710438e-01 -9.62378261e-02  9.80501450e-02  2.28739258e-01\n",
            "  7.48426609e-02  5.34722249e-02  1.17704471e-01 -1.17843817e-01\n",
            "  4.00564740e-02 -2.70983519e-01 -3.53139984e-01  1.24647117e-01\n",
            "  2.25203795e-01  2.66145095e-02 -1.90142640e-02 -1.45686699e-01\n",
            " -2.26050924e-01 -5.92262666e-02 -4.18087250e-04 -4.12672460e-02\n",
            "  1.77713236e-01  7.34285109e-02  9.75493454e-02  1.63303994e-01\n",
            " -2.44673586e-02 -1.22303619e-01 -4.40075989e-01  2.80014974e-01\n",
            " -1.47685102e-01 -1.25126212e-01  5.37329227e-02  1.13887982e-01\n",
            "  7.88758303e-02  1.30390797e-01  2.37602804e-01 -1.20948326e-02\n",
            " -1.25709883e-01 -4.71058293e-01  3.12425729e-01 -1.48288665e-01\n",
            " -6.95102760e-02 -1.60158866e-01 -3.12907568e-01 -2.58896841e-01\n",
            "  2.12937878e-01 -1.04839176e-03  1.19380301e-01  3.14151925e-01\n",
            " -4.92715041e-02  1.08942033e-01  1.11769909e-01 -1.94898319e-02\n",
            " -1.41879552e-01  1.82293280e-01 -7.27825841e-02 -2.34588242e-01\n",
            " -1.02108945e-01  1.28806728e-01  1.26995944e-01 -2.78706972e-01\n",
            "  1.60362032e-01  4.06447530e-02  5.34383159e-02  3.15147346e-01\n",
            "  5.46252995e-01 -1.02250166e-01  2.72610249e-01 -1.06633890e-01\n",
            " -1.33985442e-01 -3.71901014e-01  1.49794210e-02  3.87839171e-01\n",
            "  1.06951013e-01 -1.80107487e-02  1.11179499e-01 -1.62630101e-01\n",
            " -1.25192182e-01 -1.77571592e-01 -4.39131039e-02 -1.91216876e-01\n",
            "  1.55480055e-01  6.74871914e-01 -1.00702264e-01 -1.67390591e-01\n",
            "  1.50970990e-02 -4.59778253e-01 -1.73587235e-02 -1.76211044e-01\n",
            " -1.13952807e-01  4.57226642e-01 -5.25200499e-02 -2.12304336e-01\n",
            " -1.15382568e-01  1.97438461e-01 -1.42266041e-02  1.79351689e-01\n",
            "  1.77364224e-01  3.62866129e-02 -2.73676287e-01 -4.08214538e-01\n",
            " -4.72196738e-01 -1.42969029e-01  2.79049622e-01  3.94998377e-01\n",
            " -3.96644281e-01 -4.80711969e-01 -1.57581835e-01  1.07297105e-01\n",
            " -5.78627050e-01 -4.93653445e-01 -3.72219324e-01 -5.43232920e-01\n",
            "  2.34404141e-01 -5.91038521e-01 -1.80740511e-01  2.43171955e-01\n",
            " -2.10355789e-02 -1.90091678e-03  1.59261521e-02 -3.17579940e-01\n",
            " -3.37471504e-01  1.48600422e-01 -5.28436767e-01 -7.42137129e-01\n",
            "  1.21327158e-01 -3.57024617e-02 -1.61051617e-01 -5.31692577e-01\n",
            " -3.75372132e-01  1.57042731e-01 -8.86047879e-02  4.28517686e-02\n",
            " -7.33727199e-02  2.75157962e-01  2.31633524e-01 -9.86581856e-02\n",
            " -6.95139064e-03 -1.62069469e-02  2.03162952e-02  1.04794735e-01\n",
            "  1.86619049e-01 -4.19409804e-02  4.90805214e-02  4.37664251e-02\n",
            "  9.40285291e-02  9.40200039e-02 -1.36399908e-01 -2.29685447e-01\n",
            " -2.26289834e-03 -4.63448035e-02 -1.09648270e-01 -1.02943581e-01\n",
            "  4.29602239e-02 -1.55344370e-01 -1.96216960e-01  1.41773307e-01\n",
            "  2.85802909e-02  9.63531862e-02 -2.13768684e-01  3.04383359e-01\n",
            "  9.34157207e-02  1.38586359e-01 -1.47243483e-01  1.08203103e-01\n",
            "  1.58347163e-01 -1.37172286e-01  1.30132026e-01 -1.90630526e-01\n",
            " -2.83191643e-02 -9.84474797e-02 -4.27898929e-02 -6.50387418e-03\n",
            " -2.04720841e-02  1.93410010e-01  7.71816391e-02  2.90895854e-02\n",
            " -3.52268044e-01  3.84616054e-02  2.92804992e-01  7.34721129e-02\n",
            " -2.34940899e-01  3.60329310e-01  8.80781167e-02  1.10327866e-01\n",
            "  1.88552836e-01 -3.60674721e-02 -3.19632991e-02  1.39496495e-01\n",
            " -1.22621079e-01 -3.66487251e-02  7.85067778e-02  7.72327597e-02\n",
            "  3.61122429e-01  4.71155837e-01  2.95205809e-02  1.22683605e-01\n",
            "  2.17863710e-01 -5.49436744e-02  1.94995349e-01  1.99998404e-01\n",
            " -3.16113355e-01  9.85140148e-02  8.18542762e-02 -1.86958807e-01\n",
            "  1.13848565e-01 -1.85281478e-01 -1.24485163e-01  1.79986162e-01\n",
            "  3.79855485e-02  3.42052443e-01  2.28548538e-01  2.23539095e-01\n",
            " -1.70675134e-02  8.62707914e-02  2.80927750e-01  2.16929295e-01\n",
            "  1.79189632e-01 -4.54798195e-01 -4.77572792e-01 -9.27788796e-02\n",
            " -2.80357857e-01 -2.65658646e-01  9.39295605e-02  2.54162219e-01\n",
            "  1.93656776e-01  1.00968835e-01 -1.63035940e-01  3.04357722e-01\n",
            " -3.92626155e-02 -2.34301004e-02  8.89644795e-02 -6.91947498e-02\n",
            " -9.32182404e-02  5.44642795e-02  2.17324786e-01  3.73677799e-01\n",
            " -7.64827912e-03 -3.16953711e-01  2.18088741e-01  6.93614469e-02\n",
            "  3.17927852e-01 -1.58352556e-01 -2.08057883e-01  2.17461541e-01\n",
            "  4.96153195e-02  3.27293849e-02 -1.22709732e-01  1.92451481e-01\n",
            " -2.42131881e-01  9.34926103e-03 -3.08015046e-01  9.85310975e-02\n",
            " -1.62109893e-01 -3.61472562e-01 -1.07645143e-02 -7.46326792e-02\n",
            "  3.13858677e-01  6.03755529e-02  3.33563077e-01 -3.17443801e-01\n",
            " -9.43754413e-02 -2.98987369e-02 -6.08837634e-02  2.15186966e-01\n",
            "  3.59517030e-02  2.32675402e-01 -2.05622267e-01  1.26744988e-01\n",
            "  9.86924212e-02 -2.25579215e-02]  - intercept :  0.8994609077100229\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.25974461180913877\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:31,915]\u001b[0m Trial 196 finished with value: 0.04052828738135897 and parameters: {'count_threshold': 9, 'postag': False, 'voc_threshold': 1779}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.         0.02955699 0.02656784 ... 0.06032316 0.1272676  0.07594893]\n",
            " [0.0216503  0.01935279 0.04213534 ... 0.13643138 0.20162683 0.04338199]\n",
            " [0.40657859 0.07405428 0.6698772  ... 0.09158476 0.10718708 0.01408971]\n",
            " ...\n",
            " [0.01228691 0.03790474 0.03156946 ... 0.20170425 0.21183185 0.05363759]\n",
            " [0.51594079 0.14138105 0.71783942 ... 0.11448094 0.07695183 0.0132091 ]\n",
            " [0.         0.04036715 0.         ... 0.06032316 0.1073257  0.03543952]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 1.32281340e-03 -1.80445970e-02  3.27036543e-02 -1.57254352e-02\n",
            "  3.28579690e-02 -6.86747098e-03  3.65250040e-02  1.69750909e-02\n",
            "  2.02076680e-03  1.27999672e-04  3.52730870e-02  4.32233596e-02\n",
            "  1.36108609e-02 -4.27533521e-03  4.13816986e-02  1.42769721e-02\n",
            " -1.99296180e-02  7.46190846e-02  7.03821164e-02 -6.80317772e-03\n",
            "  9.02025373e-02 -1.06917982e-02  8.59710583e-02 -1.93095659e-02\n",
            "  6.26782864e-02  5.28339346e-02 -5.46670506e-02 -1.50585756e-02\n",
            " -2.33471330e-02 -1.25161659e-02  8.86518004e-02 -8.84399132e-02\n",
            " -2.94778834e-02 -1.55179425e-02  6.11668016e-02  2.74116534e-02\n",
            "  2.65536902e-02 -4.93956867e-02  4.34822289e-02  1.77223218e-01\n",
            "  2.74500480e-02  2.75468326e-01 -3.23876063e-03 -5.62414593e-02\n",
            "  1.41952818e-02  3.64153056e-02 -1.16551005e-02 -2.22277489e-02\n",
            "  7.59132329e-02  1.25115761e-01  4.59881908e-02  4.08416768e-02\n",
            "  5.38187341e-03 -4.51341016e-02 -2.58727656e-02  9.36656664e-03\n",
            " -1.06815999e-01 -1.64257511e-02 -6.71529521e-02 -1.79717273e-02\n",
            "  9.32912674e-03  1.36760560e-01  1.21415822e-03 -3.71467486e-02\n",
            " -8.64929356e-02 -2.16947131e-02  2.11664541e-02 -5.90225216e-02\n",
            "  5.70441558e-02 -5.39730054e-02 -2.04854535e-02 -3.66089212e-02\n",
            "  6.59494842e-02  1.43101613e-01 -5.82495571e-02  1.58598434e-01\n",
            " -1.31118996e-02  3.12025136e-02  1.04991974e-01  8.40459552e-02\n",
            "  3.10141832e-02  3.25909458e-02  4.67169048e-02  2.50374830e-02\n",
            "  1.18283205e-02  1.25604486e-02 -3.65686431e-02  2.58831112e-03\n",
            "  3.86368226e-02  4.28472678e-02  6.35685014e-02  8.69197042e-02\n",
            "  7.34105794e-02  1.54140627e-01 -4.46517428e-03 -1.95928740e-02\n",
            "  2.99624742e-02  3.22024502e-02  1.32380501e-01  1.21465359e-01\n",
            " -4.77893025e-02  1.79518741e-02  5.13336903e-02  8.65554956e-02\n",
            " -7.98436792e-03 -3.82473523e-03  2.48306843e-02  7.29223658e-04\n",
            " -1.88736026e-02 -2.45955334e-01  1.16817312e-01  5.87115247e-02\n",
            "  7.80438039e-02  4.68513482e-02 -7.37991926e-02  2.21034295e-02\n",
            "  9.42297849e-02  1.57555689e-01  9.35512587e-02 -3.97393241e-02\n",
            "  1.04535831e-02  3.77206988e-02  1.21980155e-01  6.98925501e-02\n",
            " -4.46891825e-02  2.83436127e-02  6.30055911e-03 -6.41520761e-02\n",
            "  3.24882923e-02  7.41510513e-02 -5.92200249e-02  7.93448205e-02\n",
            "  2.56107623e-02  1.02160882e-02 -1.00023699e-01  3.64235206e-02\n",
            " -3.14119669e-03  3.99764040e-02  3.11625597e-02  2.89654637e-02\n",
            "  7.17681524e-02  1.05651742e-01 -6.44080862e-02  1.42844636e-01\n",
            "  9.50142686e-02  1.62786093e-01 -1.67102187e-02  6.97833552e-02\n",
            "  2.83492002e-02 -4.22487041e-02  9.59772928e-02  2.17539259e-01\n",
            "  1.64200106e-02 -3.53408954e-02  4.87923516e-02 -5.26872635e-02\n",
            "  1.24681658e-02  1.14194077e-01  1.19491335e-01  2.64811405e-02\n",
            " -8.64913729e-02  1.52863868e-01  7.52702164e-03 -7.99074005e-03\n",
            "  3.08515153e-02  8.43070157e-02  4.07332996e-03 -1.68373750e-02\n",
            " -4.92156798e-02 -2.21677180e-02 -1.35974445e-02  1.49115891e-02\n",
            "  4.13054169e-02  1.06965040e-02 -1.22722802e-02  1.84599144e-02\n",
            "  7.78260187e-03 -5.69856198e-03 -2.09146706e-03 -9.67157084e-02\n",
            " -4.20989309e-02 -1.11526522e-02  1.43111812e-02  3.15761197e-02\n",
            " -3.29984542e-02  1.51101294e-03  4.13333189e-02 -3.44835812e-03\n",
            "  3.60661926e-02  3.53389910e-02  3.40623260e-02 -3.50098564e-03\n",
            "  2.06457958e-03 -1.94678320e-02  2.89783816e-04  3.56899586e-02\n",
            "  3.42755102e-02 -4.18326017e-02  2.12074425e-03 -6.36399701e-03\n",
            "  4.26177036e-02  8.55041625e-04 -4.50221362e-02  2.34536022e-02\n",
            " -6.79969470e-03  9.31198536e-03 -5.32733832e-02 -3.63508340e-02\n",
            "  4.66795350e-02 -3.29245489e-02  3.65015304e-02  2.40234922e-02\n",
            " -4.48515310e-02 -8.02679567e-03 -1.17383468e-02  4.31017136e-02\n",
            " -4.49249320e-02 -1.79561878e-02 -7.81852141e-03  1.85722999e-02\n",
            " -1.33695949e-02  2.43804846e-02 -1.09702259e-01 -4.11985322e-02\n",
            " -5.62163826e-03 -5.74021792e-02 -2.58327275e-02 -8.80586873e-03\n",
            "  2.12769695e-02 -1.35334376e-01  8.02419141e-03 -4.08849998e-02\n",
            "  8.90404675e-03 -2.15904796e-02  2.90809286e-02  3.07662739e-02\n",
            "  1.12257467e-02 -5.42637664e-02 -6.93116438e-02  5.26207469e-03\n",
            "  1.14168651e-01  1.32662975e-02 -7.92878612e-02  4.85612667e-03\n",
            " -5.82208375e-04 -5.00921490e-02  3.53485447e-02  4.42663737e-02\n",
            "  1.48611897e-03 -1.02583235e-01 -2.99288110e-02  2.00701937e-02\n",
            "  1.61028467e-03 -1.61104194e-02 -8.33668456e-03  3.10153147e-02\n",
            "  2.22393377e-02 -2.71936268e-02  1.16621936e-01 -2.05214578e-02\n",
            " -5.22942924e-02  5.18202914e-02  3.09906624e-02  4.23479259e-02\n",
            "  1.68432118e-02  1.00569828e-01 -2.44430189e-02  1.30280457e-02\n",
            " -2.17581281e-02 -2.34097273e-02 -1.21275288e-01  4.98846799e-02\n",
            " -8.37242477e-02 -1.19850814e-02  1.13858591e-03  4.24502352e-02\n",
            " -4.51669163e-03  3.78970311e-02  5.43544448e-02  2.38915496e-02\n",
            "  3.85732092e-02 -2.72580669e-02 -2.67453983e-02  2.70171022e-02\n",
            " -8.43631803e-02  6.06333070e-02 -3.96123793e-03  2.99207165e-02\n",
            "  3.23511912e-02 -8.72097082e-02  3.33643051e-02  1.44134018e-02\n",
            "  1.34088971e-02  1.59595614e-03 -4.54613200e-02  4.35123710e-02\n",
            " -4.59922540e-02  3.09260473e-02  5.18319278e-02  1.25450564e-03\n",
            " -1.19146113e-01  6.45248252e-02 -3.10171787e-02  6.32473605e-02\n",
            "  3.28463883e-02  2.30790207e-03 -6.20796795e-02  1.15643570e-02\n",
            " -1.51443668e-01 -7.84497105e-02 -2.60867193e-02  6.50324831e-02\n",
            "  2.01798568e-02 -6.43097256e-03 -3.00168035e-02  6.41292287e-02\n",
            "  8.79654015e-02  9.32936522e-02  3.96361427e-02  6.45401964e-02\n",
            " -1.18095785e-01  2.38808530e-02  2.44616344e-02 -2.13891363e-02\n",
            "  2.94003777e-02 -3.81625706e-02 -5.06650014e-02  5.17373250e-02\n",
            "  4.50738958e-02 -6.47305244e-02 -1.30386381e-02  4.81593111e-03\n",
            "  4.42564076e-02 -8.45551093e-03]  - intercept :  0.06433041476795531\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.04052828738135897\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:32,470]\u001b[0m Trial 197 finished with value: 0.32107147321159524 and parameters: {'count_threshold': 9, 'postag': False, 'voc_threshold': 6479}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[3.14542268e-02 6.16388960e-02 2.38257564e-02 ... 2.46414679e-01\n",
            "  1.36360973e-01 2.49881031e-02]\n",
            " [5.84618667e-04 2.53563778e-02 4.08368658e-02 ... 2.01358213e-01\n",
            "  7.62594403e-01 4.71256732e-02]\n",
            " [4.20189424e-02 2.05264350e-02 4.11129374e-02 ... 1.26157306e-01\n",
            "  1.20951004e-01 2.24790670e-02]\n",
            " ...\n",
            " [1.51541722e-02 4.82344123e-02 1.13777473e-01 ... 8.52290932e-02\n",
            "  4.90269654e-02 2.53109568e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 5.13421077e-02\n",
            "  8.94936669e-02 7.43374775e-02]\n",
            " [2.78384544e-02 1.30548419e-01 1.73577646e-02 ... 3.03607136e-01\n",
            "  1.73218676e-01 1.87500847e-02]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 0.00643846  0.0276249   0.00555879 -0.07057312 -0.02657884  0.01048894\n",
            " -0.00765541 -0.15975726 -0.07776029 -0.03523765 -0.07009709 -0.03323036\n",
            " -0.06160421 -0.04391427 -0.06037646 -0.04951128 -0.05208273 -0.06478786\n",
            " -0.09954986 -0.07398962  0.06762692 -0.03651641  0.00459941 -0.11437441\n",
            "  0.02766508  0.02924876  0.16589342  0.01392874  0.1897224  -0.03375091\n",
            " -0.04590963  0.03326991  0.03979974  0.07667146 -0.09952193 -0.04229315\n",
            "  0.06528521 -0.15996286 -0.05108902 -0.12975864  0.02175687 -0.01523637\n",
            " -0.02061756  0.09165108  0.11358824 -0.03632462  0.07438689 -0.0078953\n",
            " -0.06147297  0.16957435  0.0560736  -0.08410694 -0.03256272 -0.07055559\n",
            "  0.0651137  -0.01269842 -0.0736304  -0.23012706  0.06975038  0.04628395\n",
            " -0.02788909 -0.07716367  0.02284495 -0.05172215 -0.02210134  0.07960389\n",
            "  0.08540062  0.21496474 -0.07573185  0.01795765  0.01657711  0.07082695\n",
            " -0.0623076  -0.0022223  -0.0469262   0.04887599 -0.02694222 -0.01298297\n",
            "  0.10941491  0.07120658 -0.01732867  0.04991126  0.00841878  0.06454276\n",
            " -0.20758347  0.0319622   0.04592609  0.03248197  0.02970498  0.02092866\n",
            " -0.0182954   0.07310455 -0.03844217 -0.08690502  0.01369129  0.03895583\n",
            " -0.08889403  0.00448987 -0.0350169   0.06465798 -0.16546586 -0.1985243\n",
            " -0.02743827  0.21820379  0.00740969  0.1389502   0.15632383  0.02848456\n",
            "  0.01420075  0.00593659 -0.07508789  0.02999008 -0.00980394 -0.06725265\n",
            " -0.09842034  0.09630934  0.16506138 -0.18191581 -0.04115273  0.16005111\n",
            "  0.09876629  0.03969083 -0.00364814 -0.11306583  0.15463422  0.03148149\n",
            " -0.05938708  0.01164006 -0.06684515 -0.09939605 -0.05343279  0.11356907\n",
            " -0.09402094 -0.0255758   0.07866368  0.01955029 -0.04323471 -0.10331434\n",
            "  0.03312105  0.03552443  0.05402266  0.0635592  -0.00218118  0.00119393\n",
            " -0.00095474  0.04019228 -0.13261808 -0.02027828  0.05807617 -0.03211592\n",
            "  0.02862771  0.00117568 -0.14609007  0.02577019  0.04766183  0.01805836\n",
            "  0.12674575  0.00939935  0.09828266  0.16295336  0.01763076 -0.00504856\n",
            "  0.0117971   0.0171489  -0.05507565  0.05035029 -0.10204112  0.04889919\n",
            " -0.074693    0.01794464 -0.06453948 -0.04029756 -0.06976357  0.01361249\n",
            "  0.02769635 -0.08445677 -0.0480914  -0.09795385 -0.01029352 -0.0398869\n",
            " -0.01338115 -0.0013097   0.0459178   0.04011636 -0.07158333  0.05291027\n",
            "  0.01544417  0.07329855 -0.01771485  0.09923014 -0.0708427  -0.10455167\n",
            "  0.07283321  0.06124635  0.0229927  -0.14597693  0.05697652 -0.05424908\n",
            " -0.0326723  -0.02201543 -0.02479829  0.04270952 -0.03611989  0.05083041\n",
            "  0.00892176  0.06439497 -0.07407599 -0.05416819  0.01943986 -0.06868341\n",
            "  0.0600729   0.05919198  0.13310129 -0.00687755  0.11272737 -0.06872226\n",
            " -0.01282195  0.0229066  -0.00879892 -0.08860327 -0.12663623  0.03857206\n",
            "  0.00316707  0.08787372 -0.0254347  -0.08189295  0.03192283  0.002536\n",
            " -0.05751539  0.00366434  0.11378659 -0.04526508 -0.08293677 -0.10519648\n",
            " -0.04118808  0.01172545  0.03781308 -0.02821858  0.01707296  0.06344775\n",
            "  0.0286987  -0.01054988 -0.05563849  0.01904732 -0.10592133  0.02168312\n",
            " -0.07686912  0.03179499 -0.13212259  0.09217727  0.0317871  -0.00383446\n",
            "  0.05059766 -0.01398836 -0.08229448  0.03814085  0.02310193 -0.10870831\n",
            "  0.18347576 -0.08936194  0.06263512  0.09455838 -0.05743888  0.04849285\n",
            "  0.01332158  0.06946247 -0.10422006  0.04916872  0.00033697 -0.00076143\n",
            "  0.07539691 -0.12378869 -0.10700559 -0.01641822  0.07135631  0.00585635\n",
            "  0.11911083  0.05698368  0.01634776  0.02969899  0.09375384  0.0945401\n",
            " -0.002919    0.01823595 -0.06508157  0.06451757 -0.15307874  0.03064853\n",
            "  0.11306571  0.00228359  0.06464262 -0.10686687  0.08596331  0.04748185\n",
            " -0.04704685  0.07822767 -0.10366604  0.01258372 -0.06244389 -0.12048319\n",
            " -0.03118964  0.07975367  0.11679906 -0.04906201  0.01747133  0.01794439\n",
            "  0.00355428  0.19027502  0.10186042  0.0214122   0.052626   -0.01743819\n",
            "  0.03829316 -0.0006294   0.03225557 -0.05738193 -0.05366285 -0.05573525\n",
            "  0.01912372 -0.01317726  0.13185361 -0.00076545]  - intercept :  0.6056925955438561\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.32107147321159524\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:33,062]\u001b[0m Trial 198 finished with value: -0.25018958782494694 and parameters: {'count_threshold': 9, 'postag': False, 'voc_threshold': 6469}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.         0.01009769 0.03243157 ... 0.06490493 0.22965543 0.04234174]\n",
            " [0.         0.00757327 0.01915401 ... 0.06675936 0.         0.04414302]\n",
            " [0.00812738 0.03092456 0.02640873 ... 0.21666038 0.06844865 0.02553417]\n",
            " ...\n",
            " [0.         0.03260739 0.0073711  ... 0.03461596 0.01542565 0.06948418]\n",
            " [0.0605736  0.04894726 0.0982358  ... 0.12946048 0.05314708 0.04085791]\n",
            " [0.23069997 0.08354715 0.34260492 ... 0.09223421 0.48806993 0.01393086]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-0.0006854  -0.01279986  0.00519227  0.0248366  -0.00782368 -0.06437034\n",
            " -0.01970502 -0.07657647 -0.06277666 -0.0552175  -0.03287477 -0.02633682\n",
            "  0.03256468 -0.08384011 -0.01709639 -0.07662182 -0.05825463 -0.02170703\n",
            " -0.04622913 -0.03113823  0.02828477 -0.04547854  0.00093987 -0.04184107\n",
            " -0.01384298 -0.05981122  0.02209301  0.01116108  0.03519862 -0.06155416\n",
            "  0.03609069  0.01526912 -0.00371398 -0.08637873  0.04197509 -0.0084125\n",
            " -0.07854638 -0.02328312  0.0125824   0.01914447 -0.02887142  0.09543489\n",
            " -0.01905559  0.00796176 -0.13015632 -0.02586038  0.00035241 -0.01271326\n",
            "  0.01698693  0.08982078  0.00034425  0.01140214 -0.12649317  0.06300701\n",
            "  0.00165301  0.03823727 -0.09401176 -0.02372853  0.01509354 -0.11325572\n",
            "  0.10383716  0.0307139  -0.00924672 -0.02257787 -0.09920366 -0.03883239\n",
            "  0.03929459 -0.00466047  0.03484852 -0.06252321 -0.01212354 -0.17497824\n",
            " -0.08445773 -0.08453496  0.05242056 -0.00637552  0.15325981 -0.00479866\n",
            " -0.04475184  0.07615248  0.01559321  0.04384671 -0.01595285 -0.03153899\n",
            " -0.11285871 -0.02455206  0.05539994 -0.0686066  -0.03392337 -0.15285513\n",
            " -0.0230839   0.05941326 -0.0102585   0.07361645 -0.06039162 -0.02966578\n",
            "  0.09818386 -0.03104078  0.12487025  0.01250929 -0.02809663 -0.01916736\n",
            " -0.02373222  0.03071389 -0.00414475 -0.06574125 -0.09246701 -0.14163992\n",
            " -0.04434184 -0.01844302 -0.02178733 -0.01084695 -0.10179683 -0.01135443\n",
            "  0.01876626 -0.11515623  0.08135577 -0.05488333 -0.03406456  0.02271878\n",
            " -0.16435495  0.01878087  0.02804375  0.02374592  0.01097157  0.02833866\n",
            "  0.00182144  0.04563001 -0.01016597 -0.05901227  0.04403642 -0.00369009\n",
            " -0.00605841  0.01790679  0.06634647 -0.07331019  0.05318964  0.04380146\n",
            "  0.06123797  0.05387513 -0.03226865  0.04468464 -0.08571894  0.01796496\n",
            " -0.00342461 -0.06740222 -0.04240853  0.06793047 -0.06918786 -0.17539021\n",
            " -0.12569544  0.03483338  0.08896113 -0.07151853 -0.0332986  -0.15146171\n",
            " -0.0391686   0.0867268  -0.05613687  0.04218801  0.02773074 -0.0638385\n",
            "  0.00974216 -0.02415306  0.03175715 -0.03893244 -0.15836896 -0.02612614\n",
            "  0.02633465 -0.01692825 -0.01692825  0.06819069 -0.036354   -0.01413291\n",
            "  0.02257062  0.01847225  0.02410861  0.00793171 -0.09898967 -0.08523128\n",
            " -0.02581269 -0.00227382  0.02056963  0.00647361 -0.06525999 -0.08870482\n",
            "  0.09462898  0.02944782 -0.03113939 -0.03781114  0.00281347 -0.0620581\n",
            "  0.03157269  0.02720647 -0.02584047  0.00739336 -0.00872234  0.02012056\n",
            "  0.07524348 -0.01784919 -0.03145565 -0.00145884  0.05622442  0.01894551\n",
            " -0.0160138  -0.02216464  0.07078972  0.0506708  -0.0103361  -0.00765016\n",
            "  0.02711696 -0.05630124  0.08274166 -0.00108903 -0.00963142 -0.02821795\n",
            "  0.01864919 -0.02037289 -0.08259807 -0.00417202  0.06159694  0.12288076\n",
            "  0.00237607  0.04124806 -0.0861776  -0.02123679  0.02665405 -0.06444446\n",
            " -0.02350719  0.04493245 -0.05657061  0.05457422 -0.05475344  0.01496826\n",
            " -0.03293203 -0.00103212  0.10438101 -0.01696566  0.00824725  0.04821136\n",
            " -0.04699479 -0.05903319 -0.01431323 -0.00521354  0.03094592  0.00641091\n",
            " -0.00570297 -0.00303531  0.0360665  -0.00135785  0.07959593  0.08379672\n",
            " -0.16766523  0.07520966  0.07965784  0.17056829  0.01473529  0.05326598\n",
            "  0.01177196 -0.00966465  0.14639565 -0.0972847  -0.12467751 -0.07597367\n",
            "  0.04544855 -0.016661    0.01014704 -0.11376239 -0.05855334 -0.06022674\n",
            "  0.00048374 -0.00897519  0.10276225 -0.06349671 -0.08751224 -0.05606586\n",
            " -0.00544823  0.01357621 -0.04079782 -0.04380446  0.03848512 -0.01615562\n",
            "  0.00277942  0.03162077  0.01964497  0.10235865  0.06016038 -0.02464636\n",
            " -0.02483758 -0.05659755  0.05682944  0.0194545  -0.03841716 -0.0023412\n",
            "  0.08076299 -0.04180842  0.00713728  0.06997556  0.02061998 -0.07747633\n",
            " -0.07390676  0.0207557  -0.02688269 -0.08803083 -0.00821577  0.01065355\n",
            "  0.04807293  0.00162808  0.06216778 -0.01636846 -0.0394367   0.04437267\n",
            " -0.02456376 -0.06910101  0.05585545  0.08552836 -0.00819747  0.04413825\n",
            " -0.16826401 -0.01666147 -0.0637962  -0.00611485  0.04118934 -0.05727443\n",
            " -0.02783268 -0.00192095 -0.01669478 -0.04839049  0.07479437 -0.10442683\n",
            "  0.09271621  0.02211674 -0.13759719  0.16104191  0.10630832  0.03911369\n",
            "  0.05347314  0.01010232  0.01371263 -0.14513867 -0.00152461 -0.01544866\n",
            "  0.01492993 -0.07586046 -0.07586046  0.03457711  0.08758896 -0.00052974]  - intercept :  0.7954318418788562\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.25018958782494694\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:33,632]\u001b[0m Trial 199 finished with value: 0.05438533365246619 and parameters: {'count_threshold': 9, 'postag': False, 'voc_threshold': 6528}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.         0.01257221 0.         ... 0.0452394  0.13121314 0.08732864]\n",
            " [0.12149315 0.11940868 0.06524021 ... 0.03001123 0.         0.02301128]\n",
            " [0.32167195 0.35756599 0.01077661 ... 0.32780909 0.11481149 0.02016105]\n",
            " ...\n",
            " [0.0370147  0.02141529 0.09277359 ... 0.19469409 0.10607736 0.02724568]\n",
            " [0.42103101 0.45709219 0.11528212 ... 0.54319162 0.         0.00612566]\n",
            " [0.21561792 0.28877215 0.09765934 ... 0.2493711  0.23997166 0.017107  ]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-2.94971632e-02 -8.98377684e-02  1.42552052e-02 -6.08959656e-02\n",
            " -8.38540375e-04 -4.54413235e-02  9.30896620e-02 -3.65737855e-02\n",
            " -5.50488136e-02 -6.78007389e-03 -3.47042879e-02  2.51053457e-02\n",
            " -7.32521643e-02 -1.06079097e-02  3.49028539e-04  2.74151801e-02\n",
            " -3.45590374e-02 -2.94714439e-02  3.76364133e-02  5.51749261e-02\n",
            " -1.81540728e-02 -4.66042651e-02 -7.96189361e-02 -1.59214775e-01\n",
            "  2.24956090e-02 -3.49797499e-02  2.12946900e-02 -4.01421650e-02\n",
            " -8.70655668e-02  2.79272369e-02  2.73320852e-02 -6.54694166e-02\n",
            "  6.45939200e-02  1.83228543e-02 -3.57427835e-02  5.03409005e-02\n",
            " -7.02435507e-02 -1.35960053e-01 -9.82318333e-02 -1.83258875e-02\n",
            " -7.82746656e-02  7.46260715e-02 -3.37665102e-02 -5.10337862e-02\n",
            " -8.49670525e-03  2.30905295e-02  3.13477116e-02  1.34591566e-03\n",
            "  2.27791326e-02 -1.02842062e-02 -6.41343304e-02 -6.31689459e-03\n",
            "  2.95727590e-02 -8.87191616e-02  7.03816486e-02 -1.06140932e-01\n",
            "  1.51511493e-02  6.49540649e-02  6.46761730e-03  1.51186881e-02\n",
            " -2.14726291e-02  2.52708046e-02 -5.48720645e-02  2.29804352e-02\n",
            "  7.21112278e-02 -5.69543102e-02 -3.17342810e-02  1.14524803e-02\n",
            "  1.43731893e-01 -1.07669823e-01 -8.33636586e-02 -1.78256744e-04\n",
            " -1.67621116e-02 -1.06937409e-01  8.34053823e-02  3.95376547e-02\n",
            " -7.73985478e-02 -1.19323463e-01 -2.73680479e-02  2.59510657e-02\n",
            " -1.18157765e-03 -2.81320067e-02 -1.39211442e-01  3.06152881e-02\n",
            "  1.17539093e-02 -1.39202783e-01  3.00465769e-02 -4.13386505e-02\n",
            "  2.31578071e-02  1.54271774e-01 -1.50626630e-02  2.70438589e-02\n",
            "  6.06207550e-02  5.21397955e-02  9.89843781e-02 -5.89703808e-02\n",
            " -4.95532746e-02  1.06972140e-01  2.07051138e-02  5.23554218e-02\n",
            " -2.81222309e-03 -8.77166095e-02  6.86357606e-02 -4.24416421e-02\n",
            " -3.20740798e-02  7.55165286e-04  4.17096441e-03 -3.16372185e-02\n",
            " -1.01359745e-02  7.26581542e-02  4.70699683e-02  2.83815104e-02\n",
            "  1.60639065e-02  2.61365675e-02  1.16360828e-02 -9.36883189e-03\n",
            " -1.76965115e-02 -1.13884117e-01  7.72255502e-02 -1.37594614e-02\n",
            "  2.39783599e-03  1.15468722e-02 -6.80235708e-03 -1.31241835e-01\n",
            "  3.08749883e-02  8.91292275e-02 -9.67906146e-02  2.46023288e-02\n",
            "  5.15030790e-02 -7.45936618e-02  2.25823178e-02 -2.15350332e-02\n",
            " -2.69725832e-03 -5.14563978e-02 -7.76220473e-02  9.79384127e-02\n",
            " -1.19437656e-02 -1.35835283e-02  8.10728065e-03 -8.38711911e-02\n",
            " -9.46769312e-02 -1.30306218e-01 -2.49565503e-02 -5.62224325e-03\n",
            "  4.08569422e-02 -6.11919000e-02 -4.75865718e-02 -1.33131383e-02\n",
            " -4.15190719e-02  5.50892238e-03 -1.04856814e-02 -3.49930879e-02\n",
            "  3.43481496e-04 -7.24285404e-02 -3.14827105e-02 -2.13325432e-02\n",
            " -2.13325432e-02  9.60870687e-02  2.03691208e-02  1.04366202e-02\n",
            "  1.35286733e-03 -6.43870224e-03 -6.58469136e-03  2.78008614e-02\n",
            "  4.65370079e-03 -2.43331561e-02 -2.66751359e-02  1.54674692e-02\n",
            " -1.89643205e-02 -4.15192293e-02 -6.78173654e-02  1.34604520e-02\n",
            "  3.51416391e-01 -8.68973917e-02 -1.33385408e-02  1.55488713e-02\n",
            "  5.98197863e-02 -1.26856169e-01 -2.02771486e-02  1.50204375e-02\n",
            " -5.50567579e-03 -3.22692010e-02 -3.77896943e-02 -3.31266482e-02\n",
            " -3.19927111e-02  2.36203172e-02 -2.57363194e-02 -3.18806347e-02\n",
            " -1.06939437e-01  3.86692114e-02  3.87961083e-02 -4.32286791e-02\n",
            "  1.13738552e-03  7.16119564e-02 -9.32393930e-02 -7.35521454e-02\n",
            " -1.36774280e-02  3.56742887e-02 -1.66405027e-02  2.15759289e-02\n",
            " -5.77176019e-02 -7.73033456e-02 -9.99024288e-02  6.45714454e-02\n",
            " -3.21576187e-02 -7.86453896e-02  4.50164796e-02  4.32132990e-02\n",
            "  1.23353357e-02 -6.58343718e-02 -9.47187323e-02  9.90350726e-02\n",
            " -9.03441858e-02 -9.65393172e-04  5.46680726e-04 -8.30031859e-02\n",
            " -7.67995380e-02  1.32281768e-03  9.48051693e-03 -4.17944433e-02\n",
            "  7.25782450e-03 -5.98300921e-02 -1.70333138e-02 -3.46742433e-02\n",
            " -5.29278897e-02 -4.17937968e-02  1.85433689e-02 -2.74567704e-02\n",
            " -4.98441648e-02 -6.97539551e-02 -8.86815863e-02  2.43655210e-02\n",
            "  1.75866609e-01 -6.45004404e-02  1.58458755e-01 -1.10614220e-01\n",
            " -2.87191704e-04 -4.32361239e-03  5.28849245e-02 -1.79906531e-01\n",
            "  5.00173351e-02 -2.17971753e-02  1.35009028e-01 -4.52625273e-02\n",
            " -1.53534111e-02  3.71566336e-02 -3.20067924e-02 -2.21185245e-01\n",
            " -7.93331356e-02 -6.75906608e-02  1.56311387e-01  4.33897509e-02\n",
            "  2.78143074e-02  2.21372905e-02 -7.39098690e-02  1.61690110e-01\n",
            "  2.85085871e-02  9.61114345e-03  2.72678487e-03  8.86918535e-02\n",
            "  1.83086722e-01 -3.24830075e-02 -1.38274028e-01 -7.66677237e-02\n",
            "  7.42615755e-03 -1.09007025e-01  7.21440487e-03 -1.17821203e-03\n",
            " -2.39719124e-02 -8.97137195e-02 -7.43663344e-02 -2.72748516e-02\n",
            " -2.56024451e-02  2.28378323e-02  6.37699088e-03  7.15568896e-02\n",
            " -3.86884517e-02 -3.94954582e-02 -1.76256464e-01  2.03649031e-02\n",
            "  4.36565447e-02  3.81638744e-02  7.12950537e-02  4.28969040e-02\n",
            "  7.85567248e-02 -3.72134681e-02 -8.80847726e-03  3.91722871e-02\n",
            "  3.00461082e-02  1.03019416e-01  5.10109000e-02 -2.72265049e-02\n",
            "  5.79359097e-02 -4.03640865e-02  2.83560567e-02  7.14659385e-02\n",
            "  6.32190099e-02  6.04153058e-02 -3.60379744e-02 -8.40503628e-02\n",
            " -1.06615628e-01  1.41967809e-01  2.22398466e-01 -1.67440653e-02\n",
            "  2.04294156e-01  2.53837228e-02 -3.67413798e-02 -3.21267456e-02\n",
            "  1.00384900e-01  7.85917605e-02  6.88182021e-02  1.74266921e-01\n",
            " -1.00856239e-01  1.31338707e-02 -2.38290159e-01 -7.38065650e-02\n",
            " -6.55332098e-03  1.59754357e-01 -3.45120621e-01  4.49981271e-02\n",
            " -6.92339565e-03 -1.30512134e-01 -1.30512134e-01  9.95265150e-03\n",
            "  2.01610396e-02  9.50857909e-02  4.48892087e-02 -5.15736988e-02\n",
            "  3.01574692e-02  2.32523515e-01 -8.39625944e-02 -1.16822387e-02]  - intercept :  0.8265141080241523\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.05438533365246619\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:34,198]\u001b[0m Trial 200 finished with value: 0.2281914237552237 and parameters: {'count_threshold': 9, 'postag': False, 'voc_threshold': 6299}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.05291918 0.04738198 0.09373376 ... 0.17679086 0.0890771  0.01861982]\n",
            " [0.04035085 0.04042683 0.05028128 ... 0.14931511 0.12892368 0.03845433]\n",
            " [0.0298175  0.00421947 0.02712962 ... 0.15784899 0.12129902 0.09116267]\n",
            " ...\n",
            " [0.25900111 0.06327301 0.39330096 ... 0.04910857 0.50212898 0.00517217]\n",
            " [0.04360645 0.05381842 0.         ... 0.41568034 0.         0.06156044]\n",
            " [0.03198499 0.03139592 0.11037672 ... 0.14633773 0.23089938 0.025526  ]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-1.35688310e-02 -2.44309121e-02 -2.65210591e-02  2.21319341e-02\n",
            " -1.94654487e-02 -7.19215956e-02  2.99287473e-02  2.97276174e-02\n",
            " -1.09386383e-02  1.91474075e-02 -2.58525344e-03  1.83783021e-02\n",
            " -4.85322008e-02 -1.17470163e-02  6.71680795e-02  8.67938191e-02\n",
            " -1.24002578e-02  3.37670831e-03  3.54501326e-02 -3.46239024e-02\n",
            " -2.76529513e-02 -1.22652106e-02  4.10564433e-03 -1.10423834e-03\n",
            "  3.73620873e-02 -1.62984863e-02 -2.08753059e-02  3.45162731e-02\n",
            " -5.89064319e-03  5.81308277e-03  3.97222716e-02 -6.92731826e-03\n",
            "  5.98133825e-02 -9.84020223e-02 -6.09196216e-02 -2.99988272e-02\n",
            " -1.21595664e-03 -3.65718711e-02 -3.92003840e-02 -1.99054869e-02\n",
            "  2.08548373e-02  3.87276643e-03 -7.62546830e-03 -2.99389871e-02\n",
            " -2.31589383e-02 -3.46229386e-02  8.61878140e-02  9.80301518e-02\n",
            " -8.34747043e-02 -1.04380393e-02  1.08066857e-01  4.98343071e-03\n",
            " -6.24884688e-02 -9.38089712e-02  1.37978745e-02  4.26721986e-02\n",
            "  2.09246121e-02 -1.47237294e-02  3.00887989e-02  2.68826077e-02\n",
            "  2.20782842e-02 -3.01493672e-02 -1.84569407e-02  3.88123932e-02\n",
            " -4.20385564e-02 -1.96037773e-02 -7.30066753e-02 -1.01035228e-02\n",
            " -2.59309613e-03 -9.61673831e-03  2.26857742e-02  3.93426792e-02\n",
            " -2.56787509e-02  2.76133288e-02 -6.00151399e-04  2.98685103e-02\n",
            "  2.19205815e-02  4.09587887e-02 -7.41472508e-02 -1.31813852e-02\n",
            " -6.01459439e-02  3.25966269e-02  2.72439138e-03  7.75733574e-02\n",
            " -1.29952426e-02 -3.05422910e-02 -2.79779609e-02  1.23094201e-03\n",
            " -3.86092461e-02  4.97593650e-02  6.66339682e-02 -2.07098039e-02\n",
            "  4.91228490e-02 -2.29507923e-02 -1.10057685e-02 -2.90380902e-02\n",
            "  3.89509825e-03 -5.49017496e-02  4.32432594e-02 -1.16287123e-01\n",
            " -1.57299247e-02 -1.57127657e-02 -1.20101983e-01  1.92402536e-02\n",
            " -5.27286523e-02  3.86128740e-02  5.98305137e-03 -7.31332287e-02\n",
            " -1.75528424e-02  4.56055270e-03  1.15957910e-01 -3.32796661e-02\n",
            "  4.41033609e-02 -1.09764263e-02  1.28870921e-03 -4.99856060e-02\n",
            " -1.26208178e-02  3.56053347e-02 -2.71244662e-02  3.50768624e-02\n",
            " -5.98104303e-02 -8.32351176e-03 -4.98494962e-02  8.97490802e-03\n",
            " -1.55684629e-02  5.19131925e-02 -3.49457591e-02 -9.11509345e-02\n",
            "  1.23456070e-02 -5.25143511e-02 -2.70699919e-02  2.76275645e-02\n",
            "  3.04857761e-02  4.17754910e-02  7.99059307e-02  2.97881848e-02\n",
            "  9.81192556e-02  3.35565119e-02  1.74621470e-02  5.53706886e-02\n",
            "  2.46947912e-02  3.24249944e-02 -2.60951033e-02 -1.41970164e-02\n",
            "  4.20153960e-02 -3.81383906e-02  6.34839578e-02  7.12601212e-02\n",
            " -5.03822066e-02 -3.92653834e-02  6.40095534e-02 -9.29199033e-03\n",
            " -8.70495762e-03 -1.01951394e-01 -8.05927786e-02  4.11917446e-02\n",
            " -7.94835757e-03 -3.33756698e-03  3.35518019e-02 -1.16565347e-01\n",
            "  7.60585647e-02 -3.49822764e-02  6.98593994e-02  5.57275289e-02\n",
            " -1.57623986e-01  3.22911951e-02  1.32869011e-03  1.32869011e-03\n",
            " -4.46454075e-02  1.30932870e-02  2.04874428e-02 -2.26656962e-02\n",
            " -3.22189776e-02 -2.95407732e-02 -1.23946251e-02 -3.96183817e-02\n",
            "  3.20397322e-02  6.10781548e-02  6.75621332e-04  1.09288133e-04\n",
            "  8.51913166e-03 -9.16369123e-02 -9.59494682e-02 -2.92626034e-02\n",
            "  3.41677053e-02 -1.55717154e-02 -2.84377427e-03 -1.10309925e-02\n",
            "  4.50934230e-02 -1.99560114e-02 -6.03088189e-02  2.08112012e-02\n",
            "  7.65312479e-02 -9.92201454e-02 -1.66702465e-02 -2.44322342e-02\n",
            " -3.39909702e-02 -1.02188297e-02  3.76874003e-02 -1.32630350e-01\n",
            " -1.80465510e-01  2.19537769e-02  1.50534491e-03  5.18270731e-02\n",
            "  1.14774002e-02 -5.59245304e-04 -4.11519802e-02 -8.03156497e-02\n",
            " -9.71108447e-03 -3.37249038e-02  3.49847879e-02 -1.50283282e-02\n",
            " -4.48594273e-03 -1.13354811e-01 -1.23635461e-01 -7.94732151e-02\n",
            "  4.47496485e-02  4.64533453e-02  2.93140192e-02  6.46439125e-02\n",
            " -8.19817929e-02 -1.21886912e-02  4.67972009e-02 -1.42616964e-01\n",
            "  2.05425562e-03  6.38005834e-02 -4.34473732e-02  4.07537049e-02\n",
            "  4.00453685e-02 -2.68456012e-02 -1.06501902e-01 -4.46483434e-03\n",
            " -1.85191785e-02  2.97837585e-03 -1.08716758e-02  2.57848131e-03\n",
            " -1.79799534e-01 -1.60281912e-02 -6.98567866e-03 -1.26154457e-02\n",
            " -3.31160878e-02  1.60084666e-02  5.84249769e-02 -2.58015868e-02\n",
            " -1.43802800e-02  6.50942733e-02  5.26321123e-02  6.73595963e-02\n",
            " -4.56272776e-02  1.81645395e-02  5.33379891e-02 -1.04507403e-01\n",
            "  6.10974953e-02  4.27565968e-02 -4.30280415e-03 -6.92251145e-02\n",
            " -7.71110504e-02 -2.48493780e-01 -1.36421917e-01  5.18336881e-02\n",
            "  3.58460086e-02  6.07212588e-04  1.15800941e-03 -2.47264624e-02\n",
            "  3.87350264e-02  4.88212682e-02  1.84642100e-02 -6.05166982e-02\n",
            "  1.08766755e-01 -5.66978380e-02 -5.87601877e-02  7.00396793e-03\n",
            " -1.56348140e-01 -7.85267208e-02  6.92014145e-02  3.36376511e-02\n",
            "  5.47933425e-02 -1.14025255e-01 -9.97864764e-03  6.00324647e-02\n",
            "  1.75948222e-01 -5.80477038e-02 -1.19511021e-02 -9.04744353e-02\n",
            "  1.99785193e-02 -3.72577381e-02 -2.84324873e-02 -5.11589261e-02\n",
            "  5.78703012e-02  4.43154610e-02 -9.75738115e-03  3.27772152e-03\n",
            " -4.84691093e-02 -1.13359948e-02 -2.77580939e-02  6.49295711e-02\n",
            " -1.50506543e-01 -5.83408567e-02  6.50274336e-02 -4.39958569e-02\n",
            "  2.96790995e-02 -1.78871761e-01 -4.94338184e-03 -5.17568397e-02\n",
            "  2.86155698e-02 -9.01685753e-02  1.81505432e-01  4.54996806e-02\n",
            " -1.12059724e-01  7.83591593e-02  3.22161535e-02 -1.76056067e-01\n",
            "  2.70944212e-02  1.52362049e-02  5.71844152e-02 -3.76029705e-02\n",
            "  7.07779897e-04  8.97234212e-02  2.17713292e-02 -4.53154382e-02\n",
            "  2.55032752e-02 -4.43695004e-02  4.95342278e-04 -1.03795468e-01\n",
            "  3.16808808e-02 -4.53975021e-02  2.34360788e-02 -3.31003902e-01\n",
            "  6.54324661e-02 -1.96640960e-02 -2.50470185e-03 -5.81105869e-02\n",
            "  1.21485877e-02  2.44004407e-02 -1.05435886e-01  4.25057237e-02\n",
            "  4.63850571e-02  4.63850571e-02 -1.62036806e-02 -3.04761727e-02]  - intercept :  0.9088094015292438\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.2281914237552237\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:34,750]\u001b[0m Trial 201 finished with value: 0.21563451749648035 and parameters: {'count_threshold': 7, 'postag': False, 'voc_threshold': 6714}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[6.08860991e-01 6.36958397e-02 8.70172502e-01 ... 1.89586445e-01\n",
            "  1.33168920e-01 6.20527278e-03]\n",
            " [2.57000524e-04 1.41273740e-02 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 1.12038792e-01]\n",
            " [3.34193041e-01 3.99598356e-02 6.10130299e-01 ... 2.68123238e-03\n",
            "  1.66461151e-01 1.89710783e-02]\n",
            " ...\n",
            " [8.30341990e-03 1.10526616e-01 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 3.89060866e-02]\n",
            " [1.61771302e-01 2.01173967e-02 1.17091226e-01 ... 2.25417173e-01\n",
            "  0.00000000e+00 1.94530433e-02]\n",
            " [2.53607181e-02 5.74614147e-02 2.44075441e-02 ... 1.13605637e-01\n",
            "  8.69263423e-02 4.88790680e-02]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-2.43239280e-02 -2.20251539e-02 -2.19209330e-02 -1.74436535e-02\n",
            " -7.72548004e-02 -2.22440279e-03  2.03111584e-02 -5.80428358e-03\n",
            " -5.86915826e-03 -4.04991583e-02 -1.65223929e-02 -8.08434252e-02\n",
            " -1.15676781e-02 -2.80545448e-02 -5.46316428e-02 -1.98131514e-03\n",
            "  8.71490845e-03 -6.07590322e-02  1.06407385e-02 -4.31621768e-02\n",
            "  4.57990570e-03 -1.98827579e-02  2.41620564e-02 -8.96096322e-02\n",
            " -2.23069663e-02  1.77591560e-02  4.75190010e-03  4.19240283e-02\n",
            " -6.89644051e-03 -8.21746189e-03  3.06608622e-02  1.21304157e-02\n",
            "  1.52123649e-02  2.76573274e-02  3.36238419e-04 -1.21987382e-02\n",
            " -1.24687171e-02 -1.74121893e-02  1.06633001e-02  4.17899874e-02\n",
            "  4.05829901e-02  2.81711619e-02  2.29690238e-02 -7.52549611e-03\n",
            " -7.31380481e-02  2.22068542e-02  1.02251900e-02  1.92675975e-02\n",
            " -7.29037614e-02 -1.12463636e-01  9.11434578e-03 -7.67487970e-02\n",
            " -7.19942808e-03 -1.04255847e-02 -2.24393049e-02 -5.24921778e-03\n",
            "  1.74839586e-02  3.26759007e-02  6.38239022e-04 -1.92351745e-02\n",
            " -4.49017629e-02  9.18121465e-02  6.20483849e-02 -7.76446329e-03\n",
            " -1.11346994e-02  1.34525868e-02  9.01156743e-03 -8.29370638e-02\n",
            "  4.43658622e-02  3.09782571e-03 -8.34289581e-03  1.67040998e-02\n",
            "  2.97126649e-02 -8.46633558e-04 -1.97064763e-02  1.46743480e-02\n",
            " -4.43747370e-03  4.25842056e-02  1.38278112e-02  9.29198326e-03\n",
            " -9.88098815e-02  4.23163503e-02  1.66309032e-02  2.25103510e-02\n",
            " -4.09621244e-02 -3.38692134e-02  4.60850723e-02 -3.88623025e-02\n",
            " -3.06066058e-02  7.08513560e-03  9.29038300e-03  4.57451764e-02\n",
            "  2.29256214e-02 -1.94971260e-02 -2.80749853e-02  2.01495110e-02\n",
            " -1.38070259e-02 -7.05630144e-02 -3.35770206e-02  2.74913655e-02\n",
            " -1.54623209e-01 -7.63915098e-02  2.67496037e-03 -5.97422333e-02\n",
            " -2.81416357e-03 -7.43815482e-02  6.21718948e-02 -8.40244195e-03\n",
            " -7.56377908e-02 -4.30756567e-02  3.61239808e-03  1.94687140e-02\n",
            "  8.66926672e-02  1.24174608e-02 -8.32252255e-03  3.55360810e-02\n",
            "  1.39865398e-02  2.50428510e-02 -2.35379898e-02  3.97406485e-02\n",
            "  6.43589689e-02  9.81456024e-03  3.38012722e-02  5.05566628e-02\n",
            "  9.16136803e-02  1.28053932e-02  8.76516772e-03  4.03755575e-02\n",
            "  7.93973307e-02  1.79825650e-02 -1.35819247e-02 -5.52160115e-02\n",
            " -2.18269536e-02  6.24308688e-02  2.91668782e-03 -5.38806591e-02\n",
            " -9.70169069e-03  3.43761546e-02  2.54600108e-03 -6.32963181e-03\n",
            " -2.83310901e-02  7.66197199e-02  9.23758943e-02  1.85294201e-02\n",
            " -5.62993282e-02 -1.55534097e-02 -7.85911232e-02 -2.54755724e-02\n",
            " -3.58016876e-03  9.93394358e-02  5.87829700e-02  6.46255339e-02\n",
            "  3.71426011e-02 -3.90960414e-02 -4.97285420e-02 -9.93188775e-02\n",
            "  5.78764153e-02  2.42636015e-02 -7.32244212e-02  1.20902660e-02\n",
            "  5.75184208e-02  2.22574461e-02 -1.64264061e-03 -7.78182828e-03\n",
            " -1.87036391e-02 -5.09205542e-02 -7.67431744e-03 -3.87058776e-02\n",
            " -2.46387330e-02 -3.50414195e-02  5.27389768e-02 -6.73512141e-02\n",
            " -4.96159694e-02  1.69676587e-02  3.84383886e-02 -8.27936491e-03\n",
            "  2.41199192e-02  6.01239578e-02 -4.98811081e-02 -5.82076591e-02\n",
            "  1.86092740e-02  1.24401915e-01  1.99758241e-02  2.46952723e-02\n",
            " -1.44197707e-01  4.48290108e-02 -6.53259858e-03 -1.82518175e-03\n",
            " -7.32745661e-03 -3.30920555e-02 -7.69713536e-04  8.07480733e-03\n",
            " -1.19034584e-01 -2.41299614e-02 -2.47854290e-02  7.02488989e-02\n",
            "  3.17677463e-02  3.23986947e-02 -1.88195530e-02 -5.65149121e-02\n",
            " -1.03552183e-01 -6.41629859e-03 -1.89110509e-02 -3.49391357e-02\n",
            "  4.94978589e-02  8.31740235e-02  8.32020621e-02  1.48496225e-03\n",
            "  8.07004677e-03  7.26548805e-02 -6.62192622e-02 -1.02038605e-02\n",
            "  4.28996663e-02 -6.72852689e-02  2.31475228e-02 -5.09321710e-02\n",
            " -1.34206118e-02 -1.90092218e-03 -1.94055994e-02 -1.53599879e-02\n",
            " -2.46076261e-02 -3.63405840e-03 -6.13684254e-02  6.33288181e-03\n",
            " -9.54137408e-03 -1.43642818e-02 -9.58508476e-03 -6.28471123e-02\n",
            "  9.53708657e-03  4.26545917e-02 -4.65325503e-02 -9.02352378e-05\n",
            " -2.08855736e-02  2.68134697e-02 -7.92428358e-03 -5.73240628e-02\n",
            "  1.67072946e-02  4.12333624e-02 -2.11864559e-03  3.26791621e-02\n",
            "  4.92258654e-02  1.44623701e-02  1.98023998e-02 -3.43705756e-02\n",
            " -5.12958058e-02  5.56279459e-02  3.57566168e-02  1.05273833e-02\n",
            " -3.02665210e-03  7.96894588e-02 -2.53868562e-03  4.63258841e-02\n",
            " -1.92118727e-02  4.32397774e-02 -8.47678323e-02 -8.79476926e-02\n",
            " -3.56937991e-02 -4.56935793e-02 -4.89704254e-02 -5.65306887e-02\n",
            " -1.43610447e-02 -3.74374093e-03  3.89517040e-02  5.31149367e-02\n",
            "  5.68565116e-04 -1.84919350e-02  3.19562372e-02  3.11955025e-02\n",
            " -5.97420391e-02  5.44715465e-02 -3.31009922e-02  9.54656781e-03\n",
            " -1.20465016e-03 -2.11326772e-02 -1.82231392e-02  1.87604202e-02\n",
            "  4.39999824e-02 -4.73166876e-02  3.97003210e-02  1.04949309e-02\n",
            " -3.38086719e-02 -2.50552353e-02  6.22571964e-03  3.14857648e-02\n",
            " -2.21897624e-02  1.08697700e-03 -4.25977241e-02  1.01911537e-01\n",
            "  6.83230903e-02 -8.43620286e-02  1.02820175e-02 -7.11482830e-02\n",
            " -5.33278673e-02  6.17601803e-02 -2.62336188e-02 -8.79239315e-02\n",
            " -7.39088678e-02  7.94687905e-02 -5.53334040e-02 -2.90386234e-02\n",
            " -4.66175496e-02 -1.98115826e-04 -5.09435383e-02 -5.06040543e-04\n",
            " -1.67441322e-02 -4.49765486e-02 -3.78945884e-02 -3.32544393e-02\n",
            "  3.67508732e-02  1.19174175e-01 -5.92348866e-04 -8.19318433e-02\n",
            " -1.34830462e-02  1.95063806e-02  1.87947980e-02 -2.51754690e-02\n",
            "  1.42461644e-01  1.47181207e-01 -3.03184613e-02 -9.84142698e-03\n",
            " -1.60487596e-02  1.34805815e-02  4.28695739e-02  6.54407925e-03\n",
            " -7.43225265e-03  6.74732395e-02 -2.71819991e-04 -2.37124060e-02\n",
            " -7.42461534e-03 -6.55847776e-02  2.42927157e-02 -6.88750213e-02\n",
            "  1.06231048e-01  1.55369887e-02 -5.05079167e-02 -3.50499232e-03\n",
            "  1.27023219e-01 -1.36825963e-02  3.41653966e-02  2.04936414e-02\n",
            " -2.59949527e-02  2.75166474e-02 -7.40191444e-02 -2.16760697e-02\n",
            "  5.91886290e-02  3.28062707e-02  3.24654422e-02 -5.51731371e-02\n",
            " -7.72865739e-02 -2.77026365e-02  5.24130319e-02 -6.07607220e-02\n",
            "  3.57002937e-02 -4.50134332e-02 -4.86779758e-03  1.10543160e-02\n",
            "  2.28453795e-02 -8.24674595e-02 -1.16141577e-02  6.77349751e-02\n",
            "  2.37192414e-02 -4.91370121e-02  2.36449297e-03 -2.81670171e-02\n",
            " -1.68412099e-02  1.24008412e-02  1.63174454e-02 -6.30499853e-03\n",
            " -2.52401680e-02 -3.64383306e-02 -2.98949104e-04 -3.94876493e-02\n",
            "  2.22075801e-03  1.03069483e-02 -1.07634048e-01  5.41450285e-02\n",
            " -6.90229134e-02 -1.20533259e-02 -2.32695421e-02 -5.54871547e-02\n",
            " -2.03267041e-02 -6.14657879e-02 -2.14221469e-02  1.18551644e-01\n",
            " -1.56300040e-02  4.26402625e-02  3.07381857e-02 -1.10499550e-01\n",
            " -2.38565729e-02 -4.61843145e-03 -1.90952346e-02 -5.19668914e-02\n",
            " -1.82497656e-02  3.96378355e-02 -6.17785818e-02 -3.57112016e-02\n",
            " -1.65426852e-03  3.23522904e-02 -3.88610251e-02 -1.62081117e-02\n",
            " -3.35387282e-02 -6.22090333e-02 -3.72250394e-02 -1.43497863e-02\n",
            "  2.82440720e-02 -2.36738746e-02 -1.66764825e-04 -5.15162761e-02\n",
            " -5.21765552e-02 -6.46198212e-02 -5.07211465e-02  9.34271978e-03\n",
            "  4.25242160e-02  3.56817170e-02 -8.05487243e-03 -2.82822493e-02\n",
            " -4.78830118e-03 -2.65202930e-02 -6.31401643e-02 -3.54146024e-02\n",
            "  9.78735238e-02  2.07973365e-02 -2.54836116e-02  1.24434918e-02\n",
            "  1.47340700e-02  4.11826167e-02  2.59779829e-02  1.03531963e-02\n",
            " -1.06307189e-01 -1.95746964e-02  1.02879489e-01  2.27242319e-02\n",
            " -3.27157962e-02 -1.65189730e-02 -7.00394245e-03 -3.93302405e-03]  - intercept :  0.6955005290802566\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.21563451749648035\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:35,343]\u001b[0m Trial 202 finished with value: -0.31482869272845687 and parameters: {'count_threshold': 5, 'postag': False, 'voc_threshold': 5784}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.02096416 0.00933527 0.35344307 ... 0.04000406 0.07590612 0.08104245]\n",
            " [0.04685487 0.1243346  0.01769826 ... 0.08149287 0.22204214 0.06079761]\n",
            " [0.04192613 0.19416254 0.00453418 ... 0.07280532 0.37329016 0.0511151 ]\n",
            " ...\n",
            " [0.42129214 0.05942396 0.10113766 ... 0.37576768 0.         0.02138431]\n",
            " [0.         0.00864995 0.00166859 ... 0.07900659 0.17631566 0.08356196]\n",
            " [0.04958505 0.10941425 0.02852255 ... 0.14083302 0.12918143 0.05396601]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 6.97672306e-03  5.45844729e-04 -2.97610082e-03 -1.07560920e-03\n",
            " -2.28563157e-02 -9.12873115e-03 -2.38173466e-02 -5.42204475e-03\n",
            "  6.32392323e-03 -1.45527321e-02 -1.40071362e-02 -4.46595771e-03\n",
            " -1.36095412e-02  1.02121353e-02 -5.29020703e-03 -4.98637396e-03\n",
            " -1.52855405e-02 -1.06080194e-02  3.80587306e-03 -5.35218968e-03\n",
            "  1.14128365e-02  1.54543217e-03 -3.75691891e-03 -1.47861940e-03\n",
            " -1.69650303e-02  7.65100825e-03 -4.09235046e-03  3.79994462e-03\n",
            "  1.43481768e-02 -1.54733134e-02 -1.35413226e-02  9.84394501e-03\n",
            "  1.06813096e-02  2.03432533e-02  3.70082936e-02 -2.26084875e-03\n",
            "  4.02507800e-03 -2.50720280e-03 -2.34232750e-02  4.79790982e-03\n",
            " -6.53821942e-03 -1.39640681e-02  1.25010206e-02 -6.04040639e-03\n",
            " -2.56796840e-02 -1.99431902e-02 -2.52471564e-02  9.26116017e-03\n",
            " -8.96562420e-03 -3.52207632e-03  4.95105681e-03 -1.88003953e-02\n",
            "  1.47886427e-02 -2.24640794e-02 -1.45458817e-02 -1.39273705e-02\n",
            " -1.91912612e-02 -1.50400194e-02 -2.59750287e-02  8.97503401e-03\n",
            " -2.61135860e-03  2.20331117e-02  1.39406097e-02 -2.79086064e-03\n",
            "  8.44434021e-03  4.63203168e-03 -2.24245197e-02  1.74069092e-02\n",
            " -8.37562640e-03 -1.39190272e-02  1.67245934e-02  2.55864207e-03\n",
            "  1.28380697e-02  8.54241009e-03  2.01668018e-02 -4.72159674e-02\n",
            " -7.59148967e-03 -1.85899464e-02  7.80230201e-03  5.89083864e-03\n",
            " -4.22690541e-03 -1.06769782e-03 -5.53636976e-03  3.92337382e-02\n",
            " -2.30542934e-02 -7.22513751e-03 -1.43877678e-02 -5.37275344e-02\n",
            " -2.29147974e-03  5.36413553e-03  1.49985494e-02 -9.39776444e-03\n",
            " -7.89317770e-03  8.34895539e-03  6.22813167e-02  8.00568384e-03\n",
            " -8.05417998e-03 -1.83510942e-02  1.85769852e-02  2.31522122e-02\n",
            "  2.88570673e-02  1.95829437e-04  3.95682092e-02  1.81652586e-02\n",
            " -5.04807138e-03  3.41354885e-02  1.57814393e-02  6.36000295e-03\n",
            " -1.99230798e-02 -2.12965860e-02  1.29938399e-02  3.61367098e-03\n",
            " -8.37388425e-03 -1.63253759e-02 -3.45899783e-02  6.13710827e-02\n",
            " -1.21739462e-02  1.95098543e-02 -5.86898372e-03  2.74406260e-02\n",
            " -2.24896859e-02  3.44927762e-02  1.50972008e-02  2.33233773e-02\n",
            "  3.62459989e-03  9.76414432e-03  1.63588277e-04  1.68667875e-02\n",
            " -3.01456714e-02 -3.51285616e-02  3.24059035e-02 -5.39867642e-02\n",
            "  1.12096761e-02  1.45930795e-02 -2.63520033e-03 -6.33286076e-03\n",
            "  1.27930827e-02  2.19093444e-03 -1.98941319e-02  1.02111299e-02\n",
            "  1.45718983e-02  2.45114334e-02 -2.34252266e-03  9.46689773e-03\n",
            "  7.65015401e-03  2.43964190e-03 -1.76870880e-02  7.28011595e-03\n",
            " -1.27865869e-02  2.93728218e-02  3.98608507e-02  5.12099635e-03\n",
            " -3.23572194e-03  3.04786760e-02  1.08564346e-02 -1.55580777e-03\n",
            " -1.97722534e-02 -1.76501773e-02  1.02071818e-02  2.27569419e-03\n",
            " -4.43154461e-03 -1.14571758e-02  1.72263127e-02 -9.48328695e-03\n",
            "  1.90447289e-02 -6.60460230e-03  5.73386440e-02 -4.57125092e-02\n",
            "  1.89950018e-02  7.40740730e-03 -1.75579564e-02 -2.85048161e-02\n",
            " -9.02193572e-03  2.98749906e-02  2.57654530e-02 -2.63044341e-02\n",
            " -2.73588860e-03  2.03566373e-03 -1.80240392e-02  2.09757013e-02\n",
            "  8.45418755e-03 -6.03565737e-03  2.27420618e-02  1.25374641e-02\n",
            "  8.54323981e-03 -2.35840372e-03 -3.37176585e-02  1.69928346e-02\n",
            " -7.50598143e-03 -7.42250547e-03 -7.32876554e-03  1.17036929e-02\n",
            "  6.92757390e-03  3.29155660e-02  2.26749863e-02 -4.10082434e-03\n",
            "  1.46861843e-02 -1.69214267e-02 -1.43160164e-02 -1.99072149e-02\n",
            "  9.07217899e-03  2.48679496e-02  5.34726617e-03 -1.32630028e-02\n",
            " -1.82123809e-02 -1.79284781e-03  5.98473021e-03  5.93338320e-03\n",
            " -1.50290432e-02  1.17280807e-02  1.03726388e-02  2.77587303e-02\n",
            "  3.19942058e-02  6.74532515e-03 -5.88416796e-03 -4.16815226e-04\n",
            "  4.67322931e-02 -2.25765836e-02 -1.49611951e-03 -3.04733645e-02\n",
            " -1.32053238e-02 -5.79655245e-02  3.60747832e-02 -1.22968772e-03\n",
            "  8.05079368e-03  2.37624128e-04  1.06196479e-02  1.22615931e-02\n",
            " -1.31590561e-02 -1.19339748e-02  6.92022102e-03  9.07086607e-03\n",
            "  3.63373019e-02  1.50292431e-02  2.61065689e-02 -1.60027844e-02\n",
            "  1.65617456e-02  2.28411021e-02  3.62617490e-02 -9.03574940e-03\n",
            " -2.89236341e-02  6.17408985e-03  6.94357997e-03  5.78720538e-03\n",
            " -9.11537363e-03 -2.99159717e-03  3.52761781e-03  3.24951930e-02\n",
            "  1.28905439e-02  9.84331590e-03  2.76876469e-02  4.28209458e-03\n",
            "  1.49570453e-02 -8.79301886e-03 -4.35442561e-03 -1.94855103e-02\n",
            "  1.59096739e-02  2.67634377e-02  2.56533010e-02  3.40941476e-02\n",
            " -1.79579275e-02  1.62524688e-02  1.18288059e-02  1.64754333e-02\n",
            "  5.44703164e-03  2.55741954e-02  1.27166372e-02  1.09641940e-03\n",
            " -1.51819782e-02  1.79467427e-02  6.64172927e-03  4.02118606e-03\n",
            " -2.22480093e-03  3.18728422e-02 -9.81614110e-03  3.66827910e-02\n",
            "  6.45254026e-03  1.43522991e-02 -8.89269612e-03  5.70622408e-03\n",
            " -8.53669058e-03  9.64153423e-03  9.64153423e-03  7.05827775e-03\n",
            "  7.76995567e-03 -4.17619372e-02  1.25195066e-03 -2.89964005e-02\n",
            " -1.40350792e-02  4.71217349e-03 -1.11386346e-02  2.23421629e-02\n",
            " -1.44760038e-02  8.37272691e-04  8.37272691e-04 -2.47393700e-03\n",
            " -2.38077298e-02  3.18966155e-03  4.99504446e-02  1.86907231e-02\n",
            " -2.14598946e-02 -1.95129357e-02  1.49527074e-02  3.40265670e-02\n",
            "  2.99598586e-02  4.99094797e-03 -1.01884247e-02 -3.14919595e-02\n",
            " -7.72651832e-03  2.90109240e-02  1.46977384e-02  1.37929835e-02\n",
            "  2.20199037e-02  1.42422762e-03 -4.73472026e-02  1.38790798e-02\n",
            " -1.27078623e-03 -3.11671608e-02  2.05902262e-03  3.95756243e-03\n",
            " -1.58719911e-02 -4.28349764e-03 -9.48316603e-03  9.64353025e-03\n",
            " -8.02613255e-03  1.83034014e-02  5.46044540e-02  9.21863942e-03\n",
            " -3.50596183e-03  1.92474586e-02  7.03680896e-04  1.85397217e-03\n",
            " -2.45074051e-03 -1.99283562e-03  3.26910524e-02 -6.32734871e-05\n",
            "  1.15516933e-02  2.57508381e-02  1.20827021e-02  1.61588399e-02\n",
            "  8.56822883e-03  7.92705894e-03  4.85514422e-02  7.77732180e-03\n",
            "  2.16473489e-02 -1.26358988e-03  1.52286767e-02 -1.64803190e-03\n",
            "  7.35731302e-03 -2.02759387e-02  2.24119194e-02 -1.34705410e-02\n",
            " -1.66936975e-02  3.35847549e-03  1.49741351e-02  1.35996815e-02\n",
            "  9.41306739e-03 -2.78810568e-02  3.07000592e-03 -2.76158744e-02\n",
            " -2.95928420e-02  7.86186276e-03  2.07820962e-03  6.07826734e-03\n",
            "  1.97605192e-02  1.01365337e-02 -1.09999997e-03  1.41535448e-02\n",
            "  1.73443326e-02 -9.61383010e-03  5.22058520e-02 -7.19561331e-03\n",
            " -6.44153966e-03  3.71689184e-02  1.84477622e-02  5.14733902e-04\n",
            "  8.75927668e-03 -4.88284537e-03  1.86532496e-02 -6.98187794e-03\n",
            " -5.46234201e-03 -4.79860477e-03 -7.07762997e-03  1.54690210e-02\n",
            " -3.32462683e-03  1.28481277e-02  8.60008682e-03  4.46544940e-03\n",
            " -4.26760214e-03 -6.41720335e-04  5.68810594e-04  2.33920122e-03\n",
            " -3.21437173e-02  3.56439955e-02  1.43787074e-02  7.91081060e-03\n",
            " -1.21130508e-03  9.13463369e-03 -3.37460348e-03  4.13713841e-03\n",
            " -3.96780048e-03  2.31667855e-03 -7.59091150e-03  1.51715381e-02\n",
            " -8.41828358e-03  3.62012237e-03 -1.16917290e-02 -5.89386437e-03\n",
            " -8.02706079e-03 -1.13921934e-02  1.56724884e-02  1.53070900e-02\n",
            " -1.23058714e-02  4.11254152e-03  6.69022764e-03  1.37532442e-02\n",
            " -8.18166346e-03 -2.18144705e-03 -1.14346641e-02 -5.08085536e-03\n",
            "  2.00474491e-02 -1.10073710e-02  8.15845731e-03  1.41400679e-02\n",
            "  2.29434338e-02  1.85488166e-02  1.38266358e-02  9.57794642e-03\n",
            "  1.49807546e-02  7.58263517e-03 -1.97357712e-02 -1.34787437e-02\n",
            "  2.65712405e-02 -4.08274789e-03  1.11494518e-02 -3.72626677e-03\n",
            "  7.25681427e-03  9.29568633e-03 -4.27924037e-04  3.80980793e-03\n",
            "  1.48202370e-02  1.10351870e-02 -1.90356336e-03 -4.33280568e-03\n",
            " -6.43987261e-03  3.75227146e-02  2.98064676e-02  3.20821041e-03\n",
            "  1.16276420e-02  1.16308659e-02  1.10057554e-03  1.89087437e-02\n",
            "  7.58100285e-03 -1.21372613e-02  5.92774965e-03  1.40463824e-02\n",
            "  6.17240675e-03 -1.52841840e-02  2.07372568e-02 -6.49072498e-03\n",
            "  6.10112770e-03  5.93140411e-03  2.95746525e-03 -9.47846487e-03\n",
            " -1.17240317e-02 -1.66657272e-02 -3.83574804e-03 -1.04832782e-02\n",
            " -4.25983634e-03  1.48526705e-03 -3.92019410e-03  1.29465962e-02\n",
            "  2.44459231e-03  7.87944772e-03  1.24763262e-02  3.13502468e-03\n",
            " -4.95600735e-03  1.97258850e-02  6.10655429e-03 -1.49412198e-02\n",
            "  6.13344750e-04  1.06883927e-02 -3.89429693e-02  1.16523760e-04\n",
            " -1.64491851e-03  5.83721825e-03  4.54729115e-03  1.09657762e-02\n",
            "  1.72081329e-02  1.43754323e-02  7.18203272e-03 -1.18265300e-02\n",
            " -2.02121496e-02 -1.53364991e-02  2.05528964e-02  1.83028856e-02\n",
            "  3.71834867e-03 -1.80298572e-02  5.76382631e-04  2.28670921e-02\n",
            "  2.58382878e-02  1.75927794e-02  2.10897069e-02  2.11659328e-03\n",
            " -2.04597568e-02  9.81011097e-03 -1.70873954e-02 -7.26616736e-03\n",
            " -7.97010622e-03 -1.28332407e-02  8.10196974e-03 -2.82301814e-03\n",
            "  2.46786063e-02  1.00875069e-02 -1.68068261e-02  5.39794848e-03\n",
            " -6.06979202e-03 -1.13359657e-02  1.38152026e-02  8.87885833e-03\n",
            " -2.45265919e-03  4.02480209e-03  7.47878676e-03  7.80967684e-03\n",
            "  1.55406662e-02  1.02285913e-02  1.48947439e-02 -9.17414254e-03\n",
            "  2.11473961e-03 -5.49016095e-03 -1.67894438e-02  3.96486609e-03\n",
            "  5.18766084e-04  5.82563596e-03 -1.36189979e-02  8.41332193e-03\n",
            "  1.60996754e-02  3.03014071e-04 -3.29536960e-02 -1.32662867e-02\n",
            "  4.97985793e-03 -4.82192832e-03  2.76596301e-03  8.94046092e-03\n",
            " -9.05717443e-03 -1.72650537e-02 -1.39545413e-02  2.98802526e-04\n",
            "  6.96478150e-03 -3.74419322e-02 -2.80338557e-03 -1.31491820e-02\n",
            "  1.78808563e-03  1.33840522e-02 -2.03552426e-02 -3.56841949e-02\n",
            " -9.59746171e-03  3.27743843e-02  2.58510703e-03  5.10343976e-03\n",
            " -1.40480853e-02 -1.38912932e-02  1.33405239e-02  1.92031745e-02\n",
            " -5.77468452e-03  1.09032127e-02  2.02038687e-02  7.09049995e-03\n",
            " -1.40742582e-02  1.32415728e-02  2.11024536e-03  4.01345620e-02\n",
            "  7.14768459e-03  1.77025640e-02 -2.62122860e-03 -1.87308306e-02\n",
            " -4.03651181e-03  1.07177635e-02 -8.19174470e-03  3.59752676e-02\n",
            " -1.64795620e-03 -1.33052816e-02  1.23485012e-02  3.88508178e-03\n",
            "  1.53777102e-02  2.04054183e-02  2.04281506e-02  4.91956737e-03\n",
            "  8.21120106e-03 -1.17968556e-02 -7.01057885e-04 -7.60856455e-03\n",
            " -1.36581471e-02 -5.80253258e-04  4.29727574e-02  2.01384231e-02\n",
            " -1.64151748e-02 -7.21701065e-03 -2.19500279e-03 -1.40461241e-02\n",
            " -2.23394711e-02 -1.43426522e-02  3.92540290e-03 -1.34209897e-02\n",
            "  7.39961110e-03  3.24047844e-02  4.71135872e-03 -4.96154723e-03\n",
            " -5.99717130e-03 -2.16897611e-02 -2.01451885e-02  2.16385620e-02\n",
            "  1.39096352e-02  1.21460978e-02 -3.61206572e-02 -1.84617050e-03\n",
            "  1.12994075e-02 -1.02569782e-02 -1.96605606e-02 -8.49109664e-03\n",
            "  2.02458023e-02 -4.32209776e-03  3.61215996e-02  1.60091017e-02\n",
            " -2.91217277e-03  2.18565192e-02  2.19747461e-02  7.32805154e-03\n",
            " -3.66409965e-03 -5.49243015e-04 -1.90693896e-02 -1.08858223e-03\n",
            " -6.65377744e-03  1.10555850e-02 -7.07511791e-03 -3.52459626e-03\n",
            "  6.74147304e-03  1.62699847e-03  2.90302998e-02  1.11004497e-02\n",
            "  3.38833472e-03  1.53042692e-02  1.36567103e-02  3.25395357e-02\n",
            " -3.65763374e-02 -4.21707568e-03 -2.51061810e-03  9.68211402e-03\n",
            "  9.15215327e-04  1.00675194e-02 -8.79319152e-03  1.83177494e-07\n",
            "  8.09175254e-03 -1.68783995e-02 -8.14219877e-03  1.48381900e-03\n",
            " -6.88068509e-03  3.56691553e-02 -1.14030955e-02  6.51931088e-03\n",
            "  6.51931088e-03 -8.51512744e-03  2.06576964e-02  4.23473848e-03\n",
            " -7.37189510e-03 -4.26843548e-03  5.25395357e-03  5.02428014e-03\n",
            " -3.09053853e-03  1.27984593e-02 -2.87414550e-02 -2.01319566e-02\n",
            " -2.01319566e-02 -2.56455510e-03 -1.79692160e-02 -2.28401597e-03\n",
            "  1.93532461e-03 -1.88254812e-02 -3.06216364e-02  1.58703462e-02\n",
            " -6.18571676e-03 -5.42076797e-02 -4.25189831e-02  1.29779743e-02\n",
            "  2.40961195e-02 -9.04827585e-03 -9.51047230e-03 -3.03152540e-02\n",
            "  6.16893513e-03 -1.94137515e-03  1.35125152e-04 -6.15319718e-04\n",
            " -1.24403466e-02  4.38830739e-03  4.94527033e-03  2.16827139e-02\n",
            " -1.89237436e-02 -1.47596458e-03 -9.50461855e-04 -2.97578562e-03\n",
            "  5.92796957e-03  1.04926235e-02  4.44741961e-03 -2.31990464e-03\n",
            "  1.59472119e-02  2.75936829e-03  1.41786397e-02  2.70075198e-03\n",
            "  5.51850634e-03  1.90363112e-02  1.05764977e-02  4.80961364e-03\n",
            "  3.45907787e-02 -7.73395851e-03 -2.26177026e-02  4.63117846e-03\n",
            "  1.39295601e-04  1.32805789e-02 -9.77570001e-03  7.40216828e-03\n",
            " -4.28471539e-02  1.41861207e-02 -1.21935100e-02  5.40911780e-03\n",
            " -1.50879892e-02  4.78294329e-02  5.33354245e-02  1.07384632e-03\n",
            " -3.83142352e-02 -5.02985485e-03 -2.08506452e-02  8.15393639e-03\n",
            "  4.79203721e-04 -7.83660770e-03  1.78309166e-02 -1.67723773e-02\n",
            " -1.76036137e-03 -3.32391201e-03 -3.05377409e-03  1.26351379e-02\n",
            "  7.43956700e-03  7.99201850e-03 -1.20612435e-03  3.00575710e-02\n",
            "  9.56647141e-03 -4.54302615e-03  1.25656614e-02  9.04028442e-03\n",
            "  1.21638813e-02  2.06154930e-02 -8.36034011e-03  1.33444737e-02\n",
            " -1.22244581e-02  1.21656398e-02 -1.16208209e-02  2.59598821e-03]  - intercept :  0.4674461608416858\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.31482869272845687\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:35,957]\u001b[0m Trial 203 finished with value: 0.07970076560431415 and parameters: {'count_threshold': 6, 'postag': True, 'voc_threshold': 9826}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.06071716 0.07698372 0.04219295 ... 0.0549382  0.0549382  0.03041019]\n",
            " [0.13132233 0.18584281 0.07000343 ... 0.07908163 0.06055868 0.01893178]\n",
            " [0.56075509 0.56733596 0.03265079 ... 0.36486603 0.         0.00680146]\n",
            " ...\n",
            " [0.64104224 0.78642624 0.09064773 ... 0.17198325 0.0460277  0.00579712]\n",
            " [0.01314012 0.0584642  0.04027841 ... 0.04186675 0.         0.04084005]\n",
            " [0.42576962 0.48996577 0.10834001 ... 0.15744445 0.         0.0051011 ]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-2.57288791e-02 -2.32090191e-02 -1.66884645e-02 -1.12273963e-02\n",
            " -2.50271123e-03  2.98469306e-03 -5.03944802e-02 -8.51526523e-03\n",
            "  1.68042733e-03 -2.00389686e-02 -1.71181010e-03 -5.69908572e-02\n",
            " -4.15767300e-02 -4.19007516e-03  7.75772913e-03 -2.54060556e-03\n",
            "  1.76594099e-02  2.00911675e-02 -4.18540410e-03  3.89740009e-03\n",
            " -3.66479558e-02 -7.52185296e-02 -1.72351513e-02  1.03708088e-02\n",
            "  2.23164792e-03 -3.68648486e-03 -2.04070604e-03 -1.97660883e-03\n",
            "  2.91814928e-03 -1.97113393e-03 -4.60070241e-02 -2.12857821e-02\n",
            "  2.40013985e-02 -4.02267754e-02 -1.59229670e-02  8.89071925e-03\n",
            " -5.81147689e-02  2.19731697e-02 -8.10774041e-03 -1.11365517e-02\n",
            " -2.16247782e-02 -5.40591887e-04 -9.23907520e-03  1.90450006e-03\n",
            " -5.11446723e-02 -7.16365174e-03  1.06536075e-02 -5.09906257e-02\n",
            " -1.45443296e-02 -1.83899276e-02 -1.69709797e-02 -2.21950255e-02\n",
            " -5.27877008e-04 -4.80502072e-02  3.43611000e-02 -1.70827704e-02\n",
            "  8.21026885e-03 -3.15418645e-02  3.40456275e-03  1.63802045e-02\n",
            " -4.49463106e-02 -4.33131461e-02  1.99370471e-02 -9.51493305e-03\n",
            "  2.91339361e-02  2.50088188e-02  9.90549753e-03 -1.99194747e-02\n",
            " -2.46599094e-02  1.62190225e-02  4.09252075e-03  6.50237329e-03\n",
            " -8.92155282e-03  6.05936689e-04  3.14244179e-03  1.10603737e-02\n",
            " -9.06542207e-03  5.04483131e-03 -1.57859312e-02 -4.30297991e-02\n",
            " -3.40102484e-03 -5.20290963e-02 -1.99693260e-02 -2.83144478e-02\n",
            " -7.98901480e-02  1.58732411e-02 -3.16740927e-02  5.81289378e-02\n",
            "  7.01698943e-03  5.92432012e-02 -1.93500612e-02 -5.53558945e-04\n",
            " -4.79169540e-02 -1.97600060e-02  3.91266626e-02  1.33688028e-02\n",
            " -4.44865338e-02 -3.82565862e-02  3.22755997e-02 -4.68549316e-02\n",
            " -1.30713454e-02 -2.37883448e-02  1.72640761e-02  6.56481726e-03\n",
            " -1.18944618e-02  1.51141147e-02 -9.77324980e-03  6.67370591e-04\n",
            "  3.12318990e-02  4.12872410e-02 -3.75602958e-03  2.85010404e-02\n",
            "  1.26450672e-02 -2.34441493e-02  2.25515911e-02  2.62306970e-03\n",
            "  1.10689135e-02  2.17389713e-02 -4.84280550e-03 -1.21519095e-02\n",
            "  1.82930224e-02 -4.23973558e-02 -3.71507367e-02  1.39408130e-02\n",
            " -1.97911661e-02 -2.73522622e-02 -4.75917948e-03  3.39509942e-02\n",
            " -4.12034926e-02  1.15852550e-02 -1.37652697e-02  3.37466724e-02\n",
            " -4.17892416e-03  1.77008111e-02  8.22325995e-03 -4.95778027e-02\n",
            " -1.09612907e-02 -1.06208230e-02  1.07340561e-02 -4.21707325e-02\n",
            "  2.83940757e-02  3.23319075e-02  5.45822147e-02  1.48763085e-02\n",
            " -6.60867159e-03  4.51029208e-03 -5.04399456e-03 -2.47215557e-02\n",
            " -3.53018011e-04  2.75040659e-03  3.20460178e-02 -2.01062807e-02\n",
            " -3.15208951e-02  5.08929180e-02 -1.60242614e-02  4.90448303e-02\n",
            "  1.35990488e-02 -1.71982340e-02  9.88962795e-03  4.07365980e-02\n",
            " -2.64597081e-03  1.52193193e-02 -1.61655612e-02  1.36210357e-02\n",
            "  3.66975512e-04  1.81387742e-02 -3.24986080e-02  5.11800917e-03\n",
            " -1.41924439e-03  1.03518827e-02  1.25371888e-02 -4.11408588e-02\n",
            "  4.57424697e-03  2.34376944e-02  1.81222269e-02 -3.77063851e-02\n",
            "  1.50729334e-02  1.82854215e-02 -2.79657294e-02 -2.25945829e-02\n",
            " -1.48430492e-02 -1.01562990e-02 -3.23983435e-02  3.61676421e-04\n",
            " -5.21246095e-02  2.14496835e-02  1.49187909e-02  7.55609940e-03\n",
            " -1.84202487e-02  7.71069684e-03  2.90629429e-02  2.22667134e-02\n",
            " -4.24132611e-02 -7.80472954e-03 -3.92420243e-03 -4.72423421e-02\n",
            "  2.63340890e-02  1.46016199e-02 -2.04510565e-02 -4.06648739e-02\n",
            "  2.08248969e-02 -9.36351343e-04 -4.50456509e-02  1.36304336e-02\n",
            "  2.20137626e-02  3.59905944e-02  1.20728260e-02 -1.17813462e-02\n",
            "  2.89762762e-02  2.15581872e-02 -1.61098176e-02  2.74972956e-02\n",
            "  3.71666106e-02 -1.52777137e-02  2.05804336e-02  4.49316020e-03\n",
            "  4.49316020e-03 -6.16649857e-03  1.08810680e-03 -1.39812528e-02\n",
            " -3.91020123e-03 -1.33558388e-02 -2.08301499e-02  1.54804951e-03\n",
            " -3.00251350e-02 -3.09557810e-02 -5.29278123e-04  1.27565627e-02\n",
            "  1.01262509e-02  2.17378466e-02 -4.00492316e-02  8.98484148e-03\n",
            " -9.02972143e-03 -5.11582813e-02 -4.82238644e-02  2.41063677e-02\n",
            " -2.44342770e-02  7.88512037e-03  1.48527895e-02 -2.34095374e-02\n",
            "  1.82651414e-03 -2.39058548e-02 -3.57202439e-02  2.48958059e-02\n",
            " -2.09385092e-02  2.88591189e-02  2.54887347e-02  2.92375431e-02\n",
            "  4.57406854e-02  2.91816481e-03 -3.12696832e-02 -3.20666129e-02\n",
            "  1.86854379e-02  8.62212160e-05  2.69324664e-02 -3.58990315e-02\n",
            "  1.73943547e-03  1.18168398e-03  7.64118218e-03  4.91342737e-03\n",
            " -3.55078040e-02  3.68833496e-02  1.99478529e-02  4.02005144e-02\n",
            " -4.56492655e-02  1.41454468e-02  2.82652679e-03  1.44459307e-02\n",
            "  1.99076731e-02 -1.32958875e-03 -4.63026871e-02 -3.60433113e-03\n",
            "  1.05763974e-02  1.94092899e-02  5.50882756e-02  3.09711944e-02\n",
            "  6.68996767e-03  1.17210462e-02 -9.87349247e-03  1.08249799e-02\n",
            "  2.72055737e-02  1.49680949e-02 -3.42586876e-02 -3.35101276e-03\n",
            "  3.48439408e-02 -2.36504855e-02 -6.61355687e-03 -4.46898207e-02\n",
            " -8.02504028e-02  1.29543975e-02  3.57387589e-02  3.13816607e-02\n",
            "  2.63223087e-02  6.94802712e-03  2.08465636e-02  3.83023969e-02\n",
            " -9.50837991e-02 -2.85116165e-03  8.80329440e-03 -1.80608558e-02\n",
            "  1.37909311e-02 -5.51111654e-03 -8.63576399e-03  4.24434566e-02\n",
            " -1.42372622e-02 -2.13524023e-02  2.86260415e-02  1.32305047e-02\n",
            "  1.60469695e-03 -9.69598800e-03 -1.15218570e-02 -9.80222226e-03\n",
            " -1.19816266e-03 -1.80825094e-02 -2.66160609e-03 -1.21769987e-02\n",
            " -2.44448809e-03  1.65950565e-02 -9.30207889e-03  6.08045991e-03\n",
            " -1.32487912e-02 -8.53593339e-03  8.95469840e-04  7.69548639e-03\n",
            "  2.73937260e-04  2.41813700e-02  3.41640260e-03  6.70464608e-04\n",
            "  1.47506069e-02 -2.29727021e-02  2.98886913e-03 -3.79674465e-03\n",
            "  9.62239827e-03  1.50884872e-02  1.57529218e-02 -7.29313856e-03\n",
            "  1.78771729e-03  6.33834961e-03  2.62769346e-02  1.24671620e-02\n",
            " -1.49657554e-02  2.24387471e-02 -2.04614008e-02 -2.53794919e-02\n",
            "  1.23907919e-02  2.76338727e-02 -7.00787936e-03 -6.00535738e-03\n",
            "  8.65203845e-03  2.43596755e-02  4.37875692e-03  9.74863515e-03\n",
            " -2.12022006e-02  9.32271225e-03 -3.85501362e-02  3.67592203e-02\n",
            " -1.44428605e-02 -3.15848450e-03  2.30285784e-02  8.37237011e-03\n",
            " -8.37732774e-04 -1.64601171e-02 -2.58733881e-03  3.91765871e-02\n",
            " -1.20686683e-02  1.76046206e-02 -1.95110865e-02  1.16411466e-02\n",
            "  1.60668697e-02  3.88048978e-02 -2.25293543e-02 -2.34032973e-02\n",
            " -1.96870393e-02  1.96693868e-02  1.01114146e-02  1.08728711e-02\n",
            "  1.27699632e-02 -1.81015341e-02 -2.50524811e-03  1.75505942e-02\n",
            "  1.39427116e-02  8.72364650e-03 -4.38556470e-03  1.07391444e-02\n",
            "  2.14433971e-02  1.47592069e-02  3.87531268e-03  1.85799871e-02\n",
            "  4.00755147e-02  5.98661321e-02 -2.94226027e-02 -3.80791045e-03\n",
            " -1.51865022e-04 -1.63046039e-02  8.75373443e-03 -1.11234390e-02\n",
            " -1.55418218e-02  1.01887119e-02  2.73443156e-02  2.69737095e-02\n",
            "  2.56895108e-02 -2.70808229e-02 -5.84510200e-03  4.83809436e-02\n",
            "  1.42618425e-02  1.98671826e-02  2.84961636e-03  5.50910873e-03\n",
            " -4.83949006e-03 -8.85786518e-04  1.95661609e-02  1.89606083e-02\n",
            "  4.23869278e-03 -1.87367655e-02 -1.55204797e-03 -3.62151969e-03\n",
            " -2.91524333e-02  3.41187156e-02 -1.41024731e-02 -2.50091859e-02\n",
            "  5.45314712e-02  1.30949688e-03 -1.77964271e-02  3.69057534e-03\n",
            "  3.56776036e-03  1.43045921e-02  5.51021346e-03  1.28232930e-02\n",
            " -2.24303718e-02  3.54853944e-03  3.77109707e-02 -7.34377985e-04\n",
            " -1.86759599e-03 -2.41588737e-02  2.11662613e-02  1.14569834e-02\n",
            "  3.27476617e-02 -3.55092473e-02  1.24135057e-02 -1.55085662e-02\n",
            "  1.40456946e-02  1.11861333e-03 -2.59600218e-02 -1.91549214e-02\n",
            " -4.92566231e-02 -1.64740203e-02  1.35065771e-03 -2.16471749e-02\n",
            "  3.78765280e-03 -9.13528694e-03  1.94172987e-02  1.36609559e-02\n",
            "  1.48539343e-02  2.18997617e-02  5.36818253e-03 -2.93280096e-02\n",
            "  2.45187976e-02 -9.95329591e-03  3.20681378e-02  1.92309771e-02\n",
            " -2.22970338e-02 -2.13316236e-02  1.46388172e-02  4.12494807e-03\n",
            "  2.33001128e-02 -5.64889748e-03  4.44199361e-03  2.99269659e-02\n",
            " -1.81400833e-02 -2.70369032e-02 -1.00176556e-02 -2.32104680e-03\n",
            " -4.54182478e-03  1.59790053e-02 -1.70124380e-02  9.39491851e-03\n",
            " -3.58402681e-02 -1.45543243e-02  1.32241256e-03 -2.33876071e-02\n",
            "  1.61728939e-02  7.68443721e-03 -5.30363953e-03 -2.00773131e-02\n",
            "  1.85898053e-02  1.54098613e-02 -7.70000648e-03 -2.63287054e-02\n",
            "  1.57447086e-02 -3.22656839e-02 -2.46004522e-03  6.43950264e-03\n",
            " -3.22278504e-03  6.89340114e-03 -1.26727680e-02  4.78236079e-02\n",
            " -2.35573008e-02 -4.47542728e-03  7.64098776e-03  3.02918670e-02\n",
            "  3.82280077e-02 -5.53016608e-02  1.93516946e-02  1.86004897e-02\n",
            "  2.89520603e-03  6.41215670e-02  2.91136601e-02 -2.64364838e-02\n",
            " -1.70602543e-02  7.07213079e-03 -3.71827524e-02 -6.89521067e-03\n",
            "  5.92721860e-03  1.29202416e-02 -6.41924099e-03 -3.97631666e-02\n",
            "  8.15701536e-05  2.01371697e-02  2.21744167e-02 -1.02229224e-02\n",
            "  6.12315179e-03 -2.37394316e-02 -1.10915030e-03 -1.57004853e-02\n",
            " -9.94372458e-03 -9.94372458e-03  3.83553050e-02 -1.65802702e-02\n",
            " -3.57890814e-02 -2.38360230e-02  6.13213617e-03 -2.16086735e-02\n",
            "  4.87998987e-02 -3.94814339e-02  7.43138880e-04 -1.60068839e-02\n",
            "  1.94107310e-02  5.41486265e-03 -5.84416553e-03 -1.39241475e-02\n",
            " -3.23996173e-03  1.13157828e-02 -8.17532242e-03  2.30780782e-02\n",
            " -4.83196703e-02 -1.83280993e-02  1.95630127e-02 -2.56751735e-02\n",
            "  2.11818704e-02  4.12130380e-02 -7.87925599e-03  7.01317199e-03\n",
            "  9.01462477e-05 -1.82157165e-02 -2.82905168e-02  2.79398435e-02\n",
            " -4.83411892e-03  3.04027768e-02 -3.42639190e-02  2.42104886e-02\n",
            " -1.71159222e-02 -2.98501003e-03 -3.21203020e-03 -1.16882893e-02\n",
            " -2.82292986e-02  2.31014152e-02 -4.98615669e-03  1.31119531e-02\n",
            " -4.00250057e-02 -1.35449707e-02 -1.30021002e-03  1.45403185e-03\n",
            "  4.82614552e-02  1.68510008e-02  4.17310434e-03  8.75254154e-03\n",
            " -4.02672786e-03  2.17120299e-02  1.49543586e-02  7.74335357e-03\n",
            " -1.42872740e-02 -1.77869023e-03  2.56363671e-02 -5.48741448e-03\n",
            "  7.33141539e-03  8.43981626e-03  3.10353462e-02  4.31438349e-03\n",
            " -7.81820148e-03 -1.69711148e-03  1.83891730e-02 -3.74852948e-02\n",
            " -2.49050801e-02  2.65845570e-02  3.45917258e-02  5.55776641e-03\n",
            " -2.90283279e-02 -3.78740183e-03  2.64566757e-02  2.05781089e-02\n",
            " -7.41615195e-03  2.72783774e-02  3.37454970e-02 -4.99494317e-04\n",
            " -5.01316691e-04 -3.26848096e-02 -2.19957331e-02 -1.02686419e-02\n",
            " -2.09804515e-02 -1.64337437e-02  1.81222311e-02 -3.30344775e-02\n",
            "  3.28165169e-02  3.95454772e-02  1.82673941e-02  2.21342261e-02\n",
            "  1.00997988e-02  1.91755703e-03]  - intercept :  0.6047204312800846\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.07970076560431415\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:36,555]\u001b[0m Trial 204 finished with value: 0.2677970412222709 and parameters: {'count_threshold': 4, 'postag': False, 'voc_threshold': 9608}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.27925952 0.12962509 0.37800428 ... 0.2286656  0.07931518 0.01608181]\n",
            " [0.         0.09254577 0.00397705 ... 0.16738378 0.01166268 0.09362748]\n",
            " [0.38226553 0.09948791 0.54234324 ... 0.14222748 0.09628026 0.02389085]\n",
            " ...\n",
            " [0.01773486 0.17645023 0.06663508 ... 0.26356421 0.10710176 0.03027682]\n",
            " [0.24810507 0.02176326 0.32269626 ... 0.18916397 0.01457835 0.03900513]\n",
            " [0.02845385 0.03281306 0.05540433 ... 0.07644876 0.11470832 0.06627848]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 0.00288367  0.00133104  0.0027572  ... -0.00676632  0.00534698\n",
            " -0.00075324]  - intercept :  0.9284645411831178\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.2677970412222709\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:37,180]\u001b[0m Trial 205 finished with value: 0.0663052604816011 and parameters: {'count_threshold': 8, 'postag': True, 'voc_threshold': 7144}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[3.38189298e-03 4.41501399e-02 3.92641666e-02 ... 5.47694453e-01\n",
            "  4.14060090e-01 4.78506954e-02]\n",
            " [3.49708965e-01 3.18965684e-02 6.23797514e-01 ... 1.49460350e-01\n",
            "  1.10445697e-01 4.38343619e-03]\n",
            " [1.95460380e-01 1.15733715e-02 1.43704581e-01 ... 0.00000000e+00\n",
            "  0.00000000e+00 1.22923636e-02]\n",
            " ...\n",
            " [6.17470691e-02 7.16257548e-02 4.81680232e-02 ... 4.14065121e-02\n",
            "  7.19586183e-02 2.35554704e-02]\n",
            " [5.44256606e-01 1.68937298e-02 7.24741790e-01 ... 0.00000000e+00\n",
            "  0.00000000e+00 4.81520859e-03]\n",
            " [5.06621735e-03 4.81383556e-02 3.47408674e-05 ... 1.68477867e-01\n",
            "  5.98945412e-02 7.76034625e-02]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 1.38125972e-02  1.86248062e-02 -3.49559395e-02  1.77011161e-02\n",
            "  2.51704084e-02  6.30443499e-04 -3.41845043e-02  2.06914126e-02\n",
            " -2.39565359e-02 -2.32612933e-02 -8.79026110e-05 -7.04541446e-03\n",
            "  6.84722139e-04 -1.77051871e-02 -1.03469910e-02  2.14422085e-02\n",
            " -2.28010991e-02 -2.63158865e-02  2.28422161e-02 -1.97282130e-02\n",
            " -1.52407930e-02  4.08453912e-04 -1.62493084e-02  9.07340489e-03\n",
            " -5.16191043e-03  5.40888604e-02 -2.16760755e-02 -5.31059141e-02\n",
            "  5.89183433e-04  1.37760862e-02 -2.60388294e-02 -4.16796267e-02\n",
            "  1.91198632e-02 -2.99961403e-02  5.95434930e-02  1.97736983e-02\n",
            "  6.66098748e-03  2.63960837e-02 -1.87054146e-02 -2.12641299e-02\n",
            " -3.96515707e-02 -2.30232860e-02 -1.71305814e-02 -2.21498755e-02\n",
            "  2.20934265e-02  1.05309802e-02  2.70744067e-03 -9.75831566e-03\n",
            "  1.54746368e-02 -1.04990397e-01 -2.47095575e-03 -2.20366036e-02\n",
            " -3.02688033e-02 -1.69786891e-02 -5.12012676e-02 -4.38148832e-02\n",
            " -5.12182779e-02 -2.86228792e-03 -3.85951590e-02  2.03605695e-02\n",
            " -3.37633084e-02  6.50856426e-02  4.52686344e-02  1.18679619e-02\n",
            " -1.35581685e-02 -9.31043206e-03 -3.89586087e-02 -6.81513290e-02\n",
            " -3.07056885e-02 -1.34674740e-02 -3.90455362e-02  2.57379626e-02\n",
            "  7.72199488e-02  5.12088412e-02  4.15074497e-02 -8.92637073e-03\n",
            "  2.11807304e-02  2.48883949e-02 -2.85797332e-02  2.44413511e-03\n",
            " -8.73611714e-03  1.20353027e-02 -1.69511799e-02 -2.73172358e-03\n",
            "  2.03019177e-02 -3.07600464e-02 -2.09664078e-02 -2.21929415e-02\n",
            " -1.53927030e-02  3.55758599e-02  3.60932488e-04 -3.22584827e-02\n",
            " -3.78426958e-02  5.12230587e-02 -3.42607494e-02 -4.25637531e-02\n",
            " -4.28895321e-03  1.98203787e-02 -4.54916896e-02  5.52933355e-02\n",
            " -4.45431566e-02 -3.99615837e-02 -6.83741995e-03  7.29117384e-02\n",
            " -4.31868195e-02 -3.23864867e-02  2.92648173e-03 -2.04866942e-02\n",
            " -5.71879545e-02 -1.37457898e-02 -1.19899103e-03 -4.30644735e-02\n",
            " -5.28066940e-03  8.57146548e-03  4.21583196e-02  8.83766776e-03\n",
            "  4.28759464e-02  4.13605801e-02 -6.03239644e-02 -3.35921219e-02\n",
            " -2.51556643e-02  4.21749849e-02  1.23431704e-02 -2.66774911e-02\n",
            "  7.46212382e-02  2.98394829e-02 -2.30824782e-02 -2.41552445e-02\n",
            " -3.01948411e-02  2.30780722e-02  5.84453583e-02  1.74223530e-02\n",
            " -8.41691084e-02  1.83275689e-02  6.50277827e-02 -3.06042809e-03\n",
            "  1.20106121e-02 -2.52538149e-03 -1.36213680e-03  5.33744070e-03\n",
            "  2.78593866e-02  9.93138149e-03  1.08615381e-01  2.46126779e-02\n",
            " -3.73496066e-02 -7.69402189e-02 -9.60831826e-03  4.39835381e-02\n",
            "  3.59486254e-02  5.09513802e-02 -1.41906129e-01 -2.42129752e-02\n",
            " -4.36296139e-02 -1.37296448e-02  7.37169057e-02  8.39173968e-04\n",
            " -2.12801795e-02  1.24494967e-02  2.55739931e-02  1.87590910e-02\n",
            "  7.13600012e-03 -4.92549389e-02 -1.98877726e-02 -1.07015231e-02\n",
            "  6.07117481e-03  1.03182710e-03  2.75429038e-02 -5.88149601e-03\n",
            " -4.52489609e-02 -1.85403115e-02 -7.54790642e-03 -4.82018256e-02\n",
            " -3.92003649e-02 -2.55012477e-02 -5.37319473e-02  4.55868543e-02\n",
            "  6.14160316e-02  1.77385747e-02 -9.65617348e-02  1.02155874e-02\n",
            " -4.01493291e-02 -2.73640318e-02  6.64164815e-03  3.57467662e-02\n",
            " -6.78185475e-02 -7.64148278e-02 -2.66998773e-02  1.88009166e-02\n",
            "  4.15716573e-02  2.34788975e-02 -3.56595658e-02 -3.36118974e-02\n",
            "  1.50933892e-02 -1.41593464e-02  8.89526464e-02 -3.13160850e-03\n",
            " -7.38516291e-03 -7.38516291e-03  5.44964825e-03 -1.97950168e-02\n",
            "  4.40304860e-02 -8.09100636e-02 -2.91252801e-02  2.26318747e-02\n",
            " -6.43004465e-03  3.13185659e-02 -1.06863873e-02 -2.43209402e-02\n",
            " -1.71552567e-03  1.14655182e-02  6.54633175e-02  3.69515364e-02\n",
            " -8.07221777e-03  4.62571557e-03  1.46819666e-03 -2.77559492e-02\n",
            "  4.67662995e-02  3.14932129e-02  2.84256772e-03 -1.43554597e-02\n",
            "  3.58611416e-02  3.00052174e-02 -9.09923969e-03 -2.03748697e-02\n",
            "  2.05102617e-02  7.06069964e-03 -6.73533149e-02 -4.15273121e-02\n",
            "  2.73907377e-02 -7.89839491e-02 -3.02153950e-02  8.03662788e-03\n",
            "  2.15481618e-02 -1.87957701e-02  1.16814936e-02 -2.93632354e-02\n",
            " -1.88255323e-02 -2.70163559e-02 -3.84682430e-02 -5.89907003e-02\n",
            " -2.36262550e-02  2.65211081e-02 -1.22155403e-03 -5.13479893e-02\n",
            " -2.07479576e-02  2.99893381e-02 -4.25898397e-02  2.16242494e-02\n",
            "  1.03651565e-02 -3.71507569e-03  3.04008755e-02 -2.76208774e-02\n",
            "  1.07233135e-03 -8.38002447e-04  2.83856766e-03  4.32373092e-02\n",
            "  4.38167089e-02  2.72997519e-02 -2.50531687e-02  7.31101274e-03\n",
            "  1.97988467e-02 -2.12713415e-02 -1.31947034e-03  4.60853642e-02\n",
            " -3.37128853e-02 -2.45062633e-02 -2.37148151e-02  7.70036929e-02\n",
            " -6.31800957e-02 -2.77607059e-02 -4.40128067e-02 -4.40165978e-02\n",
            " -4.24109267e-03 -6.35295127e-02  6.76630598e-02 -1.68983398e-02\n",
            "  4.71726323e-03 -2.22536982e-02  8.98536236e-02 -9.36905065e-02\n",
            "  1.63949722e-02 -3.35337817e-02 -2.45670890e-02  2.18503538e-03\n",
            " -3.30133068e-02 -3.39554777e-03  2.15595197e-02 -7.67534400e-02\n",
            " -1.14428961e-02  2.78479659e-02  4.24414833e-03 -2.05957289e-02\n",
            " -4.61573730e-02 -1.37947432e-02  1.40776090e-02 -8.15306025e-03\n",
            " -3.14469856e-02  5.59353006e-03 -3.23738614e-02  3.96353673e-03\n",
            " -9.05870457e-03  1.50116230e-02 -3.83135956e-03 -1.49878191e-02\n",
            "  1.64321672e-02  2.37371415e-02  1.96046794e-02 -5.17291484e-03\n",
            " -5.23185171e-02 -5.09099873e-02  1.42881091e-02  4.25393715e-02\n",
            " -1.88501838e-02 -3.77347811e-02 -2.23370490e-02  3.15309683e-02\n",
            "  9.11236494e-02 -4.35989580e-02 -2.92133224e-02 -1.40774323e-03\n",
            " -3.37941074e-02 -2.10806551e-02 -3.89482879e-02  1.90090478e-02\n",
            " -2.14320462e-02 -1.93827279e-02  1.43890839e-02  1.16311626e-02\n",
            "  7.10462015e-02 -1.01748125e-02  1.04481760e-03 -9.85014592e-03\n",
            "  3.08321031e-04 -1.11619206e-03 -8.39799066e-03 -1.71726606e-02\n",
            " -2.90215785e-02  6.36400256e-02 -5.44827718e-03 -5.59564234e-02\n",
            "  1.63607210e-03 -2.83063397e-02  4.08064638e-02  3.38509514e-02\n",
            " -2.52196257e-02  6.81129170e-02  8.79821209e-03  3.86122143e-02\n",
            "  2.03394635e-02 -6.26526859e-03  1.53339173e-02 -2.47666206e-02\n",
            " -3.06677818e-02  2.68133980e-02 -1.05668715e-02  5.89977743e-02\n",
            "  1.12440612e-02 -2.35709666e-02 -1.06284039e-01 -2.40903283e-02\n",
            " -1.45485105e-02 -2.12354504e-02  7.00748905e-02  3.76371903e-02\n",
            " -4.75039113e-04 -3.33259802e-03 -2.74065475e-02  7.74862471e-02\n",
            " -2.01232812e-03  4.59334304e-03  1.16773230e-02  7.15858309e-03\n",
            "  3.39425608e-02 -6.44073917e-02 -8.37944922e-02  8.27327985e-02\n",
            "  4.95097918e-02 -2.76934891e-02 -3.60999908e-02  3.65290439e-02\n",
            " -6.34481757e-02 -2.05623665e-02 -3.50616990e-02 -3.67261592e-03\n",
            "  2.03978963e-03 -4.26907216e-02 -2.04115696e-02 -2.27658897e-02\n",
            "  4.19159069e-02  1.98531880e-02 -2.14662027e-02 -1.67703734e-02\n",
            " -3.00586312e-02 -8.61742036e-03 -6.22735315e-04  7.44847134e-03\n",
            "  2.38740009e-02 -2.75511263e-02 -6.74682183e-02  1.39445271e-02\n",
            "  2.39702436e-02  2.88277952e-02  4.24800809e-03 -1.76110010e-02\n",
            " -2.96202976e-02  4.21346549e-02 -1.10692338e-02  4.83707130e-02\n",
            "  4.48428205e-02  7.43048110e-05  7.43048110e-05  5.19745269e-03\n",
            "  6.27233366e-03  5.32480837e-02  2.35372835e-02 -3.68509187e-02\n",
            " -1.72497298e-03 -9.81752714e-02 -8.09601605e-03  1.75484097e-02\n",
            "  1.86137446e-02 -2.80872511e-02  1.19399811e-02 -2.24389462e-02\n",
            " -2.07810911e-02 -8.03125107e-03]  - intercept :  0.8319249741870723\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.0663052604816011\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:37,757]\u001b[0m Trial 206 finished with value: 0.09363544111946626 and parameters: {'count_threshold': 9, 'postag': False, 'voc_threshold': 1526}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.01473319 0.02015982 0.08256447 ... 0.09757589 0.0881252  0.03129006]\n",
            " [0.00650795 0.0301719  0.0206176  ... 0.         0.06730325 0.04796427]\n",
            " [0.         0.         0.00147543 ... 0.         0.06730325 0.05175921]\n",
            " ...\n",
            " [0.00682914 0.03360845 0.05632845 ... 0.17218931 0.31962599 0.0261923 ]\n",
            " [0.         0.06455571 0.00110657 ... 0.         0.13772629 0.12335467]\n",
            " [0.00557824 0.02586163 0.00781616 ... 0.         0.74434383 0.05806882]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 3.33986536e-03 -6.63869402e-03 -1.65343376e-02 -4.67448053e-02\n",
            " -4.46219755e-02  2.37593435e-02  4.57739592e-02  2.82780135e-02\n",
            " -6.89786718e-02 -1.61562065e-02 -1.16697554e-01  1.00000799e-03\n",
            " -4.31762014e-02 -3.59791740e-02 -1.99615816e-02 -1.05789145e-01\n",
            " -1.35329185e-01  1.02862666e-02  5.39571216e-02  1.37991772e-02\n",
            " -4.63258255e-02 -2.29639636e-02  1.21649602e-02  4.83405991e-03\n",
            "  5.78776229e-02 -6.71625877e-02  6.09330957e-02 -2.09492914e-02\n",
            " -6.12513528e-02 -7.22890540e-02 -5.26695402e-02  2.74663677e-02\n",
            " -8.44955313e-02  7.17559189e-02  5.06447096e-02  3.88101336e-02\n",
            "  4.78550389e-02 -1.35964521e-01  2.70379377e-02  2.88533975e-02\n",
            " -1.80801781e-02 -1.95919330e-01 -4.72790496e-02 -3.00005012e-02\n",
            "  2.81697827e-02 -9.60514842e-02 -1.33007554e-01  1.99274230e-02\n",
            " -1.36161171e-01  4.57007095e-02 -7.19256675e-02  3.06960102e-02\n",
            " -1.60017082e-02 -1.12043802e-01 -4.16771277e-02  8.05761203e-02\n",
            "  2.08596315e-02 -2.87815936e-02 -2.48649200e-02  1.01843147e-01\n",
            "  1.78442663e-02  1.10817624e-01 -8.80771805e-02 -1.96428700e-01\n",
            " -9.62993329e-02  1.31013560e-01  1.19492348e-01 -7.66407290e-02\n",
            " -9.94721002e-03 -6.44304307e-02 -1.38407699e-01 -1.62555665e-02\n",
            " -4.62575818e-02  1.55114313e-02 -3.54314229e-02 -5.63942895e-02\n",
            " -1.32597012e-02  1.34079613e-02  7.40300051e-02  5.40329867e-02\n",
            " -1.86410404e-02  3.78535876e-02  1.44043459e-03  3.91034852e-02\n",
            "  5.95624986e-02  1.49459981e-02 -5.29430746e-03  7.64739640e-02\n",
            " -1.24234184e-02 -8.47900731e-03  1.80778670e-02 -7.36296762e-03\n",
            "  3.50230956e-02 -4.99689875e-02  1.06381207e-01 -3.33407929e-03\n",
            "  3.18973835e-02 -8.84981182e-02  1.00174809e-01 -2.74587133e-01\n",
            "  2.30889200e-02  2.10930102e-02  3.02371293e-02 -1.35211047e-01\n",
            " -5.56417697e-02 -1.22002414e-01 -6.98800111e-02  3.47062320e-03\n",
            "  2.51887849e-02  1.05959274e-02  2.58809440e-02 -6.80320219e-02\n",
            " -6.74811432e-02 -2.17303228e-01  2.15102864e-03  1.23085929e-03\n",
            " -3.49026808e-02 -8.83676639e-03  1.07792558e-01 -6.93665379e-02\n",
            "  3.12552264e-02 -3.12281776e-03  2.76193292e-02 -1.30892523e-02\n",
            "  5.23308170e-02 -8.41864222e-03  3.02375284e-02  5.33531677e-02\n",
            "  1.76715334e-01 -5.13437471e-03 -5.79066471e-02  1.29008040e-01\n",
            " -1.72850911e-02  3.57553395e-02  3.60488083e-02 -4.59222691e-02\n",
            "  3.12451148e-02  4.88102918e-02 -1.06697821e-01  1.91146834e-02\n",
            " -2.41329070e-02  5.30834316e-02 -3.43611339e-02 -5.10302509e-03\n",
            " -1.08267069e-01  1.56271565e-02 -7.13214387e-02 -1.75105772e-01\n",
            "  9.92833462e-03  1.22641224e-02 -1.00967495e-01 -2.90898928e-02\n",
            " -5.52118429e-03 -4.86186480e-02  1.95847133e-02  9.07507652e-02\n",
            " -5.41252725e-02  3.33477729e-02 -1.44158070e-01  8.82019661e-02\n",
            "  4.96448928e-02 -4.75237421e-02  3.41877541e-03 -1.72270345e-01\n",
            "  3.74922254e-02  6.82492743e-02  1.63494657e-02 -2.70503731e-02\n",
            " -7.14061032e-03  6.40146880e-03 -1.53363658e-02 -6.76262290e-02\n",
            " -1.29960167e-03 -5.54444532e-03 -1.75493514e-02 -8.89004155e-03\n",
            " -2.62848206e-02 -2.98883720e-02  1.28282223e-02  6.89214151e-03\n",
            "  1.00712051e-02  1.79212586e-02 -8.47793898e-05 -4.74909256e-02\n",
            " -5.61566786e-02 -9.61302475e-02 -2.43434718e-02  4.34939909e-03\n",
            " -9.91374946e-02  2.35053543e-03 -8.88961932e-03  4.39937473e-03\n",
            "  2.59986411e-02 -2.51210845e-02 -3.65776589e-02 -1.16273747e-02\n",
            " -4.11975539e-02 -1.38649437e-02 -3.84958738e-03 -2.82715301e-02\n",
            " -1.10415614e-01  9.02734721e-02  2.75409118e-03  6.13781075e-02\n",
            "  3.98428546e-02 -7.00689026e-03 -3.03067282e-02  1.38135248e-02\n",
            " -5.83612593e-02 -7.61500192e-02  2.62973019e-02  4.18498980e-02\n",
            "  2.89460845e-02 -1.23012361e-02  5.53950962e-02  2.26915203e-02\n",
            " -3.43873028e-02  3.42924480e-02 -1.22982503e-02  4.71915376e-02\n",
            "  5.93485047e-02 -2.12763456e-03  2.86542929e-02 -1.01173364e-02\n",
            "  2.58374052e-02  2.29156138e-02 -4.08990402e-02  1.16150016e-01\n",
            " -7.15759738e-02  3.36423300e-02 -6.40697595e-02 -4.46455823e-02\n",
            " -2.46532706e-02  5.30336459e-02 -7.50371735e-03 -6.95872766e-02\n",
            "  1.86910451e-02  8.83583547e-02  3.47756895e-03 -1.86785286e-02\n",
            " -9.31697879e-02  2.74128434e-02  4.78636910e-02 -7.51162481e-02\n",
            " -5.38726254e-02  4.72524169e-02 -3.04468731e-02 -7.88756052e-02\n",
            "  9.95667204e-03  5.90202040e-02 -1.05369748e-02  2.65010524e-02\n",
            " -8.77527909e-03 -4.22367307e-03 -8.65589685e-02  8.35852790e-02\n",
            "  2.86153124e-02  4.53735850e-02 -4.42271271e-02 -2.90395105e-02\n",
            " -1.12681653e-02  6.42947507e-02  1.53909264e-02 -9.95273064e-02\n",
            " -6.15472692e-02 -6.32959336e-02  6.69475972e-03 -1.75440548e-02\n",
            " -5.07153198e-02  1.62273555e-02  2.34659247e-02  1.96092431e-02\n",
            "  2.86549615e-02  7.70568716e-03 -1.65887537e-03  1.19546605e-02\n",
            " -5.27531496e-02  5.51699735e-02 -2.40259551e-02  9.88122228e-03\n",
            " -1.25109153e-02  3.67942319e-02 -8.90068589e-02 -4.24423689e-02\n",
            "  3.21032172e-02  3.18917416e-02  3.16009064e-02 -2.73443352e-02\n",
            "  3.43045091e-02  1.55292653e-02 -8.18976657e-02  3.16343273e-02\n",
            " -2.35182750e-02  2.77697791e-02  1.36132538e-01 -1.45260733e-02\n",
            "  5.89907018e-02 -2.13676822e-02 -8.30501629e-02  2.57453204e-02\n",
            "  7.34984229e-02  1.25529264e-02 -5.48014427e-02 -1.11351639e-01\n",
            " -1.38720162e-02  2.39763788e-02  2.72244988e-04  3.68067955e-02\n",
            " -1.82153558e-02  7.70893931e-02  4.85339833e-02 -7.36495120e-02\n",
            "  6.17918231e-02 -5.90958229e-02  4.60764905e-02  4.86960487e-02\n",
            "  9.92860912e-03 -5.84252327e-02 -9.41433993e-02  2.11377376e-02\n",
            "  3.55205220e-02  6.76009844e-02 -2.66325268e-02  7.40396939e-02\n",
            " -4.74524121e-03 -1.97215199e-02  2.47545894e-02  9.93943289e-02\n",
            "  9.39661902e-02 -3.14796293e-03 -7.76315257e-02  7.20273934e-02\n",
            "  1.21218829e-01  8.92449785e-02 -8.56480306e-03 -1.64579024e-02]  - intercept :  1.1455522991607177\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.09363544111946626\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:38,326]\u001b[0m Trial 207 finished with value: -0.05490775581770933 and parameters: {'count_threshold': 9, 'postag': False, 'voc_threshold': 1223}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.52012375 0.73087531 0.06424395 ... 0.06162461 0.21088007 0.0098191 ]\n",
            " [0.03205698 0.08590501 0.02017534 ... 0.39977819 0.15949781 0.10644223]\n",
            " [0.10124773 0.09468501 0.06575943 ... 0.11483742 0.1205848  0.02542868]\n",
            " ...\n",
            " [0.04678479 0.03689335 0.05648556 ... 0.12407368 0.14337116 0.03763301]\n",
            " [0.23311578 0.44209352 0.06509184 ... 0.09537587 0.11730266 0.01738849]\n",
            " [0.40845322 0.52635345 0.10080832 ... 0.03650015 0.3528959  0.02622635]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 0.00896663 -0.00082349  0.02008213 -0.01137601  0.12703334  0.00092654\n",
            " -0.08930739  0.0401786  -0.02819758  0.00508803 -0.03105851  0.08734059\n",
            "  0.05215546 -0.00447338 -0.01115543  0.03382942 -0.00658687 -0.00542234\n",
            "  0.04147063 -0.02229616 -0.17395165  0.02498472 -0.00416836  0.00300908\n",
            " -0.0215234  -0.02015711 -0.05790632  0.07206666  0.02111737  0.0620423\n",
            " -0.04183625 -0.05934009  0.06969613 -0.06392806  0.04658099 -0.029927\n",
            "  0.03201418 -0.03841378 -0.12577986  0.03572961  0.0358566  -0.00359009\n",
            " -0.03709366  0.07112455 -0.06842615  0.00914054  0.03451916  0.03075825\n",
            "  0.02890985 -0.02646485 -0.09734269 -0.00282099 -0.01149972  0.05270582\n",
            "  0.03570389  0.03506586 -0.04150356  0.00385302  0.02155991  0.03231916\n",
            "  0.02510566 -0.08712191 -0.09278871  0.0007585   0.0204398   0.09589431\n",
            "  0.17105904 -0.02144856  0.07487171  0.02346203  0.0482674   0.1174685\n",
            " -0.03163504 -0.07108706 -0.00062415  0.06657602  0.03626535 -0.02440573\n",
            " -0.00968163  0.08943492  0.05187781  0.01957372  0.01778518  0.04333208\n",
            " -0.07267356  0.06402939  0.13118823  0.03612713  0.05909888 -0.1130505\n",
            "  0.01099811 -0.04357562  0.09603182  0.00318106 -0.03003289  0.06768076\n",
            "  0.02168197  0.03267367 -0.03335521  0.02286107 -0.09810148 -0.05877702\n",
            "  0.06703788  0.0822963  -0.05117839  0.00379042  0.08268666  0.00714822\n",
            "  0.06140332  0.02293778  0.17708815 -0.01587186 -0.10705852 -0.06850716\n",
            "  0.05039263 -0.01812328 -0.03323922 -0.02097978 -0.05193108 -0.07294076\n",
            "  0.08132951 -0.01228524 -0.07951945 -0.047524   -0.05556602 -0.03491824\n",
            "  0.03674012  0.07076578 -0.16473849 -0.0406993  -0.03171964 -0.02482505\n",
            " -0.01339415 -0.06059173  0.03509308  0.10389492  0.0950528  -0.03221892\n",
            "  0.08029031 -0.04890981  0.03327948  0.1045078   0.04678029  0.06350887\n",
            " -0.06417847  0.06393145  0.04183549  0.06572065  0.13029915  0.15995525\n",
            "  0.03643163  0.02867286 -0.05188571  0.09021244  0.00979088 -0.0157725\n",
            "  0.00706229 -0.07064689  0.02349509  0.00607091 -0.03389239  0.00777294\n",
            "  0.06074862 -0.02161455 -0.05006198 -0.02350401  0.00701766  0.01734482\n",
            "  0.04000871  0.01845917 -0.06253846  0.10280996 -0.07133746 -0.03080129\n",
            " -0.02174197  0.00899868  0.00187768 -0.07450972  0.06781113 -0.00323474\n",
            " -0.02804822  0.04750682 -0.02583584  0.07115419  0.04841231 -0.04086357\n",
            " -0.03099174  0.02961198 -0.23249786  0.08136557 -0.01140333 -0.10430547\n",
            "  0.00056161 -0.11268473 -0.03483785 -0.14245184 -0.1295992  -0.0843587\n",
            "  0.01365184 -0.03922454 -0.03071596 -0.13668853  0.1282817   0.01617494\n",
            "  0.06077037 -0.01280058 -0.02434115  0.04469163 -0.07814189 -0.01781009\n",
            "  0.00475486  0.03317194  0.02720197 -0.01550815 -0.07338619  0.03747582\n",
            "  0.05276521  0.00326794 -0.04273339  0.04019454  0.06827651 -0.02846845\n",
            "  0.0240884   0.03378626 -0.08296942  0.10936301  0.01237323 -0.03211622\n",
            " -0.0227701  -0.03192865  0.00082658 -0.02419825 -0.08592335  0.12821156\n",
            "  0.00270503  0.07841052  0.03534134 -0.06166754 -0.06742558  0.00140481\n",
            " -0.0324086   0.0659162   0.07935245 -0.06296578 -0.10802355  0.02073391\n",
            "  0.06687198 -0.02222549 -0.0803884   0.0930385  -0.1036358  -0.17741603\n",
            "  0.06290349 -0.09248733 -0.0361153  -0.06116641 -0.02183893 -0.07090468\n",
            " -0.01229841  0.04610331  0.0059681  -0.12144423  0.08372144  0.05228046\n",
            "  0.16090951  0.04281328  0.05780822  0.05154813 -0.09880521 -0.00110269\n",
            "  0.1063759   0.14211405 -0.05851288 -0.01639985  0.0410824  -0.05755602\n",
            " -0.10588898  0.11298883 -0.06240688 -0.16326683  0.04795987 -0.17757373\n",
            "  0.15358489  0.01486665  0.08246519 -0.10443343 -0.04601233 -0.10958311\n",
            " -0.00157226 -0.07224384 -0.07183728 -0.15432216 -0.0694983  -0.07469\n",
            "  0.05388243  0.0761735   0.13237853 -0.02596496  0.11192885 -0.00298662\n",
            " -0.01680745  0.02683298  0.05383411  0.15842081 -0.07558238 -0.01794524\n",
            "  0.02181031  0.10483002  0.02984417  0.06347982 -0.0303162   0.0042011\n",
            " -0.02607946 -0.05964111 -0.03310491 -0.06023242 -0.05484659 -0.11300224\n",
            " -0.02455308 -0.00776909]  - intercept :  0.7951363907423561\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.05490775581770933\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:38,887]\u001b[0m Trial 208 finished with value: -0.28287393713603404 and parameters: {'count_threshold': 9, 'postag': False, 'voc_threshold': 2026}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[6.12364468e-01 2.75623394e-02 1.10358208e+00 ... 6.01069735e-01\n",
            "  5.42867967e-02 6.67715520e-03]\n",
            " [2.18709041e-01 3.15071699e-02 3.63945979e-01 ... 3.76844083e-01\n",
            "  5.41395060e-02 8.76376620e-03]\n",
            " [6.70778768e-02 3.43436732e-02 5.52289648e-02 ... 1.32439096e-01\n",
            "  5.36976339e-02 4.37929843e-02]\n",
            " ...\n",
            " [9.56717572e-04 0.00000000e+00 4.65136035e-02 ... 0.00000000e+00\n",
            "  6.54889801e-02 3.70636162e-02]\n",
            " [4.24560329e-01 6.86873464e-02 6.44932626e-01 ... 1.06023036e+00\n",
            "  2.16976291e-01 4.93396935e-03]\n",
            " [0.00000000e+00 1.49187109e-02 1.10929911e-01 ... 0.00000000e+00\n",
            "  2.65542356e-02 6.84646656e-02]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-0.0117873   0.00895374  0.03546237 -0.00192198  0.03232787 -0.05904719\n",
            "  0.03481812 -0.07054039  0.03382519  0.00114931  0.02885663 -0.04141653\n",
            " -0.0277884  -0.07656632 -0.00916356  0.01597375 -0.05913571 -0.03961203\n",
            "  0.04930867  0.03844502  0.02254829  0.10288086  0.09267704  0.06313777\n",
            "  0.05641507  0.01467733  0.00271732 -0.04654243  0.03694282  0.17181965\n",
            "  0.10524571 -0.10969527  0.13499052  0.00813764  0.05257493 -0.06718523\n",
            " -0.00802214  0.01132281  0.03594369 -0.04498191  0.12256885 -0.00664645\n",
            " -0.02986113  0.03119756 -0.09035491  0.07982868  0.01244639 -0.02722018\n",
            " -0.04307356  0.02860661  0.0300221   0.0727653   0.1203926   0.03381084\n",
            "  0.02792993 -0.06556198  0.16763414  0.04894078 -0.0647284   0.01017906\n",
            "  0.10520432 -0.07249759 -0.01205643 -0.02705758  0.0792796  -0.0052619\n",
            "  0.0227761   0.17977044  0.11563343 -0.0201113   0.06523627 -0.04385215\n",
            "  0.05903234  0.06328348 -0.02182826 -0.10311826  0.11475223 -0.01034119\n",
            " -0.14938341  0.04021965 -0.07745215 -0.06097291  0.05669188  0.04729104\n",
            "  0.03834787  0.16698262 -0.03086599  0.13596104  0.0193763  -0.06877791\n",
            "  0.0031095   0.10974453  0.05477435  0.09624232  0.02110141 -0.0019179\n",
            "  0.02472009 -0.02729411  0.10541249 -0.00090864 -0.05518458  0.0062779\n",
            " -0.06046312  0.00502066  0.00390479  0.06883556  0.04368825  0.06333503\n",
            " -0.04596051  0.1282732  -0.04630502 -0.11233344 -0.00927003  0.02858069\n",
            "  0.1845785   0.18435907 -0.12681396  0.14946851  0.04140394  0.07342664\n",
            "  0.04388745 -0.0353217   0.0688951   0.09172352 -0.0644432  -0.05136151\n",
            " -0.07242733  0.0704103   0.11846065  0.12056905  0.0701959   0.08147452\n",
            "  0.05689797  0.0042816  -0.03376204 -0.05060947  0.12022285 -0.05259634\n",
            "  0.09420928 -0.02768962  0.04094839  0.00176676 -0.0436927  -0.01447824\n",
            "  0.14278936 -0.01606437  0.10175414 -0.11310427  0.01603429  0.08829753\n",
            "  0.05799937 -0.05046167 -0.01558112 -0.07749631  0.21716801  0.0985669\n",
            " -0.03358892  0.04429526  0.020994    0.00288849  0.11087413  0.04065597\n",
            "  0.12136567  0.01037341 -0.05308603  0.05299771  0.00231815  0.09963023\n",
            "  0.10939118 -0.02037234  0.03062396  0.00036698  0.08619226 -0.05517784\n",
            "  0.099844   -0.05693135  0.05032064  0.08307486 -0.052981    0.0208943\n",
            "  0.09698149 -0.03073493  0.01217868 -0.02546354  0.08879333  0.04457868\n",
            " -0.05193276  0.03967226  0.16747738 -0.01001564  0.06655005  0.08831802\n",
            "  0.00852498  0.09096769  0.03777973  0.2284111   0.03781442 -0.03325312\n",
            " -0.05989982  0.02655339  0.11440481  0.16297948  0.09046998 -0.00041192\n",
            " -0.03755089  0.07620574  0.02791193 -0.027097    0.0709268  -0.03799199\n",
            "  0.04381193  0.07060943 -0.09111625 -0.10676303 -0.03400119  0.12675741\n",
            "  0.00973362  0.18508998  0.00788645  0.06074665 -0.14656173 -0.01540163\n",
            " -0.14833825  0.00259893  0.00050846 -0.04436589  0.09232516  0.07615586\n",
            "  0.08574581 -0.01094922 -0.17381499 -0.01873591  0.01964922 -0.03776453\n",
            " -0.00130648  0.03304692  0.03952543  0.09443298  0.00450022 -0.0142795\n",
            " -0.08531235  0.11150427 -0.09577185 -0.18136982  0.02090105 -0.04856378\n",
            "  0.14805526  0.09567484  0.10531474 -0.06508587  0.02639946 -0.01653343\n",
            " -0.08189821 -0.01482901 -0.13263816 -0.12898887 -0.03174812 -0.01512603\n",
            "  0.05508906 -0.12215517 -0.07332893 -0.15394916  0.02964118 -0.01071798\n",
            "  0.04185836  0.02694551  0.05038865 -0.06619005 -0.09287452  0.11666392\n",
            " -0.08564799 -0.14287691 -0.03659197  0.06031568  0.04659356 -0.18065156\n",
            " -0.06474827 -0.0334756   0.04357507 -0.02768236 -0.08383949 -0.04328466\n",
            " -0.03987948  0.00991188  0.03156245  0.1602877  -0.16360939  0.12802401\n",
            "  0.04547916  0.03095886 -0.17301416 -0.09109752 -0.00594239 -0.02530696\n",
            " -0.03130583 -0.01783147  0.12348058  0.00376297 -0.13598043 -0.01971494\n",
            " -0.00392523  0.04820188 -0.18220392 -0.034332    0.01062606  0.16510425\n",
            " -0.03017493 -0.11944935  0.06677366  0.03309896 -0.06613693 -0.14386579\n",
            "  0.05459399  0.0429311  -0.19891683 -0.23694368 -0.06223554  0.01714439]  - intercept :  0.15660519603777384\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.28287393713603404\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:39,472]\u001b[0m Trial 209 finished with value: 0.266910220847362 and parameters: {'count_threshold': 9, 'postag': False, 'voc_threshold': 1715}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[3.69074585e-01 3.74583369e-02 5.36630913e-01 ... 8.79369098e-01\n",
            "  3.39107382e-01 5.79666745e-03]\n",
            " [0.00000000e+00 2.22236227e-03 3.09406004e-02 ... 0.00000000e+00\n",
            "  2.29050286e-02 5.04948838e-02]\n",
            " [1.04211720e-03 3.80976390e-03 4.78847558e-02 ... 0.00000000e+00\n",
            "  1.45746135e-01 5.21076545e-02]\n",
            " ...\n",
            " [4.44171815e-01 2.88975090e-01 4.02492570e-01 ... 1.69573048e+00\n",
            "  9.40279506e-02 6.57722677e-03]\n",
            " [1.25361845e-02 6.10276168e-02 3.35014536e-02 ... 1.06628364e-01\n",
            "  8.40539746e-02 3.85949921e-02]\n",
            " [8.10535600e-04 3.95086626e-03 3.72436989e-02 ... 1.22127306e-01\n",
            "  2.48196653e-01 5.98945840e-02]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-1.60603191e-02  1.60642958e-02 -2.99740845e-02  1.79096913e-02\n",
            " -5.25472054e-02 -5.79018353e-02  2.62267085e-02  3.63063327e-02\n",
            "  1.53185825e-02 -8.19274448e-03  1.13634635e-02  3.54764717e-02\n",
            " -3.01540821e-02 -2.73754119e-02  8.02184047e-04  3.75316481e-02\n",
            "  4.10245062e-03  5.33490569e-02  1.84756884e-02  6.19486943e-02\n",
            " -1.26652903e-02 -1.03826528e-01 -1.00813904e-02 -4.14926572e-02\n",
            " -3.52324972e-02 -2.75231715e-02  4.67310713e-02 -4.21933075e-02\n",
            " -2.03307110e-02  6.57989917e-02  6.28115906e-02 -5.44144503e-02\n",
            " -5.23591783e-02  3.16201329e-02 -5.92419460e-03  6.94226612e-02\n",
            "  3.61909974e-02  5.60454245e-03 -6.77945141e-02  2.79810240e-02\n",
            "  2.92091254e-02 -4.82783793e-02  7.80347469e-02  4.17361243e-02\n",
            "  7.04767463e-02 -1.85495656e-02  3.08321357e-02  2.78919525e-03\n",
            " -3.83134200e-02  5.48844756e-02  2.58840127e-02 -2.52142827e-02\n",
            "  6.00528368e-02 -8.29206826e-02  5.79118463e-02 -5.55182386e-02\n",
            "  1.00505471e-01 -1.02880883e-01  7.14473933e-02  1.74968188e-02\n",
            " -2.18486263e-02 -7.65597181e-02  4.59116192e-02 -2.56617889e-02\n",
            "  3.01237304e-02  6.52520949e-03 -1.97251470e-02  2.28150248e-02\n",
            "  3.40279478e-02  3.62662731e-03  2.80186817e-02 -4.60742122e-02\n",
            "  1.02953491e-02 -8.52140975e-02  7.20583033e-02  4.03939454e-02\n",
            " -1.57326884e-02  5.61750767e-02  5.13878895e-02  2.07989852e-02\n",
            "  4.94495704e-02  2.05042445e-02  9.43563935e-02 -2.67576635e-02\n",
            "  3.84745135e-02  7.69221635e-03 -8.74746070e-02 -3.33578887e-02\n",
            " -5.63562328e-02  1.18479729e-01  5.53357129e-02  4.96135760e-02\n",
            " -1.33591492e-02  1.99834171e-02 -2.76905408e-02  2.43055870e-03\n",
            " -7.72594161e-02  1.05649298e-02 -2.61801401e-02 -9.16344969e-02\n",
            " -8.52360247e-02  7.17206952e-02  4.61095458e-02  4.38813876e-02\n",
            "  7.08494307e-02  4.59109971e-03  7.65379474e-04  1.13032841e-01\n",
            "  1.21085736e-02 -7.20695496e-02  4.43583443e-02  1.14681700e-01\n",
            " -5.92744787e-02  9.75552114e-02 -8.09080478e-03  1.18014313e-01\n",
            " -7.58246800e-02 -7.58246800e-02  6.00738167e-02  9.99940099e-02\n",
            "  3.98473102e-02 -2.71649417e-02  3.67709628e-02 -7.16037430e-02\n",
            " -2.66546173e-02  6.49255221e-02  9.43137324e-02  3.36389054e-02\n",
            "  3.25114240e-02  6.35430621e-02 -1.28088973e-01 -2.91697506e-02\n",
            " -9.64960753e-02  1.12778252e-02 -5.54633957e-02 -5.35187288e-02\n",
            " -1.03078479e-01  3.05367758e-02 -3.29033891e-02  1.40930748e-02\n",
            " -6.77284294e-03 -1.14296093e-01 -5.24128275e-02  1.09747865e-01\n",
            " -5.22215789e-02 -7.06189055e-02 -1.24415132e-02  1.93607819e-02\n",
            "  8.44908161e-02 -8.73470392e-02  8.78659211e-03 -2.91279591e-02\n",
            " -8.56969655e-03 -5.04882707e-02  8.43181794e-02 -1.40029142e-02\n",
            " -5.08478662e-02 -1.21788908e-01  3.55737944e-02  9.98378622e-03\n",
            " -2.88564106e-02 -7.12362346e-02  6.84357803e-03 -6.61568794e-02\n",
            "  5.06237976e-02 -2.34930514e-02 -7.68400968e-03  8.47399626e-03\n",
            "  1.95361989e-03 -5.53872052e-02 -3.47940446e-02 -5.87688823e-02\n",
            " -5.21391298e-02  1.04312446e-01  5.39149324e-02 -1.63778768e-01\n",
            " -6.34070395e-02 -1.76036720e-02 -1.57860923e-01 -1.90841683e-02\n",
            "  1.11623630e-01  8.02340658e-02  4.81056185e-02 -2.49707107e-02\n",
            "  3.73950343e-02 -1.06987340e-01  4.00046075e-04 -1.18521785e-02\n",
            " -5.39072072e-02  5.40613745e-02  1.99169403e-02  1.54433329e-02\n",
            " -4.61692337e-03  8.49949772e-02 -1.18010136e-01 -3.60512317e-03\n",
            " -1.26745101e-01 -8.75394053e-03 -6.14477913e-02 -8.93690212e-02\n",
            "  1.68930917e-02  2.91267276e-02  2.52902451e-02  7.51011360e-02\n",
            " -4.98246970e-03  6.21078418e-02 -2.04698630e-01  7.65965816e-02\n",
            " -1.05693019e-01  1.15631801e-01 -4.08154114e-02 -8.66460372e-02\n",
            "  5.58337403e-02  8.24994516e-02 -1.09216934e-03 -7.72171510e-02\n",
            "  5.27538887e-02 -5.93002379e-02  2.51609371e-02 -5.14990072e-02\n",
            " -8.56206955e-02 -5.98960226e-02  1.02650440e-01 -4.24056698e-02\n",
            "  4.56403990e-02  5.42607004e-02  1.57586350e-01  3.58953182e-02\n",
            "  1.63254976e-02  9.32895781e-02  5.14434954e-02 -7.85944633e-02\n",
            " -1.51896970e-02 -9.65256923e-02 -3.64507289e-02 -1.51348265e-02\n",
            "  9.92727058e-02  1.20198090e-01  4.11437074e-02 -1.78627867e-01\n",
            "  1.09123033e-01 -7.60866681e-02  8.69839772e-03 -5.30738466e-02\n",
            "  1.56681302e-02 -1.46195015e-01  6.98856038e-04 -7.58773721e-02\n",
            " -8.63949859e-03 -1.45633881e-01  4.01579506e-02  1.76802235e-01\n",
            " -9.73871833e-03 -1.53069353e-01  1.65815598e-01  2.95830748e-02\n",
            "  4.24140903e-02  1.09785239e-01  9.27820006e-03  1.29772477e-02\n",
            "  3.36461348e-02  1.21720844e-01 -1.79477855e-02 -9.18075682e-02\n",
            " -4.11213074e-02  1.62487314e-01 -9.29687549e-02 -7.58904006e-02\n",
            " -7.58904006e-02  3.81878153e-02  7.39129891e-02  2.31350881e-02\n",
            " -1.06339165e-01  1.97398085e-01 -1.22268516e-01 -1.02994206e-01\n",
            "  8.62209918e-02 -1.83159963e-02  9.61345704e-02 -1.95741957e-02\n",
            "  1.49849273e-01  6.77763952e-02 -4.52051743e-02 -7.50144067e-02\n",
            " -6.58868713e-02 -1.63801673e-01 -4.19750271e-02 -2.00640072e-02\n",
            "  1.32993155e-02 -1.23086723e-03 -3.20217459e-02  1.20369768e-01\n",
            "  4.07609386e-02 -1.08390748e-01  8.88502134e-02  7.46386053e-02\n",
            "  1.33986519e-01  6.78646449e-02  1.70182381e-02 -1.10359592e-01\n",
            " -1.21361601e-01 -1.54974100e-04]  - intercept :  0.7086731733649669\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.266910220847362\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:40,042]\u001b[0m Trial 210 finished with value: 0.07436933112020747 and parameters: {'count_threshold': 10, 'postag': False, 'voc_threshold': 6131}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.1532021  0.1904191  0.08010839 ... 0.         0.         0.01779828]\n",
            " [0.10395223 0.22458699 0.01267028 ... 0.45241642 0.         0.07136463]\n",
            " [0.         0.         0.01297143 ... 0.45241642 0.         0.04512734]\n",
            " ...\n",
            " [0.32845974 0.4128326  0.20687212 ... 0.         0.         0.0055117 ]\n",
            " [0.         0.         0.01297143 ... 0.         0.         0.02290631]\n",
            " [0.00106718 0.05857643 0.04545495 ... 0.         0.         0.01837234]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-1.09219144e-01 -4.09944948e-02 -2.34057873e-02 -3.81812696e-02\n",
            " -9.62456119e-02 -6.59423472e-02 -9.99937460e-02  5.42995052e-02\n",
            " -7.34012234e-02  1.78839524e-02 -5.63424301e-02  2.86512423e-02\n",
            "  4.62877008e-03  4.21398788e-02  7.20304534e-02 -2.07450128e-02\n",
            " -6.34801045e-02  3.39025928e-02 -9.08089176e-03  5.39098338e-02\n",
            "  7.52176845e-02 -6.23993160e-02  1.12008411e-01  1.67949442e-02\n",
            " -3.70742370e-02  1.20305530e-02  6.88557486e-02 -1.43215707e-02\n",
            "  1.70653938e-02  8.72168431e-02 -2.41129200e-01  3.20206096e-02\n",
            " -5.71808773e-02 -6.81756756e-03  1.49860533e-01 -9.55186234e-02\n",
            "  1.83012061e-04 -7.79329485e-02 -1.71468512e-02  1.84321333e-01\n",
            "  6.14908528e-03  1.17428117e-01  2.11199041e-03 -1.50103123e-01\n",
            " -4.76443541e-02 -9.00094609e-02 -2.77565006e-02  1.16975419e-01\n",
            " -3.60713588e-03 -7.26744681e-02 -3.40427439e-02  3.95704705e-02\n",
            " -9.58439967e-02  8.26634634e-02 -7.68900200e-02  4.59021850e-02\n",
            "  6.25503905e-02  1.72058138e-01 -3.63155986e-02 -5.81113353e-02\n",
            "  1.82034190e-02  4.45677978e-02  1.36357610e-03  1.54838241e-01\n",
            "  1.84229002e-01 -2.23637101e-01  9.17079155e-02 -1.51043964e-01\n",
            " -7.18986913e-02  2.71023360e-01 -6.12487408e-02 -7.67874097e-02\n",
            "  1.95909189e-01 -1.33330302e-01  5.85856460e-02 -1.12667760e-01\n",
            " -1.19944280e-02  1.18075934e-02 -1.81115609e-02  5.28165472e-02\n",
            " -1.39553552e-01  1.61002486e-01 -5.04917887e-02 -8.11602790e-02\n",
            "  2.18772890e-03 -2.06650798e-02 -1.17989064e-01  1.08808563e-01\n",
            "  6.94111264e-02  1.27770812e-01 -3.21687972e-02  2.41803899e-02\n",
            " -1.14515954e-01  1.76514178e-01 -6.07662730e-02 -7.69697848e-03\n",
            " -7.95674293e-02 -4.08783667e-02 -5.88592937e-02  1.19986540e-01\n",
            " -1.26918963e-01  1.81398467e-02  1.17204707e-02  3.52726249e-03\n",
            "  1.14583462e-01 -7.31939216e-02  2.30509699e-01 -1.44489455e-01\n",
            "  1.27336636e-01 -5.32639938e-02 -3.02959191e-01  2.83877242e-01\n",
            " -1.30787017e-01  2.61464056e-01 -4.38686283e-02  1.72713679e-01\n",
            "  5.35931308e-02  7.38500965e-02  1.03946776e-01 -8.93485989e-02\n",
            "  9.98147948e-02  2.89755607e-02  7.81794415e-02 -7.63223637e-03\n",
            " -1.55942256e-02  1.10088383e-01  1.49417572e-01  2.23799131e-01\n",
            "  1.01550336e-01  1.02383977e-02  9.14134207e-02 -1.18201798e-01\n",
            " -3.63908719e-02  2.07982997e-01 -5.98854927e-03  9.46016310e-02\n",
            " -9.00686673e-02  1.88241889e-01 -5.57635798e-02  5.47430948e-03\n",
            " -4.44870493e-02 -1.20366707e-01 -2.22661283e-02  1.98734938e-02\n",
            " -1.63055777e-03 -3.43740205e-02  3.27836277e-03  9.31761058e-02\n",
            "  7.40098553e-02 -1.80429496e-02  2.41107301e-02  4.60847810e-02\n",
            "  5.13714517e-02 -5.57024936e-02 -2.90126869e-02  4.92131790e-02\n",
            "  2.24440369e-02 -5.62581636e-03  2.33201357e-02  1.58030740e-01\n",
            " -7.29043875e-03 -3.50017736e-02  2.00648859e-01  1.30586059e-02\n",
            " -8.73117253e-02 -1.97946778e-02 -1.09668407e-01  2.56302213e-02\n",
            " -1.54544707e-03  8.97663122e-02 -5.51887986e-02 -1.16531532e-01\n",
            "  1.30147934e-01 -2.89223529e-02  2.80745119e-02  9.29728188e-02\n",
            " -1.10410126e-01 -1.23425054e-02 -5.43309421e-02  1.50269712e-01\n",
            "  6.87137519e-02  1.49108281e-02  1.59838288e-01  1.38378977e-01\n",
            " -7.13591795e-02 -6.32254692e-02  2.80475470e-02  1.72442409e-01\n",
            "  9.73366876e-02  1.19747699e-01 -6.08840940e-02  3.22168006e-04\n",
            " -4.25281947e-02  2.38216015e-01  4.11385873e-02 -1.23136290e-02\n",
            "  6.44639096e-02  1.02992157e-01  1.63473343e-01 -6.06821851e-02\n",
            " -9.11534121e-02  2.10562938e-02 -9.94074043e-02  1.43886689e-02\n",
            " -8.67011372e-02  8.36259894e-02  8.47029672e-02 -1.92899084e-02\n",
            " -2.09855459e-02 -1.15281489e-02 -4.35393545e-02  9.15718588e-02\n",
            "  3.33450773e-02 -1.04678970e-01  5.83743258e-03  1.19052856e-01\n",
            " -8.76111114e-02  3.28620659e-02  3.19240743e-02 -2.91113056e-01\n",
            "  2.55397008e-02 -2.00199622e-01  3.34998167e-02 -4.04981786e-02\n",
            " -3.08561439e-02  3.53631908e-01 -2.15139279e-03  5.37795571e-02\n",
            "  1.06135716e-01  2.91643027e-02  5.68779722e-02  1.06654381e-01\n",
            " -7.64827990e-03 -2.77854472e-02  6.71042379e-02  1.40292130e-01\n",
            "  3.26020519e-02 -1.86171878e-02  5.53526933e-02 -7.28852035e-02\n",
            "  5.48986439e-02  1.07312006e-01  1.30998568e-01 -1.04648527e-01\n",
            "  1.12901202e-01 -6.02269804e-02  4.89231073e-02  1.07823050e-01\n",
            "  4.51162541e-02  3.40935300e-02 -1.00710338e-01  2.17844367e-01\n",
            "  3.95720219e-02 -3.39591718e-02 -9.54160958e-02 -1.29082494e-01\n",
            "  9.61633809e-02 -6.68697557e-02  4.51778116e-02  1.31645899e-01\n",
            " -7.97389822e-02  3.24466228e-02 -8.31063541e-02  1.45902958e-01\n",
            "  1.26004582e-01  2.12090664e-01 -2.25486277e-02  2.18678113e-01\n",
            " -6.81777173e-02 -1.33627157e-01  8.24942419e-02  8.49512219e-02\n",
            " -5.62540778e-02  1.44203728e-01 -2.51980347e-02  7.70282364e-02\n",
            "  3.41649731e-02  2.23327239e-01 -1.20226651e-01  7.32225028e-03]  - intercept :  -0.04398306379991357\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.07436933112020747\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:40,549]\u001b[0m Trial 211 finished with value: 0.20848700289679128 and parameters: {'count_threshold': 10, 'postag': False, 'voc_threshold': 1022}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.06792573 0.18433591 0.06815297 ... 0.05945644 0.         0.02825969]\n",
            " [0.30868415 0.42301442 0.15569216 ... 0.00432991 0.         0.0101594 ]\n",
            " [0.35006338 0.13938963 0.10409883 ... 0.01301353 0.         0.00710026]\n",
            " ...\n",
            " [0.02069547 0.03369269 0.01191496 ... 0.04460166 0.         0.0894536 ]\n",
            " [0.01793289 0.02593619 0.07067168 ... 0.19922049 0.         0.0329577 ]\n",
            " [0.26783604 0.20376711 0.17202491 ... 0.00650676 0.         0.01627269]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-2.82020457e-03  3.07731203e-02 -4.23677920e-02 -5.89583743e-02\n",
            " -4.38517321e-02  1.94156651e-04  8.23165748e-02 -1.19135625e-02\n",
            " -1.24429664e-02 -9.87914488e-03 -1.23930236e-01  1.76719791e-02\n",
            " -1.09137682e-01 -5.03265679e-02  1.10194180e-01  1.48096459e-03\n",
            "  1.60532458e-02 -5.95612813e-02 -5.20691112e-02  2.21434629e-02\n",
            "  4.57397926e-02 -1.41727934e-01  5.14060171e-02  1.80947447e-02\n",
            " -1.23187873e-02  5.82602694e-02 -4.15217131e-02  3.26712977e-02\n",
            "  6.08564122e-02 -3.84937220e-02 -6.23175061e-02 -7.84222842e-03\n",
            " -8.24762969e-02  8.53937178e-02  1.10530305e-02  2.26500842e-01\n",
            " -3.00408614e-02  2.74903885e-02 -7.76074416e-02  5.88112597e-02\n",
            "  7.28721776e-02  6.62924405e-02 -8.08090153e-03  1.09819464e-01\n",
            " -6.19020293e-02 -4.73674326e-02 -1.10824409e-01  6.02723216e-02\n",
            " -7.13257723e-02 -7.20133002e-02  9.97799538e-02 -7.01889956e-03\n",
            " -6.23084390e-02 -1.82993861e-02  5.80777979e-02 -1.24573590e-02\n",
            " -7.58437439e-02  6.88746732e-02 -6.08939422e-02  3.82031055e-02\n",
            "  1.84366586e-02 -2.71460173e-02  4.07366259e-02 -5.81861034e-02\n",
            " -1.80469958e-01  1.36329595e-01 -4.08631438e-02 -4.05421126e-02\n",
            " -3.61429137e-02  4.26137338e-02  1.98365365e-01  1.25149039e-01\n",
            " -5.46960001e-02  8.27944117e-03 -1.90865908e-03 -6.70147012e-02\n",
            "  5.93720311e-02  8.62065206e-02  2.43445869e-02 -4.37093846e-02\n",
            "  3.40120714e-03  1.46859822e-02  4.20786925e-02  2.69885807e-02\n",
            "  2.40237513e-02 -9.76682439e-02 -9.42535310e-02  4.17221291e-02\n",
            " -1.72983076e-01 -8.45049401e-03 -2.04026810e-01 -4.66507782e-02\n",
            "  2.11019548e-02 -3.85473069e-02 -1.58749744e-01 -9.95186392e-02\n",
            " -1.74169598e-02  7.73011198e-02 -1.46581430e-01 -7.18597892e-02\n",
            "  1.98122301e-01  4.44266239e-02  1.11863590e-01  1.66464976e-02\n",
            "  5.92584597e-02  1.03888820e-01 -1.48486572e-02  4.84991205e-02\n",
            " -3.05334732e-02 -6.08427587e-02 -9.26078542e-02 -1.33759984e-01\n",
            " -1.69470470e-03 -1.23911652e-01 -1.71085331e-01  3.29703843e-02\n",
            " -1.15255066e-01  7.60603996e-02  3.44128079e-02  6.08887629e-02\n",
            " -8.16304088e-02 -2.13666223e-01  2.55821417e-02 -1.26393923e-01\n",
            " -3.43318686e-02 -2.85561806e-02  1.52308481e-02  7.23143116e-02\n",
            "  9.15808598e-02  1.76089065e-01  9.71193106e-02 -7.34789825e-02\n",
            " -1.41638081e-01 -1.95613494e-02 -7.43826242e-02 -6.21481011e-02\n",
            "  3.51785026e-02  6.56057173e-02 -1.31189380e-01  2.51844477e-02\n",
            " -5.15329730e-02  2.84067019e-03 -8.37936458e-03 -4.21960561e-02\n",
            " -1.51735617e-02 -3.46849755e-02 -3.88496059e-02 -1.21853510e-02\n",
            " -2.48439674e-02  3.44681871e-02  6.11107689e-02 -2.33862173e-02\n",
            " -9.39757261e-04 -6.13589967e-02 -6.38245841e-02 -2.34475313e-02\n",
            " -6.14347415e-02 -1.64351843e-01 -4.99514201e-02  3.53504146e-02\n",
            " -1.36012102e-01 -6.36508155e-02 -1.58918456e-03  3.23989905e-02\n",
            " -1.11308480e-01  7.32376596e-03 -3.95783130e-02 -6.04775005e-02\n",
            " -1.37355112e-01  6.21227487e-02 -7.93410182e-02 -1.42740807e-02\n",
            " -4.32412113e-02  6.51031907e-02 -4.81754847e-02  3.90597421e-02\n",
            " -6.26158641e-03  2.80974031e-02 -5.83277391e-02 -3.26945189e-02\n",
            " -3.76262664e-02 -1.72461780e-02  1.35922931e-01  3.34033312e-02\n",
            "  6.36315582e-02  1.81922147e-02  4.52261696e-02 -4.51454498e-02\n",
            " -7.72437221e-02 -6.41905389e-02  1.01941202e-01  6.55858630e-03\n",
            "  3.80967827e-02  1.10584353e-01  1.01668603e-01 -1.19670449e-01\n",
            "  6.07680104e-02  5.37361323e-02 -8.00513880e-03  3.17504448e-02\n",
            "  7.89292882e-04 -1.06826840e-01 -6.97341233e-02  1.57044936e-01\n",
            "  9.06587988e-02  9.63106908e-02 -1.93008163e-01  3.61752998e-02\n",
            "  4.61163212e-02 -2.01718311e-01 -8.94217937e-03 -1.06802781e-01\n",
            "  8.73091924e-02  3.19403327e-01  9.36979651e-02 -5.94477193e-02\n",
            "  1.57734713e-01  7.55730480e-03 -7.79680141e-02  4.53139613e-02\n",
            " -1.62668300e-02  1.14591907e-03 -5.28369316e-02  2.20474480e-01\n",
            "  9.63482752e-02  1.35545960e-01  2.73172837e-02 -1.79173458e-01\n",
            "  5.61831655e-02 -9.02562816e-02  1.16110949e-01 -1.02797993e-01\n",
            " -9.05587138e-02 -1.29601058e-01  5.19801548e-02  1.17316970e-01\n",
            "  6.96338076e-02 -4.67613898e-02  2.37463830e-02 -1.87567758e-02\n",
            "  3.94061248e-02 -1.22033753e-01  4.65802629e-02 -1.41855860e-01\n",
            "  3.30850236e-02 -6.60197052e-02  5.00960209e-02 -8.11238346e-02\n",
            " -1.72519098e-02 -1.79846814e-01  1.52431090e-01 -7.75010347e-02\n",
            "  3.10493691e-02  5.62723474e-02 -3.95136209e-02 -9.50571647e-02\n",
            " -5.09558199e-02 -7.21403711e-02 -1.22145581e-01 -5.06229950e-02\n",
            " -5.62562322e-02  3.36050455e-02  2.09986197e-01 -5.19106551e-02\n",
            " -1.60042936e-01  5.96647287e-04  4.61547421e-02  1.17471071e-01\n",
            " -1.48506822e-01  1.39933118e-01 -4.69668749e-02  1.84053048e-01\n",
            "  9.39529844e-02 -4.83201260e-02  9.05666957e-02 -7.57592823e-02\n",
            "  3.71826321e-02 -9.75166082e-03 -4.49059350e-02 -1.38009494e-01\n",
            "  2.52587477e-02  1.02758287e-01 -8.97863501e-02 -7.36578560e-02\n",
            " -8.00099400e-02 -1.65659921e-02]  - intercept :  0.8342406030163427\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.20848700289679128\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:41,144]\u001b[0m Trial 212 finished with value: 0.1404430646398099 and parameters: {'count_threshold': 7, 'postag': True, 'voc_threshold': 1344}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.11117766 0.07678889 0.04477085 ... 0.01683823 0.40758453 0.02780128]\n",
            " [0.16155719 0.24589693 0.187241   ... 0.0378561  0.21349752 0.01167128]\n",
            " [0.         0.03326246 0.03328674 ... 0.0550484  0.23249062 0.06467617]\n",
            " ...\n",
            " [0.         0.         0.06088073 ... 0.01731932 0.16075223 0.10809426]\n",
            " [0.22580344 0.38798098 0.01346455 ... 0.01515441 0.1460553  0.01606318]\n",
            " [0.01722616 0.1096981  0.03436309 ... 0.27540836 0.25171518 0.03592762]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 2.69786931e-02  2.67501451e-03 -4.49141980e-03  1.33764268e-02\n",
            "  1.87723054e-02 -7.19675413e-02  1.27023665e-03 -3.44659151e-02\n",
            "  3.03517562e-02 -1.80223571e-02 -3.93581691e-02  6.50840585e-02\n",
            " -3.35656657e-02  4.45628415e-02  6.87115885e-04  6.25216641e-03\n",
            "  2.15843383e-02  7.62373375e-03  4.54624071e-02  7.25043713e-02\n",
            "  1.61241954e-02 -2.21132589e-02  7.40543787e-03 -1.78373237e-02\n",
            " -3.00629175e-02  5.47165696e-03  2.54935097e-02  2.52169533e-02\n",
            "  1.92766638e-02 -1.38859354e-02  5.37506385e-02  6.54488311e-02\n",
            " -1.08111026e-02 -2.50371991e-02  2.55952698e-02 -4.10860888e-02\n",
            "  1.78131358e-02 -8.65009012e-03  1.86101911e-02 -2.61768250e-02\n",
            " -1.44261250e-03  1.61846756e-04 -3.52635672e-02  2.10878436e-02\n",
            " -1.89780384e-02 -3.94001985e-02 -7.30035493e-02  9.75707358e-03\n",
            " -2.39371159e-02  7.58216402e-02  8.97224456e-03  8.67534624e-03\n",
            "  3.59566514e-02  2.47847687e-02  8.17212955e-02 -1.56974416e-03\n",
            "  1.06358608e-02  4.85295576e-02 -6.70475189e-02  1.36188723e-02\n",
            "  1.05057259e-02  1.49470164e-02 -1.86747592e-02 -2.28439234e-02\n",
            " -4.81158856e-02  1.14561801e-02  1.43759960e-02 -1.75324366e-03\n",
            "  2.92914998e-03 -1.40985244e-02 -1.38672339e-02  1.38461495e-02\n",
            "  2.40704219e-02 -9.49851690e-03  3.43508011e-02 -6.74063520e-03\n",
            "  5.91551484e-03 -5.44834890e-02  1.88194104e-02 -1.21273640e-02\n",
            "  4.06699968e-03  1.17224271e-02  2.92438558e-02 -1.53610838e-02\n",
            "  5.04152973e-02  4.80679075e-03 -2.06852243e-02 -6.21084833e-02\n",
            " -1.93110422e-02  9.25318056e-03  2.32604715e-03  2.51140246e-02\n",
            " -4.64839411e-02 -2.83680276e-02 -5.28493798e-02 -6.86234769e-02\n",
            "  1.53210286e-02  4.13572152e-02 -1.05827790e-02  1.74312435e-02\n",
            "  9.79955042e-04 -3.10059127e-02 -2.62552332e-02 -3.75194668e-03\n",
            "  2.57239929e-02  2.81747460e-02  4.72745681e-03  2.91262335e-02\n",
            " -1.26325309e-02 -1.41757705e-02  3.40655033e-02 -1.85548024e-02\n",
            "  3.72449654e-02  6.72460247e-02 -5.46883153e-02  1.03874157e-02\n",
            " -1.75603302e-02  7.30596161e-02  9.17141385e-03 -2.39670871e-02\n",
            " -1.05476459e-02 -1.00867978e-02  3.82162389e-02  6.69277217e-02\n",
            " -6.38206917e-04 -5.21523708e-02  2.81453859e-02 -2.43467962e-02\n",
            " -6.21879742e-02  4.81031940e-02  9.06026395e-02 -5.09596785e-02\n",
            " -5.63399412e-02 -2.03726574e-02 -4.13451938e-02  9.29134603e-03\n",
            " -1.23601762e-02 -8.66069269e-03  7.33187295e-02  1.93576103e-02\n",
            "  2.77875959e-02 -2.73405796e-02  2.12595093e-02 -2.45543797e-02\n",
            "  2.50298679e-02  3.52723150e-02 -8.48455268e-02  6.59963015e-03\n",
            " -6.09999975e-02  6.46823372e-02 -2.33435886e-02  2.61886270e-03\n",
            "  7.00372401e-02 -1.87299786e-02  1.91785862e-02  7.73484098e-03\n",
            " -3.47450790e-02  2.82075486e-04 -1.76984774e-03 -4.02485021e-02\n",
            "  1.74976438e-02 -1.07952450e-02  1.90857101e-02  6.33810766e-03\n",
            "  3.81607505e-02  1.29012448e-02  3.37748351e-05  7.97417622e-02\n",
            " -4.74544148e-05 -4.55396253e-02  5.97010205e-02 -1.70339288e-03\n",
            " -1.10448040e-03  2.00174278e-02 -5.17367397e-02 -6.41003235e-02\n",
            "  9.55299335e-03  1.56253836e-02 -3.78326558e-02  1.87433912e-02\n",
            "  1.21003754e-02  3.60144162e-02 -1.48882994e-02 -2.24142224e-02\n",
            " -2.29033819e-03  5.50779390e-03  9.06798067e-03  8.42752442e-02\n",
            "  1.95104100e-02 -5.57557664e-02  1.61070758e-04 -6.40804278e-02\n",
            "  3.44389605e-02 -1.72329595e-02 -1.90368623e-03  2.16147486e-02\n",
            " -3.84988473e-02 -3.23996883e-02  8.02294803e-02 -3.22527822e-02\n",
            "  6.89651317e-02 -4.28236056e-02 -9.67365884e-02 -4.55918404e-03\n",
            "  6.06213525e-02  3.75469003e-03  5.23814539e-02 -7.01913206e-03\n",
            " -3.70436981e-02 -9.97647959e-02 -5.23639285e-02  7.60466633e-04\n",
            "  1.21295514e-02 -1.31024770e-01 -1.07869966e-01  1.03548184e-02\n",
            " -3.78397817e-02 -4.01850729e-02  4.85375109e-02 -4.21239126e-02\n",
            "  5.86482839e-02 -2.76007848e-02  5.88077803e-03  2.00095621e-02\n",
            " -9.79508128e-02  1.44581072e-02  3.74738736e-03 -4.35230582e-02\n",
            "  1.57230110e-02 -1.26836891e-02 -4.28743216e-02 -8.94766213e-02\n",
            "  3.61041756e-02  1.11259838e-03  2.42150086e-02 -1.67724073e-02\n",
            " -1.59273415e-02  1.42079093e-02 -1.11015209e-02  1.88291367e-03\n",
            " -1.13551400e-02 -4.48222813e-02  3.70435614e-02  1.96459035e-02\n",
            "  1.73498381e-02  2.29741072e-02  2.45118030e-03  1.96167876e-02\n",
            "  3.09444145e-02  1.69595228e-02  3.79652636e-02 -2.52681380e-02\n",
            "  1.41493066e-02 -2.87579058e-02  2.89745534e-02  2.35311296e-03\n",
            "  9.75832555e-03 -8.28176924e-03 -2.08988589e-02  2.95511282e-02\n",
            "  5.59042591e-02  1.04187568e-02  1.33780293e-02 -7.48089524e-03\n",
            "  1.54368169e-02 -3.34086059e-02 -2.78469613e-03 -3.93958421e-02\n",
            "  2.25081508e-02  3.53174967e-02 -1.81881413e-02 -4.40154949e-03\n",
            "  5.39285789e-02 -3.27358704e-03  8.17052415e-03 -2.81498669e-02\n",
            "  8.00963773e-03  4.00746813e-02 -1.59206907e-03  3.35960714e-02\n",
            " -8.17697260e-02 -1.44763093e-02  2.48145283e-02  4.84756633e-02\n",
            " -5.35441680e-02 -4.46316918e-03  1.57272726e-02 -4.65875804e-02\n",
            "  7.99684472e-02  7.28338410e-02 -1.41479301e-02 -5.39671794e-02\n",
            " -3.51390360e-03  3.30614111e-02 -7.06504493e-03  6.06249042e-03\n",
            "  2.00887215e-02 -1.42614557e-02  2.63501344e-03  4.89596320e-03\n",
            " -2.54242899e-02 -5.26630690e-02  8.48318803e-03  7.92145181e-02\n",
            " -3.10680534e-02  8.19799575e-02 -4.98132430e-02 -1.62691020e-02\n",
            "  6.97304781e-02  3.43209301e-02  1.30735381e-02  5.93810796e-02\n",
            "  4.88153377e-02 -7.32402769e-03 -1.98060596e-02  1.44809158e-02\n",
            " -4.83557675e-02  1.52506872e-02  5.35276827e-02 -2.80496790e-02\n",
            "  7.58290557e-03  8.71346672e-03  1.30647010e-02 -4.80427947e-03\n",
            "  2.69847681e-02  7.76191629e-02  7.64233787e-03  2.69656922e-02\n",
            "  1.55606416e-02  1.29516252e-02  1.04196187e-01 -5.75601362e-03\n",
            " -6.81742392e-02  1.64169831e-02 -4.97071804e-02 -4.70819414e-02\n",
            "  2.50414113e-02 -9.91641202e-03 -1.16052686e-02  2.82431180e-02\n",
            "  4.14034416e-02 -3.32123313e-02  8.79098089e-02 -2.56890033e-02\n",
            " -2.20247542e-02 -3.23428468e-02  5.09136084e-02  1.27947099e-02\n",
            " -4.16864977e-02 -1.13418121e-01  5.21133322e-02  1.30141247e-03\n",
            " -1.22081477e-01  2.03860489e-02 -9.98175615e-02 -4.61790120e-02\n",
            " -1.46344033e-02  1.78486974e-02  4.73800657e-03 -1.70833000e-02\n",
            "  8.06535774e-02  4.74782766e-02 -7.05052741e-03  5.73351625e-02\n",
            " -3.57866524e-02 -3.10686356e-02 -4.33789356e-02 -5.68969927e-03\n",
            " -5.27243419e-02  7.56185463e-03 -2.26289112e-02 -1.64069866e-02\n",
            " -7.11922658e-02  5.53401990e-02  1.59806306e-02  2.60595382e-03\n",
            "  6.21446864e-03 -1.37202213e-03  6.57189797e-02 -3.87207262e-02\n",
            " -8.82678360e-02 -7.63945475e-03 -3.56447654e-02  1.49046946e-02\n",
            " -6.06948273e-02  7.20465655e-03  9.50351479e-03 -1.19431712e-02\n",
            " -1.61780890e-02 -3.55830106e-02 -1.63174432e-02 -1.49532859e-02\n",
            " -3.77488985e-02 -2.16092570e-02 -2.95636436e-02 -6.51682629e-02\n",
            " -9.90047848e-03 -1.65452600e-02  7.90093708e-02 -8.51458580e-02\n",
            "  5.35893803e-03 -1.84414333e-02 -4.29499247e-02 -3.22616779e-02\n",
            "  6.40912856e-02  8.82131544e-02 -2.40719830e-02 -2.70341372e-02\n",
            "  1.46156376e-02 -5.62759590e-02  1.08851292e-02 -3.01841673e-03\n",
            " -2.96836840e-02  6.04611269e-04 -5.88693634e-02 -3.31050953e-02\n",
            " -2.79563282e-02 -4.03862653e-02 -5.96978977e-02 -6.25013150e-02\n",
            "  1.81376029e-02  1.08950447e-01 -4.62691539e-02 -1.19409666e-02\n",
            "  3.89113469e-02 -5.12624935e-02 -5.46562668e-02 -5.28120136e-02\n",
            "  1.08856518e-02 -3.51724546e-02  4.84566306e-03  1.89024632e-03\n",
            "  1.29649169e-02 -2.54020831e-02  4.24255595e-02 -6.28448780e-02\n",
            " -2.58888656e-02 -1.62587743e-02 -1.31634193e-02  2.22111375e-02\n",
            "  1.73930603e-02 -2.28167225e-02 -6.74527353e-02 -9.25945429e-02\n",
            " -2.58032150e-02  2.21309494e-02  3.81865896e-02 -1.20144721e-01\n",
            " -4.66582189e-02 -4.34131430e-02  3.79544545e-02 -1.04917040e-01\n",
            " -3.18704258e-02 -3.11692999e-02 -5.27559972e-02 -5.15393259e-02\n",
            "  4.62063056e-03  2.26475895e-02 -4.12742012e-02  2.97675352e-02\n",
            "  3.60443340e-02 -5.91326374e-02 -7.35130579e-02 -7.93987530e-02\n",
            " -1.21588061e-01 -1.78831923e-01 -1.38531339e-02 -1.19224464e-04]  - intercept :  0.7543904202635541\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.1404430646398099\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:41,726]\u001b[0m Trial 213 finished with value: -0.13960897657227572 and parameters: {'count_threshold': 8, 'postag': False, 'voc_threshold': 9296}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.29854464 0.01703717 0.40732599 ... 0.05187518 0.00332638 0.01040468]\n",
            " [0.         0.00339367 0.         ... 0.13833381 0.         0.02774582]\n",
            " [0.35486342 0.02628789 0.51199321 ... 0.         0.00380158 0.        ]\n",
            " ...\n",
            " [0.05882365 0.01808453 0.09154346 ... 0.10060641 0.00713814 0.0364365 ]\n",
            " [0.         0.00796765 0.         ... 0.10375036 0.         0.09773285]\n",
            " [0.09933244 0.06995486 0.28278512 ... 0.03074085 0.06084091 0.01515026]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-0.00056969  0.02599087 -0.06459308  0.01788143  0.02773174  0.0080379\n",
            " -0.02710383 -0.08670223 -0.02300331  0.00233434 -0.0431728  -0.03247557\n",
            " -0.00145477  0.08521921 -0.01288262 -0.00513684 -0.08746151  0.05370517\n",
            " -0.00369092 -0.0146215  -0.08846061  0.07099415  0.03833707  0.04414654\n",
            "  0.00682503  0.08477205  0.07100279 -0.04617815 -0.00099685 -0.05705219\n",
            " -0.01297791 -0.13413902 -0.07009879  0.02533752  0.05161234 -0.08465631\n",
            " -0.04600307 -0.07880569  0.00956988 -0.00919574  0.05275359 -0.03190012\n",
            "  0.01466935 -0.0152182   0.0064409   0.00632674 -0.01764849  0.0378319\n",
            "  0.00530101  0.03868982 -0.03804464  0.06542651  0.05665416 -0.14457849\n",
            "  0.02868034  0.06594865 -0.00059064 -0.05587406 -0.01072268  0.03802592\n",
            "  0.01528975 -0.01753884 -0.10342729  0.05342284 -0.00268375  0.0138464\n",
            "  0.01285194 -0.05735315 -0.09152451  0.08907808 -0.01582924  0.045048\n",
            "  0.01743659 -0.05394529 -0.00026339  0.04171836 -0.08451426  0.01466836\n",
            " -0.06301012  0.04774709 -0.04266576  0.04493247 -0.00568213 -0.05487401\n",
            " -0.03496223  0.0833077   0.05685285  0.04554961 -0.08445249  0.00485582\n",
            " -0.01074345  0.07792494 -0.07566086 -0.13502372 -0.0659542   0.08122536\n",
            " -0.01943512  0.01765045 -0.00256685 -0.02405742  0.01623112 -0.09442693\n",
            "  0.05122158  0.04257791  0.00683527  0.00879233 -0.0197052  -0.05430713\n",
            "  0.06126656  0.01196321 -0.0547163   0.01846344 -0.01556825 -0.00468426\n",
            "  0.03519401 -0.05973221  0.10785577  0.01463945 -0.01069949  0.06279097\n",
            " -0.0367978  -0.03842364 -0.03842364  0.00598299  0.00256167  0.01579859\n",
            "  0.01050353 -0.07554904  0.00936593 -0.09869697  0.08849512  0.04605416\n",
            "  0.02757841 -0.05175249 -0.01429419  0.03669718 -0.01921395 -0.07507295\n",
            " -0.0235521  -0.04108566 -0.04057883  0.03772779  0.02171608  0.00033993\n",
            " -0.0590621  -0.03907751 -0.11415819 -0.008709    0.0531202  -0.06271945\n",
            "  0.0184117   0.00309538 -0.03866364 -0.00252661  0.04810878  0.01021176\n",
            " -0.00408508 -0.09297816 -0.01991845 -0.08471347 -0.04032413 -0.02136399\n",
            "  0.03600855  0.02813674 -0.02175959 -0.12205325  0.07147192  0.00818501\n",
            "  0.0503528  -0.05528633  0.07474869 -0.01036917  0.01594383  0.01880044\n",
            " -0.01427972 -0.0132961  -0.02853352 -0.05888592  0.04878311  0.00174245\n",
            " -0.02557924  0.00845935 -0.11197524  0.09598451 -0.14017182  0.003009\n",
            "  0.00696114  0.09594239 -0.04864569 -0.01691409  0.07127586  0.02568904\n",
            " -0.01044583  0.00418731 -0.02979138 -0.03785388 -0.01589347 -0.05165331\n",
            "  0.11121087 -0.0705973  -0.0804463  -0.04510008  0.01988409  0.13210935\n",
            "  0.01527927  0.00327043 -0.03436782  0.0307921  -0.02102984 -0.00532803\n",
            "  0.0132598  -0.03116628  0.01531246 -0.01065034 -0.0076077  -0.04162443\n",
            " -0.05722748 -0.0008926   0.03273805 -0.01826408 -0.00665695  0.00768836\n",
            "  0.0292343   0.00332244  0.00452037 -0.02737059  0.03782221 -0.00616695\n",
            "  0.00266964 -0.03571362  0.01933239  0.04242252  0.05020198 -0.00805043\n",
            " -0.00858144  0.06269951 -0.04027526  0.02805087  0.00647207 -0.01223753\n",
            " -0.02271493 -0.02933191  0.04029754  0.01752989 -0.00438927 -0.0344375\n",
            " -0.01404315  0.01226107 -0.02560973 -0.01475993 -0.02790205  0.02095093\n",
            " -0.02624422  0.00911644 -0.01184707 -0.00728648  0.00741687  0.00646578\n",
            "  0.09526977  0.0215126   0.01553178  0.06046826 -0.09491355 -0.044872\n",
            " -0.01269107  0.04406294  0.03114424 -0.047789    0.01291781  0.00581301\n",
            " -0.00122438 -0.12944022 -0.00812611  0.01240292  0.00576014  0.04051347\n",
            "  0.01664544 -0.10478229 -0.03925325 -0.05596447 -0.06534824 -0.07170141\n",
            " -0.0233701   0.00731331  0.02038139  0.00409456  0.01444739 -0.07068792\n",
            " -0.02168266  0.00533312 -0.06057156  0.02214758  0.0047281   0.00290378\n",
            "  0.0101375  -0.01538224 -0.00657295 -0.0735719   0.04729161  0.01397511\n",
            "  0.00261373 -0.05664038 -0.00603554  0.04458958  0.05887806 -0.01354208\n",
            " -0.04497264 -0.02256263 -0.01250301  0.0746137  -0.07372763 -0.00820768\n",
            "  0.06275851  0.02701897  0.00277225 -0.01760657  0.02833715  0.03247581\n",
            "  0.01330047  0.00556416  0.0172293   0.0196472  -0.02518769 -0.0185691\n",
            "  0.08336971 -0.0342797   0.00436178  0.00501188  0.07227235 -0.05268644\n",
            "  0.01014928  0.01014928  0.01391275 -0.04904159 -0.05531725  0.00752005\n",
            " -0.01146606  0.04508169 -0.01094913  0.05566904 -0.00620283 -0.03644036\n",
            " -0.01592198  0.00264049 -0.00234338  0.03763399 -0.05686962 -0.01812854\n",
            " -0.02900092 -0.0176359   0.0686657   0.03285381 -0.012534   -0.03537286\n",
            "  0.01016128 -0.04977795 -0.04054665  0.02061616 -0.06007824 -0.01315345\n",
            " -0.00303892 -0.14481425  0.0520121   0.0714152  -0.02921477 -0.0148425\n",
            " -0.16155667 -0.0070372   0.02633494 -0.04110079 -0.05182688 -0.02226211\n",
            "  0.03179404  0.03959805 -0.07466707 -0.01809296  0.03250252  0.04042146\n",
            " -0.05485397 -0.06428673 -0.01716905  0.02423442 -0.05623804 -0.03168227\n",
            " -0.02755826  0.07666846 -0.00309021  0.05863846 -0.02190219  0.01916506\n",
            "  0.00914303 -0.05051545  0.03040756 -0.06280062  0.03611462  0.00378476\n",
            "  0.03672869  0.04789212  0.0254648   0.0184487   0.01964983 -0.00494928\n",
            "  0.01466953 -0.01662444  0.03717886 -0.00657833 -0.04889172 -0.03780103\n",
            " -0.04904044  0.02158653 -0.00194836 -0.03225554 -0.03016715  0.02136385\n",
            " -0.00602189 -0.00713371 -0.04989916 -0.01213373]  - intercept :  0.8540674232375693\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.13960897657227572\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:42,329]\u001b[0m Trial 214 finished with value: -0.2532248640069667 and parameters: {'count_threshold': 6, 'postag': False, 'voc_threshold': 6421}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.05426154 0.0658559  0.03941534 ... 0.07672112 0.06920654 0.0439052 ]\n",
            " [0.         0.00462516 0.02519869 ... 0.         0.         0.0302039 ]\n",
            " [0.06485895 0.09259715 0.08758067 ... 0.18253741 0.11109976 0.03324561]\n",
            " ...\n",
            " [0.00406337 0.00513906 0.01679912 ... 0.         0.         0.03123412]\n",
            " [0.56353088 0.14393561 0.61119998 ... 0.         0.         0.        ]\n",
            " [0.0241659  0.04715056 0.05128901 ... 0.12074058 0.05572592 0.06992155]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-1.74393346e-02 -1.41746662e-02 -5.75147509e-03 -2.97764595e-02\n",
            "  3.16361217e-03 -6.50546962e-03 -3.93341353e-02 -5.97207439e-03\n",
            "  1.12691391e-02 -6.96899974e-03 -3.17296891e-02  9.49858935e-03\n",
            " -1.25103593e-02 -1.90163441e-02  5.32034162e-03  1.55579907e-02\n",
            " -5.40123713e-03 -2.07298605e-02  3.48307228e-03 -1.90426336e-03\n",
            "  4.73783786e-04  8.79326710e-03  9.47634339e-03 -4.90051104e-03\n",
            " -1.35742134e-03  6.78095834e-03 -2.42614148e-02  3.79450984e-03\n",
            "  6.27890704e-03 -3.12560002e-02  1.09772518e-02 -2.33177155e-02\n",
            " -1.35180416e-03 -1.15152633e-02  6.06169642e-03  1.98706613e-02\n",
            "  4.22784227e-02  7.72711618e-03  3.36247914e-02 -6.81809008e-03\n",
            "  1.03909582e-02 -6.81508197e-03 -1.12262855e-02 -1.76108617e-02\n",
            " -2.35500414e-02 -3.34034745e-02 -2.18301615e-02  1.61777430e-02\n",
            "  6.10176667e-03 -1.76176772e-02  1.31839063e-02 -3.54973212e-02\n",
            " -3.19092754e-04  5.92945254e-03 -5.41108667e-02 -2.30501650e-03\n",
            " -3.65825992e-03 -9.05539007e-03 -1.02466863e-01  9.26488443e-04\n",
            "  1.19965894e-02  9.72654733e-03  1.64955213e-02  9.55810695e-03\n",
            "  7.95729937e-03 -1.89907032e-03  7.57805256e-03  5.50862362e-03\n",
            " -4.78028336e-02  2.17641739e-03  9.81994831e-03 -6.08578221e-02\n",
            "  9.13590563e-03 -1.46970393e-02  1.94579147e-02 -2.93464063e-02\n",
            " -1.10794155e-02 -2.29314124e-02  5.28573022e-03 -4.70614070e-03\n",
            "  9.31131786e-03  4.56031835e-03 -6.92037559e-03  1.27256371e-02\n",
            " -1.73548296e-03  1.45871564e-02 -3.11819019e-02  8.38739369e-03\n",
            "  3.94004115e-02  7.15018277e-03 -3.09462425e-02 -4.06502819e-02\n",
            "  2.32633958e-02  1.64563030e-03  2.64621829e-02 -3.01988583e-02\n",
            " -1.76245617e-02 -8.19225001e-04  1.55950824e-03  4.99472144e-02\n",
            " -1.02901042e-03 -5.25908798e-03  2.60754659e-02  4.67443806e-03\n",
            "  1.88516163e-02 -7.77211427e-04 -6.56266870e-02  3.14821202e-03\n",
            " -2.30126576e-03 -2.46825337e-02  2.22933694e-02  4.67847818e-02\n",
            " -1.83080538e-02 -1.32050103e-02 -1.37701324e-02 -1.32599569e-03\n",
            "  4.97830530e-03 -6.11696754e-03 -9.65361610e-03 -1.99155303e-02\n",
            " -1.28720092e-03  1.33255565e-02  2.21852283e-03 -9.19838492e-02\n",
            "  4.19680537e-03 -6.84670155e-03 -1.79072961e-02  3.69198124e-02\n",
            "  2.30696387e-03  3.37984399e-02 -2.49488348e-02 -5.95459578e-03\n",
            " -8.45817330e-03  3.13348865e-02  3.53403441e-04  1.61258615e-02\n",
            " -2.55286541e-02  1.54973730e-02 -3.27886301e-03  3.11564161e-02\n",
            " -8.53038785e-03  4.67593935e-02 -1.94130549e-02 -1.79132350e-02\n",
            "  8.76948047e-04 -4.90342450e-02  1.12362997e-02  1.39287378e-02\n",
            "  6.90463254e-03  1.36290805e-02  5.67059309e-02 -1.71826523e-02\n",
            "  2.34706140e-02  3.53542202e-02  1.57581999e-02 -6.63694267e-03\n",
            " -5.37248897e-03 -4.58770113e-03  1.22595752e-02 -3.35191590e-02\n",
            " -9.78489590e-03  3.58872098e-02  4.58242176e-02 -3.87076692e-03\n",
            "  3.57662522e-02  7.23980874e-02 -3.34508390e-03 -4.37752927e-03\n",
            " -1.14691499e-02 -1.14691499e-02 -2.77643216e-02  2.58278749e-02\n",
            " -1.99053889e-02  4.35718000e-02  7.38587966e-03  2.74572181e-03\n",
            "  2.78479861e-02  8.82356405e-03  5.53192083e-02 -2.12936856e-02\n",
            "  5.76043986e-03 -4.86447951e-03  3.09229706e-02  3.47479579e-02\n",
            "  1.93663863e-02 -5.18589094e-02  2.88003460e-02 -1.00906901e-02\n",
            "  4.26093081e-03  2.34825107e-02  6.23493158e-04 -3.71441613e-02\n",
            "  1.99798911e-02  3.13506527e-02  2.16730813e-02 -2.31352736e-02\n",
            "  6.03363646e-03 -3.04226197e-03  1.10827878e-02  1.38583372e-02\n",
            " -3.10755198e-03  9.47722486e-03  5.24126223e-03  4.42058788e-03\n",
            "  1.12164406e-02  6.01047999e-02 -1.99280277e-02  1.33718361e-02\n",
            "  5.80822574e-03 -9.37541447e-03  1.28261086e-03 -3.20465441e-03\n",
            " -1.91740623e-02  2.44076029e-02  3.99078805e-02 -1.88196291e-03\n",
            " -1.54617576e-03 -1.61592528e-02  1.57119906e-02 -1.11432272e-01\n",
            "  3.30374414e-02  1.37770677e-02  2.05276499e-02  5.63273214e-03\n",
            "  1.96290829e-02 -2.46060124e-03  3.99286860e-02  1.17369041e-02\n",
            " -1.05124985e-02  3.51583999e-03 -4.46568780e-02 -2.40064167e-02\n",
            " -5.27713768e-04  1.19043865e-02 -4.77216643e-03  1.64970789e-02\n",
            "  2.17876537e-04 -7.39754215e-03 -6.04459252e-03 -3.56921031e-03\n",
            "  4.02885291e-02  1.65846001e-02  3.06850904e-03  3.38596564e-02\n",
            "  1.96335645e-02 -1.87100758e-02  7.41849982e-03 -2.52247398e-03\n",
            "  2.49426041e-02  5.80581655e-02  2.93537173e-02  5.30933362e-03\n",
            "  2.07392837e-03  1.79537376e-02  3.32836151e-03  2.32004694e-02\n",
            "  1.58594539e-02  1.31945094e-02 -2.61572243e-02  1.97432768e-02\n",
            " -4.47222970e-03  1.78877798e-02  1.39300770e-02  2.56085910e-02\n",
            " -5.55050949e-03 -9.48332981e-03 -2.92567578e-03  5.11963055e-02\n",
            " -5.27855756e-03  3.66151774e-02  1.85940718e-02  5.39497730e-04\n",
            " -1.10882450e-02  1.08092563e-02  4.91765410e-03  1.98051806e-02\n",
            " -1.94594716e-02 -2.48160668e-02 -1.06355867e-02 -1.05091534e-02\n",
            "  6.13279585e-03  1.93700587e-02  1.83548935e-02 -1.26945145e-02\n",
            "  2.29744037e-02 -5.16610599e-03  2.49092856e-02  1.02107437e-02\n",
            "  2.13603583e-02  1.87577163e-02  3.77981413e-02  3.82597334e-02\n",
            "  2.07877012e-02 -3.20894746e-03  8.87540588e-03  4.68920811e-03\n",
            " -1.80526970e-03  2.81233329e-02 -2.06757403e-03 -1.28416033e-03\n",
            "  1.78148709e-02 -1.53030122e-02  9.72277255e-03  1.26379254e-02\n",
            "  1.43725925e-02  6.46070575e-03  2.01010009e-02  2.93845451e-02\n",
            " -2.95035243e-03 -9.47276171e-03  3.21386176e-05 -7.01071649e-04\n",
            "  1.39831177e-02 -1.75969655e-02 -7.27600485e-03  3.97320220e-04\n",
            " -7.51152911e-03 -4.50040446e-03 -2.43902642e-03  7.09401448e-03\n",
            " -1.13033522e-02  4.68232297e-03 -7.73304055e-03 -1.01243859e-02\n",
            " -2.18112493e-03 -9.65173952e-03  1.99053704e-02  1.03904211e-02\n",
            " -2.34582940e-02  3.51976742e-03  2.49618094e-02  2.63762480e-02\n",
            "  1.34337369e-02 -6.92578563e-03  6.10746107e-03  8.48929888e-03\n",
            " -1.37294170e-02 -2.62428007e-02 -8.97205417e-03 -1.12032725e-02\n",
            "  1.00131407e-02  9.44250434e-04 -1.18857792e-02 -1.25105458e-02\n",
            "  7.13818809e-03  2.07221409e-04  2.58848666e-02  7.71895714e-03\n",
            "  1.77925370e-04 -1.81472914e-02  8.76931453e-04  6.61299646e-03\n",
            " -2.35606025e-02 -3.13847818e-02 -1.08735040e-02 -1.74917392e-02\n",
            "  3.12075167e-02 -1.65913748e-02  9.71438102e-04  1.01130254e-02\n",
            "  6.10214741e-03  1.28942679e-02  1.00413187e-02  1.20678980e-02\n",
            "  3.76236020e-03  1.88076362e-02 -4.77325185e-02  1.37478057e-02\n",
            " -4.91003978e-03 -3.53413186e-02 -8.99990351e-04 -3.55205435e-02\n",
            " -3.76127949e-02  1.34715745e-02  1.83137424e-02  2.71763782e-02\n",
            "  8.14314866e-03  1.38688054e-02  8.62710007e-03 -2.59169562e-02\n",
            "  1.79501651e-02  2.67486781e-03 -6.18853498e-02  9.00406949e-03\n",
            " -1.40340213e-03  4.65945682e-03 -3.84049795e-02 -4.23098546e-03\n",
            "  1.55155086e-02  1.64605876e-02  1.85055228e-02 -1.98384103e-02\n",
            " -2.48169314e-02  3.19527625e-03  6.99495059e-03  1.64148123e-02\n",
            "  2.57367074e-02 -4.50373149e-02  9.79683276e-03 -1.45056759e-02\n",
            " -1.49799957e-03 -1.93591599e-02 -1.93270708e-02  3.65542155e-02\n",
            " -6.30376435e-02  4.83952039e-02 -3.18423867e-03 -1.63771765e-02\n",
            " -4.71009217e-03  2.38609188e-02 -1.36424405e-03  1.89925182e-02\n",
            "  1.32718898e-02 -1.33506196e-02 -2.46736885e-02  1.47014631e-03\n",
            "  3.51691053e-03  2.15323008e-02 -2.70441211e-03 -5.40639319e-03\n",
            " -1.68630361e-02 -2.71153689e-03  7.74292441e-03 -3.91424336e-02\n",
            " -7.95019855e-03 -9.50714258e-03  5.46002720e-03 -1.88621142e-02\n",
            " -2.74825214e-02 -7.70770081e-03 -2.66082072e-02 -1.02074957e-02\n",
            "  8.48517270e-03  1.67511987e-02  2.46209117e-02  1.21991258e-03\n",
            " -2.43026066e-02  2.60309133e-02  2.03469251e-03  2.93640241e-03\n",
            "  4.46354171e-03 -2.09422607e-02  2.94678859e-02  1.47834662e-02\n",
            "  9.58136102e-03  3.33312798e-04  1.36142780e-02 -1.23460324e-02\n",
            " -1.09121005e-02  4.64305425e-03  3.85858571e-02  2.46411839e-02\n",
            "  7.72411979e-02 -2.54664860e-03 -8.38572350e-03  1.59590637e-02\n",
            "  1.22825204e-02  1.68458626e-02  1.90565904e-02 -3.23872351e-02\n",
            "  2.13665718e-02  1.00742873e-02 -2.41264480e-02  1.03479502e-02\n",
            "  2.15686615e-02 -1.24056938e-02 -4.77703392e-03 -8.48037188e-03\n",
            "  6.42342639e-03 -2.74831352e-03 -1.16333354e-02  8.52357142e-06\n",
            "  2.48914068e-02  1.06835102e-04 -5.02081404e-04 -2.29123595e-02\n",
            "  2.45584764e-02 -8.20260111e-04  2.01986878e-03 -3.98028298e-03\n",
            " -3.98028298e-03 -2.00847958e-03  1.14730373e-02  1.19980342e-02\n",
            " -1.61595764e-02  4.17131506e-03  4.33832939e-03 -1.07070253e-02\n",
            " -1.51245110e-02  2.59115146e-02 -1.81165647e-02  1.26254804e-02\n",
            "  2.09433018e-02 -4.55004571e-02  6.63421820e-03  2.68994957e-02\n",
            " -4.46132786e-02  1.88324398e-03 -1.56050976e-02 -2.00250961e-02\n",
            " -2.48846883e-02 -5.47585511e-03 -1.49525326e-02 -6.86838862e-04\n",
            " -9.83452072e-03  2.77220487e-02 -1.16945612e-02 -4.06284990e-03\n",
            " -5.95128925e-03  4.00791871e-03  2.90453463e-02  2.11689682e-02\n",
            " -4.36499530e-03 -1.40479566e-02  1.16800602e-02 -7.86656630e-03\n",
            "  4.27827815e-02  1.45095872e-03  2.27811293e-02  1.87157798e-02\n",
            " -2.63790263e-03 -2.02040212e-02 -2.81421506e-02 -3.93193535e-02\n",
            "  1.08377639e-04 -3.60674169e-02 -1.09837701e-02 -2.24284186e-02\n",
            "  3.82013092e-03 -5.45244986e-03 -1.68529092e-02 -1.28712014e-02\n",
            "  3.58286031e-02 -4.88269987e-03  1.73742705e-02 -8.83080180e-03\n",
            "  4.27258448e-03  2.51426585e-02 -7.22912400e-03  1.13028914e-03\n",
            " -5.37552633e-03 -5.90006717e-03 -7.32287864e-02 -7.45820587e-03\n",
            " -1.16549512e-02 -1.70273848e-02  5.16493941e-02 -1.67919215e-02\n",
            "  9.04149421e-03  1.55055924e-03 -1.88392193e-03 -4.85386089e-03\n",
            " -1.29244683e-02  3.04498366e-04 -5.93444769e-03 -4.71646013e-02\n",
            "  8.89904295e-03  1.66222891e-02 -4.68741522e-02  4.03738023e-02\n",
            "  1.62695901e-02  4.49963994e-04 -2.09782402e-02 -3.21360048e-03\n",
            "  3.60959370e-03 -4.11841610e-02 -2.00120709e-03  5.24251694e-03\n",
            "  1.08579364e-02  9.57734426e-03 -1.37414091e-02  5.51688533e-03\n",
            " -3.57361502e-02  3.13088129e-03  2.18450712e-02 -1.11733638e-02\n",
            "  2.10737601e-02  1.50985939e-02 -2.83766369e-04 -4.24903314e-03\n",
            " -5.84753253e-02 -6.73621615e-03  1.06248495e-03 -3.83182024e-03\n",
            " -1.62400469e-02  4.30183739e-03  7.98285872e-03  4.63118993e-03\n",
            "  1.38294179e-03 -2.44311859e-02 -3.41654382e-02 -1.59320019e-02\n",
            " -5.82323957e-02  2.60010546e-03 -7.13555781e-03  1.28791701e-02\n",
            " -1.23528077e-03 -1.07778408e-02  1.25595749e-03  5.71008594e-03\n",
            " -3.71179516e-03  1.33788325e-02  1.31457562e-02  5.17206207e-03\n",
            " -1.79770864e-02  1.16218551e-02 -4.45043268e-03 -1.90201328e-02\n",
            "  4.52620987e-03  5.96101373e-03  9.54605021e-03  3.18869942e-02\n",
            "  3.38958091e-02 -6.15357132e-03 -9.96458773e-04  1.27309639e-02\n",
            "  1.40129080e-02  3.48035231e-02  2.67640725e-02 -8.90519343e-03\n",
            " -4.71092306e-03 -1.59594592e-03]  - intercept :  0.5258741476131644\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.2532248640069667\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:42,949]\u001b[0m Trial 215 finished with value: -0.08739602241841327 and parameters: {'count_threshold': 7, 'postag': True, 'voc_threshold': 6659}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.34436574 0.45729199 0.02476612 ... 0.02019286 0.02156793 0.00764467]\n",
            " [0.42292069 0.62438242 0.05087989 ... 0.02019286 0.02156793 0.00764467]\n",
            " [0.06192409 0.10152657 0.07700251 ... 0.05023184 0.34771814 0.02768319]\n",
            " ...\n",
            " [0.00562311 0.03808825 0.09195572 ... 0.13033579 0.         0.02915784]\n",
            " [0.25666214 0.29874165 0.05735075 ... 0.03230858 0.03450868 0.01223147]\n",
            " [0.06848129 0.06637971 0.13881528 ... 0.16138797 0.16876372 0.03689272]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-0.00222251  0.00192935 -0.00049619 -0.00740827 -0.00454202 -0.00446876\n",
            " -0.06816803 -0.02507301  0.00329628  0.0057524   0.00101332  0.01342365\n",
            " -0.00425634 -0.01994865 -0.03990057  0.03757727  0.00144292  0.02118036\n",
            "  0.00163215 -0.04409965  0.0028998  -0.00950393 -0.03715072  0.01801931\n",
            " -0.01934437 -0.02916348 -0.00928914 -0.01146681 -0.01666965  0.02045618\n",
            " -0.0102948  -0.00228719 -0.0184846  -0.01610314  0.02389031  0.02842261\n",
            " -0.05146045  0.01073778  0.0216532  -0.0127045  -0.03171745  0.01265108\n",
            " -0.02389885  0.01880889 -0.00416723 -0.02198581 -0.03268485 -0.0258377\n",
            " -0.02995344 -0.06795361 -0.00552843  0.00473415 -0.02050503 -0.00475618\n",
            "  0.0131586  -0.02787923 -0.05182054  0.02337367 -0.01951017 -0.0619583\n",
            " -0.07173698  0.02083279  0.05205609  0.02206876 -0.01393509  0.054507\n",
            " -0.00632879 -0.02092255 -0.04542677  0.05281203  0.00158327 -0.03221969\n",
            "  0.00075566  0.07293372 -0.05772003 -0.00361352 -0.02373905 -0.04928156\n",
            " -0.01175055 -0.06386985 -0.04341336 -0.0135756  -0.05866053  0.03816644\n",
            "  0.04088273 -0.0069012   0.01026554  0.00069099  0.02154518 -0.00054723\n",
            "  0.01008146 -0.00533398  0.02300276 -0.01994737 -0.01217719  0.10322326\n",
            " -0.00279636 -0.05186866 -0.02432288 -0.00046359 -0.01993534 -0.06531109\n",
            " -0.03597753  0.03242528  0.09366372 -0.01612388 -0.08421352 -0.00140215\n",
            "  0.04132239  0.00152447  0.01026611  0.03858993  0.02022547 -0.00771642\n",
            " -0.01257726 -0.02195026 -0.00265393  0.00936954 -0.01236176 -0.04076671\n",
            " -0.05963287 -0.01227508 -0.00539935  0.03444114 -0.00799271 -0.03859018\n",
            "  0.03882308  0.05024455  0.01769229  0.03929473 -0.01625618  0.03473061\n",
            " -0.01998364 -0.0205573   0.00320839  0.044252   -0.03886416 -0.04140319\n",
            " -0.08209234  0.00253412 -0.03658166 -0.04194164 -0.04806942  0.00103766\n",
            "  0.0141066  -0.04468217 -0.03426625 -0.00805778  0.00442454 -0.06609303\n",
            " -0.03779386  0.0318097  -0.01894333 -0.05781699  0.00664928  0.03860828\n",
            "  0.05228904 -0.05419169  0.05446677 -0.00691369  0.01307732 -0.02756825\n",
            " -0.02499993  0.01292719 -0.03057641 -0.07631186 -0.07427046  0.01396598\n",
            "  0.00715299  0.02206251 -0.01553489  0.00442852  0.02347411 -0.00814356\n",
            " -0.01324155  0.05769143 -0.02763123 -0.03126919 -0.01123884 -0.0687889\n",
            " -0.00610779 -0.05133837 -0.02435134  0.0278556  -0.09457842  0.01055362\n",
            "  0.02473262 -0.04295354 -0.04406669  0.00466411 -0.02759597  0.02280355\n",
            " -0.02788398  0.04919571 -0.06477404  0.05887546 -0.00084068  0.01110839\n",
            " -0.02934438  0.02998692 -0.03600905 -0.01515382  0.02011302  0.01995298\n",
            "  0.0461671   0.10062041 -0.01864911  0.02383962  0.00774446  0.01595323\n",
            " -0.05898307  0.03331466 -0.00367796  0.00070589  0.04542081  0.01520516\n",
            " -0.0150447   0.04894747  0.04894747  0.00364445 -0.05744932 -0.0492027\n",
            " -0.03420667  0.02271037 -0.02672206  0.05563291  0.00078584  0.00536831\n",
            " -0.02649896 -0.03776131 -0.0048439  -0.01241088 -0.00083091  0.0016115\n",
            " -0.01586711 -0.01646819 -0.02240165 -0.02755317 -0.00828683  0.00025742\n",
            " -0.00146735 -0.00566364 -0.03453633  0.0179305  -0.02556139 -0.02448624\n",
            "  0.02490833  0.00883674 -0.06539588 -0.06235247 -0.02364497 -0.00647923\n",
            "  0.01317263 -0.01507644 -0.02370231  0.03718065 -0.01055801  0.02585624\n",
            " -0.03290108 -0.03230864  0.01402608  0.00419282  0.04238822 -0.00360923\n",
            " -0.05888464  0.040525   -0.03091076 -0.02464481  0.00928332 -0.05651457\n",
            " -0.0472028  -0.03581834 -0.03426221  0.01989459  0.02695633  0.00192261\n",
            " -0.05685364 -0.02651623  0.00419708  0.04679111  0.02459107 -0.02512641\n",
            " -0.0279554  -0.00950479 -0.01138452  0.03694894  0.00233999  0.05572843\n",
            " -0.01293168 -0.00988987 -0.04190953 -0.06182167  0.03821292  0.01558988\n",
            "  0.00132538 -0.03385387 -0.01980898 -0.07543656  0.00608026  0.03570354\n",
            "  0.0419274   0.01962822  0.00140352  0.00426994  0.05918781  0.02310842\n",
            " -0.00860716  0.01988949 -0.02916505  0.02458414 -0.00167969 -0.0120186\n",
            " -0.02504398  0.01118053  0.01374271  0.03070369 -0.02189387  0.02528967\n",
            " -0.00192279 -0.04206026  0.06039016  0.01496845  0.00578474  0.03430452\n",
            " -0.00924366  0.00809911  0.08881919  0.00125904  0.0277863   0.00557795\n",
            "  0.08522273 -0.07592083 -0.03960433 -0.00186366 -0.00438811 -0.01289356\n",
            "  0.01057565  0.01529361  0.02297553  0.00287593  0.02867245 -0.02111428\n",
            " -0.01634486  0.05470751  0.02138286 -0.05147287  0.03128883 -0.02838304\n",
            "  0.02797465 -0.04636935 -0.01829382 -0.03068255 -0.0243703   0.0329513\n",
            "  0.01771028 -0.04304299 -0.01974756 -0.00931561  0.01195611  0.02402815\n",
            "  0.00875014 -0.05546782  0.0251347   0.04672187 -0.02044849 -0.01700873\n",
            " -0.00893713  0.01591396 -0.01505474 -0.01866358  0.01678512  0.03235193\n",
            "  0.04302698 -0.01108184  0.09005358  0.05370578 -0.06585296 -0.02079333\n",
            "  0.02937514  0.04612528  0.00465213  0.01826968  0.02416487  0.01633589\n",
            "  0.00027876  0.00915983 -0.00800401  0.0246867   0.01526829  0.07036616\n",
            "  0.05865694 -0.08322217 -0.02291924 -0.04872271  0.02767689 -0.06812749\n",
            " -0.02897751 -0.01603991 -0.02508289  0.01983182  0.004005   -0.03372604\n",
            "  0.04479195 -0.00027938  0.02418209 -0.04674131  0.04371127  0.01910766\n",
            " -0.02098959  0.03263674 -0.01523924  0.01826318  0.01386904  0.02606288\n",
            "  0.01090237  0.02848423  0.00555802  0.00536629  0.02605186 -0.03370891\n",
            " -0.06724772 -0.0187758   0.00668141 -0.03433393 -0.00254516 -0.00882636\n",
            "  0.07992124  0.02596192 -0.00505633  0.01926965  0.03068053 -0.08434496\n",
            " -0.03260765 -0.02690512  0.0800023  -0.01220207 -0.01456188  0.01402552\n",
            "  0.02625928 -0.0775923   0.00187378 -0.02223017 -0.00816404  0.04001289\n",
            "  0.00039745  0.07557912  0.01012006  0.00833479  0.01778536  0.01778536\n",
            "  0.02955217  0.02753859  0.03228169  0.06135682  0.00504643 -0.00241284\n",
            " -0.00421828 -0.00764117 -0.03381263  0.01368398  0.056672    0.00255037]  - intercept :  0.7588181677919033\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.08739602241841327\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:43,558]\u001b[0m Trial 216 finished with value: 0.45242462541020284 and parameters: {'count_threshold': 4, 'postag': False, 'voc_threshold': 9890}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.00083355 0.03219502 0.3213202  ... 0.01217199 0.0071965  0.11294604]\n",
            " [0.         0.         0.33332585 ... 0.04381917 0.02590739 0.14129273]\n",
            " [0.02025338 0.08068651 0.00759383 ... 0.18524538 0.06397009 0.08605104]\n",
            " ...\n",
            " [0.30850173 0.15731678 0.15373452 ... 0.05514334 0.2187967  0.        ]\n",
            " [0.01773555 0.05306026 0.11839503 ... 0.04381917 0.02590739 0.13793397]\n",
            " [0.34450879 0.10296748 0.10220984 ... 0.10212566 0.27649948 0.028767  ]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [0.01301889 0.00667825 0.0073238  ... 0.00271118 0.00115852 0.00192581]  - intercept :  0.8781072245457686\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.45242462541020284\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:44,143]\u001b[0m Trial 217 finished with value: 0.36767957422106606 and parameters: {'count_threshold': 7, 'postag': False, 'voc_threshold': 9969}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.         0.00858556 0.         ... 0.14725272 0.03004534 0.03653493]\n",
            " [0.21871143 0.01548257 0.40831722 ... 0.03926739 0.0915464  0.00788889]\n",
            " [0.22352165 0.02406813 0.34465058 ... 0.05890109 0.19511778 0.01715753]\n",
            " ...\n",
            " [0.         0.07193891 0.01892739 ... 0.06626373 0.0135204  0.0742719 ]\n",
            " [0.55639066 0.05150163 0.65486607 ... 0.06626373 0.0135204  0.01930223]\n",
            " [0.22089908 0.1779167  0.21796479 ... 0.08835163 0.23692899 0.01619794]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 0.02615517 -0.02215842 -0.00654097  0.02308697 -0.00577143  0.04798511\n",
            "  0.00812326  0.06064179 -0.01568044  0.00325708 -0.00599622  0.01119645\n",
            "  0.04011753  0.06675755 -0.04752622 -0.00751556 -0.0524738   0.01103748\n",
            "  0.02092028 -0.01795154  0.00195635 -0.01981846  0.01510206 -0.0327142\n",
            "  0.0264398   0.00468822 -0.0368377   0.06165488  0.02542881  0.01689585\n",
            "  0.05108248 -0.02519912 -0.00187292  0.02437819  0.00596588  0.0026958\n",
            " -0.00622249  0.00409276 -0.03387713  0.01612645 -0.04733253 -0.02039269\n",
            "  0.00558607 -0.03130798  0.0213677  -0.11479698 -0.02410652  0.02186498\n",
            "  0.03059423  0.03262594 -0.07998483  0.00308    -0.0010159   0.00372372\n",
            " -0.06673053 -0.01862337 -0.01983569 -0.02341794 -0.02675892 -0.01250723\n",
            " -0.04017175  0.00102563 -0.0023811   0.0070132  -0.04792693 -0.06558304\n",
            "  0.01281916  0.01720998  0.00707777  0.0559433   0.03894632  0.00818201\n",
            "  0.03460703 -0.00081912  0.02494126 -0.04081266 -0.01525795 -0.02695229\n",
            " -0.08299412  0.04733863 -0.02066374 -0.05324148 -0.01813512 -0.04810656\n",
            " -0.05772966 -0.01332663 -0.04084578  0.02885385 -0.01578245  0.03586195\n",
            "  0.01405204 -0.00980493  0.04884039 -0.009392   -0.01769881  0.03862631\n",
            " -0.02852054 -0.0260806   0.07303167 -0.04461425  0.01985305 -0.04095367\n",
            " -0.00374053 -0.02351292 -0.07282456  0.04110092  0.00045902 -0.0092567\n",
            " -0.01431707 -0.00698793 -0.01702239 -0.02768354 -0.04811813  0.03084987\n",
            "  0.08336943 -0.01611244 -0.0514105  -0.01269766  0.01544425  0.00230572\n",
            "  0.02612153  0.06495514 -0.03734889  0.01861765  0.07247844 -0.06339324\n",
            " -0.00427582 -0.00086455 -0.02279664  0.0083349  -0.01216434  0.02509281\n",
            " -0.01373899 -0.02233898 -0.00249472  0.03124056 -0.00640591  0.01648808\n",
            " -0.02226604 -0.01311109  0.01617904 -0.00232851 -0.01032117 -0.03141657\n",
            "  0.01654389 -0.02942102 -0.04134703  0.02261424  0.00182023 -0.01898417\n",
            "  0.01901139 -0.04989655 -0.09912997  0.02984299  0.07945086  0.00149305\n",
            " -0.00552482  0.06354742  0.00238179  0.0020378  -0.03239729 -0.04292664\n",
            " -0.01519838 -0.02767352 -0.01062166 -0.0692289   0.01501677 -0.02931115\n",
            "  0.02315872 -0.02914857 -0.02914857 -0.0082643  -0.01799831  0.05143111\n",
            "  0.01975196  0.03188404  0.04727849 -0.00460214  0.00451999 -0.00987095\n",
            "  0.01051668 -0.00266967 -0.04105849  0.0281514  -0.0194451   0.03290385\n",
            "  0.0150736  -0.09043412 -0.03248562 -0.00639895 -0.00947813 -0.02787742\n",
            " -0.02872151  0.03952418  0.02102474 -0.05331249 -0.00381485 -0.02857362\n",
            " -0.03451396 -0.01379905  0.04501964  0.00901611 -0.03689944 -0.00113391\n",
            " -0.01192952  0.01781691  0.02072406  0.00492658 -0.01957613 -0.00442186\n",
            " -0.01331734 -0.02496946  0.01669795 -0.00721555  0.02384391 -0.00570209\n",
            "  0.01876377  0.05053814 -0.02963438  0.02191393 -0.054543   -0.02012506\n",
            " -0.047467   -0.04402277 -0.02174711 -0.02174711  0.01021654  0.03484982\n",
            " -0.03660883  0.012532    0.00064505 -0.0299232  -0.0070737  -0.00531761\n",
            " -0.00298854 -0.03704062 -0.00969952  0.01531187 -0.00294157  0.0050513\n",
            "  0.01332072 -0.01245068  0.0378365  -0.00393625  0.02794248 -0.01322858\n",
            " -0.0039219   0.00493854  0.01026699  0.00945994  0.0362433  -0.03346602\n",
            " -0.03379022 -0.01632145 -0.003911   -0.0073859  -0.03255112 -0.00748015\n",
            " -0.02354151 -0.00533598 -0.01386378 -0.00030167 -0.00407222  0.0043408\n",
            "  0.02729538  0.06431441 -0.01584544  0.00279216 -0.03668487 -0.05714343\n",
            "  0.03859305 -0.01990554  0.01906722 -0.01952404  0.00462793 -0.00552431\n",
            " -0.02889933 -0.04358661 -0.07150412  0.00220092 -0.00512652  0.01440961\n",
            "  0.03977041 -0.03550517 -0.01717728  0.03341031 -0.07220549 -0.02178706\n",
            " -0.00608161  0.00619319 -0.00793855 -0.02827915 -0.05407477 -0.03349881\n",
            " -0.04377187 -0.00424888  0.00351695 -0.03934534 -0.02320877 -0.00645282\n",
            " -0.04514145 -0.02792753 -0.02253997 -0.03729383  0.0045464   0.03571278\n",
            "  0.01495577  0.12328798  0.054879    0.03469599  0.05429185  0.07738799\n",
            " -0.06960823 -0.03627419 -0.05628268 -0.13227845 -0.03191278 -0.02489815\n",
            "  0.01496887 -0.03366377  0.03080212 -0.0734803  -0.03833878 -0.01978875\n",
            " -0.01742016  0.05232142  0.07658112  0.05306932 -0.05008417  0.04375318\n",
            "  0.02397012  0.0199932   0.04001556  0.01127759 -0.05047258  0.02156563\n",
            " -0.07618809 -0.0156914   0.00345939  0.00452059 -0.07180831 -0.03187293\n",
            " -0.0249076  -0.00751692  0.07445844 -0.02417698 -0.03158243 -0.05146646\n",
            " -0.01636887 -0.02726573  0.05717115  0.07072681  0.00147137 -0.00875428\n",
            "  0.01966303 -0.03011676 -0.00115469  0.02886424  0.02808444 -0.0527884\n",
            "  0.06724791  0.02288775 -0.08107257  0.01098812 -0.03129774  0.00081888\n",
            "  0.01680831 -0.01241687  0.00530854 -0.0097371  -0.00426832 -0.03329074\n",
            " -0.00049366  0.01259148 -0.01430517 -0.02970158 -0.0344282  -0.03079702\n",
            " -0.0409567  -0.03245351 -0.02091432  0.00483721 -0.04800146 -0.03152155\n",
            "  0.00794465 -0.01258723 -0.01054789 -0.05604459 -0.0337037  -0.09288525\n",
            " -0.01013503  0.07638244 -0.0659543  -0.091879    0.00209281 -0.01304027\n",
            " -0.02467683 -0.05552091 -0.02552499 -0.03818518 -0.01366668  0.01873045\n",
            " -0.06478842 -0.03012061  0.02830201  0.05352666  0.00888756  0.00888756\n",
            "  0.00184141 -0.0044492   0.03238194 -0.04706799 -0.03668469  0.0357339\n",
            " -0.0011359  -0.01049173 -0.0046482  -0.02855557  0.04071963 -0.0372959\n",
            " -0.02237009 -0.07698532 -0.00049673 -0.0390916  -0.15345705 -0.01555006\n",
            " -0.01218705 -0.01311668 -0.05234104 -0.01327174  0.0229703   0.00277042\n",
            "  0.01604523  0.04135236  0.06043666  0.02053907  0.02717392 -0.00815334\n",
            " -0.07105628 -0.06629897 -0.04163487 -0.01475482  0.00945811  0.00924236\n",
            "  0.0084991  -0.0296841  -0.01019027  0.00264462 -0.0595988  -0.01577089\n",
            "  0.03287989  0.02194734  0.00125059  0.00981961  0.03391464 -0.00554919\n",
            " -0.07732309 -0.01756085 -0.00496113  0.00153123  0.02695881 -0.04445112\n",
            " -0.04445112  0.00131152  0.00287267 -0.01291615  0.01947012 -0.03619013\n",
            " -0.0148845  -0.01525997 -0.00749672  0.00049682 -0.03000457 -0.00540276]  - intercept :  1.1702172515283946\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.36767957422106606\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:44,734]\u001b[0m Trial 218 finished with value: 0.2116556037911116 and parameters: {'count_threshold': 4, 'postag': False, 'voc_threshold': 9872}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.13133399 0.13739195 0.2179054  ... 0.10538636 0.10538636 0.05203398]\n",
            " [0.10592577 0.06793589 0.19050741 ... 0.14731596 0.14731596 0.05980644]\n",
            " [0.09266129 0.0435751  0.00342489 ... 0.39033533 0.39033533 0.03674413]\n",
            " ...\n",
            " [0.24607708 0.38776027 0.12957615 ... 0.03977767 0.03977767 0.00810024]\n",
            " [0.31777094 0.46519759 0.07997849 ... 0.17778139 0.17778139 0.0136105 ]\n",
            " [0.13754095 0.23491219 0.04435809 ... 0.18346614 0.18346614 0.02824263]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [0.00923717 0.00431072 0.00215487 ... 0.00070855 0.00070855 0.00172221]  - intercept :  0.6627000498622468\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.2116556037911116\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:45,336]\u001b[0m Trial 219 finished with value: -0.051204150391198945 and parameters: {'count_threshold': 4, 'postag': False, 'voc_threshold': 9733}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.46749636 0.29025212 0.36584046 ... 0.23435171 0.54676408 0.00233303]\n",
            " [0.         0.00544373 0.02088425 ... 0.05678842 0.01249655 0.05955054]\n",
            " [0.00656862 0.1006356  0.02691099 ... 0.08908302 0.07797182 0.06496916]\n",
            " ...\n",
            " [0.15588072 0.11444037 0.15601112 ... 0.25705474 0.32664463 0.02043687]\n",
            " [0.         0.04692369 0.02062236 ... 0.06049698 0.         0.06439072]\n",
            " [0.37758822 0.08371097 0.45306823 ... 0.02467685 0.42787725 0.01963271]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 0.00805413  0.00371403 -0.00545603 ... -0.01736993 -0.01567026\n",
            "  0.00054647]  - intercept :  0.7253166705556672\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.051204150391198945\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:45,926]\u001b[0m Trial 220 finished with value: -0.07117587171219489 and parameters: {'count_threshold': 3, 'postag': False, 'voc_threshold': 9594}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.32114344 0.12838664 0.34690354 ... 0.20128494 0.13043216 0.03374599]\n",
            " [0.76767749 0.16743233 0.76919864 ... 0.         0.         0.        ]\n",
            " [0.         0.01638551 0.         ... 0.22043339 0.         0.05576748]\n",
            " ...\n",
            " [0.         0.00879209 0.         ... 0.01526679 0.02300062 0.0828963 ]\n",
            " [0.25558067 0.07337485 0.17821682 ... 0.0339262  0.29983639 0.12713853]\n",
            " [0.         0.00589588 0.05778663 ... 0.15745242 0.         0.00608419]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 0.00395796  0.00888461  0.00412409 ... -0.00793542  0.01472857\n",
            "  0.00351479]  - intercept :  1.0634159075938097\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.07117587171219489\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:46,493]\u001b[0m Trial 221 finished with value: 0.3580640957953633 and parameters: {'count_threshold': 4, 'postag': False, 'voc_threshold': 9856}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.3069185  0.09100028 0.46893247 ... 0.03835117 0.01361052 0.0048124 ]\n",
            " [0.17118578 0.11841769 0.11279091 ... 0.32269553 0.10379484 0.05665273]\n",
            " [0.36918231 0.0804439  0.50448364 ... 0.05752676 0.28259889 0.00942862]\n",
            " ...\n",
            " [0.62250678 0.07527403 0.82271641 ... 0.05752676 0.         0.        ]\n",
            " [0.         0.         0.04151773 ... 0.         0.34460207 0.13109862]\n",
            " [0.02921926 0.06041355 0.02259433 ... 0.09380669 0.23366493 0.09207251]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-0.00790865  0.00461982  0.00125776 ... -0.00483005  0.00032092\n",
            " -0.00251569]  - intercept :  0.6620029629059729\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.3580640957953633\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:47,089]\u001b[0m Trial 222 finished with value: 0.002220487231948281 and parameters: {'count_threshold': 4, 'postag': False, 'voc_threshold': 9901}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.         0.02671893 0.05993452 ... 0.         0.         0.02323044]\n",
            " [0.38409486 0.0587629  0.47191023 ... 0.09802075 0.1364935  0.02493389]\n",
            " [0.61061793 0.03938496 0.79206407 ... 0.11129916 0.         0.02172519]\n",
            " ...\n",
            " [0.37912978 0.04762693 0.63070017 ... 0.         0.         0.01353308]\n",
            " [0.         0.02558377 0.         ... 0.41455997 0.         0.11040135]\n",
            " [0.         0.08553652 0.         ... 0.03312594 0.         0.0660799 ]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-0.00315065  0.00230684 -0.00754279 ...  0.01376499 -0.00820102\n",
            " -0.00074033]  - intercept :  0.8143329178881153\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.002220487231948281\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:47,695]\u001b[0m Trial 223 finished with value: 0.17859533898970814 and parameters: {'count_threshold': 4, 'postag': False, 'voc_threshold': 9734}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.05376939 0.16780141 0.01004342 ... 0.3769255  0.1466266  0.0369446 ]\n",
            " [0.38970826 0.58371104 0.02533984 ... 0.4057352  0.02187476 0.01899244]\n",
            " [0.         0.         0.01989372 ... 0.         0.05369259 0.08988222]\n",
            " ...\n",
            " [0.31749715 0.32283445 0.26036058 ... 0.08761205 0.12456828 0.05454202]\n",
            " [0.0677596  0.09057066 0.11113227 ... 0.20514273 0.36524088 0.05608383]\n",
            " [0.35104541 0.46105714 0.02275321 ... 0.47353945 0.22788554 0.02717389]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 0.00112219  0.03315544  0.01488924 ... -0.01311661  0.02736856\n",
            " -0.00145449]  - intercept :  0.9622986436386776\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.17859533898970814\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:48,282]\u001b[0m Trial 224 finished with value: 0.25876307238728125 and parameters: {'count_threshold': 4, 'postag': True, 'voc_threshold': 9994}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.74478913 0.92665749 0.05441784 ... 0.         0.         0.        ]\n",
            " [0.50538967 0.57001753 0.01537607 ... 0.10475684 0.         0.01606477]\n",
            " [0.51793867 0.50813206 0.02465863 ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.26876952 0.28598954 0.08523806 ... 0.10818242 0.41146888 0.01852105]\n",
            " [0.03669609 0.09344534 0.06255965 ... 0.27718966 0.         0.0491733 ]\n",
            " [0.         0.         0.15882637 ... 0.33946314 0.         0.11448588]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 8.23901427e-04  8.62765252e-03 -7.51547714e-05 ... -1.47586523e-04\n",
            " -7.26847051e-03  8.39549057e-04]  - intercept :  0.5822760769689765\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.25876307238728125\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:48,888]\u001b[0m Trial 225 finished with value: -0.20155205001804846 and parameters: {'count_threshold': 3, 'postag': False, 'voc_threshold': 9512}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.         0.05622429 0.01278502 ... 0.         0.         0.21114778]\n",
            " [0.08945608 0.07091493 0.04162489 ... 0.11011178 0.04324108 0.10671531]\n",
            " [0.64472468 1.01389248 0.00994705 ... 0.92849527 0.75107445 0.        ]\n",
            " ...\n",
            " [0.         0.00750383 0.21020558 ... 0.33309906 0.42370163 0.18820842]\n",
            " [0.01476715 0.0334674  0.0623122  ... 0.10431563 0.03891697 0.10801635]\n",
            " [0.53865555 0.8516736  0.01326274 ... 0.42061684 1.11049947 0.02304618]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-6.53669362e-03 -7.11161087e-03 -9.80947145e-03 ... -1.72170535e-02\n",
            "  1.41528014e-02  6.83376828e-05]  - intercept :  0.3873126389005589\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.20155205001804846\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:49,508]\u001b[0m Trial 226 finished with value: -0.011980739738445092 and parameters: {'count_threshold': 4, 'postag': False, 'voc_threshold': 9800}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.         0.01681925 0.04784435 ... 0.04805393 0.07007546 0.13142712]\n",
            " [0.6536412  0.0954956  0.74680472 ... 0.0954606  0.         0.02663144]\n",
            " [0.         0.10226696 0.01145575 ... 0.35354051 0.37923229 0.11275439]\n",
            " ...\n",
            " [0.13760152 0.033092   0.19627715 ... 0.45524709 0.05918372 0.01382947]\n",
            " [0.00973016 0.11834365 0.01743768 ... 0.11959798 0.01919766 0.08260748]\n",
            " [0.41040534 0.0111284  0.52872198 ... 0.01850101 0.         0.02961267]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 0.00606147  0.00293343  0.00430702 ...  0.00143921 -0.00678932\n",
            " -0.00488083]  - intercept :  0.6016643001519477\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.011980739738445092\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:50,078]\u001b[0m Trial 227 finished with value: 0.13254778203287626 and parameters: {'count_threshold': 7, 'postag': False, 'voc_threshold': 9956}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.38496133 0.1073029  0.49577724 ... 0.01371449 0.05107127 0.00901487]\n",
            " [0.46924074 0.07956005 0.49966249 ... 0.35527069 0.12767817 0.02253717]\n",
            " [0.06486627 0.0633426  0.07736566 ... 0.26235281 0.11607106 0.05267025]\n",
            " ...\n",
            " [0.47360438 0.08486617 0.51622769 ... 0.00979607 0.         0.        ]\n",
            " [0.         0.024243   0.02761504 ... 0.02929721 0.58414286 0.07551959]\n",
            " [0.08454664 0.03676997 0.         ... 0.0140982  0.30641945 0.07258449]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-7.84913426e-03  6.63520606e-03 -4.16715378e-03  4.14959778e-03\n",
            " -1.46793976e-03 -2.99485591e-02 -3.86689753e-02 -5.83466883e-03\n",
            " -2.22681629e-02 -1.14584212e-02 -4.23560159e-03 -2.19195018e-02\n",
            "  1.97475263e-02  2.84911491e-02  3.08279623e-03  5.68185600e-02\n",
            "  2.69574376e-02 -2.81112116e-02  4.60302834e-02  1.51726579e-02\n",
            " -8.95005351e-03  7.50942474e-03  5.10738465e-03 -7.92377506e-03\n",
            " -3.67684636e-03  1.08616183e-03 -2.38433791e-02 -6.09174337e-03\n",
            "  2.97360896e-02 -3.42997330e-02 -2.61161223e-03 -4.58313703e-02\n",
            " -5.82405518e-03 -5.08855245e-03 -3.95188425e-03 -4.18465729e-02\n",
            "  2.16656941e-02 -4.54644015e-03  3.76647244e-02  2.30351663e-02\n",
            "  1.57558458e-02 -3.74882677e-02  1.35298495e-02  2.78147931e-02\n",
            " -6.28934963e-03  7.42895432e-03  3.06917249e-02  3.14786058e-02\n",
            "  1.30841283e-02  2.68902899e-02  4.25416260e-02  7.13199295e-02\n",
            "  5.78810207e-02  5.12869568e-03  2.65213259e-02  1.02445200e-03\n",
            " -1.10136414e-02 -5.64478302e-03 -1.59407937e-02 -4.28798416e-02\n",
            " -5.11979957e-03  1.65338286e-02  4.82364757e-02  3.26667400e-02\n",
            "  5.55590138e-03  3.06463749e-02 -6.25483000e-03  4.11526895e-02\n",
            " -6.36437582e-03  8.18264394e-03  2.43144444e-02  2.87258167e-02\n",
            "  5.08452222e-02  6.19367490e-02  3.15268212e-02 -2.47123783e-02\n",
            "  4.27393782e-02  2.23732856e-02  6.69576022e-02 -3.78271299e-02\n",
            "  1.72854333e-03  3.10422971e-03  3.43751855e-02  2.51300682e-02\n",
            "  4.40905643e-02  9.86508718e-04  1.95350562e-02  7.88512597e-03\n",
            "  5.56087731e-02  2.96040000e-02  1.59412663e-02  4.53629099e-04\n",
            "  4.74523132e-02 -4.19444154e-02 -3.40928203e-03 -1.88172797e-02\n",
            " -3.20497952e-02 -3.85640911e-02  2.56951024e-02  9.63740098e-03\n",
            " -1.68386584e-02  1.98675833e-02 -4.55086880e-02 -1.94670354e-02\n",
            "  2.37363161e-02 -3.24691861e-04  2.76240744e-02  7.24361721e-02\n",
            " -2.86091633e-02  2.89682622e-03  4.06576951e-02 -2.85707690e-02\n",
            "  1.58163369e-02  1.13160423e-02  5.54692062e-02 -1.17857665e-02\n",
            "  5.57486402e-02 -2.94742018e-02  2.04913434e-02 -5.43048589e-02\n",
            "  5.66084214e-02 -1.82496842e-02 -4.94566450e-03  1.85746362e-02\n",
            "  1.27390707e-02 -1.57724682e-02  6.64042191e-03  7.82890569e-04\n",
            "  1.06389160e-02  1.06389160e-02  2.33127693e-02 -7.52448401e-02\n",
            " -4.77456802e-02 -3.05511767e-02 -6.51647454e-03  2.43997870e-02\n",
            "  3.21163030e-03 -1.23196682e-02  1.50163538e-02 -1.45750951e-03\n",
            "  1.08542570e-02 -3.91998116e-02 -7.33767846e-02  4.61915620e-03\n",
            " -2.89948267e-02  2.60674270e-02  4.40693421e-02 -4.86615880e-03\n",
            "  3.78061290e-02 -3.89846555e-02  4.15772356e-02  5.16211250e-02\n",
            "  1.38536629e-02  4.47596035e-02  1.05225614e-02  4.96956398e-02\n",
            "  8.05341462e-02  3.31370235e-02  6.63785633e-02  4.97951061e-02\n",
            "  1.37196659e-04  2.22740853e-02  3.19530423e-03  3.67499681e-02\n",
            "  3.78424349e-02  4.23870571e-02 -2.74710275e-03  1.13682147e-02\n",
            "  1.66229824e-02  6.67926943e-02 -1.97395375e-02  2.04176235e-02\n",
            " -2.88346132e-02  9.05189300e-02  5.04386085e-03  1.64012996e-02\n",
            " -4.18812282e-02 -7.62374696e-02  6.82163460e-03  3.34755299e-03\n",
            " -1.28917541e-02  2.37870224e-02  2.87444789e-02 -1.60896070e-02\n",
            "  2.83406867e-03 -1.91881590e-04  5.93881956e-03  5.67281981e-02\n",
            "  9.62154012e-03  6.50932178e-02 -3.57445124e-03  3.75930132e-02\n",
            "  1.34994351e-02  1.72232479e-02  5.84072312e-02 -4.91569277e-02\n",
            " -1.76485003e-02  7.54627517e-03  2.49181267e-02  5.37062964e-02\n",
            "  1.20546069e-01  4.56455215e-02 -1.52153468e-02  4.02160381e-02\n",
            "  6.59811389e-02 -4.00880404e-02  4.82870966e-02 -1.32829775e-02\n",
            "  1.20552740e-02  1.42880630e-02  7.16060447e-02  1.67903788e-02\n",
            "  2.86222921e-02 -5.52611952e-03  3.23357051e-02  4.89360950e-02\n",
            "  5.44748686e-02 -4.17965950e-02 -6.86241311e-03  3.20652914e-02\n",
            " -1.47518360e-02  2.03489487e-02  3.28699314e-02  4.16432422e-02\n",
            "  4.22397837e-02  1.15620713e-02 -2.18085740e-02 -8.83365995e-02\n",
            "  2.32547716e-02  3.76844553e-02  4.24536405e-03  9.35103405e-03\n",
            " -5.54659465e-03  4.07098237e-03 -1.33362672e-02  4.04153477e-02\n",
            "  4.39979257e-03 -2.52584572e-02 -3.10104939e-04  2.05360354e-03\n",
            " -1.21382276e-03 -6.90376820e-03  8.53229593e-03 -3.33631921e-02\n",
            "  1.54946912e-02  2.25390566e-02 -1.22121370e-02  1.69425807e-03\n",
            "  1.66073082e-02  3.18734122e-02  3.01996896e-02  1.31802782e-02\n",
            " -3.42931296e-02 -2.32530166e-02 -1.34914714e-02 -1.06244423e-04\n",
            " -2.00421887e-02 -3.30730802e-02 -6.89920801e-03 -5.34386442e-03\n",
            "  7.31535352e-05 -2.26274036e-02 -1.63675393e-02 -7.76628901e-02\n",
            " -6.16068323e-03  8.09438742e-03 -4.23260345e-02 -2.21444167e-02\n",
            "  2.01010207e-02  6.09743892e-02 -3.59262287e-02  5.61446019e-02\n",
            "  3.68549990e-02  2.40789649e-02  4.25232429e-02  2.46297974e-02\n",
            " -4.98841974e-03 -2.60946007e-02  5.08744605e-02  4.38213022e-02\n",
            "  2.58079164e-02 -1.15521102e-03 -1.55628506e-02 -3.31895865e-02\n",
            " -5.99485620e-03 -6.36530917e-02 -1.90346718e-03  3.14851380e-02\n",
            "  2.83039411e-02 -2.42302939e-02  2.84341501e-02  2.11806210e-02\n",
            "  5.42684817e-03  2.91342648e-02 -1.38801854e-03  1.16803159e-02\n",
            " -2.14796828e-02 -5.08006092e-03  1.28136861e-02  3.77730128e-03\n",
            "  8.67141808e-03 -7.91954411e-02 -2.02002588e-02  2.04979585e-03\n",
            "  2.74039932e-02  8.29280916e-02  1.38027025e-02 -6.37041257e-02\n",
            " -4.42047405e-03  2.64066790e-02  8.68630638e-02 -3.01835089e-02\n",
            "  5.46892346e-02 -2.18042202e-04  3.89448594e-03 -1.00594900e-02\n",
            "  2.69738578e-02  7.27718475e-03  4.52685668e-02  4.93880931e-03\n",
            " -1.74905396e-02  1.32890166e-02  3.37958585e-03  1.46531054e-02\n",
            "  3.18591940e-02 -7.16238275e-02  9.60831064e-03 -2.51883751e-02\n",
            "  1.42471275e-03 -3.92924336e-02  2.81451536e-02  6.36567478e-02\n",
            " -2.34351510e-02  4.64853919e-05 -1.15992147e-01 -2.55500490e-02\n",
            " -4.27866468e-02  1.02593516e-02  3.49465977e-02  1.74865173e-02\n",
            " -2.28712111e-02 -3.30222505e-02  4.29207582e-02 -2.00775216e-02\n",
            " -2.90468961e-02 -1.16586321e-02 -8.09631777e-03 -7.60509342e-02\n",
            "  4.05743922e-02 -9.39680710e-03  2.28405314e-02  6.99597689e-05\n",
            "  6.79753550e-03 -3.95650736e-02  4.73334398e-03  2.17246185e-02\n",
            "  1.27545132e-02 -3.46571257e-02 -4.40815061e-02 -2.80962213e-02\n",
            "  6.91237042e-04  6.91237042e-04  6.26846959e-02 -2.86597889e-02\n",
            " -5.17973160e-02 -8.88374506e-04  2.71651891e-03  7.48697590e-03\n",
            " -2.06248831e-03  2.56881372e-03 -3.52010913e-02 -5.27029832e-03\n",
            "  3.62034347e-03 -2.64933597e-02 -6.94974064e-02  5.73853495e-03\n",
            " -2.40444199e-02  4.75575592e-02  4.01177033e-02 -1.42878641e-02\n",
            "  7.00416871e-02 -5.22766710e-02  2.25916206e-02  6.27664800e-03\n",
            " -2.64585251e-02 -5.54232740e-03  2.80397866e-02  6.95209273e-02\n",
            " -3.17511611e-03 -1.35672326e-02  2.06460888e-04  1.11071680e-01\n",
            " -5.13508154e-03  5.47294717e-02  4.09122644e-06  2.17260995e-02\n",
            "  5.87731080e-02  2.86793794e-02 -1.39112770e-03  4.24319237e-02\n",
            " -1.49525982e-02 -3.54140641e-03 -5.11574779e-02 -6.11163104e-02\n",
            "  2.25754842e-02 -5.56065459e-03 -2.32055906e-02  1.00315879e-02\n",
            " -3.95143865e-02 -1.03711446e-01 -3.41245677e-02  2.50643478e-02\n",
            "  2.28654019e-02  8.75690429e-03 -1.62555343e-02 -3.20042287e-03\n",
            " -2.22124346e-02 -5.47925343e-02  5.13041284e-02 -2.31611986e-02\n",
            "  2.26089581e-03  4.42087748e-02 -2.14814290e-03  3.84536234e-02\n",
            "  2.74057831e-02 -4.57653317e-02  1.65254564e-02 -1.36363507e-02\n",
            " -3.97482140e-04 -2.20185789e-02  3.20963292e-02  3.31528871e-02\n",
            " -9.74043123e-03 -7.78689756e-02  3.27947618e-02 -1.77905319e-02\n",
            "  1.78901448e-02 -6.81630634e-03  4.42185964e-02  1.61104651e-02\n",
            "  1.55958660e-02  7.43285365e-03  3.58750515e-02  5.51644262e-02\n",
            "  1.90527547e-02  5.65738887e-02  4.87711777e-02  3.89766181e-02\n",
            " -7.41444330e-03 -2.50091927e-02  5.67825276e-02  3.86484635e-02\n",
            "  2.31695403e-02  1.86924968e-02 -2.16406442e-02  1.93963815e-02\n",
            " -2.79392487e-02  9.09757044e-03 -2.73876418e-02 -7.61748834e-02\n",
            "  2.48951862e-03  1.93762030e-02 -7.00764900e-02  7.82100166e-03]  - intercept :  0.21224681817162083\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.13254778203287626\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:50,669]\u001b[0m Trial 228 finished with value: -0.3530001855991077 and parameters: {'count_threshold': 7, 'postag': False, 'voc_threshold': 9706}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.05709786 0.05889324 0.10291353 ... 0.02287895 0.05113297 0.02588104]\n",
            " [0.03805115 0.06592761 0.1195642  ... 0.05090486 0.2219969  0.03692419]\n",
            " [0.02812578 0.02069368 0.02384861 ... 0.08158908 0.0686091  0.03994155]\n",
            " ...\n",
            " [0.10705876 0.1941444  0.07840602 ... 0.22623904 0.00649138 0.01258368]\n",
            " [0.46653881 0.76934231 0.03274407 ... 0.06609475 0.08145621 0.00834415]\n",
            " [0.         0.03908619 0.07267062 ... 0.         0.09981445 0.09454224]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-2.24148747e-02 -1.97178320e-02  8.41378718e-05 -2.60925316e-02\n",
            "  1.65595192e-03 -7.49264513e-02 -3.73771008e-02 -1.08868841e-02\n",
            " -3.59759373e-03 -4.56022014e-03 -2.77213811e-02  4.49454547e-03\n",
            "  2.61581569e-02 -3.77312535e-02 -1.01638741e-02 -3.20168523e-02\n",
            "  1.76177290e-02  5.97157534e-02 -1.87685454e-02 -3.75877783e-02\n",
            " -1.54429792e-02  1.76193787e-03 -2.73863440e-03  2.10738941e-03\n",
            "  2.42430167e-02  5.39757333e-03  6.59467003e-03 -1.85386447e-02\n",
            " -1.41692542e-02 -2.60067913e-03 -2.07564713e-03 -1.32216592e-02\n",
            " -2.28718994e-03 -1.62995133e-02  1.56066528e-03 -3.74639642e-02\n",
            "  8.38250907e-04 -3.30621911e-03  7.74952577e-03 -6.21637956e-03\n",
            " -4.35050832e-02  1.35941226e-02  2.18373986e-03  4.17770488e-03\n",
            " -4.87838131e-02  2.47226694e-02 -8.65606786e-02 -5.27539265e-02\n",
            " -5.97240502e-02 -3.65822708e-03  2.59485330e-02  3.48512093e-02\n",
            " -2.79171850e-03 -1.58097398e-02 -5.10903504e-02  1.87337762e-02\n",
            " -5.55478391e-02 -2.33094810e-02 -1.12558083e-02  8.23546273e-03\n",
            " -1.31607609e-02 -7.71414656e-03 -1.02876171e-01 -4.87925346e-02\n",
            " -4.17480592e-02 -1.07407742e-02 -2.71305929e-02 -2.78611699e-03\n",
            "  3.97274611e-03 -6.60933669e-03 -2.50337199e-02  5.21852848e-03\n",
            " -4.52492915e-02 -4.65277537e-02  6.23078552e-02 -3.05470333e-02\n",
            " -6.74922049e-04 -8.30534016e-03  6.22326615e-02  2.25614227e-03\n",
            " -4.68839949e-04  5.74891174e-02 -1.63358255e-03 -1.64365882e-02\n",
            " -3.24645331e-03  2.47542809e-02 -2.71061921e-02 -1.05582686e-02\n",
            "  1.00849930e-02  2.42328456e-02  1.01215848e-02 -3.39340401e-02\n",
            " -1.72935434e-02 -7.53624223e-02 -4.66141225e-02  1.24296274e-02\n",
            "  3.26617642e-03  1.48683266e-02 -4.71353591e-02  4.25907104e-02\n",
            " -6.00425984e-03  1.49392673e-03  1.37152861e-02  3.06753368e-02\n",
            " -3.79406624e-02 -4.92605098e-02 -2.96462903e-02 -2.61400357e-02\n",
            " -1.39673389e-02 -2.49878336e-02 -5.62497936e-02  2.75943517e-02\n",
            " -2.27877831e-03 -4.08237599e-03 -2.15536304e-02  1.34633053e-02\n",
            " -1.86303499e-02 -6.15633912e-02 -1.98896471e-02 -2.56572393e-02\n",
            "  1.46482971e-02 -3.23742102e-02 -7.57364555e-03 -7.57364555e-03\n",
            "  4.47003563e-02  1.74478590e-03 -4.41438383e-02 -3.55329890e-02\n",
            "  6.82653845e-03 -6.56934226e-02 -3.20316971e-05  2.58323125e-02\n",
            "  1.06892223e-03 -5.01674718e-02 -6.08352684e-02 -1.14483224e-01\n",
            " -4.31205966e-03  7.41841626e-03  1.36110488e-02 -8.96615318e-02\n",
            " -1.04452906e-02 -1.69692712e-03 -4.58580930e-02 -1.12684048e-03\n",
            "  3.35174221e-02 -8.41483341e-02 -2.04359698e-02 -2.13468011e-02\n",
            " -1.12811075e-02 -4.28003918e-04 -2.48942653e-02 -4.31131997e-02\n",
            " -8.00039544e-02  1.14009027e-02 -6.59905770e-03 -4.36240531e-03\n",
            " -6.45346596e-02 -1.83270862e-02  1.13712382e-02  1.41944804e-03\n",
            " -3.48087977e-02 -1.44917850e-02  5.36989073e-02  3.11275188e-02\n",
            " -2.91994409e-03 -1.90753835e-02 -6.92357111e-05 -5.46496499e-03\n",
            "  1.76871724e-02  3.54031948e-02 -3.24726497e-02 -6.42416926e-02\n",
            "  2.05254990e-02  5.53777787e-02 -3.54353780e-03 -2.23666628e-02\n",
            "  1.55522404e-02  1.86025355e-03  3.46258348e-02  3.73832531e-02\n",
            "  3.53121462e-02  6.60175950e-03  6.60175950e-03 -1.33441023e-02\n",
            " -2.38450662e-02  1.22673804e-02  5.15853924e-02 -4.62252580e-02\n",
            "  1.34891812e-02 -1.48458529e-02 -5.45549942e-04 -1.20544979e-01\n",
            "  1.28941461e-02 -4.84560237e-03  2.07156475e-02  9.91913776e-04\n",
            "  1.01330015e-03 -8.84052905e-03  3.30601905e-02  9.87771401e-04\n",
            " -1.61571062e-02 -5.58715107e-02 -2.03825236e-02 -3.21510139e-02\n",
            " -4.91182828e-03 -1.48327705e-02  2.47054383e-02  1.19063660e-02\n",
            "  6.92909485e-04 -1.38230595e-02 -1.44560521e-02 -1.26063009e-02\n",
            " -2.26342794e-03 -3.00087783e-02  3.47085363e-02  2.45732161e-03\n",
            "  6.01923186e-03 -1.88246286e-02 -7.82214883e-02  1.78393528e-02\n",
            " -1.46466226e-02  6.50650211e-03  8.82534349e-02  2.03401116e-02\n",
            "  1.72404671e-02 -2.45877638e-02 -8.18380538e-03 -1.18426641e-02\n",
            " -4.51709845e-03 -3.77817912e-03 -3.98970148e-03 -1.35683883e-01\n",
            "  8.75574830e-03  2.53155695e-02 -5.02898806e-02  3.99465595e-02\n",
            " -1.62892037e-02  5.86338251e-03 -5.47522879e-03  1.11462183e-02\n",
            " -3.59288385e-02  3.15581507e-02 -1.91766980e-02 -2.59602260e-02\n",
            " -1.68158414e-03 -1.74018564e-03 -2.37979436e-02 -3.18826114e-02\n",
            " -1.58100144e-03 -2.33935214e-02  4.84056514e-03 -9.18511793e-02\n",
            " -2.10626739e-02 -3.80228157e-02 -1.18471230e-02 -5.84167668e-03\n",
            " -7.20542037e-03 -2.15438366e-02 -1.05095096e-02 -3.22798805e-02\n",
            " -1.79953101e-03 -2.49905604e-02 -1.32351802e-02 -1.63450636e-02\n",
            "  2.21273808e-02 -3.71578515e-02 -8.79984442e-03 -1.03767286e-02\n",
            "  8.04949636e-03 -3.30843162e-03  8.49038192e-03  9.07008788e-03\n",
            "  3.56022987e-02  5.00942627e-02 -2.98654893e-02  1.25715268e-02\n",
            "  9.00683314e-03 -6.58695706e-03  3.92043233e-02 -1.04037918e-02\n",
            "  1.78159127e-03  1.02217053e-02 -2.41546951e-03 -6.17540754e-03\n",
            " -5.21787781e-02 -9.91143555e-03 -2.84509487e-02  3.16605497e-02\n",
            "  1.30171983e-02 -4.02467086e-02 -5.24157367e-02 -2.32623631e-02\n",
            " -3.42461541e-02 -2.79673676e-02 -2.71349635e-03  7.64410131e-04\n",
            "  6.24070202e-02  2.14486782e-02  1.50334359e-02 -4.91832516e-02\n",
            " -3.62969303e-02 -7.60990724e-03  2.34239789e-04 -1.62809507e-02\n",
            " -8.48279353e-03  2.25663404e-02 -9.11963929e-03 -2.33011830e-02\n",
            " -3.08649469e-03 -3.63808078e-02  5.98084889e-02 -4.52918893e-02\n",
            " -1.94291416e-03 -4.44618535e-02  4.95573432e-02  8.90429948e-03\n",
            "  7.54768055e-03  6.86783559e-02 -7.47427920e-02 -8.87074683e-02\n",
            "  7.61074134e-02 -1.17739117e-02  1.91791220e-02  3.44845458e-03\n",
            "  8.06721208e-02  1.73835161e-02 -1.65149811e-02  6.97809644e-02\n",
            "  1.40800621e-03 -3.84089909e-02 -3.31143161e-02  1.14148595e-02\n",
            " -2.00756634e-02 -8.64535360e-03  3.41284988e-02 -1.24136841e-03\n",
            " -7.05901192e-03 -5.57905051e-02 -8.32512838e-03  1.01484415e-02\n",
            " -1.11119491e-02  2.52707895e-02  1.51705290e-02  2.83239134e-02\n",
            "  3.79557622e-03  4.45127641e-02  2.80526983e-02 -5.81824775e-02\n",
            "  2.43626319e-02  1.21720636e-02 -5.82277138e-02 -1.33811035e-02\n",
            " -5.69064943e-02 -1.84158034e-02 -5.01302545e-02 -4.70441353e-02\n",
            "  6.18527206e-03  1.61439990e-02  1.37345151e-02  2.77169262e-02\n",
            " -1.59518735e-02  1.77856127e-02 -1.00349268e-02  5.40281439e-02\n",
            " -1.69593098e-02 -4.42448268e-02  3.30934530e-02  2.12389663e-02\n",
            "  9.42191803e-03  9.42191803e-03  4.03844661e-02  2.67347398e-02\n",
            " -4.23629220e-03 -5.62507850e-02 -2.04221978e-02  1.67760788e-02\n",
            " -2.61030286e-03  2.22155292e-02  1.35567647e-02  3.49936962e-02\n",
            " -2.76488491e-02 -7.30538242e-03 -3.65698211e-02 -1.53037566e-02\n",
            "  4.49898921e-02  6.02737858e-03  1.07158522e-02  5.63102205e-03\n",
            "  1.59123823e-03 -3.69279531e-02  3.20520498e-02 -9.07691894e-02\n",
            "  1.83211502e-02 -1.34514603e-02 -6.55126329e-02 -5.13273650e-02\n",
            " -5.89772329e-02 -6.81661130e-02 -2.66025305e-02 -5.37465519e-02\n",
            "  7.17362840e-03 -3.34674042e-02 -3.50515874e-02  5.07155083e-02\n",
            "  3.01477300e-02  3.89233800e-02 -5.94341871e-02  4.03191498e-03\n",
            "  3.29838398e-02  1.65855399e-02  6.28175196e-02  2.30361549e-02\n",
            " -3.89031835e-02 -3.50689785e-02 -3.94624240e-03 -4.31131395e-02\n",
            " -3.83979015e-02  4.21484353e-02  1.93586496e-02  3.97147878e-02\n",
            "  6.56735727e-02  1.25943778e-02  2.84797508e-02 -2.23705486e-02\n",
            "  4.83807251e-02  7.84581841e-03  2.13886686e-02 -9.60178589e-03\n",
            " -9.60178589e-03 -1.29643919e-02 -3.71144549e-02 -3.26818422e-02\n",
            "  5.59053840e-02  2.00433991e-03  4.16741407e-02  2.96043926e-02\n",
            " -1.28715363e-02 -9.69587113e-03 -2.64652833e-02 -1.65713003e-02\n",
            "  2.33414675e-02  1.70639337e-02 -7.78358739e-03 -9.78510313e-03\n",
            "  1.74467140e-02  2.59706638e-02 -1.01989895e-01  5.13801287e-02\n",
            " -4.17933314e-02 -2.91499699e-02  2.73939673e-03  2.26045563e-02\n",
            "  1.25441638e-02 -2.91919803e-02 -2.63910393e-02  1.68463361e-02\n",
            " -2.82473717e-02  1.70649294e-02  3.37410828e-02  1.70370992e-02\n",
            "  2.10271749e-02 -8.19177645e-03  1.68220985e-02  6.00979968e-03\n",
            " -7.25048782e-03  2.09832244e-02 -4.00128824e-02  1.87416350e-02\n",
            "  8.24756786e-02  2.62088560e-02  3.02495951e-02 -4.94207573e-02\n",
            "  2.26951899e-02 -3.74053229e-02  4.15567796e-02 -2.45953866e-02\n",
            " -6.31428952e-03 -1.98281551e-02  2.47915932e-03 -6.09606182e-03\n",
            "  2.42549176e-03 -1.88319830e-02 -8.27810728e-03  2.67287169e-02\n",
            "  9.56977503e-03  2.99617653e-02 -5.61214573e-03  2.59636239e-02\n",
            " -1.61978961e-02 -2.63561026e-02  1.39660906e-02  2.25203017e-04]  - intercept :  0.9901539906167935\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.3530001855991077\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:51,261]\u001b[0m Trial 229 finished with value: -0.22887706551179252 and parameters: {'count_threshold': 4, 'postag': False, 'voc_threshold': 9460}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.34742891 0.62193535 0.08365125 ... 0.09895919 0.03322286 0.00590436]\n",
            " [0.         0.         0.00345992 ... 0.01151802 0.86576615 0.12592715]\n",
            " [0.26758816 0.20661823 0.20470493 ... 0.00806262 0.09363742 0.01429087]\n",
            " ...\n",
            " [0.48427691 0.52554598 0.22954986 ... 0.10746237 0.26297838 0.01387795]\n",
            " [0.02356702 0.06151731 0.00632966 ... 0.02015654 0.         0.06903956]\n",
            " [0.52238879 0.53027505 0.29514958 ... 0.         0.         0.00609168]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 7.23709295e-04  5.53480232e-03  2.22603961e-03 ... -1.96110948e-05\n",
            " -4.59621923e-04 -1.41793497e-03]  - intercept :  0.6724068171528829\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.22887706551179252\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:51,857]\u001b[0m Trial 230 finished with value: 0.20317734158209869 and parameters: {'count_threshold': 7, 'postag': False, 'voc_threshold': 5667}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.0591196  0.05646837 0.03454011 ... 0.30041875 0.09846856 0.05718536]\n",
            " [0.         0.00291039 0.         ... 0.         0.         0.02937181]\n",
            " [0.01652896 0.03416126 0.07762396 ... 0.02723338 0.04475844 0.06015304]\n",
            " ...\n",
            " [0.20114247 0.04116921 0.40413169 ... 0.07915706 0.19046346 0.02669742]\n",
            " [0.         0.00227374 0.03118124 ... 0.         0.         0.04897734]\n",
            " [0.38436981 0.15674167 0.39690988 ... 0.         0.69741254 0.00881154]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 3.97753277e-03 -1.21467505e-03  2.33052153e-02  8.28105035e-03\n",
            " -6.44924038e-03  2.01840700e-02  1.98228346e-03 -6.35796592e-03\n",
            " -1.07305921e-02  2.19831085e-02 -6.87833828e-03  2.09151413e-02\n",
            " -2.18928143e-03  1.90542839e-02  3.30248063e-02  2.87099038e-02\n",
            " -1.42368174e-02  2.51595306e-02  7.20720608e-03  1.41476903e-02\n",
            "  2.44034886e-02  4.79729262e-03 -1.29522912e-03  7.92247158e-03\n",
            " -2.34633671e-02  6.83999294e-02  2.91263146e-02 -3.23956972e-02\n",
            " -2.41632469e-02  2.44140506e-02  8.65211873e-03 -2.10299304e-02\n",
            " -9.19955762e-03 -1.70999510e-02 -3.79876946e-02 -1.14846988e-02\n",
            "  1.10836309e-02 -1.91720223e-02 -1.09928591e-03 -1.85904603e-03\n",
            " -1.22042199e-03 -2.02084250e-02 -2.14625854e-02 -1.95634823e-03\n",
            "  2.00103940e-02  3.53951299e-02 -1.87193730e-02  4.58203357e-02\n",
            " -4.34227575e-02 -3.28084745e-02  2.07674657e-02 -5.12892892e-03\n",
            "  1.45160381e-02 -4.31544539e-02  4.73986725e-02 -6.92337328e-03\n",
            " -1.51300470e-02  1.69260398e-02  3.15727952e-03  7.76197526e-03\n",
            " -1.38963589e-02 -2.89006605e-02 -6.55702317e-03 -1.75252057e-02\n",
            " -2.57148880e-02  5.34457205e-02 -2.76643179e-02  8.31149762e-03\n",
            " -7.55333432e-03 -3.82025455e-02  1.31536039e-02  8.99751455e-03\n",
            " -6.89192123e-03 -6.29810457e-03  3.33642965e-02 -2.32450704e-02\n",
            " -1.01061731e-02  3.68842060e-02 -2.84641374e-02 -2.46617669e-02\n",
            "  2.04497644e-02  7.01221268e-03  3.25121678e-02  1.05863481e-02\n",
            "  1.72673199e-02  1.58175755e-02  4.40829795e-03  1.10117355e-02\n",
            " -9.01657050e-03  4.28176546e-02 -8.01255192e-03 -4.60153737e-02\n",
            " -1.48367216e-04  3.47483403e-02 -2.68063239e-02  3.93110587e-02\n",
            " -7.40780984e-03  2.95842135e-04  5.19206054e-02  4.60454144e-02\n",
            " -5.35295851e-02  2.96475179e-02 -2.08609566e-02  1.06756522e-03\n",
            " -3.88978363e-03  2.37588859e-02 -3.89034732e-02 -7.81408956e-02\n",
            "  3.04458994e-02  5.14206533e-03 -2.04113240e-02 -2.62579740e-02\n",
            "  1.52526183e-02  3.36235038e-02 -1.75781736e-02 -5.61256021e-02\n",
            "  2.76082713e-02 -1.14760161e-02  9.02290557e-03 -1.66928191e-02\n",
            " -6.11113107e-02 -2.10547133e-02  6.21675532e-02  9.45182294e-03\n",
            " -2.85432954e-02  2.05569101e-02 -1.74753478e-03 -1.83259058e-02\n",
            "  3.36581294e-02  8.90921223e-03  7.60248165e-02 -1.63743160e-03\n",
            " -1.47752851e-02 -5.51790886e-02 -3.73162624e-02  7.52436573e-02\n",
            " -1.40497593e-03  2.60464737e-02  1.09872603e-02 -4.17112210e-03\n",
            " -2.65792317e-02  1.34431009e-02  9.91968061e-03 -1.45677158e-02\n",
            "  7.81992106e-03 -1.81293074e-02  2.17808999e-02 -7.29428607e-03\n",
            " -3.06651637e-02 -2.76541031e-02 -2.61513596e-02 -1.00758628e-02\n",
            "  9.88739781e-03  1.63864467e-02 -1.99896515e-02 -2.01677585e-02\n",
            " -4.53855766e-03  2.12465667e-02 -2.75458426e-04  8.55251563e-02\n",
            "  3.12153587e-02 -3.94093290e-02 -3.43975037e-03 -3.33066284e-02\n",
            " -1.76743716e-02 -6.09553085e-03 -4.44963264e-02 -1.14870830e-02\n",
            " -2.38320805e-02  2.24836453e-02  1.71888028e-03  8.03816410e-02\n",
            " -2.07744510e-03 -9.90595978e-03 -2.85130027e-02  1.79405890e-02\n",
            "  1.10555911e-02 -2.03413422e-02 -3.59973929e-04 -3.45555192e-02\n",
            "  1.04802895e-02  1.47860723e-02  3.72894878e-02 -2.56155820e-02\n",
            "  3.21366774e-02  2.10849127e-02 -3.33155508e-02  2.12935693e-02\n",
            " -7.82376583e-02 -9.73608558e-03 -7.96153766e-02  2.17019394e-02\n",
            "  2.78363785e-03  4.31994141e-02 -3.28949160e-02  2.12160100e-02\n",
            "  3.84515100e-02  9.43193971e-03  4.64944929e-03 -4.00464695e-02\n",
            " -2.41081264e-02 -2.51000751e-03  3.78909670e-03 -8.57075399e-03\n",
            "  2.64231371e-02  1.78472323e-02  2.11718073e-02  3.00495676e-02\n",
            " -1.64174770e-02  4.67379379e-03  1.99963961e-02 -1.13825966e-02\n",
            "  2.55953152e-02 -6.42759233e-02 -3.63128825e-03 -5.03745236e-02\n",
            "  2.45689819e-03 -3.12192310e-03 -2.24587602e-02  7.69517808e-04\n",
            " -1.56789816e-03  4.83627690e-03 -4.66947197e-02 -4.00864528e-03\n",
            "  1.14653326e-02 -3.26605540e-02  1.72006552e-02  1.43345398e-02\n",
            "  3.75230746e-02 -4.03242295e-02 -2.19066149e-02 -3.78123216e-02\n",
            "  5.05528207e-03  2.96195911e-02 -1.10648734e-02  3.01135847e-04\n",
            "  1.01859352e-02 -1.33107368e-04 -5.31654515e-04  5.48651729e-03\n",
            "  8.72965119e-03  2.58083666e-02 -6.87903417e-03  1.99328843e-02\n",
            " -2.61810704e-02  3.97888379e-02 -8.10611087e-03  1.11245638e-02\n",
            " -5.06789209e-03 -2.77946764e-02  3.81118198e-02  2.58083796e-02\n",
            " -1.97246035e-03  3.02322966e-02 -1.69467226e-02 -3.00535933e-03\n",
            " -1.48980465e-02  1.56860383e-02 -4.07911779e-03  1.69434925e-02\n",
            " -5.89203775e-03  5.21306621e-02 -7.31410024e-04 -1.43665753e-02\n",
            "  6.71103054e-03  3.92296706e-02 -2.69545267e-02 -1.23889339e-02\n",
            " -2.13294508e-02  2.00677265e-02 -6.79598720e-02  4.18171216e-02\n",
            " -5.63664821e-02 -1.21475245e-02  1.83841837e-02 -2.94640678e-02\n",
            " -2.22837986e-02  1.53046340e-02 -1.37714538e-02 -2.01452675e-02\n",
            " -8.35164005e-02  5.66043790e-02  8.51308927e-03 -6.92217757e-03\n",
            " -3.62628075e-02 -2.11657506e-02  1.17821578e-03 -3.34800151e-02\n",
            "  1.69268413e-02 -2.36158442e-02  5.22049531e-02  1.58944874e-03\n",
            " -9.46892695e-03  5.95586415e-02  8.83165730e-03 -3.49894943e-02\n",
            " -1.82338127e-03  4.00861305e-02 -4.68172325e-02 -9.53724533e-02\n",
            "  3.36857382e-02 -3.97605337e-02  1.75055779e-02 -5.93339439e-02\n",
            " -1.34615365e-01  3.24768954e-02  2.81224688e-02  4.54541601e-03\n",
            " -4.23647017e-02 -1.44994613e-02  7.79338629e-02 -5.97148627e-02\n",
            "  3.23450770e-02  3.37285017e-02 -5.57663815e-02  3.35368030e-02\n",
            " -4.38996052e-02 -2.81215577e-02  5.69305854e-02 -7.08279488e-03\n",
            " -1.83857454e-04  7.41965533e-03  1.62680811e-02 -7.05344978e-03\n",
            " -1.32084197e-02 -1.67888006e-02  8.94332604e-03 -3.75687825e-04\n",
            "  1.64205100e-02  3.41958228e-02 -6.57095728e-02 -1.90986436e-02\n",
            " -1.01624006e-01  2.18726028e-03 -1.07220369e-01  2.31728553e-02\n",
            " -4.18601722e-02  6.96856281e-02 -4.11751448e-02  1.77150481e-02\n",
            " -8.86068550e-03  7.76023412e-03 -4.40457657e-02 -7.86837080e-02\n",
            " -3.89954870e-02  1.39934193e-02  2.80029402e-02  1.32309498e-03\n",
            "  2.25983593e-02 -6.21561445e-02  3.88974603e-02 -3.95925357e-02\n",
            "  1.19171124e-03 -3.52477988e-04  2.52677537e-03 -1.12026015e-01\n",
            " -3.31043528e-02 -1.04536087e-01  2.50413674e-02  3.78404699e-02\n",
            " -2.53077861e-02 -5.33082262e-02  4.24177857e-02 -5.74386933e-02\n",
            " -4.48255809e-02 -4.00423904e-02  8.48349214e-02 -2.19540936e-03\n",
            "  2.57374169e-02 -4.60930022e-02 -7.70509744e-02  5.67170980e-02\n",
            " -5.69960076e-02  2.93253951e-02 -7.35239721e-02  8.52467326e-03\n",
            " -2.08099839e-02 -3.27745592e-02  1.28565908e-02 -1.22159791e-02\n",
            " -6.32091425e-02 -2.97641679e-02  1.44341515e-02  1.77079193e-03\n",
            " -3.44919273e-02 -3.93346620e-02  6.16038961e-02  1.98733609e-02\n",
            " -9.85835120e-02 -4.18740674e-02 -1.41559921e-02 -9.74652720e-02\n",
            " -3.09494003e-02  2.53627528e-03 -4.29396555e-02 -4.76284214e-02\n",
            "  3.67926672e-02 -3.23143182e-02  3.25825698e-02 -1.22102729e-01\n",
            " -6.49317407e-02 -3.33608014e-03 -6.60079265e-02 -1.25239312e-02\n",
            " -8.07312114e-02  8.19915334e-03  2.26613068e-02  3.28849690e-02\n",
            "  4.48791506e-02 -5.21251494e-02 -1.19324853e-02 -7.17616895e-02\n",
            " -6.48114135e-02  6.32548910e-02 -2.31553186e-02 -1.30040166e-02\n",
            " -5.88971015e-02 -2.46668147e-02  4.78429128e-02 -3.32325361e-02\n",
            " -1.55369194e-03  4.16354692e-02 -4.63522676e-03  7.87003111e-03\n",
            " -9.70440661e-02  3.12155057e-03 -6.35347935e-02 -5.25536048e-02\n",
            "  2.95659371e-02 -7.05573633e-02 -1.01176875e-01 -4.61804914e-02\n",
            "  5.08114080e-02  9.28358935e-03  2.19655382e-02 -5.12849884e-02\n",
            " -1.77044721e-02 -2.12021693e-02  1.67086872e-02  1.77882396e-02\n",
            " -7.59611481e-02 -6.98220428e-02 -3.94939604e-03 -2.58869748e-02\n",
            " -3.34214839e-02  1.08565947e-02 -1.26140374e-02  2.06904058e-02\n",
            " -3.83815440e-03 -9.67958477e-02 -3.93325649e-03  3.39426289e-02\n",
            " -3.66700282e-02 -9.38061024e-02  2.56753455e-02  2.60550532e-02\n",
            " -1.85129881e-02 -1.11983213e-01 -1.22018979e-02 -1.30655363e-02\n",
            " -3.69412945e-02 -3.01538735e-02  2.59079370e-02 -2.63331062e-02\n",
            "  6.57323792e-03 -4.05669248e-02 -8.25693274e-03  9.53859501e-03\n",
            "  1.35221257e-02 -1.76022892e-02 -3.34126756e-02  1.52265654e-03]  - intercept :  1.1052297693386417\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.20317734158209869\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:52,441]\u001b[0m Trial 231 finished with value: 0.11774082694838428 and parameters: {'count_threshold': 7, 'postag': True, 'voc_threshold': 9988}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.12384887 0.09590299 0.025742   ... 0.15306057 0.07529546 0.01114822]\n",
            " [0.         0.01518689 0.00956456 ... 0.40178398 0.19765057 0.08199817]\n",
            " [0.06985625 0.05805101 0.0496781  ... 0.30731378 0.13176705 0.08227866]\n",
            " ...\n",
            " [0.07272239 0.12512695 0.03215779 ... 0.20089199 0.09882528 0.03383365]\n",
            " [0.01419357 0.         0.02778274 ... 0.42856958 0.21082727 0.06040644]\n",
            " [0.020432   0.06058572 0.0151901  ... 0.4591817  0.22588637 0.03344467]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 1.66622927e-02  3.17342413e-02  1.21102108e-03 -2.85929831e-02\n",
            "  6.17013505e-03  2.04157298e-02 -2.19888282e-02 -1.35216208e-02\n",
            " -3.43983604e-02  3.66635161e-02  2.26230010e-02 -1.77533927e-02\n",
            " -2.97565707e-02 -4.42713963e-03 -8.47877457e-03 -4.49297494e-02\n",
            " -9.05127559e-03  3.49742978e-02 -1.29032143e-02 -8.90970858e-03\n",
            " -2.00124419e-02 -3.10914998e-03 -1.13624360e-02 -3.11295114e-02\n",
            " -1.08577051e-02  4.27626687e-02 -8.85974721e-04 -2.37439807e-02\n",
            " -8.42800024e-02  3.28424134e-02  3.27292328e-02  2.34484385e-02\n",
            "  4.09506789e-02  2.58511783e-02 -1.08330131e-03  4.15909902e-02\n",
            "  5.93154064e-03  1.13119701e-04 -6.70820706e-04  1.95632579e-02\n",
            "  3.94851539e-02  1.58065278e-02 -2.25262526e-02 -1.69956330e-02\n",
            " -1.12295618e-02  4.19729718e-02 -9.46132393e-03  6.65080312e-03\n",
            "  8.87304614e-02 -9.26384493e-03 -7.25521681e-03  2.76974106e-02\n",
            "  5.92104628e-02  3.22727241e-02  4.15985166e-02 -2.20572293e-02\n",
            " -2.73785988e-02  2.49287551e-03 -8.16664087e-03  1.20130610e-01\n",
            "  4.71958901e-02  4.41997388e-03 -1.78181879e-02  1.50875885e-02\n",
            " -6.49606157e-03 -2.33217974e-02  5.12282287e-02 -1.27154616e-02\n",
            "  2.38906396e-02 -1.94859020e-02  1.36131480e-02  4.01570894e-02\n",
            " -3.87351133e-02  3.30772230e-02 -3.08268198e-02 -1.48099182e-02\n",
            " -4.87220207e-02  4.34294704e-02 -1.14613312e-02  1.07629351e-02\n",
            "  2.95888075e-02  3.50167553e-02 -1.93028551e-02 -1.81086357e-03\n",
            "  4.15936440e-02  3.92614454e-03  6.29734470e-02  1.72950389e-04\n",
            "  2.45403158e-03  2.33273272e-02 -5.37365697e-03 -2.81289317e-02\n",
            "  1.35277278e-02  5.59192558e-03  1.99009426e-03 -2.67755310e-02\n",
            "  6.46548872e-02  8.74388833e-03 -1.60973185e-02  1.69949631e-03\n",
            " -6.18421464e-03  1.51999663e-02  5.95692579e-02 -1.69278486e-02\n",
            "  5.51264779e-02 -1.88283370e-02  5.53696731e-03  9.82138572e-03\n",
            " -4.38913887e-03  2.23595217e-02 -2.29971158e-02  1.27996311e-02\n",
            "  7.43950130e-02  1.07988536e-01 -1.34604206e-02 -3.71312634e-02\n",
            " -1.33710460e-01 -1.41603438e-02  1.84986937e-02  3.85990260e-02\n",
            " -6.40508491e-02  3.94669797e-02  4.96177318e-02 -4.59512045e-03\n",
            " -4.40328833e-02 -5.71558012e-03  2.83350068e-03 -1.63849621e-02\n",
            "  2.80678578e-03  4.93150042e-02 -5.57216375e-02 -4.50118660e-02\n",
            "  1.65995737e-02 -3.22517422e-02  5.79203396e-02  1.36721296e-02\n",
            " -6.34684732e-02  2.34861399e-02 -3.20196202e-02 -3.68699826e-02\n",
            "  4.74450752e-02  9.97574900e-03  7.38224062e-02 -1.61038451e-02\n",
            "  4.54060826e-03 -5.28801528e-02  3.48177444e-02  1.06771752e-02\n",
            "  1.32955671e-02 -4.17304367e-02  7.45215801e-02 -2.59710100e-03\n",
            "  3.60612160e-02  1.98570941e-02 -4.61276322e-02  1.90854503e-02\n",
            "  2.72715843e-02 -2.85405939e-02  8.37811368e-02  1.26936953e-01\n",
            " -1.14703893e-02  4.77081083e-02  1.57490020e-03 -2.97099263e-02\n",
            " -2.23459924e-02  1.36016803e-02  2.63949395e-02 -4.56655780e-02\n",
            "  2.19996825e-02  2.57571817e-02  2.89506405e-02  2.50810704e-02\n",
            " -5.87435269e-03 -1.46506190e-02 -4.95182388e-02  7.63223938e-02\n",
            "  6.96522695e-02  7.04980571e-02  6.27112458e-02  3.93291912e-02\n",
            " -2.03886908e-03  2.53058508e-02 -1.71352287e-02 -1.87899864e-02\n",
            "  8.30933530e-03 -4.71182792e-02  3.50619273e-02  6.32221024e-02\n",
            "  6.45614825e-02  9.64247362e-02 -5.13322464e-03 -4.48481524e-03\n",
            " -1.69415104e-03  4.07286029e-02 -9.41208217e-02  2.03070402e-02\n",
            " -1.43729136e-02 -4.11911980e-02  1.98926197e-02  1.98926197e-02\n",
            "  6.96576347e-03  3.67391024e-02  5.43564844e-03  9.18646626e-03\n",
            "  4.55204683e-03  9.82785649e-03  4.22303817e-03  4.03960742e-02\n",
            "  3.91389325e-04 -1.22424302e-01  3.42150831e-02 -3.80706145e-02\n",
            "  9.47452073e-04 -4.85744174e-02 -6.97129510e-02  3.92689039e-02\n",
            " -1.05418510e-02  3.38683572e-02 -1.71531125e-02 -4.99863184e-03\n",
            "  2.00484630e-02  1.13080288e-02 -3.95364940e-04 -6.02173609e-03\n",
            " -7.47303847e-03  3.25792728e-02 -1.45870784e-03 -2.52636835e-02\n",
            " -3.02889222e-03  1.98335739e-02  1.57861708e-04  4.11399199e-02\n",
            " -7.37601143e-03 -8.12099285e-03 -6.30248047e-03  1.16957650e-02\n",
            "  5.60130850e-03  1.15136932e-02 -4.72812846e-03 -1.29409026e-03\n",
            " -3.24843438e-02 -2.23480148e-03  3.09315754e-02 -9.89469667e-03\n",
            " -5.63249114e-05  2.32766625e-02  1.08341434e-02 -1.14224324e-02\n",
            " -2.12359447e-02 -4.41019302e-02 -1.67163671e-02  2.18426824e-02\n",
            " -2.41882343e-03  1.77307810e-02 -1.30850451e-02  1.35106016e-03\n",
            " -1.00796338e-02  1.68210985e-02 -3.08621529e-02  1.56711975e-02\n",
            " -9.22979462e-03 -8.50766499e-03 -2.43041739e-02 -2.00743983e-02\n",
            "  2.51501121e-02  1.36760838e-02 -7.12881475e-03  2.55827109e-04\n",
            "  4.10558581e-02  2.24863280e-02 -2.63459464e-02  3.15515251e-02\n",
            " -3.32164913e-02 -2.24836034e-02 -4.08799487e-02 -1.11232387e-02\n",
            "  1.32516387e-04  2.87181715e-03  3.74778618e-03  4.53190918e-02\n",
            " -1.62604800e-02  1.17684402e-02 -2.33657770e-02  3.59637027e-02\n",
            "  3.87661230e-02  2.25616298e-02 -3.10325294e-03  1.45234593e-02\n",
            "  1.70581600e-02 -3.44295571e-02  1.86462427e-02 -1.99335638e-03\n",
            "  5.46314269e-02  3.70579209e-02 -5.52404893e-03  3.00970337e-02\n",
            " -1.56253925e-02  1.80571467e-03 -1.68422145e-02  1.53354505e-02\n",
            " -4.58198777e-02 -1.93794496e-02 -6.90396761e-04 -1.90877951e-03\n",
            " -1.92156832e-02  1.42804563e-02  2.56195863e-02  8.91047461e-03\n",
            " -3.82296497e-02  2.82508897e-02 -1.77485078e-02  1.53431794e-02\n",
            " -4.48729829e-02  8.69365028e-03  6.12888121e-03 -2.38142307e-02\n",
            " -7.67831276e-02  1.98642453e-02 -9.95815270e-03  1.54980581e-02\n",
            " -4.54135145e-03  3.12760640e-02  4.96510700e-03  3.80826090e-02\n",
            " -1.49760543e-02 -5.23744028e-02 -2.40383836e-02  2.90443105e-02\n",
            "  8.93952180e-04 -1.81517501e-02 -7.46927931e-03  3.13188105e-02\n",
            "  1.83899428e-02 -1.63764290e-02  1.80084091e-02  8.31882875e-04\n",
            " -9.76459955e-02 -6.15847130e-02  6.20403361e-02 -3.28186458e-02\n",
            " -2.08085905e-04 -4.89029020e-02  4.29381418e-02  1.84780458e-02\n",
            " -3.68346153e-03  2.62693336e-03 -2.41212019e-02  7.18263044e-03\n",
            "  1.39947993e-02 -3.90593194e-02 -7.22580006e-02  8.03079030e-03\n",
            "  3.43296940e-02 -1.32490177e-02 -6.95059596e-03  1.24611079e-02\n",
            " -1.21387339e-01 -4.25867481e-02 -7.25378650e-02 -7.73059116e-03\n",
            "  7.72728826e-03 -3.36753546e-02  5.64202222e-02 -6.94060115e-02\n",
            "  1.97730925e-02  1.93242279e-02 -8.08974863e-03 -8.75775956e-03\n",
            " -1.44555988e-03 -5.91948426e-02  2.38467836e-02  1.29501982e-02\n",
            "  1.38065272e-02 -1.13699492e-02 -3.63379151e-03  5.56518036e-02\n",
            "  2.66707208e-02 -3.31487110e-02  2.28765532e-02 -3.58546773e-02\n",
            "  8.86393556e-03  5.17668631e-04 -2.04933325e-02 -5.74417387e-03\n",
            "  2.22756192e-03 -5.29116850e-02 -1.42490936e-03  3.53048472e-02\n",
            " -1.55753193e-02 -9.33315190e-03  8.86080437e-03 -2.39050898e-02\n",
            "  3.25133824e-02 -1.14547259e-02 -3.57970677e-02 -1.80121192e-03\n",
            "  3.87259680e-02  3.43950528e-02 -2.23051314e-02 -4.60369555e-02\n",
            " -2.80829907e-02 -8.81888163e-04 -1.30719287e-01 -7.63076505e-02\n",
            "  7.84844109e-04 -1.49218329e-02 -2.89751437e-02 -7.97632679e-03\n",
            "  3.22013336e-03  3.82090689e-02 -1.96878741e-02 -9.95228016e-03\n",
            " -5.61195269e-02  3.88990864e-02  1.77360148e-02 -2.89237526e-03\n",
            " -1.37182756e-03  4.60426696e-02  1.07803795e-02  1.07803795e-02\n",
            " -4.35040511e-02  1.23903616e-02  4.85056543e-02 -3.53148352e-02\n",
            "  2.02087772e-02 -5.77466886e-02  6.17813110e-03  3.40838099e-03\n",
            " -2.84237257e-02 -8.34485463e-02  6.31142824e-02 -5.22407648e-02\n",
            "  3.16947881e-02 -9.05151514e-03 -6.40890434e-03 -1.92153540e-02\n",
            " -1.89290770e-02  3.27266012e-02  5.98176194e-02 -1.51976628e-03]  - intercept :  0.34962391034255913\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.11774082694838428\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:53,067]\u001b[0m Trial 232 finished with value: 0.013938101387618949 and parameters: {'count_threshold': 5, 'postag': False, 'voc_threshold': 9196}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.21657037 0.17701921 0.28324176 ... 0.37752419 0.16443716 0.02421176]\n",
            " [0.32054062 0.02050191 0.51603244 ... 0.20865837 0.28021632 0.04116753]\n",
            " [0.39501956 0.09361918 0.63947253 ... 0.1147621  0.29572622 0.01536771]\n",
            " ...\n",
            " [0.318885   0.02542893 0.57790755 ... 0.10919672 0.03980943 0.00767871]\n",
            " [0.03704013 0.03465751 0.0766055  ... 0.31884585 0.22705142 0.06295689]\n",
            " [0.11836452 0.08846539 0.07370768 ... 0.15301614 0.07961886 0.0554487 ]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-1.67100267e-03 -1.17979963e-02 -2.69148973e-03 -1.60675251e-03\n",
            " -1.43923580e-02 -5.07748089e-03 -2.85349450e-03 -2.12609224e-02\n",
            "  2.32884251e-02 -1.37080701e-02 -2.19469398e-02 -3.48113785e-03\n",
            "  7.26763296e-03 -1.70953645e-02  5.08438495e-03 -1.57986482e-04\n",
            " -5.37954869e-03 -1.93097560e-03  4.25437098e-04  6.22108551e-03\n",
            "  5.01307509e-03 -1.41942319e-02 -1.77284658e-02 -1.98178746e-02\n",
            "  8.89519103e-04 -1.88251650e-02 -6.54323602e-04 -4.01416015e-02\n",
            " -1.06307156e-02  4.71152574e-02  1.83509596e-04 -2.13403560e-02\n",
            " -1.40480031e-02 -3.77789091e-02 -1.80143751e-02 -9.39114927e-03\n",
            " -4.85582703e-03  3.14165091e-02  1.00798365e-02  9.28190505e-03\n",
            "  3.67981487e-03  4.41403403e-02 -3.86741097e-03 -2.48405165e-02\n",
            " -1.01970227e-02  3.86372800e-03 -3.17031146e-02 -1.38512433e-02\n",
            "  3.11331431e-02  1.44402420e-02 -1.08279381e-02 -2.84765258e-02\n",
            "  2.82085838e-02  5.21611835e-03 -7.54343213e-03 -7.08787453e-03\n",
            " -3.65914174e-02 -2.57757852e-03  2.24458605e-02  2.35972749e-02\n",
            " -2.88580517e-02 -1.15237603e-02  2.50215146e-02 -1.91355196e-03\n",
            " -1.26656509e-02  2.01798382e-02  2.67196203e-02 -2.36086669e-02\n",
            "  9.15225802e-03 -1.74307918e-03 -3.05876731e-02 -1.19970936e-02\n",
            " -2.21399304e-03  1.66836782e-02 -4.35504706e-03 -3.03186378e-02\n",
            "  2.94271689e-02  1.29879805e-02 -1.20637748e-02  4.26267545e-02\n",
            "  1.48548111e-02  4.39335276e-03  4.56444190e-03 -1.30227895e-02\n",
            "  3.15526066e-02  3.85254141e-02  3.85254141e-02 -3.54229299e-02\n",
            "  9.27046078e-03 -2.14306437e-02  2.16817392e-02 -2.44841789e-02\n",
            "  6.51692772e-03  1.97272669e-02 -4.07926708e-02 -1.05843880e-02\n",
            "  2.81720705e-02 -1.03722468e-02  4.37776538e-03  4.31954813e-03\n",
            " -3.40324020e-02  2.78065116e-02  9.88333864e-03 -1.90288243e-03\n",
            "  4.37160459e-03  3.84660323e-02  3.26025636e-02 -1.85886859e-02\n",
            " -1.02644893e-02  3.54795939e-02 -8.54543382e-03  2.27786635e-02\n",
            "  1.50062531e-02  1.89030014e-02  2.93291808e-02 -1.15965907e-02\n",
            "  2.41864253e-02  3.77849074e-04  8.87087466e-03  2.23220616e-02\n",
            "  3.03497706e-02 -4.78069179e-02 -3.35794656e-02 -1.96936337e-02\n",
            "  9.76700548e-03  9.45500221e-03  2.65684579e-03  2.24076157e-02\n",
            "  2.12938985e-02 -2.30272223e-03  4.11609532e-04  9.19963177e-03\n",
            " -6.75829293e-02 -4.91238545e-03  6.94062202e-03 -1.61449783e-02\n",
            " -3.28166347e-02 -1.34318029e-02  2.58202691e-02 -9.61645780e-03\n",
            " -1.31141863e-02  1.47841816e-02  5.92241296e-03 -3.32868120e-02\n",
            " -1.25761309e-03 -3.13242125e-03  2.65084435e-02  2.64861207e-02\n",
            " -2.17315157e-02 -1.90033356e-02  5.17346508e-03  5.23040187e-03\n",
            "  1.26788186e-02  8.46973731e-03 -1.45694358e-02 -7.85280360e-02\n",
            " -4.40681853e-03  4.50803495e-03  8.72135499e-03  4.59774634e-02\n",
            " -4.70847099e-03 -3.77126802e-02 -4.08265013e-02 -6.58037880e-03\n",
            "  7.55783847e-03  2.01880512e-03  3.57858811e-02 -1.43911610e-02\n",
            "  4.40730760e-03  1.83120065e-02  5.28065355e-02  4.29739189e-02\n",
            "  3.03871207e-02 -1.29180217e-02 -3.88291109e-02  4.13834197e-03\n",
            "  2.36728276e-02  7.42690260e-03 -2.52910616e-02  1.75152555e-02\n",
            " -2.93334801e-02 -5.41607826e-03  2.12163263e-02 -3.26954657e-02\n",
            "  3.59123633e-02 -5.19474113e-02  1.91634246e-02  6.43288815e-03\n",
            "  3.30338301e-02  8.89311216e-03 -1.68190640e-02 -2.20672703e-02\n",
            "  3.12275094e-02 -3.12426156e-02  1.09337809e-02  1.86290937e-02\n",
            "  2.79444735e-02 -3.30334960e-02  1.63801516e-03 -2.52572223e-02\n",
            "  4.52100508e-03  3.45826589e-02  4.00656347e-02  1.46306294e-02\n",
            "  5.74920757e-03  6.01880434e-02 -4.07780830e-02  1.42917745e-03\n",
            " -2.64514731e-03  4.98751072e-03 -2.36825175e-02 -1.31596110e-02\n",
            "  4.17225050e-04  1.26080548e-02 -7.67411753e-03 -2.43993885e-03\n",
            "  8.64895290e-03  6.22844181e-04 -9.29817674e-04  2.45760137e-02\n",
            "  1.25188837e-03  2.73165511e-03 -8.74649073e-03  2.49473643e-02\n",
            "  3.27451171e-02  4.70239992e-02 -2.17647563e-02  2.26873891e-02\n",
            "  5.08634544e-03  3.58755871e-02  9.06855694e-03 -1.69980237e-02\n",
            " -1.63241177e-02  4.10015924e-03  1.25978489e-02  5.49531866e-03\n",
            " -1.97032807e-02  2.49582757e-02  5.83379294e-03  3.74971162e-03\n",
            " -6.98908069e-03 -1.89736263e-02  2.35604049e-02  1.92651717e-03\n",
            "  1.26730539e-02  3.35087454e-02 -1.72489567e-02  1.55540542e-02\n",
            " -2.41718277e-02 -1.48378717e-03  1.51149299e-02 -2.11725720e-02\n",
            " -8.55516784e-03 -2.57041761e-02  8.22157011e-04  2.05362281e-03\n",
            " -7.34664632e-03 -2.11344494e-02  1.90402051e-02  2.23666330e-02\n",
            "  3.25621333e-02  1.19959490e-02 -7.48670281e-03  2.88462343e-02\n",
            " -3.48731615e-02 -8.97676571e-03  2.26423825e-03 -3.15813838e-02\n",
            "  2.17678011e-02  1.03983808e-02  1.50344947e-02  2.46970016e-02\n",
            "  4.19395277e-02  5.43641314e-03 -8.66125651e-05 -9.44108514e-03\n",
            " -9.62089525e-03 -1.99855026e-02 -5.66283126e-03  1.81264776e-02\n",
            " -1.31992580e-02  1.08840870e-02  4.05665820e-02  8.96537587e-03\n",
            " -3.38387849e-02  1.84860064e-02  2.64668932e-02  1.76322817e-02\n",
            " -1.58924242e-02  1.61076091e-03  1.96948724e-02 -2.61474538e-02\n",
            " -4.53110160e-02 -6.04730751e-02 -5.77902300e-02  4.71624871e-03\n",
            " -2.15083888e-02  9.67845382e-03  2.43276686e-03  1.09909863e-02\n",
            "  2.92790479e-02 -2.49211431e-02 -3.79772228e-02 -1.71271626e-02\n",
            "  2.56963085e-02  2.92925068e-02  2.86450780e-02  1.08920579e-02\n",
            "  9.66635535e-03 -1.00341365e-02  1.51572447e-02 -3.98778490e-02\n",
            " -2.18261794e-02 -1.79982241e-02 -1.12549976e-02 -3.49678147e-03\n",
            "  3.58928383e-02  1.23098159e-02  6.08794402e-03 -1.92197115e-03\n",
            " -6.43288292e-03 -2.53011978e-03  9.86689362e-03 -3.13699453e-02\n",
            "  1.87643749e-03 -3.15262784e-02 -1.08009784e-02  5.27603385e-02\n",
            " -1.46174791e-02  3.95709243e-03  5.72973096e-03  6.15380301e-04\n",
            "  5.30418572e-03  3.98098012e-03  3.10425736e-02  1.51369472e-03\n",
            "  1.29239168e-02  5.66115069e-02 -2.15325208e-02 -5.23815503e-04\n",
            "  8.21316437e-03  5.02259791e-03  3.42955130e-03 -5.79566280e-03\n",
            " -1.37626004e-02  2.10221951e-02  8.14331235e-03 -2.57410046e-03\n",
            "  3.95679070e-02 -9.75655804e-03  4.79903943e-03 -2.24952400e-02\n",
            " -1.94745406e-02 -1.38747470e-02 -1.35378758e-02  1.35151674e-02\n",
            " -3.09180722e-03 -4.29049728e-03 -4.19693393e-03 -1.90044740e-02\n",
            "  3.98755590e-03 -1.02769990e-02  6.95383824e-03  4.94497559e-03\n",
            " -2.06702569e-03 -1.01766940e-02  1.61269144e-02  8.00630125e-03\n",
            " -8.54521026e-03  2.33493178e-02  3.88011717e-03  2.91540265e-02\n",
            "  2.25287267e-02  4.42211844e-03  1.85522725e-02  7.16770397e-03\n",
            "  7.08559934e-03 -4.10698682e-02  3.27214201e-02 -1.27166550e-02\n",
            " -4.18168412e-03 -1.22070628e-02  3.40440969e-02 -1.53790612e-02\n",
            " -9.45550215e-03  1.54892724e-02  6.27975410e-03 -3.17883511e-02\n",
            " -1.77379316e-03  1.22429317e-02  9.69385505e-03 -8.94502276e-04\n",
            " -6.53754818e-05 -4.55769790e-03 -6.54678315e-03  8.52451473e-03\n",
            " -2.36388225e-02  4.30302197e-03 -5.73584472e-03 -1.72518072e-02\n",
            "  1.19004133e-02 -4.24305226e-03 -2.71519416e-02 -2.41973041e-02\n",
            "  4.72531975e-03  9.59160486e-03 -1.43307151e-03 -3.00797421e-03\n",
            "  4.65246098e-03 -5.95481920e-03 -1.55797069e-03 -2.11452061e-03\n",
            " -2.97486744e-02  1.92004449e-03 -1.85634988e-03 -1.37295114e-02\n",
            " -1.81175646e-02 -3.60540329e-02 -7.56630206e-05 -1.39589622e-02\n",
            "  9.67977238e-03  2.23337894e-02  1.66861523e-02  7.41773213e-03\n",
            " -2.54392653e-03 -2.02612359e-02 -2.11575454e-03  9.34584671e-03\n",
            " -1.08037472e-02  1.62362951e-02  8.49999029e-03  1.25110809e-02\n",
            " -3.62325378e-03  3.76069528e-04 -1.56982432e-02 -1.40454725e-02\n",
            "  2.83563471e-02  1.28440577e-02 -1.93975251e-02 -2.54796315e-02\n",
            "  1.82680500e-02  8.08126892e-03 -3.56969693e-03 -2.78961442e-02\n",
            " -7.16238647e-03 -7.43275504e-04  1.80654016e-02  3.38894100e-03\n",
            " -1.83292372e-02  1.09300117e-02  4.66815871e-03  9.76074259e-03\n",
            " -2.13758723e-02 -1.92186727e-02  6.97841908e-03 -9.88422551e-03\n",
            " -5.53021098e-03  1.63518007e-02  5.73172584e-02  3.05274915e-04\n",
            " -2.02055175e-02 -1.48475796e-03 -3.99113132e-02  3.40818912e-03\n",
            " -2.69693909e-03 -1.17895850e-02 -1.62969732e-02 -1.44537407e-02\n",
            "  4.58443404e-02  3.26007270e-03  2.06251486e-02  2.28400244e-02\n",
            "  3.21449840e-03  1.16338695e-02  4.29842745e-03 -9.82265766e-03\n",
            "  1.06867187e-02  1.02015116e-02  1.02839500e-02  5.97821750e-03\n",
            "  9.32633651e-03 -1.60800434e-02 -2.38145230e-03 -3.40440328e-02\n",
            "  6.44561222e-03 -3.04748759e-03 -2.70219555e-02 -1.10189001e-02\n",
            " -6.34813780e-03 -1.55404489e-02  1.90425765e-03 -6.13750719e-03\n",
            " -2.89274830e-02 -8.79836528e-03  2.77233558e-02  3.27589919e-02\n",
            " -1.40404376e-02  3.43689831e-02 -1.87907117e-02 -3.36862685e-02\n",
            " -1.32113382e-02  4.56171467e-03  5.36096088e-03  2.08024185e-02\n",
            " -6.92785605e-03  4.14744341e-03  4.00988356e-02 -2.27137732e-02\n",
            " -7.53157787e-03 -3.34818438e-03 -1.70287328e-02  1.70870395e-02\n",
            " -1.19712963e-03 -8.61974516e-03 -2.34047965e-02 -3.13807792e-03\n",
            "  1.21428269e-02  6.57980383e-03 -3.79897358e-03  2.29407842e-02\n",
            " -7.53419003e-03 -6.40395057e-04  1.64695795e-03  3.13770671e-03\n",
            " -2.56795128e-02 -5.44499784e-03 -1.23499298e-02 -1.87381165e-02\n",
            " -2.27379642e-02 -1.55759794e-02 -1.07614630e-02 -2.95450039e-03\n",
            "  7.48946503e-03 -4.54464238e-03 -1.51561180e-02 -1.75867842e-02\n",
            " -3.74387871e-03 -1.62203096e-02 -6.73986279e-03  1.77473748e-02\n",
            "  5.46494629e-03 -3.26896915e-03  3.31610096e-03  7.36421100e-03\n",
            " -9.47835251e-03  8.32100794e-03 -6.01522510e-03 -1.56546853e-02\n",
            " -2.88834651e-03 -2.88406868e-03  2.12954770e-03  1.08500867e-02\n",
            " -2.97927426e-02 -2.49244070e-02 -1.92025951e-02 -8.13590907e-03\n",
            "  1.18229876e-02  1.67665850e-02  1.12087599e-02  6.31288157e-03\n",
            "  1.66542506e-02  4.80255511e-03 -7.80465869e-04  3.13290023e-02\n",
            " -2.03964860e-02 -1.03200958e-02  2.88116493e-02  6.35130826e-03\n",
            "  8.87528642e-03  9.62752604e-02 -2.00260983e-03 -2.75126558e-03\n",
            "  6.91174341e-03  1.76076103e-03 -2.56592344e-04 -2.46984958e-02\n",
            " -1.69254622e-02 -6.02307872e-02 -2.68461452e-02  1.72510405e-02\n",
            "  1.26844130e-02  1.84925024e-02 -2.04907155e-02  1.92900397e-03\n",
            "  5.35396311e-03  4.15830641e-03 -6.00965227e-03  2.89701908e-02\n",
            "  1.25656735e-03 -2.36006957e-02  6.27116044e-02 -3.37457717e-02\n",
            "  1.89334538e-02  1.09262397e-02  2.56538515e-02 -4.08676900e-03\n",
            " -1.44887718e-02  2.44580571e-02 -2.99218371e-02  1.07313959e-02\n",
            " -2.10413764e-02 -7.19571700e-03  1.83675139e-02 -1.19432007e-02\n",
            "  1.47234917e-02  6.79343306e-03  1.02640767e-02 -1.02627298e-02\n",
            " -1.43799082e-02 -1.94025563e-02 -6.39894744e-03  1.50863306e-02\n",
            "  4.38493012e-03  9.09282015e-03 -2.58326959e-02  1.37381513e-02\n",
            "  5.63769804e-02  1.41227110e-02 -1.86426728e-02  4.87533966e-03\n",
            " -3.38399143e-02  4.20959091e-02  1.29586450e-02 -5.13118709e-03\n",
            " -1.05332773e-03  5.98167637e-03  1.84562777e-02 -6.57941044e-03\n",
            " -1.89471489e-02  1.06380590e-02  6.21339896e-03 -5.76324184e-04\n",
            "  2.94924929e-02 -9.22747653e-03 -8.75365365e-03 -1.92145171e-02\n",
            "  6.55653608e-03  1.79774104e-02 -1.89975887e-02 -1.19976878e-02\n",
            " -5.77028757e-03  6.64845948e-03  2.12272546e-02  1.79204837e-02\n",
            "  4.86024037e-03 -3.25885587e-02  1.22248914e-02  6.71292596e-03\n",
            " -2.83319650e-05 -1.85948842e-03  1.28821850e-03 -5.81701228e-03\n",
            " -1.01693784e-02 -3.17238707e-04 -2.03497158e-02  5.47692556e-03\n",
            " -2.23078504e-02  1.11498295e-02 -1.60983842e-02 -3.29633390e-02\n",
            "  1.58943840e-02 -1.92067863e-03  7.05666426e-03  2.14797325e-02\n",
            " -1.59734862e-03  2.09356001e-02 -8.86134177e-03  1.72976852e-02\n",
            " -1.55399791e-02 -1.81014260e-02  5.12241071e-03 -1.93954470e-02\n",
            "  6.74171417e-03  2.26461232e-02  2.49898897e-02  5.40471651e-03\n",
            " -1.21255395e-02  1.56337210e-02  1.75954998e-02  1.38177694e-02\n",
            " -1.97501185e-02 -2.76735365e-02  2.17632466e-02 -1.37637680e-02\n",
            " -3.86830412e-03 -3.74048411e-02 -4.87967301e-02  2.47610167e-02\n",
            " -3.14624204e-02 -1.31667425e-02 -1.99592295e-02  4.42678878e-02\n",
            "  3.65665002e-02 -1.16890320e-02 -2.99552098e-03 -2.75989734e-02\n",
            "  9.83147897e-03  4.16423968e-02  3.60317915e-02  1.73790964e-02\n",
            "  7.68193867e-03 -2.25188243e-02  1.13999817e-02 -1.01268196e-02\n",
            " -1.01676104e-02  5.93020586e-03  1.44994395e-02  3.27469548e-03\n",
            "  8.90298716e-03 -3.80156955e-03 -2.90972069e-02 -9.81385097e-03\n",
            " -1.59429792e-02  2.44431552e-02 -4.31846095e-03  1.71277699e-02\n",
            "  1.99439546e-02 -1.74300840e-02 -1.11375521e-02 -4.50670930e-04\n",
            " -1.42615784e-04  7.28638545e-04  8.53676584e-03 -2.15357364e-04\n",
            " -3.37478600e-02 -1.30844280e-02  1.88936771e-02  2.10573141e-02\n",
            "  1.38033341e-02 -3.63889998e-04 -1.35357524e-02 -2.02756293e-02\n",
            "  1.50108847e-03  2.50919085e-02  2.39252626e-02  1.32225375e-02\n",
            " -1.39151107e-02 -7.80893752e-03 -3.88495920e-03 -1.01710375e-02\n",
            "  1.89746089e-02  1.09063584e-02  3.77890506e-03 -2.82896520e-02\n",
            " -3.94655159e-02 -1.96774061e-02  3.22658126e-02 -2.43127218e-02\n",
            "  2.22676224e-02 -6.57404565e-03  7.25238337e-04 -1.29986513e-02\n",
            " -2.13358171e-02 -1.57873327e-02 -2.76401870e-04 -2.11743756e-02\n",
            " -2.15731298e-02 -9.61803278e-03 -7.15121751e-03 -1.62002744e-02\n",
            " -3.75978771e-03  3.40649149e-03 -1.11329476e-03  8.32237050e-03\n",
            "  5.20611166e-03 -2.49163369e-02 -2.16229217e-02 -7.88521761e-03\n",
            " -1.46134266e-02 -1.70323434e-02 -1.33490056e-02 -3.01582983e-02\n",
            " -7.79336052e-03 -2.44062030e-02  1.14970043e-02  1.28891256e-02\n",
            "  3.13080586e-02  6.95014290e-03  2.07057118e-02 -4.11895138e-02\n",
            " -1.09605330e-02  1.89738052e-02 -9.19311592e-03  2.31340614e-03]  - intercept :  0.5135531341053484\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.013938101387618949\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:53,639]\u001b[0m Trial 233 finished with value: 0.16414916227632942 and parameters: {'count_threshold': 7, 'postag': True, 'voc_threshold': 9725}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.54397003 0.31479087 0.62270858 ... 0.04650857 0.08538659 0.00809609]\n",
            " [0.02025478 0.04754328 0.05581094 ... 0.18560412 0.06282775 0.04632652]\n",
            " [0.02991014 0.07925393 0.03499591 ... 0.23736874 0.06853936 0.03062197]\n",
            " ...\n",
            " [0.02160127 0.00890969 0.06081889 ... 0.15850175 0.18038093 0.04056514]\n",
            " [0.06640561 0.04071594 0.06710012 ... 0.03964184 0.12562859 0.03829893]\n",
            " [0.36282015 0.01936051 0.46282762 ... 0.09514782 0.05874802 0.03166925]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 0.00766908 -0.00241059 -0.01207392 -0.02068859 -0.00573864  0.01295258\n",
            "  0.00047139 -0.00215294 -0.03665386 -0.0568061  -0.01016441  0.00789514\n",
            " -0.04989706  0.01296418  0.00730523 -0.02084116 -0.04845657 -0.00747758\n",
            " -0.00175303 -0.04858644  0.00461396  0.00804558 -0.02122854 -0.06779958\n",
            "  0.00629758 -0.04307571 -0.03412988 -0.05373046  0.00287753 -0.02672171\n",
            " -0.03350862 -0.01883064 -0.03489806 -0.02983788 -0.02646365  0.00975697\n",
            " -0.0022015  -0.01025295  0.08514784 -0.0114938  -0.00275838  0.03111104\n",
            "  0.01048457 -0.0559493  -0.00781012 -0.00662173  0.00465712 -0.00110594\n",
            "  0.04653317  0.00939798 -0.03778292  0.02853087  0.00670394 -0.0234709\n",
            " -0.06730021  0.01350628 -0.04747792 -0.06617728 -0.02911026 -0.01041667\n",
            " -0.00176174 -0.01232054 -0.01934756  0.0173139  -0.01887256 -0.00778766\n",
            "  0.0204663   0.04435627  0.04768598  0.00026531 -0.06057089 -0.03753383\n",
            "  0.01975341  0.00731146 -0.00415359 -0.04178571 -0.01742145  0.02896626\n",
            "  0.02461783  0.06217581 -0.06364024  0.07101627 -0.02795723  0.03576651\n",
            "  0.00949701 -0.0120137   0.02988269  0.00023894 -0.06162129 -0.01859687\n",
            " -0.06289193 -0.05568976  0.01504284  0.01542137  0.04177836 -0.0804595\n",
            "  0.00335692  0.00914635  0.04297553 -0.00032368 -0.00912326  0.00278452\n",
            "  0.00363388  0.00783131 -0.011534    0.02661619 -0.04109681  0.00496976\n",
            " -0.03086367 -0.02027134 -0.0154786   0.0374082   0.00467355 -0.01022644\n",
            " -0.01789619  0.00614038  0.00501387  0.01416926 -0.09522333 -0.01948879\n",
            " -0.0149634  -0.05759629 -0.01901132 -0.05161007 -0.02804464  0.02915886\n",
            " -0.01514904 -0.00387893 -0.04626071 -0.00359039  0.04168396  0.023369\n",
            "  0.00898089 -0.00031379  0.00691032 -0.00226518  0.03102062 -0.04262324\n",
            "  0.00756191  0.00517921  0.05627855 -0.05669521  0.02668227 -0.02935013\n",
            "  0.0105062  -0.06875521  0.00260266 -0.0161855   0.00777331 -0.01788739\n",
            " -0.02120234 -0.01633325  0.03001048  0.00413663  0.00336548 -0.02011492\n",
            "  0.03663181  0.01563445 -0.05112549  0.01737719  0.02915733  0.06992591\n",
            " -0.04316544  0.0238523  -0.04975918 -0.00178284 -0.02262183  0.01462128\n",
            "  0.01083716 -0.01519987  0.01224084  0.00346655 -0.00987847  0.01564053\n",
            " -0.05507532  0.04877576 -0.02133256 -0.01544862 -0.01084156 -0.00929207\n",
            "  0.04511026  0.00589299 -0.03752905 -0.02244731 -0.00589816 -0.03375406\n",
            "  0.04074195  0.02484913 -0.03795151 -0.05321774  0.0155357   0.01616211\n",
            "  0.01284246  0.03088186  0.02630829  0.01107386  0.00930213 -0.05965183\n",
            " -0.02440293  0.00923821  0.02304267  0.00519437  0.05364977  0.01081821\n",
            " -0.03431813  0.00042542  0.00294155 -0.00737079  0.0220444  -0.01557876\n",
            " -0.07341946  0.02267242 -0.02959824  0.00816608 -0.03803784 -0.00573164\n",
            "  0.05817416  0.0199435   0.03899891  0.00833694 -0.01340452 -0.04590439\n",
            "  0.02049941  0.00973967 -0.06669433  0.01932111  0.0185635   0.02272722\n",
            "  0.04761497 -0.03593531 -0.02923117 -0.01893009 -0.00399996  0.01591531\n",
            "  0.02850585 -0.05237881 -0.0169833  -0.02449951  0.06185817  0.01507592\n",
            " -0.02519104  0.04517052  0.00818938 -0.00844331  0.01512941  0.01769676\n",
            " -0.00056244 -0.00954076 -0.00952665  0.00031122 -0.01614205 -0.00064711\n",
            "  0.05357748 -0.01854853  0.01431421 -0.00817204 -0.03390841 -0.00366405\n",
            "  0.00367663 -0.02926138 -0.09480016  0.03716133 -0.03966459 -0.05522006\n",
            " -0.01469042 -0.00040708  0.01582755 -0.02058511 -0.01854728 -0.0172859\n",
            " -0.08873446  0.01159735 -0.00504038  0.00220677 -0.04148888  0.011588\n",
            " -0.04275521 -0.00520541  0.03566025 -0.0147431  -0.04844776 -0.03471009\n",
            "  0.069875    0.0266875  -0.02669171  0.0113581  -0.02123611 -0.03166507\n",
            " -0.02562482 -0.0162594  -0.01513085  0.00754418  0.06014944 -0.00716535\n",
            " -0.04155151 -0.01861117  0.01305854 -0.04468383 -0.02743678  0.08678678\n",
            " -0.04488621 -0.1304822   0.03438362  0.0066151  -0.0102848  -0.00852971\n",
            " -0.00971304  0.05751921  0.009895   -0.02763928  0.03713249  0.01208582\n",
            " -0.00247126 -0.01837615 -0.00909011 -0.00849793  0.02336835 -0.01412797\n",
            " -0.01561532 -0.0227144  -0.02426593  0.00412697  0.00771871  0.02568103\n",
            " -0.04166378  0.07123363 -0.0272556   0.0142753   0.03044452 -0.03823047\n",
            " -0.01094245  0.01073722 -0.01616491  0.02091386  0.02642405  0.07352498\n",
            " -0.03864536  0.03531226 -0.04274897 -0.0129047   0.0145098  -0.01310142\n",
            "  0.01019499 -0.02301272  0.02246376 -0.07340098 -0.10060053  0.05424999\n",
            " -0.06160424 -0.04662742 -0.0515611   0.04429018 -0.07242057  0.00322174\n",
            "  0.04996372  0.03894378 -0.03258684  0.04929379 -0.03308348 -0.03283563\n",
            " -0.06341167 -0.01210209  0.01968425 -0.03306656 -0.033259   -0.06829475\n",
            " -0.05800023  0.00905936 -0.00093268  0.01638257 -0.009814   -0.04254156\n",
            "  0.07131616 -0.0207861  -0.04344357 -0.00620749 -0.00524356 -0.08224445\n",
            " -0.02881902  0.03487411  0.01453904  0.02139198  0.12780504  0.00222307\n",
            "  0.02268579 -0.00806258 -0.04312174 -0.07233532 -0.02170365  0.00793989\n",
            " -0.02183315 -0.00668758  0.01167065 -0.01741096  0.01272157  0.01311049\n",
            "  0.00920943  0.02080163  0.01461615 -0.07416212  0.03845662  0.00334637\n",
            "  0.0310419  -0.07197774 -0.07110218 -0.02315605  0.00285232  0.02185488\n",
            "  0.06478101  0.02265129 -0.05236407 -0.03474196 -0.00921404  0.03394888\n",
            " -0.00040094  0.0032735  -0.00589997 -0.00850692 -0.08288192  0.01708821\n",
            " -0.01745161  0.00021765  0.04814901  0.02112439 -0.00441047 -0.05730727\n",
            " -0.04001505  0.06779868 -0.01508301 -0.07155554 -0.0222851   0.00226568\n",
            "  0.00370711 -0.025471   -0.02989738 -0.03889026 -0.04426901 -0.00383717\n",
            " -0.04634992  0.05036736 -0.07656088 -0.0459915   0.06795444 -0.04300013\n",
            "  0.00985916 -0.06196753  0.0219302  -0.020264   -0.02491916  0.02743306\n",
            " -0.0502201  -0.00825831  0.02884495 -0.04780414 -0.13055726 -0.02884309\n",
            " -0.07801095 -0.06219695 -0.01663239  0.02165831 -0.03770463 -0.02347196\n",
            "  0.05510659  0.03611218 -0.04596325  0.00473704  0.02434021  0.00023362\n",
            "  0.00934188  0.02075887 -0.05027598 -0.05004744  0.01165798 -0.0020286\n",
            "  0.01033522  0.02457837  0.02623071 -0.00889578 -0.12947185 -0.02335222\n",
            " -0.02547491 -0.01988841 -0.02137472 -0.0313483   0.04864732 -0.00531875\n",
            "  0.00566676 -0.00072611]  - intercept :  1.0479301476571403\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.16414916227632942\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:54,217]\u001b[0m Trial 234 finished with value: -0.15175766206825175 and parameters: {'count_threshold': 4, 'postag': False, 'voc_threshold': 5475}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.         0.02876461 0.         ... 0.02246286 0.2518603  0.09567395]\n",
            " [0.11154975 0.04488299 0.11585081 ... 0.20121974 0.20988358 0.09034059]\n",
            " [0.16387132 0.05665247 0.25859593 ... 0.01142995 0.23380845 0.02602888]\n",
            " ...\n",
            " [0.0041187  0.01098186 0.00189987 ... 0.08912095 0.04197672 0.01978173]\n",
            " [0.34862108 0.05473013 0.75471925 ... 0.11973788 0.07164501 0.00878942]\n",
            " [0.         0.01791284 0.         ... 0.00962694 0.10794013 0.15435424]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 0.00414932  0.00476507 -0.00161939 ...  0.02094552  0.00845471\n",
            "  0.00354417]  - intercept :  0.8062518431223472\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.15175766206825175\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:54,819]\u001b[0m Trial 235 finished with value: 0.026106955664307492 and parameters: {'count_threshold': 7, 'postag': False, 'voc_threshold': 9602}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.05706276 0.0943582  0.09111925 ... 0.11479457 0.22742245 0.04617433]\n",
            " [0.03083152 0.09112758 0.03289103 ... 0.24004304 0.05219996 0.1585704 ]\n",
            " [0.         0.         0.05685144 ... 0.1063276  0.82369984 0.0443504 ]\n",
            " ...\n",
            " [0.         0.07214006 0.08344532 ... 0.26097607 0.06089995 0.06838558]\n",
            " [0.00790501 0.05545487 0.01817314 ... 0.05025263 0.08525993 0.0510392 ]\n",
            " [0.03293984 0.08643499 0.0462156  ... 0.46859066 0.06959994 0.09464916]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-1.58564115e-02 -3.19508791e-02 -6.95781939e-03  2.06689645e-03\n",
            " -8.75069579e-05 -3.71210083e-02 -2.16681179e-02  1.55010726e-03\n",
            "  6.64078945e-03 -2.42155453e-02  2.27237023e-02  1.66761996e-02\n",
            " -9.37646809e-03 -3.48520710e-02  8.56556930e-03  9.22185367e-03\n",
            "  1.84981847e-02 -3.76494959e-02 -2.51214907e-02  4.07758491e-02\n",
            "  2.77102556e-02  1.65045729e-03 -1.58780066e-02 -4.89015472e-02\n",
            " -9.28560143e-03  2.05217322e-02 -7.78798017e-03  3.88967492e-02\n",
            " -7.14721824e-03  2.10710542e-02  5.19457708e-03 -1.92606629e-02\n",
            " -2.82079727e-03 -1.87845576e-03  1.27799903e-02 -2.75899999e-02\n",
            " -1.69257523e-02  1.17215995e-02  2.08122730e-02 -3.70325470e-02\n",
            "  5.46755522e-02 -2.46380888e-02  3.77799477e-02  2.14272789e-02\n",
            " -6.39849576e-02 -4.62936919e-03  4.19818153e-02  4.30714654e-02\n",
            " -3.18933404e-02  1.90210923e-02 -6.15884949e-04  3.87580088e-02\n",
            " -3.73557775e-02  6.95157303e-03  1.98639321e-02  2.79966119e-02\n",
            "  3.98215026e-02 -2.16920941e-02  1.48433558e-02 -5.47199015e-02\n",
            " -1.32860461e-02  9.06009608e-03  6.39320456e-03  9.90253642e-03\n",
            "  2.26175441e-02 -4.07447277e-03  1.82180611e-02 -3.24138984e-02\n",
            "  6.17792188e-02  3.26715389e-02  3.29732552e-02 -3.29790588e-03\n",
            " -4.33673641e-04 -1.70377552e-02  3.71203605e-02 -1.18228284e-02\n",
            "  1.41529499e-03  8.67932056e-03  8.67932056e-03 -4.60596570e-02\n",
            " -2.03246331e-02  1.48273964e-02 -2.42030503e-02 -6.60949292e-02\n",
            "  4.59685910e-02 -3.14703141e-02  7.31120277e-03 -1.19187260e-02\n",
            "  6.45011332e-03 -3.18504404e-03  1.79436028e-02 -1.51035762e-02\n",
            "  4.09311452e-02 -4.31183732e-04 -2.96660451e-02 -4.69201303e-02\n",
            " -8.14302370e-02  4.35618643e-02 -1.58158966e-02 -3.94421684e-02\n",
            " -6.44117558e-03  2.66302695e-03 -2.87047242e-02 -4.20382888e-03\n",
            " -9.42011443e-03 -3.14811816e-02  2.06583961e-02 -2.16482303e-02\n",
            "  4.95221383e-03  5.18473828e-03  7.90038810e-04 -1.84037891e-02\n",
            " -4.30251479e-02 -4.85037075e-03 -3.07639045e-02  3.71645326e-02\n",
            "  3.01542219e-02 -1.11850906e-02 -2.81234312e-02 -1.83121529e-02\n",
            " -1.05404190e-02 -5.42939224e-03 -3.45965237e-03  5.88608018e-03\n",
            " -8.61475052e-02 -4.35703981e-02  5.92336728e-02  3.85343007e-02\n",
            " -1.36521783e-02  7.41950681e-03 -7.92907397e-02  1.86629377e-02\n",
            " -2.23745638e-02  4.16514633e-02  1.65363869e-02 -8.05912302e-03\n",
            "  3.56591121e-02  4.98942738e-02  3.44487344e-02 -1.50987002e-02\n",
            " -2.68193335e-02  2.81794893e-02 -9.94548795e-04  3.62087419e-02\n",
            "  2.91408690e-02  1.87866456e-02  1.94997954e-02 -1.43208214e-02\n",
            " -2.84976756e-02  3.91205837e-02  2.73756434e-02  4.07706755e-02\n",
            "  4.33613362e-02 -1.05719297e-01 -5.54151902e-03  3.19117803e-02\n",
            " -1.23447304e-02  5.72009495e-03 -2.81552283e-02  2.95327750e-02\n",
            "  3.00256467e-02 -5.20320923e-03  6.13763780e-02  1.49149324e-02\n",
            "  5.31826393e-03  4.60985819e-02 -1.30907859e-02 -7.23643959e-02\n",
            " -2.59555327e-02 -6.57726805e-02 -8.49466881e-03  3.69673693e-03\n",
            " -1.65125938e-02  1.20138427e-02  4.18407597e-02 -6.73110187e-04\n",
            "  1.54352136e-02  8.62195021e-03 -5.06595116e-03 -1.29054454e-02\n",
            "  9.06315203e-03  1.23569212e-02  4.74239786e-03 -1.99237195e-02\n",
            "  2.19430867e-02  1.92155390e-02 -4.15335835e-04  6.19836974e-02\n",
            " -2.58337506e-02 -2.86689968e-02 -1.47486743e-02  1.73686661e-02\n",
            " -4.81230293e-02 -1.17622575e-02 -1.75701489e-02  1.72169116e-02\n",
            " -4.11225664e-02  1.18394632e-02 -8.21572743e-03  3.91599420e-02\n",
            " -1.66596426e-02 -1.18475074e-02 -2.21417156e-02  4.38172169e-02\n",
            "  4.54953619e-02 -3.09970620e-02  2.63089632e-02 -3.60896424e-02\n",
            " -5.75673411e-02 -3.11289122e-03  2.37558257e-04  1.54831324e-02\n",
            " -6.31434746e-02 -9.49782862e-04  2.74467792e-02  2.40945635e-03\n",
            " -2.87582078e-02  1.38734578e-02  1.87411258e-02 -1.03703381e-04\n",
            "  4.58020995e-02 -1.85635436e-02  1.51078418e-02 -6.38117846e-03\n",
            " -1.89821914e-02 -1.14633155e-02 -1.41087717e-02 -3.54612364e-03\n",
            "  3.93170154e-02 -3.04994548e-02  5.42825675e-02  5.05644464e-03\n",
            " -8.60497592e-03  2.83128120e-02  1.66221945e-02  1.76219610e-04\n",
            " -3.46604871e-03  2.54335637e-02  1.38236681e-02  3.60624364e-02\n",
            " -4.58827769e-02  4.35981860e-03  1.53516989e-02  2.67120785e-02\n",
            " -1.84970625e-03  1.90616347e-02  1.76226424e-02  2.20149228e-03\n",
            "  3.33075334e-02  4.76629800e-02  6.15116676e-02  5.47370349e-02\n",
            "  1.21897289e-02 -2.28421872e-02  2.77941527e-02  1.99572632e-02\n",
            "  3.45808782e-03  5.38969242e-03  1.01121863e-02 -6.42423930e-03\n",
            " -1.00712133e-03 -2.49157788e-02  2.06238538e-02  3.43490942e-02\n",
            " -4.32616528e-03 -1.75098292e-03  3.41849698e-04  8.41350818e-03\n",
            " -9.35540530e-04  1.27362748e-02 -3.79576156e-02 -8.15374811e-03\n",
            "  2.14315322e-02 -1.99271066e-04 -1.02640372e-03  4.71798079e-02\n",
            "  2.38330274e-02 -6.07699345e-04 -5.43816764e-02  1.34673852e-02\n",
            "  1.73382087e-02  2.83027290e-02 -1.25800704e-02  1.77476396e-02\n",
            "  5.07360741e-02 -2.28361619e-02  3.40858457e-02  2.23467052e-02\n",
            " -1.73884694e-02  1.50325854e-02 -3.21260322e-02  3.68736167e-02\n",
            " -5.38786457e-02  2.58024203e-02 -1.38883870e-02 -9.94707071e-03\n",
            "  2.12499056e-02  4.53578603e-03  6.07758543e-03  2.12518635e-02\n",
            "  1.35572838e-02 -3.37582714e-02 -2.03962304e-02 -1.38116149e-02\n",
            " -6.18360783e-03  5.37754886e-03 -1.47334573e-02  2.86421751e-02\n",
            "  3.28635152e-02 -1.96434390e-02  3.40664267e-02  1.61390199e-02\n",
            " -3.50134936e-02  2.84758077e-02  3.67233085e-02  4.40147070e-02\n",
            " -2.86778845e-02  1.98535990e-02 -5.00860189e-02 -6.24578979e-03\n",
            " -3.56619655e-02  7.49090075e-03  6.99315250e-02  8.44986766e-04\n",
            "  1.36424744e-02  3.70990379e-02 -2.15034372e-02  4.71447427e-02\n",
            " -1.55074575e-02 -8.28363212e-04 -1.21597417e-02 -2.88888744e-02\n",
            " -3.11732290e-03  1.53307342e-02 -1.82078917e-02 -1.82078917e-02\n",
            " -1.49231800e-02  9.67833182e-03 -8.63955452e-03 -3.67405975e-02\n",
            " -7.72367618e-03  3.08544809e-02  1.79030355e-02  1.97216795e-02\n",
            " -5.89006313e-02  5.92701972e-02  2.04006015e-02 -4.59218110e-02\n",
            " -7.97001486e-03 -3.75891366e-02  1.73885318e-03  2.17378836e-02\n",
            " -2.85273847e-02 -9.47917416e-03  3.11471828e-02  2.21802985e-02\n",
            " -9.57083046e-03 -4.69610666e-02  2.63559941e-02 -7.10959929e-02\n",
            " -2.02804973e-02 -1.73503438e-03  6.27905732e-02  5.35343828e-02\n",
            "  5.39485592e-02 -1.34582656e-02  8.16550982e-03 -4.13811729e-02\n",
            "  1.72834606e-02 -4.10156016e-02  5.45261358e-03 -3.23269539e-02\n",
            "  1.18006526e-03 -4.05406103e-02 -2.36195716e-02  2.95048620e-03\n",
            " -4.66860497e-03 -1.07360933e-02 -7.69847867e-03 -1.56406309e-02\n",
            " -1.70624475e-03  1.65313811e-02 -3.53970604e-02  3.71292425e-02\n",
            "  8.23844358e-02  2.82667356e-02  2.87201661e-02  1.04680585e-02\n",
            "  1.09363401e-02 -2.09414037e-02  3.22687199e-02  2.96157711e-02\n",
            " -3.36378636e-02  6.35196287e-02  3.24901231e-02 -6.79216365e-02\n",
            "  3.78005970e-03 -3.66342776e-02 -2.59471852e-02  5.57232420e-02\n",
            "  5.30599679e-02  2.04309652e-02  2.31199146e-02 -6.83021563e-02\n",
            "  2.81704583e-03  1.65244312e-02  4.43713849e-02  4.95209975e-02\n",
            " -3.73634433e-02  1.80377493e-02  7.89996488e-03 -1.06943822e-03\n",
            "  6.76100417e-02 -1.20578811e-02  1.83191771e-02  5.32872214e-02\n",
            " -2.60225903e-02  1.92116742e-03  2.58910964e-02 -8.09225190e-03\n",
            "  3.62255703e-02 -9.70657132e-03 -3.04436739e-02 -6.22449400e-02\n",
            "  2.25956532e-02 -1.46015361e-02 -2.35321906e-02  1.71644931e-02\n",
            " -3.85335805e-02  4.35391557e-03  1.73156724e-02 -1.58550464e-02\n",
            " -5.75212457e-03  2.46561851e-02 -1.04555898e-02  3.30640544e-02\n",
            "  6.74515466e-03 -2.17023193e-02  4.24630300e-02  1.22817948e-03\n",
            " -7.45616162e-02  5.41173556e-02  6.52997925e-03  2.00550606e-02\n",
            "  8.29145676e-02 -4.63877755e-02  2.17093023e-03  5.83103359e-02\n",
            " -2.44929398e-02 -2.93395651e-02  9.62088616e-03  4.63627303e-02\n",
            "  6.21216986e-02  1.66080689e-02  2.85230268e-02  1.10279075e-02\n",
            "  4.44810644e-02 -2.48883945e-03 -1.26586279e-02 -1.59441617e-02\n",
            "  4.20283559e-02  3.22628697e-02 -5.35449574e-02 -3.12008224e-02\n",
            " -4.00409101e-02 -2.47429679e-02 -4.24712931e-02 -2.58818752e-02\n",
            "  2.45297897e-02  2.35772402e-03  9.59075607e-03  3.18575847e-02\n",
            " -7.90801910e-03 -1.57555406e-02 -1.14491314e-02 -4.90434069e-03\n",
            " -3.77360461e-02  4.78056326e-03  4.43079817e-02  1.52787542e-02\n",
            " -7.71058838e-04  3.67208280e-02 -2.04503980e-02  8.22373291e-04\n",
            "  1.22614131e-02  1.13060584e-01 -1.34368087e-02  2.81919048e-04\n",
            "  2.66034739e-02  1.25054568e-02 -2.72544829e-02 -3.19336989e-02\n",
            "  2.12930987e-02 -1.74522128e-02  1.12243942e-02 -5.82134187e-02\n",
            "  1.42462738e-02  8.16272429e-03  2.36565912e-02  4.90947282e-03\n",
            "  2.36867601e-02 -6.18442844e-03 -6.34470493e-02 -2.31232634e-02\n",
            " -2.40676360e-02 -1.55383081e-03  2.96269089e-02  2.21709012e-02\n",
            "  7.17893277e-03  9.45517946e-03 -2.03948277e-02 -4.38574881e-02\n",
            " -1.84035856e-02  4.42853827e-03]  - intercept :  0.5466925282801436\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.026106955664307492\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:55,388]\u001b[0m Trial 236 finished with value: 0.13612683833692113 and parameters: {'count_threshold': 10, 'postag': False, 'voc_threshold': 7531}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[1.84295819e-01 4.63951854e-02 3.30528952e-01 ... 2.21641204e-01\n",
            "  3.05526008e-01 1.46834190e-02]\n",
            " [0.00000000e+00 1.14391666e-01 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 6.96660564e-02]\n",
            " [3.08337067e-02 6.12443785e-02 6.29858268e-02 ... 5.11400140e-02\n",
            "  7.40813241e-02 3.43413557e-02]\n",
            " ...\n",
            " [3.26496301e-01 4.93700437e-02 4.37955278e-01 ... 6.01315265e-01\n",
            "  1.07914771e-01 8.31377554e-03]\n",
            " [5.58699275e-02 4.75340693e-02 6.78531952e-02 ... 1.44246179e-01\n",
            "  1.44869123e-01 2.45457087e-02]\n",
            " [0.00000000e+00 5.21908190e-02 1.38456923e-02 ... 5.18805900e-04\n",
            "  1.21287148e-01 1.79110868e-02]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-0.04567312 -0.02012706  0.08051578 -0.03041704  0.06315824 -0.02824211\n",
            " -0.16092061 -0.02657129  0.00214335 -0.08073325 -0.10105378 -0.07230712\n",
            " -0.00552644 -0.06809385 -0.12713736 -0.05056927  0.02107272 -0.00966906\n",
            "  0.05040279  0.05435586 -0.08013098 -0.12252298 -0.15341448  0.00510705\n",
            " -0.06592127 -0.01348253  0.09142392  0.0043247  -0.10755323 -0.1474578\n",
            "  0.02932438  0.0372568   0.0727699  -0.07507391 -0.00797009  0.02397489\n",
            " -0.07772134 -0.01098066  0.02012306  0.06237643 -0.00283952 -0.02203097\n",
            "  0.15040594 -0.15599956  0.01149862  0.02332724  0.02839004 -0.00160455\n",
            " -0.12922078 -0.14328269 -0.04520765  0.11784474 -0.02478864 -0.07782261\n",
            " -0.00175306  0.10561186 -0.12813636 -0.04062377 -0.10676593 -0.16176448\n",
            "  0.04640777 -0.0767348   0.03728489 -0.00714588  0.03496976 -0.11745674\n",
            "  0.00555677  0.01388211 -0.07248777  0.16408682  0.04410612  0.02619986\n",
            " -0.21945558 -0.08151283 -0.02433318  0.009633    0.10490697 -0.11635549\n",
            "  0.06142801 -0.01897243  0.02607794 -0.06067321 -0.01068796 -0.10446413\n",
            " -0.01475725  0.13304648  0.10106026  0.13639168  0.06281828  0.17155107\n",
            "  0.05129194  0.05663245 -0.02562891 -0.03532255 -0.15054157  0.22735919\n",
            "  0.02887696  0.04786047  0.02556587  0.03492214  0.00695108  0.06019166\n",
            " -0.13301168  0.19210495  0.17728367  0.09404285 -0.04460871 -0.01861302\n",
            " -0.10130614  0.01409255 -0.04050873 -0.0846714   0.13293795  0.20906565\n",
            "  0.12468809 -0.06415349  0.03780281  0.14965887  0.05784744  0.09589603\n",
            "  0.07097417 -0.14286068 -0.01385142  0.05023652  0.08902054 -0.0231879\n",
            "  0.0086486  -0.15470938  0.02537908  0.00620301 -0.14600741  0.16844105\n",
            " -0.14275917  0.00184847 -0.06306879 -0.15960131 -0.02010856 -0.10617046\n",
            "  0.20246787  0.00134498  0.08678429  0.12553155 -0.01275265  0.05259166\n",
            " -0.02922984  0.12578162 -0.01200244  0.06319927  0.3057416   0.05465567\n",
            " -0.05472973  0.01985131  0.02660947  0.0431798  -0.04106295  0.05908822\n",
            " -0.11352259  0.03873323  0.08220388  0.12151965 -0.0706047   0.13164316\n",
            "  0.08351462  0.08038674 -0.03201823  0.02242888  0.03247443  0.1123608\n",
            "  0.14247168  0.10297103  0.13828413 -0.02301855 -0.05501468  0.03099645\n",
            " -0.01532347  0.0588967  -0.06733763 -0.05370421  0.02760748  0.11316049\n",
            " -0.05824338 -0.08668948  0.20928135 -0.04278272  0.04475879 -0.0143023\n",
            "  0.04545214 -0.06876995  0.07800628  0.08569626  0.21162861  0.01165689\n",
            "  0.21632789  0.15542474  0.01506777  0.25376249  0.10634139 -0.04732036\n",
            " -0.04604241  0.17529714  0.01173099  0.02935449 -0.04204354  0.15904174\n",
            "  0.02670428  0.11672384  0.13613149  0.11758204 -0.11200461  0.03610796\n",
            "  0.0074021   0.05804845  0.08374228 -0.04250205 -0.01510992 -0.06759603\n",
            " -0.05772619  0.18853256  0.04000221  0.10558009  0.00313838  0.11900492\n",
            " -0.0966559  -0.19195776 -0.08304782  0.14144627  0.30496568 -0.06282944\n",
            "  0.26794192  0.23994089  0.0927089   0.06633577  0.07921842  0.24093286\n",
            "  0.12875845 -0.00646165 -0.09180885  0.02966786  0.01118895  0.16162915\n",
            "  0.05312035  0.05027086  0.04314884  0.05125574 -0.00714013 -0.12173717\n",
            "  0.14626061 -0.07226705 -0.02393254  0.03440518  0.00935953  0.2503459\n",
            " -0.23543509  0.13836694  0.00042678  0.11981336  0.03449027  0.10721715\n",
            " -0.03562664  0.08555644 -0.09057169 -0.07212973 -0.00948424  0.19871337\n",
            " -0.0812762  -0.09078636 -0.01055368  0.10154099 -0.01660085 -0.01811778\n",
            "  0.12119637  0.07249928  0.18049318 -0.02106624  0.12012332 -0.01393866\n",
            " -0.07717311 -0.1150827   0.09424832 -0.14198785 -0.03270562 -0.04577956\n",
            " -0.10560137  0.06995274 -0.07154838  0.01796982]  - intercept :  -0.1104288371051661\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.13612683833692113\n",
            "yes\n",
            "Creating vocab and co-occ matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-01-31 22:47:55,954]\u001b[0m Trial 237 finished with value: -0.04240355056586351 and parameters: {'count_threshold': 9, 'postag': False, 'voc_threshold': 1440}. Best is trial 128 with value: 0.5489420817004809.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.         0.00133564 0.03262716 ... 0.5471432  0.13416051 0.13457294]\n",
            " [0.01778486 0.12992732 0.03761685 ... 0.17216693 0.08744405 0.03158451]\n",
            " [0.22409778 0.01366891 0.46216546 ... 0.06020306 0.11176369 0.02112564]\n",
            " ...\n",
            " [0.03707788 0.04701297 0.06939082 ... 0.32635211 0.06923423 0.0364123 ]\n",
            " [0.00729097 0.03737623 0.06068018 ... 0.51734204 0.01903296 0.05350553]\n",
            " [0.05261245 0.00708376 0.0930276  ... 0.42479656 0.26854956 0.05633676]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [ 5.31121924e-02  1.60109498e-02 -3.00343725e-02  4.21466582e-02\n",
            "  4.06341605e-02  3.83710339e-02  3.57526564e-02  1.71405715e-02\n",
            " -7.65294635e-03 -2.63696317e-03  4.92267585e-02  1.52974478e-01\n",
            "  8.05864204e-02 -1.33431061e-03 -6.81755426e-02  2.69908997e-02\n",
            "  3.73990545e-02 -7.17866466e-02 -1.99268065e-02  2.65630085e-02\n",
            "  2.43390686e-02  5.30686003e-03  1.53061559e-02  4.97085940e-03\n",
            "  9.62677043e-03  8.64896022e-02  5.76370297e-03 -2.01371506e-02\n",
            "  9.84176105e-03  6.56064853e-02  7.77729937e-02 -4.33302362e-02\n",
            " -7.04913879e-02  7.41834918e-02 -4.39677571e-02 -1.45229291e-01\n",
            " -2.15458884e-02  4.45587331e-02  8.01017073e-02 -3.93182642e-02\n",
            "  4.28622610e-02  4.75457246e-02  3.42363566e-03  8.91292680e-02\n",
            "  1.54243495e-02  9.83121770e-02  9.65418419e-02  1.18639924e-02\n",
            "  5.67966186e-02 -2.61541665e-02 -1.04706274e-01  8.34452033e-02\n",
            " -1.24157211e-04 -3.61624431e-02  1.13526519e-02  9.10545212e-02\n",
            " -7.94706418e-02  8.17985872e-02 -5.49600016e-02  1.56343440e-01\n",
            " -2.39553397e-02 -5.12069163e-02  6.66851826e-02 -3.25608301e-02\n",
            " -7.72127414e-03  1.51838073e-02 -9.54232336e-02  6.32116080e-03\n",
            "  5.64762843e-02  2.96946852e-02  5.73052515e-02  5.54861138e-02\n",
            "  2.69495817e-02 -3.90197024e-03 -7.22334122e-02 -4.86609846e-03\n",
            "  3.10237592e-03 -5.40428990e-02  3.55859001e-02  8.56711816e-02\n",
            " -8.98858281e-04  8.76907962e-02  5.75911392e-03  4.30669907e-02\n",
            " -2.93623665e-02 -9.15272985e-03  2.05902251e-01  3.28648805e-02\n",
            " -7.57809432e-03  6.27307512e-02  1.08472211e-01 -8.06938595e-02\n",
            " -1.18089950e-02  6.41190230e-02  3.07982356e-02  8.39072421e-02\n",
            " -5.17389367e-02  5.89657332e-02  3.02160410e-02 -6.07980812e-02\n",
            " -5.73072278e-02 -8.16703056e-02 -1.24692736e-01 -1.89578070e-01\n",
            " -1.67955038e-01 -4.78038309e-02  3.51732078e-02 -2.72481798e-02\n",
            " -1.52740781e-02  8.99453599e-02  6.19984155e-02  1.54348875e-02\n",
            " -2.44311187e-02 -1.31431356e-01  8.32715648e-02 -1.08776295e-01\n",
            " -5.42153797e-02  9.30528651e-02  9.20299192e-02  6.56099669e-02\n",
            " -5.86527736e-02 -2.75669170e-02  7.74845312e-02 -2.43484060e-03\n",
            "  4.61078474e-02  3.75459312e-02 -1.50690616e-01  1.55313756e-01\n",
            " -6.70309849e-03  1.24364906e-01 -2.10499566e-03  7.31126441e-02\n",
            "  6.76861835e-02 -1.38375129e-01 -5.63774989e-02  7.85955866e-02\n",
            "  1.75962641e-02 -1.84661545e-02  4.83173066e-04 -5.70242783e-02\n",
            "  2.39683374e-02 -5.48343813e-02 -6.40541296e-02 -9.42963054e-02\n",
            " -4.86505693e-02 -5.79120233e-02 -5.79120233e-02  8.25176126e-02\n",
            " -1.56191861e-02 -2.19859447e-02 -2.21979197e-02 -2.19593025e-01\n",
            "  2.16600268e-03  1.01261415e-02  9.33685148e-02  1.41846332e-01\n",
            "  1.62773295e-03 -2.80136282e-02  5.27911878e-02  6.76157111e-03\n",
            "  5.20245702e-03 -1.18730352e-02 -7.84746814e-03 -1.28369260e-02\n",
            " -4.39717862e-02  2.80187697e-02 -2.16930407e-02  2.93029855e-03\n",
            "  5.39988960e-02  4.87418039e-02  5.54685947e-02 -1.11912654e-01\n",
            "  4.40670998e-02 -1.80571917e-02  1.22097901e-02  2.49937613e-02\n",
            " -7.13958434e-02  3.39689654e-02  1.44131232e-02 -5.17578701e-02\n",
            "  1.40942894e-03 -1.90233567e-02  1.62257657e-02 -4.50023463e-02\n",
            "  2.60272152e-02  1.49214106e-02 -1.55725085e-02 -1.76282134e-02\n",
            "  2.17853207e-02  3.86577938e-02  3.74982545e-02 -6.54454279e-02\n",
            " -3.91327632e-02 -1.18507508e-02 -8.21125302e-02  7.83147167e-02\n",
            "  2.49322848e-02 -7.38623995e-02  1.60148629e-02 -1.66009673e-02\n",
            "  2.27691188e-02 -6.91825345e-02  5.03080103e-02  2.76355188e-02\n",
            "  2.70799917e-02 -1.80135514e-02  3.76231748e-02  1.20072826e-02\n",
            " -6.88636939e-02  2.29903881e-02 -6.81950190e-02  1.94131572e-02\n",
            "  5.68467706e-02 -5.85307648e-02  3.74322116e-02 -1.70871321e-02\n",
            "  1.23402951e-01 -1.00965490e-02  5.47582369e-02  3.66327221e-02\n",
            "  4.24662817e-02  1.01282079e-02  8.04355457e-02  3.57778141e-03\n",
            " -6.71399030e-03 -4.76522006e-02 -5.53768685e-02  1.00549832e-01\n",
            " -5.71914347e-02  2.47613282e-02  1.38866165e-01  1.03225496e-01\n",
            "  2.07524926e-02 -6.09779857e-02 -6.38711320e-02 -1.08579820e-01\n",
            "  3.30514785e-03  3.35552107e-02  6.72258104e-02  7.48667520e-02\n",
            "  1.48494634e-02  6.38327511e-02 -2.16599960e-02 -7.69763857e-02\n",
            " -2.57682178e-03  4.87422428e-02 -6.66282578e-02  3.29909841e-03\n",
            " -1.08009323e-01  1.03122112e-02  2.55567166e-03 -2.63592844e-02\n",
            " -4.88607574e-02  6.93515943e-02  1.11866412e-02  1.57624152e-02\n",
            " -8.92776177e-02  1.26090531e-01 -1.35577732e-01  7.27020527e-02\n",
            "  9.12230510e-02 -2.93121557e-02  4.32161410e-02 -1.75267628e-01\n",
            " -1.34839643e-01 -2.26299098e-01  3.80554628e-02 -9.03993448e-02\n",
            " -6.51700377e-04 -2.11537093e-03  1.33775040e-02  4.70694206e-02\n",
            "  2.32647999e-02  2.62587535e-02  6.34122583e-02 -1.07393733e-01\n",
            " -2.26989989e-02  4.75893573e-02  1.20858426e-02  1.00430759e-01\n",
            " -9.26765538e-02  3.68526938e-02  2.53860356e-02  1.85803950e-01\n",
            " -7.43755602e-02  3.63890662e-02 -1.27921035e-03  9.97052610e-02\n",
            " -9.41898287e-02  5.97524958e-02  9.75849683e-02 -1.16144604e-01\n",
            "  8.62980245e-02 -1.00478565e-01 -1.20285663e-01 -1.45536047e-02\n",
            "  8.28985410e-02  3.73243450e-02  7.26958440e-02 -6.54157081e-02\n",
            "  4.00109645e-02  6.60738758e-02 -2.71588487e-02 -1.46935739e-02\n",
            " -4.12215129e-02  3.26003438e-02  3.26003438e-02 -1.16772647e-02\n",
            "  3.34615052e-02 -7.30956395e-02 -4.40658895e-02  3.11328247e-02\n",
            " -3.40685274e-02 -3.88681653e-02  1.17656553e-02  3.70880511e-02\n",
            "  5.23304059e-02 -6.44228255e-02 -3.46666205e-02  8.32057497e-03]  - intercept :  0.5435059231458044\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.04240355056586351\n",
            "yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Résultats de l'étude\n",
        "print(\"Number of finished trials: \", len(study.trials))\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: \", trial.value)\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))"
      ],
      "metadata": {
        "id": "HZoJ8RGvL2kC",
        "outputId": "71dd37be-c38a-4fd7-8889-4b39f4fbdbb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of finished trials:  238\n",
            "Best trial:\n",
            "  Value:  0.5489420817004809\n",
            "  Params: \n",
            "    count_threshold: 5\n",
            "    postag: False\n",
            "    voc_threshold: 7224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDv3OV2XW0ov",
        "outputId": "7433ac98-c12c-4748-acd1-77941f1c4cc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Evaluate the best model on the test set\n",
        "## TO COMPLETE\n",
        "res = evaluate_model(dataset=dataset, count_threshold=trial.params['count_threshold'], voc_threshold=trial.params['voc_threshold']\n",
        "                     , stopwords=set(), postag=trial.params['postag'], lemmatization=False, stemming=False\n",
        "                     , size=30, sim_or_dist=False, lowercase=False, PMI=False\n",
        "                     , agg=lambda x: np.mean(x, axis=0), regression=\"linear\")\n",
        "print(\"Pearson : \", res[-1])\n",
        "resultats.append(res[-1])"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating vocab and co-occ matrix\n",
            "Assigning vectors\n",
            "Normalizing scores\n",
            "train_x[:size]  [[0.33708258 0.01922778 0.53414367 ... 0.         0.         0.01955799]\n",
            " [0.70033481 0.03938426 0.894298   ... 0.         0.         0.        ]\n",
            " [0.4966451  0.05107072 0.6511207  ... 0.00306105 0.03505706 0.03098073]\n",
            " ...\n",
            " [0.43735191 0.08001309 0.47027286 ... 0.00420894 0.04820346 0.06958022]\n",
            " [0.57608372 0.03409997 0.71347249 ... 0.00187064 0.25445962 0.01893267]\n",
            " [0.62838541 0.06379912 0.75824579 ... 0.         0.         0.        ]]\n",
            "isnan(ar) False\n",
            "Coeff regression lineaire :  [-1.64393752e-03 -9.36442737e-04 -3.14576706e-03 ... -5.79446885e-04\n",
            " -5.26085288e-04 -5.64115422e-05]  - intercept :  1.4483313227306749\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.5677403587421394\n",
            "Pearson :  0.5677403587421394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJVrNfsnSUMz"
      },
      "source": [
        "### Distributed representations: word2vec experiments\n",
        "\n",
        "In this experiment you don't have to create your own word representations, we will use pre-trained word embeddings which can be loaded with the code below.\n",
        "What you will have to do is similar to what you did once you built the distributional representations: preprocess sentences and combine word embeddings to create sentence representations.\n",
        "**Additionally**, you will experiment with **Word Mover's Distance**. This measure uses word vectors and looks for the shortest \"traveling distance\" between the vectors in the two sentences. You will find its implementation (slightly modified from that in the [gensim](https://radimrehurek.com/gensim/auto_examples/tutorials/run_wmd.html) library) in `wmdistance`. you can use it as a/the feature in your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9BZvNXSv1hL",
        "outputId": "1f94159a-3f37-494d-8cbc-d533b0ffa5e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# This may take a while...\n",
        "! pip install pymagnitude"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymagnitude\n",
            "  Downloading pymagnitude-0.1.143.tar.gz (5.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.4 MB 7.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pymagnitude\n",
            "  Building wheel for pymagnitude (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pymagnitude: filename=pymagnitude-0.1.143-cp37-cp37m-linux_x86_64.whl size=360966747 sha256=b8a1658584c47b0354fb9e00a2466497557067aed3ab4d149e32209966700567\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/96/d6/b765a1ce34517c193d764b634b1ff7db5e1dcfea2520f17273\n",
            "Successfully built pymagnitude\n",
            "Installing collected packages: pymagnitude\n",
            "Successfully installed pymagnitude-0.1.143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax7ZpkXgIk4X"
      },
      "source": [
        "### How pymagnitude works\n",
        "from pymagnitude import *\n",
        "\n",
        "# Loading vectors\n",
        "## TO COMPLETE : specify the path to the magnitude file in \"\"\n",
        "vectors = Magnitude(\"/content/drive/MyDrive/Colab Notebooks/GoogleNews-vectors-negative300.magnitude\") \n",
        "\n",
        "# This is how you obtain the vector for a word (for example, \"cat\"):\n",
        "cat_vector = vectors.query(\"cat\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqrA7nRBho_A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b47966b2-a059-44bd-e543-ddfffafb203a"
      },
      "source": [
        "! pip install pyemd\n",
        "! pip install gensim\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "import pyemd\n",
        "\n",
        "## A function that calculates Word Mover's Distance, slightly modified from https://tedboy.github.io/nlps/_modules/gensim/models/word2vec.html#Word2Vec.wmdistance\n",
        "   \n",
        "def wmdistance(words1, words2, vectors):\n",
        "    \"\"\"\n",
        "    words1 and words2 are lists of str containing the words of the first and second sentence in a sentence pair. \n",
        "    These should include only words that you are interested in (e.g., maybe you exclude stopwords or words of some pos)\n",
        "    vectors is a Magnitude object (created above with the downloaded word embeddings)\n",
        "    \"\"\"\n",
        "    # Remove out-of-vocabulary words.    \n",
        "    if len(words1) == 0 or len(words2) == 0:\n",
        "        print('At least one of the documents had no words that were'\n",
        "                    'in the vocabulary. Aborting (returning inf).')\n",
        "        return float('inf')\n",
        "    \n",
        "    dictionary = Dictionary(documents=[words1, words2])\n",
        "    vocab_len = len(dictionary)\n",
        "\n",
        "    # Sets for faster look-up.\n",
        "    docset1 = set(words1)\n",
        "    docset2 = set(words2)\n",
        "\n",
        "    # Compute distance matrix.\n",
        "    distance_matrix = np.zeros((vocab_len, vocab_len)) #, dtype=double)\n",
        "    for i, t1 in dictionary.items():\n",
        "        for j, t2 in dictionary.items():\n",
        "            if not t1 in docset1 or not t2 in docset2:\n",
        "                continue\n",
        "            # Compute Euclidean distance between word vectors.\n",
        "            distance_matrix[i, j] = np.sqrt(np.sum((vectors.query(t1) - vectors.query(t2))**2))\n",
        "\n",
        "    if np.sum(distance_matrix) == 0.0:\n",
        "        # `emd` gets stuck if the distance matrix contains only zeros.\n",
        "        print('The distance matrix is all zeros. Aborting (returning inf).')\n",
        "        return float('inf')\n",
        "\n",
        "    def nbow(words):\n",
        "        d = np.zeros(vocab_len) #, dtype=double)\n",
        "        nbow = dictionary.doc2bow(words)  # Word frequencies.\n",
        "        doc_len = len(words)\n",
        "        for idx, freq in nbow:\n",
        "            d[idx] = freq / float(doc_len)  # Normalized word frequencies.\n",
        "        return d\n",
        "\n",
        "    # Compute nBOW representation of documents.\n",
        "    d1 = nbow(words1)\n",
        "    d2 = nbow(words2)\n",
        "\n",
        "    # Compute WMD.\n",
        "    return emd(d1, d2, distance_matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyemd in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from pyemd) (1.21.5)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY60iXaXhWd0"
      },
      "source": [
        "def assign_distributed_vectors(data, vectors, sim_or_dist=True, postag=False, lowercase=False, wmdistance=False, agg=lambda x: np.mean(x, axis=0), distance_measure=lambda x,y: euclidean_dist(x,y)):\n",
        "  # Write a function like assign_distributional_vectors which uses distributed\n",
        "  # vectors. You can also simply modify that function.  \n",
        "  '''This functions assigns each sentence a vector and optionally calculates the similarity/distance \n",
        "  between the representations of s1 and s2.\n",
        "  Parameters\n",
        "    data: list of tuples (like dataset['train']['data'])\n",
        "    vectors: vectors in Magnitude format    \n",
        "    sim_or_dist: bool. If True, we will use a similarity or distance as the only feature. If False,\n",
        "    we will use the concatenation of the representations of s1 and s2.\n",
        "    postag: whether we want to apply a postag-based filter to obtain sentence representations\n",
        "    lowercase: bool. If True, words are lowercased.\n",
        "    wmdistance: bool. If True, Word Mover's Distance is used.\n",
        "  Returns:\n",
        "    features: an array with the data transformed into features '''\n",
        "    ## TO COMPLETE\n",
        "\n",
        "  if sim_or_dist:\n",
        "    features = np.zeros((len(data), 1))\n",
        "  else:\n",
        "    features = np.zeros((len(data), M.shape[1]*2)) \n",
        "\n",
        "  for i, (s1, s2) in enumerate(data):\n",
        "    # Tokenize, lowercase if lowercase=True, and if postag=True, postag s1 and s2    \n",
        "    ## TO COMPLETE\n",
        "    if (lowercase) :\n",
        "      s1, s2 = s1.lower(), s2.lower()\n",
        "    \n",
        "    # Now create two lists, one for each sentence, with the word representations that you want to use\n",
        "    # You can go through the words (or word, pos) in each sentence and decide whether you keep their representation or not\n",
        "    s1_tok, s2_tok = word_tokenize(s1), word_tokenize(s2)\n",
        "    s1vecs, s2vecs = [], []\n",
        "    if (postag) :\n",
        "      keep_pos = ['NN', 'VBP', 'VBZ', 'VBG', 'VB', 'JJ'] #nouns, verbs and adjectives\n",
        "      s1_pos, s2_pos = pos_tag(s1_tok), pos_tag(s2_tok)\n",
        "      for (w, pos) in s1_pos :\n",
        "        if (pos in keep_pos) :\n",
        "          s1vecs.append(vectors.query(w)) # on ajoute la representation vectorielle du mot si dispo, sinon vect 0\n",
        "      for (w, pos) in s2_pos : \n",
        "        if (pos in keep_pos) :\n",
        "          s2vecs.append(vectors.query(w))\n",
        "    else : \n",
        "      for w in s1_tok :\n",
        "        s1vecs.append(vectors.query(w))\n",
        "      for w in s2_tok :\n",
        "        s2vecs.append(vectors.query(w))\n",
        "\n",
        "    # It is possible that some sentences will not have any word representation available.\n",
        "    # We assign them a 0-vector in this case (be careful, because this could result in a cosine of NaN)\n",
        "    if not s1vecs:\n",
        "      print(\"zero s1vecs\")\n",
        "      s1vecs = [np.zeros(300)] # les vecteurs retournes par Magnitude sont de taille 300\n",
        "    if not s2vecs:\n",
        "      print(\"zero s2vecs\")\n",
        "      s2vecs = [np.zeros(300)]\n",
        "    \n",
        "    # Aggregate the representations of words in a sentence, for example by averaging them\n",
        "    s1vec = agg(s1vecs) #np.sum(s1vecs, axis=0) #s1vecs.mean(axis=0)\n",
        "    s2vec = agg(s2vecs) #np.sum(s2vecs, axis=0) #s2vecs.mean(axis=0)\n",
        "\n",
        "    # Fill in features[i] with the desired feature (one or more similarity/distance measures if sim=True, \n",
        "    # a concatenation of the representations otherwise)\n",
        "    if (sim_or_dist) :\n",
        "      features[i] = distance_measure(s1vec, s2vec)#np.linalg.norm(s1vec - s2vec)\n",
        "      #features[i] = cosine(s1vec, s2vec) #features[i] = np.linalg.norm(s1vec - s2vec)\n",
        "    else : \n",
        "      features[i] = np.concatenate(s1vec,s2vec)   \n",
        "   \n",
        "  return features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " np.linalg.norm(vectors.query('cat') - vectors.query('dog')), np.linalg.norm(vectors.query('cat') - vectors.query('butterfly')), np.linalg.norm(vectors.query('cat') - vectors.query('moon'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aoyj7gE1tMYI",
        "outputId": "dd30e840-ba60-4c3a-af95-d26a3fb8cb45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.69145405, 1.2446976, 1.3381146)"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assign_distributed_vectors(dataset[\"train\"][\"data\"][:10], vectors, sim_or_dist=True, postag=False, lowercase=False, wmdistance=False, agg=lambda x: np.mean(x, axis=0))"
      ],
      "metadata": {
        "id": "HCv7Y1kpgCKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain train_x, dev_x, test_x\n",
        "# Try different combinations (using M, PMI_M, different aggregation functions, with and without postag filtering...)\n",
        "## TO COMPLETE\n",
        "from nltk.stem import WordNetLemmatizer, LancasterStemmer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "train_y = np.array(dataset['train']['scores']) / 5\n",
        "dev_y = np.array(dataset['dev']['scores']) / 5\n",
        "test_y = np.array(dataset['test']['scores']) / 5\n",
        "\n",
        "def evaluate_model_distributed(dataset=dataset, count_threshold=2, voc_threshold=None, stopwords=set(), postag=False, \\\n",
        "                   lemmatization=False, stemming=False, size=-1, sim_or_dist=False, lowercase=False, wmdistance=False, \\\n",
        "                   agg=lambda x: np.mean(x, axis=0), regression=\"linear\"):\n",
        "    \"\"\"regression : \"linear\" or \"rf\" (random forest)\"\"\" \n",
        "\n",
        "    # Train, Dev, Test\n",
        "    print(\"Assigning vectors\")\n",
        "    train_x = assign_distributed_vectors(dataset[\"train\"][\"data\"][:size], vectors, sim_or_dist=sim_or_dist, postag=postag, lowercase=lowercase, wmdistance=wmdistance, agg=agg)\n",
        "    dev_x = assign_distributed_vectors(dataset[\"dev\"][\"data\"][:size], vectors, sim_or_dist=sim_or_dist, postag=postag, lowercase=lowercase, wmdistance=wmdistance, agg=agg)\n",
        "    test_x = assign_distributed_vectors(dataset[\"test\"][\"data\"][:size], vectors, sim_or_dist=sim_or_dist, postag=postag, lowercase=lowercase, wmdistance=wmdistance, agg=agg)\n",
        "    \n",
        "    print(\"Normalizing scores\")\n",
        "    train_y = np.array(dataset['train']['scores'][:size]) / 5\n",
        "    dev_y = np.array(dataset['dev']['scores'][:size]) / 5\n",
        "    test_y = np.array(dataset['test']['scores'][:size]) / 5\n",
        "\n",
        "    # Initializing the model\n",
        "    if (regression == \"linear\"):\n",
        "      linreg = LinearRegression()\n",
        "      linreg.fit(train_x[:size], train_y[:size])\n",
        "      predictions = linreg.predict(dev_x[:size])\n",
        "      print(\"Coeff regression lineaire : \", linreg.coef_, \" - intercept : \", linreg.intercept_)\n",
        "\n",
        "    else : # regression ==\"rf\"\n",
        "      regr = RandomForestRegressor(max_depth=5, random_state=0)\n",
        "      regr.fit(train_x[:size], train_y[:size])\n",
        "      predictions = dev_x[:size]\n",
        "\n",
        "    # Evaluating\n",
        "    print(\"Evaluating regression model\")\n",
        "    pears = evaluate(predictions[:size], dev_y[:size])\n",
        "    print(\"Pearson's r obtained on the dev set:\", pears)\n",
        "    return train_x, dev_x, test_x, pears\n"
      ],
      "metadata": {
        "id": "8WovriNsUszy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BASIC (mean)\n",
        "p = evaluate_model_distributed(size=20, sim_or_dist=True, postag=False, lowercase=False, wmdistance=False, agg=lambda x: np.mean(x, axis=0))\n",
        "print(p[-1])\n",
        "\n",
        "# BASIC (mean + wm)\n",
        "p = evaluate_model_distributed(size=20, sim_or_dist=True, postag=False, lowercase=False, wmdistance=True, agg=lambda x: np.sum(x, axis=0))\n",
        "print(p[-1])\n",
        "\n",
        "# LOWERCASE + mean (pas wm)\n",
        "p = evaluate_model_distributed(size=20, sim_or_dist=True, postag=False, lowercase=True, wmdistance=False, agg=lambda x: np.mean(x, axis=0))\n",
        "print(p[-1])\n",
        "\n",
        "# LOWERCASE + mean + wm\n",
        "p = evaluate_model_distributed(size=20, sim_or_dist=True, postag=False, lowercase=True, wmdistance=True, agg=lambda x: np.mean(x, axis=0))\n",
        "print(p[-1])\n",
        "\n",
        "# LOWERCASE + mean + wm + POS\n",
        "p = evaluate_model_distributed(size=20, sim_or_dist=True, postag=True, lowercase=True, wmdistance=True, agg=lambda x: np.mean(x, axis=0))\n",
        "print(p[-1])"
      ],
      "metadata": {
        "id": "TVUDs_F-iXpn",
        "outputId": "1f18b67b-4d0f-466c-a1eb-1472b95e1644",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning vectors\n",
            "Normalizing scores\n",
            "Coeff regression lineaire :  [-2.54423878]  - intercept :  1.1169786921538933\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.1510558574725457\n",
            "0.1510558574725457\n",
            "Assigning vectors\n",
            "Normalizing scores\n",
            "Coeff regression lineaire :  [0.07203825]  - intercept :  0.6030533704959241\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.3268491427613787\n",
            "0.3268491427613787\n",
            "Assigning vectors\n",
            "Normalizing scores\n",
            "Coeff regression lineaire :  [-1.6355485]  - intercept :  0.9788672131748936\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.15697118896503526\n",
            "-0.15697118896503526\n",
            "Assigning vectors\n",
            "Normalizing scores\n",
            "Coeff regression lineaire :  [-1.6355485]  - intercept :  0.9788672131748936\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: -0.15697118896503526\n",
            "-0.15697118896503526\n",
            "Assigning vectors\n",
            "Normalizing scores\n",
            "Coeff regression lineaire :  [-1.29304022]  - intercept :  0.9157562111685156\n",
            "Evaluating regression model\n",
            "Pearson's r obtained on the dev set: 0.4548165071748151\n",
            "0.4548165071748151\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resultats = []\n",
        "\n",
        "# Basic : no stop words, no pos, no lemmat, no stemm, no dist, no lowercase, no pmi, reg lineaire\n",
        "res = evaluate_model(dataset=dataset, count_threshold=2, voc_threshold=None, stopwords=set(), postag=False, lemmatization=False, stemming=False, size=30, sim_or_dist=False, lowercase=False, PMI=False, agg=lambda x: np.mean(x, axis=0), regression=\"linear\")\n",
        "resultats.append(res)\n",
        "\n",
        "# PMI & lower\n",
        "res = evaluate_model(dataset=dataset, count_threshold=2, voc_threshold=None, stopwords=set(), postag=False, lemmatization=False, stemming=False, size=30, sim_or_dist=False, lowercase=True, PMI=True, agg=lambda x: np.mean(x, axis=0), regression=\"linear\")\n",
        "resultats.append(res)\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# PMI & lower & stemming\n",
        "res = evaluate_model(dataset=dataset, count_threshold=2, voc_threshold=None, stopwords=set(), postag=False, lemmatization=True, stemming=False, size=30, sim_or_dist=False, lowercase=True, PMI=True, agg=lambda x: np.mean(x, axis=0), regression=\"linear\")\n",
        "resultats.append(res)\n",
        "\n",
        "# PMI & lower & postag\n",
        "res = evaluate_model(dataset=dataset, count_threshold=2, voc_threshold=None, stopwords=set(), postag=True, lemmatization=False, stemming=False, size=30, sim_or_dist=False, lowercase=True, PMI=True, agg=lambda x: np.mean(x, axis=0), regression=\"linear\")\n",
        "resultats.append(res)\n",
        "\n",
        "# PMI & lower & postag & sim or dist\n",
        "res = evaluate_model(dataset=dataset, count_threshold=2, voc_threshold=None, stopwords=set(), postag=True, lemmatization=False, stemming=False, size=30, sim_or_dist=True, lowercase=True, PMI=True, agg=lambda x: np.mean(x, axis=0), regression=\"linear\")\n",
        "resultats.append(res)\n",
        "\n",
        "# PMI & lower & postag & sim or dist & stemming\n",
        "res = evaluate_model(dataset=dataset, count_threshold=2, voc_threshold=None, stopwords=set(), postag=True, lemmatization=False, stemming=True, size=30, sim_or_dist=True, lowercase=True, PMI=True, agg=lambda x: np.mean(x, axis=0), regression=\"linear\")\n",
        "resultats.append(res)"
      ],
      "metadata": {
        "id": "hTXlGpdvjk2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_vrbBdZafjJ",
        "outputId": "91142c77-1d9d-4d64-d694-8c85505fc0d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Train and evaluate the models on the dev set\n",
        "## TO COMPLETE\n",
        "# Initializing the model\n",
        "linreg = LinearRegression()\n",
        "# Training\n",
        "linreg.fit(train_x[:size], train_y[:size])\n",
        "# Predicting\n",
        "predictions = linreg.predict(dev_x[:size])\n",
        "\n",
        "# Evaluating\n",
        "print(\"Pearson's r obtained on the dev set:\", evaluate(predictions[:size], dev_y[:size]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pearson's r obtained on the dev set: 0.27980040830580816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvzeJ0RJXEIF"
      },
      "source": [
        "## 3. A Neural model that computes sentence representations\n",
        "\n",
        "In this third part, you will train a neural model which learns representations of sentences and optimizes them so their cosine similarity reflects their semantic similarity.\n",
        "\n",
        "The model is a Siamese bidirectional Long Short-Term Memory (biLSTM) network. In short, it is called *Siamese* because it works with two input vectors (each corresponding to one sentence) which need to be compared.\n",
        "An *LSTM* is a type of recurrent neural network and so it is used when inputs consist of sequences. We choose a *bidirectional* model because it processes an input sentence in two directions, from left to right and from right to left, which helps in better preserving information from both ends of the sentence. \n",
        "\n",
        "You will find an almost-complete implementation in PyTorch. You are expected to complete some pieces of code and to experiment with different parameters, such as the hidden sizes or the number of layers and/or epochs.\n",
        "\n",
        "Overall, the steps are:\n",
        "- 3.1 Creation of a pytorch **Dataset** \n",
        "- 3.2 Defining and instantiating **model** \n",
        "- 3.3 Building the **training loop**\n",
        "- 3.4 **Training** the model with **at least 5 different configurations**, **evaluating** on the dev set\n",
        "- 3.5 **Evaluating** the best model on the test set.\n",
        "\n",
        "But first, let's see a **brief introduction** to (recurrent) neural networks and PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVp8wO-DLPA6"
      },
      "source": [
        "### On neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW7XqJORknJq"
      },
      "source": [
        "An in-depth study of neural networks is not the object of this project, but it can be useful to have a rough idea of how they work. We will present them here briefly; the curious reader can consult the sources cited below, refer to the Deep Learning course of the curriculum or search by themselves, on the Internet for example.\n",
        "\n",
        "The perceptron, also called artificial neuron or formal neuron, tries to reproduce the function of a biological neuron. The objective of a neural network is to reproduce an arbitrarily complex function that associates an input $x$ with an output $y$. For example, we can create neural networks to recognize a dog from a cat: the input is then an image, and the output the word \"dog\" or \"cat\".\n",
        "\n",
        "A layer of a neural network is governed by the following equation:\n",
        "\n",
        "$$ \\hat{y} = f(\\mathbf{W} \\mathbf{X} + b) $$\n",
        "\n",
        "where\n",
        "- $\\mathbf{X}$ is the input matrix, usually consisting of several vectors $x_1, \\dots, x_m$\n",
        "- $\\hat{y}$ is the output vector\n",
        "- $\\mathbf{W}$ is a matrix of weights (parameters specific to the layer, which can be updated)\n",
        "- $b$ is a vector of weights called *bias* (also subject to evolution)\n",
        "- $f$ is a function called *activation function*, generally non-linear like the sigmoid, $\\mathrm{ReLU}$ or $\\tanh$ functions.\n",
        "\n",
        "<img src=\"https://user.oc-static.com/upload/2018/12/10/15444553183515_neuroneformel-1.png\" width=\"450\">\n",
        "\n",
        "The main thing to remember is that the layers of a neural network combine inputs $x_1, \\dots, x_m$ by linear (the $\\mathbf{W} \\mathbf{X} + b$) and non-linear ($f$) operations in the hope of obtaining an output $y$.\n",
        "\n",
        "In practice, at the initialization of the network, given an input $x$, we usually obtain an output $\\hat{y}$ which can be very different from $y$ : the parameters $\\mathbf{W}$ and $b$ are not yet appropriate.\n",
        "\n",
        "We then perform a *training* of the neural network which allows to reduce the distance between $\\hat{y}$ and $y$, i.e. to minimize the *loss function* $\\mathcal{L}(y, \\hat{y}) = \\| \\hat{y} - y \\|^2$. This is usually done by an optimization process such as gradient descent. We will skip the technical details (look at the sources for more information); just remember that by updating the parameters $\\mathbf{W}$ and $b$ incrementally, it is possible to improve the performance of the network as it will produce a $\\hat{y}$ similar to the desired $y$.\n",
        "\n",
        "<img src=\"https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_01_04-GradientDescent-WHITEBG.png\" width=\"500\">\n",
        "\n",
        "By stacking such layers, the network becomes more complex, hence, more elaborate mechanisms can be approximated. However, this usually requires a longer and more data-intensive training. The \"*Deep*\" in *Deep Learning* reflects the fact that we use multi-layered neural networks.\n",
        "\n",
        "As an illustration, here is a neural network that learns to separate crosses and circles: the more the training progresses (a full pass through the dataset is commonly called *epoch*), the better it works.\n",
        "\n",
        "<img src=\"https://user.oc-static.com/upload/2018/12/12/15446484526497_linearsep_anim.gif\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGY1hba_Lq5V"
      },
      "source": [
        "**Sources**\n",
        "- [OpenClassrooms - Initiez-vous au Deep Learning](https://openclassrooms.com/fr/courses/5801891-initiez-vous-au-deep-learning)\n",
        "- [IBM - What are neural networks?](https://www.ibm.com/cloud/learn/neural-networks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYPnMYkKbR41"
      },
      "source": [
        "### Recurrent Neural Networks\n",
        "Recurrent neural networks (RNN) are neural networks that are particularly adapted to process sequential data such as time series or text. Indeed, in a text, there are sequential dependencies between words; we cannot write a sentence in any order, in French or English at least.\n",
        "\n",
        "The difference between RNNs and classical neural networks (usually called *feed-forward neural networks*) is that the former have a \"memory\" represented by a hidden state that evolves along a \"time\" axis in any given layer.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/875/1*AQ52bwW55GsJt6HTxPDuMA.gif\" width=\"600\">\n",
        "<img src=\"https://miro.medium.com/max/875/1*o-Cq5U8-tfa1_ve2Pf3nfg.gif\" width=\"600\">\n",
        "<img src=\"https://miro.medium.com/max/875/1*WMnFSJHzOloFlJHU6fVN-g.gif\" width=\"600\">\n",
        "\n",
        "There are many ways to implement such a memory; here we will use a type of RNN called LSTM (Long-Short Term Memory), which was the best performing model until the introduction of the *Transformer* in 2017 (the *Transformer* will not be on the agenda of this project; the curious reader may read the corresponding paper in the sources).\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/875/1*0f8r3Vd-i4ueYND1CUrhMA.png\" width=\"600\">\n",
        "\n",
        "The LSTM exhibits several interesting mechanisms; in particular, it has the ability to choose what it wishes to retain or forget in the long term. Refer to the sources for more detail.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6C9rnAUlAa4"
      },
      "source": [
        "**Sources**\n",
        "\n",
        "- [colah's blog - Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "- [Towards data science - Illustrated Guide to LSTM’s and GRU’s: A step by step explanation](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n",
        "- [Distill - Attention and Augmented Recurrent Neural Networks](https://distill.pub/2016/augmented-rnns/)\n",
        "- [Transformer - Attention Is All You Need](https://arxiv.org/abs/1706.03762)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N30J_iCf3fEC"
      },
      "source": [
        "### Pytorch\n",
        "PyTorch is a Python-based library for scientific computing that provides three main features:\n",
        "- An n-dimensional Tensor, which is similar to numpy but can run on GPUs\n",
        "- Easily build big computational graphs for deep learning\n",
        "- Automatic differentiation for computing gradients \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FITe7Hhp3fEK"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2eOF7O2pmiN"
      },
      "source": [
        "#### Tensors\n",
        "\n",
        "**NB**: Tensor are the basics block of pytorch. Tensor allows to store data (input data or target data) as well as the parameters (also called weights, neurons,...) of your neural network.\n",
        "\n",
        "(image by [Matthew Mayo](https://www.kdnuggets.com/author/matt-mayo))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RS8V2QrQLVw"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDkgzVe13fEd"
      },
      "source": [
        "**What is a pytorch tensor?** : A torch.Tensor is a multi-dimensional matrix containing elements of a single data type.\n",
        "\n",
        "Tensors are similar to NumPy’s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing.\n",
        "\n",
        "**How to define a pytorch tensor?**\n",
        "- using existing constructors : _torch.ones_ , _torch.zeros_ _torch.rand_\n",
        "- based on existing object\n",
        "    - from another tensor (or only using the shape of the other tensor)\n",
        "    - from a python list \n",
        "    - from a numpy array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzPgaeY13fEn"
      },
      "source": [
        "# define \n",
        "ones = torch.ones(3,2)\n",
        "# a tensor can be printed\n",
        "print(ones)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eW3NOwG3fFE"
      },
      "source": [
        "# other basic definition \n",
        "print(torch.zeros(5,3), \"\\n\", \n",
        "      torch.rand(2,3), \"\\n\", \n",
        "      torch.empty(2,2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS_i19lt3fFQ"
      },
      "source": [
        "# from a python list \n",
        "ls = [[[1,3,5,6],[-1,4,4,4]],[[-1,-3,-5,-6],[10,-4,-4,-4]]]\n",
        "tensor = torch.Tensor(ls)\n",
        "print(tensor)\n",
        "# from a numpy array : \n",
        "array = np.array([0,1])\n",
        "#array\n",
        "tensor = torch.from_numpy(array)\n",
        "print(tensor)\n",
        "# symetrically  tensor.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwb5o07oqSp-"
      },
      "source": [
        "If the list is not a proper matrix shape, an exception is returned!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRaSA_R-3fFp"
      },
      "source": [
        "# list must be in a proper matrix shape\n",
        "ls = [[[1,3,5,6],[-1,4,4,4]],[[-1,-3,-5,-6],[10,-4,-4]]]\n",
        "torch.Tensor(ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-KGQ2hK3fGe"
      },
      "source": [
        "**Basic manipulations**\n",
        "- access type / change data types \n",
        "- access elements \n",
        "- reshape \n",
        "- maths opertions : add, multiply , ..\n",
        "- differentiate / derive\n",
        "- set to a specific _device_ : GPU , GPU:0, GPU:1 , CPU ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu2aGF3j3fGt"
      },
      "source": [
        "# get type \n",
        "print(tensor,tensor.dtype)\n",
        "# change type \n",
        "tensor = tensor.float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz2gxNen3fHS"
      },
      "source": [
        "**NB**: types are important in Deep Learning  because: \n",
        "- some types are more memory consumming than others : e.g : float16 vs float32\n",
        "- some operations require a specific type (cf. Embedding layer ...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FThm_Rqv3fHg"
      },
      "source": [
        "tensor = torch.rand(5,2,2)\n",
        "print(tensor)\n",
        "# access one element\n",
        "print(tensor[0,1,1])\n",
        "# access several element\n",
        "print(tensor[:3,0,:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5SGNZwf3fHv"
      },
      "source": [
        "**NB**: pytorch tensor indexing exactly match numpy indexing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEuGn5Js3fHx"
      },
      "source": [
        "# get the shape of a tensor\n",
        "tensor.size()\n",
        "# reshape it \n",
        "print(tensor, \"\\n\",\n",
        "      tensor.view(2,2,5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD2AQmQH3fIN"
      },
      "source": [
        "intTensor = torch.ones(3,2, dtype=torch.float32)\n",
        "print(intTensor, intTensor.dtype)\n",
        "intTensor.int()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mte8663zp5kh"
      },
      "source": [
        "#### Operations on Tensors\n",
        "\n",
        "❗**Beware of Broadcasting** ❗ issues if your tensors does not have the same size. As in [Numpy broadcasting](https://numpy.org/devdocs/user/basics.broadcasting.html#general-broadcasting-rules), some operations above can broadcast, i.e. extend, their application to the size of the targeted tensor.\n",
        "\n",
        "One really good explanation about PyTorch specific broadcasting can be found here: https://stackoverflow.com/questions/51371070/how-does-pytorch-broadcasting-work\n",
        "\n",
        "Let's take the same example and reproduce it here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqfZT_FqrrQR"
      },
      "source": [
        "<img src=\"data:image/png;base64,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\" width=\"350\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLvARLY8sLHt"
      },
      "source": [
        "In PyTorch we reproduce it as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaG40nunrxm_"
      },
      "source": [
        "tensor_a = torch.Tensor([[1,2], [3,4], [5,6]])\n",
        "tensor_b = torch.Tensor([1,2])\n",
        "tensor_a + tensor_b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0d2Wd-u3fIi"
      },
      "source": [
        "#### Automatic Differentiation\n",
        "\n",
        "The core component of any modern deep learning library is _Automatic Differentiation_. \n",
        "\n",
        "**Recall**\n",
        "- Training any deep learning model requires backpropagation \n",
        "- Backpropagation is an algorithm that efficiently computes the gradient of a neural network's output based on its input and with regard to all its parameters (or also named weights)\n",
        "\n",
        "_Automatic Differentiation_ provides a way of automatically computing gradients of any function. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yYxdNxpcQPZ"
      },
      "source": [
        "### (3.1) Creating a Pytorch **Dataset**\n",
        "\n",
        "The best way to prepare our dataset to be used in a pytorch model is by using the `torch.utils.data.Dataset` class. We will create a class inheriting from Dataset which should have the `__len__` and `__getitem__` methods. You can find a complete description and example [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files).\n",
        "\n",
        "We will also use the [**DataLoader**](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#preparing-your-data-for-training-with-dataloaders), which takes care of batches, shuffling the data..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtQWcM67kOZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54e64967-293d-463f-81d3-f4dd71a770f8"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import CosineSimilarity, MSELoss\n",
        "import torch.optim as optim\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f749a898810>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRnzTl3hIwi2"
      },
      "source": [
        "class STSDataset(Dataset):  \n",
        "    def __init__(self, data, max_length=30, vocab=None, min_word_freq=2,lowercase=True):\n",
        "        '''Params:\n",
        "        data: dict with 'data' and 'scores' (as in dataset['train'] from load_data())\n",
        "        vocab: (word2idx, idx2word). A tuple with two dicts: word -> index and index -> word. \n",
        "        If it is None, the vocabulary is created within the function.\n",
        "        For the dev and test sets, you should use the training vocabulary.        \n",
        "        max_length: maximum sequence length (in words) allowed. Longer sentences will be truncated. Shorter ones\n",
        "        will be padded.\n",
        "        min_freq: int. Used only if vocab=None, to create the vocabulary.\n",
        "        lowercase: bool. If True, words are lowercased.'''\n",
        "        \n",
        "        # Get the data\n",
        "        self.data = data['data']\n",
        "        # Set the maximum length we will keep for the sequences\n",
        "        self.max_length = max_length\n",
        "        # Allow to import a vocabulary (for valid/test datasets, that will use the training vocabulary)\n",
        "        if vocab is not None:\n",
        "            self.word2idx, self.idx2word = vocab\n",
        "        else:\n",
        "            # If no vocabulary imported, build it (and reverse)\n",
        "            self.word2idx, self.idx2word = self.build_vocab(self.data, min_word_freq, lowercase=lowercase)        \n",
        "                \n",
        "        s1s = []\n",
        "        s2s = []\n",
        "        for s1, s2 in self.data:        \n",
        "            s1_idcs = []\n",
        "            s2_idcs = []\n",
        "            # Tokenize each sentence and turn it into a list of vocabulary indices (remember to take care of UNK). Lowercase if necessary            \n",
        "            ### TO COMPLETE\n",
        "            ## Considérer les stopwords, la ponctuation..\n",
        "            for w1 in word_tokenize(s1.lower()):\n",
        "                if w1 in self.word2idx:\n",
        "                    s1_idcs.append(self.word2idx[w1])\n",
        "                else:\n",
        "                    s1_idcs.append(self.word2idx['UNK'])\n",
        "            for w2 in word_tokenize(s2.lower()):\n",
        "                if w2 in self.word2idx:\n",
        "                    s2_idcs.append(self.word2idx[w2])\n",
        "                else:\n",
        "                    s2_idcs.append(self.word2idx['UNK'])\n",
        "            \n",
        "            # Truncate sequences that are longer than <max_length>. Append them to s1s and s2s.\n",
        "            ### TO COMPLETE\n",
        "            for s_idcs in (s1_idcs, s2_idcs):\n",
        "                if len(s_idcs)>max_length:\n",
        "                  s_idcs = s_idcs[:max_length]             \n",
        "            s1s.append(s1_idcs)\n",
        "            s2s.append(s2_idcs)            \n",
        "            \n",
        "        # Apply padding: for sequences shorter than <max_length>, we fill them with 0 values \n",
        "        # This way they all have the same length (max_length) and we can create a pytorch tensor with them            \n",
        "        self.tensor_s1 = torch.LongTensor(pad_sequences(s1s, maxlen = max_length)) # padding 'pre' by default\n",
        "        self.tensor_s2 = torch.LongTensor(pad_sequences(s2s, maxlen = max_length))\n",
        "\n",
        "        # Make a tensor with the targets - we map them to the [-1, 1] interval for convenience (same range as the cosine)\n",
        "        self.tensor_y = (((torch.FloatTensor(data['scores']) + 1) / 6) * (1+1)) -1 \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # The iterator just gets one particular example with its category\n",
        "        # The dataloader will take care of the shuffling and batching\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        return self.tensor_s1[idx], self.tensor_s2[idx], self.tensor_y[idx]\n",
        "  \n",
        "    def build_vocab(self, corpus, count_threshold, lowercase):        \n",
        "        word_count = {}\n",
        "        for s1, s2 in corpus:\n",
        "            if lowercase:\n",
        "              s1 = s1.lower()\n",
        "              s2 = s2.lower()            \n",
        "            s1_tokens = word_tokenize(s1)\n",
        "            s2_tokens = word_tokenize(s2)\n",
        "            for token in s1_tokens + s2_tokens: \n",
        "                if token not in word_count:\n",
        "                    word_count[token] = 1\n",
        "                else:\n",
        "                    word_count[token] += 1\n",
        "        filtered_word_counts = {word: count for word, count in word_count.items() if count >= count_threshold}\n",
        "        words = sorted(filtered_word_counts.keys(), key=word_count.get, reverse=True) + ['UNK']\n",
        "        # But we need to shift the indexes by 1 to put the padding symbol to 0\n",
        "        word_index = {words[i]: i + 1 for i in range(len(words))}\n",
        "        idx_word = {i + 1: words[i] for i in range(len(words))}\n",
        "        return word_index, idx_word\n",
        "\n",
        "    def get_vocab(self):\n",
        "        # A simple way to get the training vocab when building the valid/test\n",
        "        return self.word2idx, self.idx2word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFfbJIg6cDc5"
      },
      "source": [
        "training_dataset = STSDataset(dataset['train'], max_length=30, vocab=None, min_word_freq=2,lowercase=True)\n",
        "# create dev_dataset and test_dataset as above using the training vocabulary\n",
        "test_dataset = STSDataset(dataset['test'], max_length=30, vocab=training_dataset.get_vocab(), min_word_freq=2,lowercase=True)\n",
        "dev_dataset = STSDataset(dataset['dev'], max_length=30, vocab=training_dataset.get_vocab(), min_word_freq=2,lowercase=True)\n",
        "\n",
        "# Prepare the DataLoaders:\n",
        "training_dataloader = DataLoader(training_dataset, batch_size = 40, shuffle=True)\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size = 25)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = 25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "sentence_length = [(len(word_tokenize(x)) + len(word_tokenize(y)))/2 for x, y in dataset['train']['data']]\n",
        "plt.hist(sentence_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "VXVqGXkU293U",
        "outputId": "34ad8e54-c7f3-48b9-f5ac-3c0431744f31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2.787e+03, 1.692e+03, 5.350e+02, 3.860e+02, 2.620e+02, 7.400e+01,\n",
              "        7.000e+00, 4.000e+00, 1.000e+00, 1.000e+00]),\n",
              " array([ 3.5 ,  8.55, 13.6 , 18.65, 23.7 , 28.75, 33.8 , 38.85, 43.9 ,\n",
              "        48.95, 54.  ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD7CAYAAABnoJM0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAASH0lEQVR4nO3dX2xT9f/H8dc6v50iw7o5tBtEItGlYRcITbjCxILZNGPjxrA0YCKgRiMxIChG3MyA6P5I1DgzE41XC7sxYQ6QQUKMfxIJRElsMEKMoEkrf7ohA9z40nO+F8T+fv5hXbut7fp+Pq7Y+bQ7n0/OyLPnbD0tcl3XFQDALE+uJwAAyC1CAADGEQIAMI4QAIBxhAAAjCMEAGAcIQAA427J9QQyNTR0RY5TWG+BKC+fqXj8cq6nkTXW1ivZWzPrzR8eT5HuvPP2fx2btiFwHLfgQiCpINc0FmvrleytmfXmPy4NAYBxhAAAjCMEAGAcIQAA4wgBABhHCADAOEIAAMZN2/cRZKp01m26tST7yx4Zva7hS39kfb8AkIq5ENxacotWvNiX9f32v9Wo4azvFQBS49IQABhHCADAOEIAAMYRAgAwjhAAgHGEAACMIwQAYBwhAADjCAEAGEcIAMA4QgAAxhECADCOEACAcYQAAIwjBABgHCEAAONSfjDN0NCQXnrpJf3yyy/yer2699571draqrKyMlVXV+uBBx6Qx3OjJ+3t7aqurpYkHT58WO3t7UokElqwYIHeeOMN3XbbbSnHAADZlfKMoKioSOvXr9fAwID6+/s1d+5cdXZ2Jsd7e3vV19envr6+ZASuXLmi1157Td3d3Tp06JBuv/12ffTRRynHAADZlzIEPp9PS5YsSX69cOFCRaPRMZ/zxRdfqKamRvPmzZMkNTU16bPPPks5BgDIvrQ+s9hxHO3evVuhUCi5bc2aNUokEnrooYe0YcMGeb1exWIxVVZWJh9TWVmpWCwmSWOOAQCyL60QbN++XTNmzNDq1aslSZ9//rn8fr8uX76sLVu2qKurSxs3bpySif5defnMrOxnMlVUlE7KYwqJtfVK9tbMevPfuEPQ1tamM2fOqLu7O/nLYb/fL0maOXOmHn/8cX388cfJ7UeOHEk+NxqNJh871lg64vHLchw37efl8iCdPz885nhFRWnKxxQSa+uV7K2Z9eYPj6fopi+gx/Xno7t27VIkElFXV5e8Xq8k6ffff9fIyIgk6fr16xoYGFAgEJAkLV26VN9//71Onz4t6cYvlB999NGUYwCA7Et5RnDq1Cl98MEHmjdvnpqamiRJc+bM0fr169Xc3KyioiJdv35dDz74oF544QVJN84QWltb9cwzz8hxHAUCAb366qspxwAA2ZcyBPfff79+/PHHfx3r7++/6fOWL1+u5cuXpz0GAMgu3lkMAMYRAgAwjhAAgHGEAACMIwQAYBwhAADjCAEAGEcIAMA4QgAAxhECADCOEACAcYQAAIwjBABgHCEAAOMIAQAYRwgAwDhCAADGEQIAMI4QAIBxhAAAjCMEAGAcIQAA4wgBABhHCADAOEIAAMYRAgAwjhAAgHGEAACMSxmCoaEhPfXUU6qtrdWKFSv0/PPPa3BwUJJ0/PhxNTQ0qLa2VmvXrlU8Hk8+L9MxAEB2pQxBUVGR1q9fr4GBAfX392vu3Lnq7OyU4zjasmWLmpubNTAwoGAwqM7OTknKeAwAkH0pQ+Dz+bRkyZLk1wsXLlQ0GlUkElFJSYmCwaAkqampSQcOHJCkjMcAANmX1u8IHMfR7t27FQqFFIvFVFlZmRwrKyuT4zi6ePFixmMAgOy7JZ0Hb9++XTNmzNDq1at16NChqZrTuJSXz8zp/jNRUVE6KY8pJNbWK9lbM+vNf+MOQVtbm86cOaPu7m55PB75/X5Fo9Hk+ODgoDwej3w+X8Zj6YjHL8tx3LSeI+X2IJ0/PzzmeEVFacrHFBJr65XsrZn15g+Pp+imL6DHdWlo165dikQi6urqktfrlSTV1NRoZGREx44dkyT19vaqrq5uQmMAgOxLeUZw6tQpffDBB5o3b56ampokSXPmzFFXV5fa29vV0tKi0dFRVVVVqaOjQ5Lk8XgyGgMAZF+R67rpX1/JAxO5NLTixb4pmNHY+t9q5NLQ31hbr2Rvzaw3f0z40hAAoHARAgAwjhAAgHGEAACMIwQAYBwhAADjCAEAGEcIAMA4QgAAxhECADCOEACAcYQAAIwjBABgHCEAAOMIAQAYRwgAwDhCAADGEQIAMC7lZxZjclz7b0IVFaUpHzeex6RrZPS6hi/9MenfF0BhIARZ4v1PcU4+K1m68XnJ+fkpqgDyAZeGAMA4QgAAxhECADCOEACAcYQAAIwjBABgHCEAAOMIAQAYN64QtLW1KRQKqbq6WidPnkxuD4VCqqurU2NjoxobG/Xll18mx44fP66GhgbV1tZq7dq1isfj4xoDAGTXuEKwbNky9fT0qKqq6h9j7777rvr6+tTX16elS5dKkhzH0ZYtW9Tc3KyBgQEFg0F1dnamHAMAZN+4QhAMBuX3+8f9TSORiEpKShQMBiVJTU1NOnDgQMoxAED2TfheQ5s3b5brulq8eLE2bdqkWbNmKRaLqbKyMvmYsrIyOY6jixcvjjnm8/kmOh0AQJomFIKenh75/X5du3ZNO3fuVGtra9Yu85SXz8zKfgrFVNzVdDLk67ymkrU1s978N6EQ/Hm5yOv1KhwO69lnn01uj0ajyccNDg7K4/HI5/ONOZaOePyyHMdNe87T8SBNhvPn8+/+oxUVpXk5r6lkbc2sN394PEU3fQGd8Z+PXr16VcPDNxbsuq7279+vQCAgSaqpqdHIyIiOHTsmSert7VVdXV3KMQBA9o3rjGDHjh06ePCgLly4oCeffFI+n0/d3d3asGGDEomEHMfR/Pnz1dLSIknyeDxqb29XS0uLRkdHVVVVpY6OjpRjAIDsG1cItm3bpm3btv1j+549e276nEWLFqm/vz/tMQBAdvHOYgAwjhAAgHGEAACMIwQAYBwhAADjCAEAGEcIAMA4QgAAxhECADCOEACAcYQAAIwjBABgHCEAAOMIAQAYRwgAwDhCAADGEQIAMI4QAIBxhAAAjCMEAGAcIQAA4wgBABhHCADAOEIAAMYRAgAwjhAAgHGEAACMIwQAYFzKELS1tSkUCqm6ulonT55Mbv/555+1atUq1dbWatWqVTp9+vSExwAA2ZcyBMuWLVNPT4+qqqr+sr2lpUXhcFgDAwMKh8Nqbm6e8BgAIPtShiAYDMrv9/9lWzwe14kTJ1RfXy9Jqq+v14kTJzQ4OJjxGAAgN27J5EmxWEx33323iouLJUnFxcWaPXu2YrGYXNfNaKysrGySlgQASEdGIcgH5eUzcz2FaaWiojTXU/hX+TqvqWRtzaw3/2UUAr/fr7NnzyqRSKi4uFiJRELnzp2T3++X67oZjaUrHr8sx3HTft50PEiT4fz54VxP4R8qKkrzcl5TydqaWW/+8HiKbvoCOqM/Hy0vL1cgENDevXslSXv37lUgEFBZWVnGYwCA3Eh5RrBjxw4dPHhQFy5c0JNPPimfz6d9+/bp9ddf19atW/X+++9r1qxZamtrSz4n0zEAQPYVua6b/vWVPDCRS0MrXuybghmNrf+txpzs98995+Ppaj6fRk8Va2tmvflj0i8NAQAKByEAAOMIAQAYRwgAwDhCAADGEQIAMI4QAIBxhAAAjCMEAGAcIQAA4wgBABhHCADAOEIAAMYRAgAwjhAAgHGEAACMIwQAYBwhAADjCAEAGEcIAMA4QgAAxhECADCOEACAcYQAAIwjBABgHCEAAOMIAQAYRwgAwLhbJvoNQqGQvF6vSkpKJEmbN2/W0qVLdfz4cTU3N2t0dFRVVVXq6OhQeXm5JI05BgDIrkk5I3j33XfV19envr4+LV26VI7jaMuWLWpubtbAwICCwaA6OzslacwxAED2TfiM4N9EIhGVlJQoGAxKkpqamrRs2TK98cYbY45halz7b0IVFaVZ3+/I6HUNX/oj6/sFkJ5JCcHmzZvluq4WL16sTZs2KRaLqbKyMjleVlYmx3F08eLFMcd8Pt9kTAd/4/1PsVa82Jf1/fa/1ajhrO8VQLomHIKenh75/X5du3ZNO3fuVGtrqx555JHJmNuYystnTvk+MHGpzkRycaaSa9bWzHrz34RD4Pf7JUler1fhcFjPPvusnnjiCUWj0eRjBgcH5fF45PP55Pf7bzqWjnj8shzHTXu+0/EgTWfnz9/8nKCionTM8UJkbc2sN394PEU3fQE9oV8WX716VcPDNxbtuq7279+vQCCgmpoajYyM6NixY5Kk3t5e1dXVSdKYYwCA7JvQGUE8HteGDRuUSCTkOI7mz5+vlpYWeTwetbe3q6Wl5S9/IippzDEAQPZNKARz587Vnj17/nVs0aJF6u/vT3sMAJBdvLMYAIwjBABgHCEAAOMIAQAYNyW3mACk8d3aYqre18HtLYDxIwSYMrm6tYXE7S2AdHBpCACMIwQAYBwhAADjCAEAGEcIAMA4QgAAxhECADCO9xGgIPE5zcD4EQIUJD6nGRg/Lg0BgHGEAACMIwQAYBwhAADjCAEAGEcIAMA4QgAAxhECADCOEACAcYQAAIwjBABgHCEAAOO46RwwicZz19Opuisqdz5FpggBMIlydddTiTufInM5uzT0888/a9WqVaqtrdWqVat0+vTpXE0FAEzLWQhaWloUDoc1MDCgcDis5ubmXE0FAEzLyaWheDyuEydO6OOPP5Yk1dfXa/v27RocHFRZWdm4vofHU5Tx/mffeVvGz52IXO03l/tmzdk1kf8XUyUf5zSV8nW9Y82ryHVdN4tzkSRFIhG9/PLL2rdvX3LbY489po6ODi1YsCDb0wEA0/jzUQAwLich8Pv9Onv2rBKJhCQpkUjo3Llz8vv9uZgOAJiWkxCUl5crEAho7969kqS9e/cqEAiM+/cDAIDJk5PfEUjSTz/9pK1bt+rSpUuaNWuW2tradN999+ViKgBgWs5CAADID/yyGACMIwQAYBwhAADjCAEAGEcIcqCtrU2hUEjV1dU6efJkcnuh3ohvaGhITz31lGpra7VixQo9//zzGhwclCQdP35cDQ0Nqq2t1dq1axWPx3M828nx3HPPqaGhQStXrlQ4HNYPP/wgqXCP8Z/ee++9v/xcF+rxlaRQKKS6ujo1NjaqsbFRX375paRpumYXWXf06FE3Go26Dz/8sPvjjz8mt69Zs8bds2eP67quu2fPHnfNmjW5muKkGhoacr/55pvk12+++ab7yiuvuIlEwl2+fLl79OhR13Vdt6ury926dWuupjmpLl26lPz3oUOH3JUrV7quW7jH2HVdNxKJuOvWrUv+XBfy8XVd9x//f13XnbZr5owgB4LB4D/eRf3njfjq6+sl3bgR34kTJ5KvnKczn8+nJUuWJL9euHChotGoIpGISkpKFAwGJUlNTU06cOBArqY5qUpL/+/DZy5fvqyioqKCPsbXrl1Ta2urXn/99eS2Qj6+NzNd18wH0+SJWCymu+++W8XFxZKk4uJizZ49W7FYrKDece04jnbv3q1QKKRYLKbKysrkWFlZmRzH0cWLF+Xz+XI4y8nx6quv6uuvv5bruvrwww8L+hi/8847amho0Jw5c5LbCv34StLmzZvluq4WL16sTZs2Tds1c0aArNq+fbtmzJih1atX53oqU27nzp36/PPPtXHjRrW3t+d6OlPmu+++UyQSUTgczvVUsqqnp0effvqpPvnkE7muq9bW1lxPKWOEIE9YuBFfW1ubzpw5o7ffflsej0d+v1/RaDQ5Pjg4KI/Hk9evnDKxcuVKHTlyRPfcc09BHuOjR4/qp59+0rJlyxQKhfTbb79p3bp1OnPmTEEf3z+Pm9frVTgc1rfffjttf6YJQZ4o9Bvx7dq1S5FIRF1dXfJ6vZKkmpoajYyM6NixY5Kk3t5e1dXV5XKak+LKlSuKxWLJrw8fPqw77rijYI/x008/ra+++kqHDx/W4cOHdc899+ijjz7S+vXrC/L4StLVq1c1PHzjE6Jd19X+/fsVCASm7c809xrKgR07dujgwYO6cOGC7rzzTvl8Pu3bt69gb8R36tQp1dfXa968ebr11lslSXPmzFFXV5e+/fZbtbS0aHR0VFVVVero6NBdd92V4xlPzIULF/Tcc8/pjz/+kMfj0R133KGXX35ZCxYsKNhj/P+FQiF1d3frgQceKMjjK0m//vqrNmzYoEQiIcdxNH/+fG3btk2zZ8+elmsmBABgHJeGAMA4QgAAxhECADCOEACAcYQAAIwjBABgHCEAAOMIAQAY9z/oN/uN9xIJ6gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train']['data'][0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "FzV5tYuIz8aM",
        "outputId": "49c41b0a-d251-4c1f-b269-0bae92f22fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'A plane is taking off.'"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s1 = dataset['train']['data'][0][0]\n",
        "l = []\n",
        "for w1 in word_tokenize(s1):\n",
        "  if w1 in training_dataset.word2idx:\n",
        "      l.append(training_dataset.word2idx[w1])\n",
        "  else:\n",
        "      l.append(training_dataset.word2idx['UNK'])\n",
        "l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFvhHZ6GpCzR",
        "outputId": "7040318a-0563-4266-8802-5aa69b4f6691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7833, 246, 6, 445, 144, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(dataset['train']['data'][0][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLVuVEPe1TnT",
        "outputId": "9d45f64e-aa54-450b-8375-d45b9110b823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['An', 'air', 'plane', 'is', 'taking', 'off', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset.idx2word[24]\n",
        "# training_dataset.word2idx['a']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "0QerRt-CvPqc",
        "outputId": "0fde3091-a984-497a-96eb-838bc5c4d4da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'an'"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset.tensor_s1[0], training_dataset.tensor_s2[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvkei2DCyKjS",
        "outputId": "7e8d2b98-863f-42fe-8234-5f58e6a8a6c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   2, 246,   6, 445,\n",
              "         144,   1]),\n",
              " tensor([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 185, 246,   6, 445,\n",
              "         144,   1]))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqAY64CZkdwD"
      },
      "source": [
        "### (3.2) Implementing the model\n",
        "\n",
        "Below is the implementation of our model, inspired from [this post](https://towardsdatascience.com/a-friendly-introduction-to-siamese-networks-85ab17522942). \n",
        "It consists of an embedding layer and a biLSTM. After two sentences have been passed through the model, we calculate their cosine similarity, which is the output value. \n",
        "When you instantiate the model, you can experiment with changing the dimension of embeddings (`embedding_dim`), the biLSTM layers dimension (`hidden_dim`), the number of biLSTM layers `num_lstm_layers`.\n",
        "\n",
        "You don't have to, but if you wanted to try more advanced modifications, you could replace the biLSTM by a unidirectional LSTM (setting `bidirectional` to `False` and modifying `CosineSimilarity` and `forward_once`).\n",
        "By default all weights are randomly initialized, but you could also try to use (and fine-tune) pre-trained word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpV8P1GoaXfQ"
      },
      "source": [
        "# Implementing the model\n",
        "\n",
        "class SiameseBILSTM(nn.Module):\n",
        "    def __init__(self, embedding_dim, vocabulary_size, hidden_dim, num_lstm_layers=1):\n",
        "        super(SiameseBILSTM, self).__init__()\n",
        "        # create an embedding layer (a lookup table for word representations), randomly initialized\n",
        "        self.embeddings = nn.Embedding(vocabulary_size + 1, embedding_dim, padding_idx=0)\n",
        "        # create the LSTM layer(s). bidirectional=True means it is a biLSTM. \n",
        "        # Each LSTM (in each direction) will have <hidden_dim> dimension\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_lstm_layers, batch_first=True, bidirectional=True)        \n",
        "        # Instantiate the cosine similarity function\n",
        "        self.cos = CosineSimilarity(dim=1)  \n",
        "\n",
        "    def forward_once(self, x):\n",
        "        # Forward pass of one input\n",
        "        # Pass it through the embedding layer\n",
        "        output = self.embeddings(x)\n",
        "        # Then through the biLSTM\n",
        "        h, (h_n, h_c) = self.rnn(output) \n",
        "        # h is of shape (Batch size, sequence length, 2 * hidden_dim) (2 because bidirectional)        \n",
        "        # Pick the representations of the last tokens:\n",
        "        out_both = h[:, -1, :]        \n",
        "        return out_both\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        # Make a forward pass of each input\n",
        "        output1 = self.forward_once(input1)\n",
        "        output2 = self.forward_once(input2)\n",
        "        # Calculate the cosine similarity between the two sentence representations\n",
        "        sim = self.cos(output1, output2)\n",
        "        return sim \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfAU4M1udWlj"
      },
      "source": [
        "training_word2idx = training_dataset.get_vocab()[0] # pour corriger un appel à un vocabulaire non défini dans l'appel à SiameseBILSTM() plus bas\n",
        "\n",
        "\n",
        "# Instantiate the model with the desired parameters\n",
        "embedding_dim = 100 ## TO COMPLETE\n",
        "hidden_dim = 150 ## TO COMPLETE\n",
        "num_lstm_layers = 1 ## TO COMPLETE\n",
        "\n",
        "# You can run the model on GPU although it doesn't take too long on CPU.\n",
        "# To use a GPU, go to \"Runtime\" -> \"Change runtime type\" -> select \"GPU\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SiameseBILSTM(embedding_dim, len(training_word2idx), hidden_dim=hidden_dim, num_lstm_layers=num_lstm_layers)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Create an optimizer\n",
        "opt = optim.Adam(model.parameters(), lr=0.0025, betas=(0.9, 0.999))\n",
        "# The criterion is MSE since we have a regression problem\n",
        "criterion = MSELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTpsmD7Wwq4B"
      },
      "source": [
        "### (3.3) Building the training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuMv2-O8dZFd"
      },
      "source": [
        "# Training function which trains the model for one epoch\n",
        "\n",
        "def train_epoch(model, opt, criterion, dataloader):    \n",
        "    # set the model to training mode\n",
        "    model.train()\n",
        "    losses = []             \n",
        "    preds = []\n",
        "    all_ys = []\n",
        "    for i, (x1, x2, y) in enumerate(dataloader):\n",
        "        # empty gradients at each step\n",
        "        opt.zero_grad()      \n",
        "        model.zero_grad()       \n",
        "        # (1) Forward        \n",
        "        out = model(x1.to(device), x2.to(device))        \n",
        "        # (2) Compute the loss\n",
        "        loss = criterion(out, y.to(device))        \n",
        "        # (3) Compute gradients\n",
        "        loss.backward()\n",
        "        # (4) update weights\n",
        "        opt.step()          \n",
        "        # store losses and also predictions and gold ys (to calculate pearson at the end of the epoch)      \n",
        "        preds.extend(out.cpu().detach().numpy())\n",
        "        all_ys.extend(y.cpu().detach().numpy())\n",
        "        losses.append(loss)\n",
        "        # The loss at this batch - commenté pour clarifier le gridsearch\n",
        "        # if (i%20 == 0):\n",
        "            # print(\"Batch \" + str(i) + \" : training loss = \" + str(loss.item()) + \"; training r = \" + str(evaluate(out.cpu().detach().numpy(), y.cpu().detach().numpy())))            \n",
        "    # Average losses to obtain the epoch loss. Calculate pearson's r on the whole dataset.\n",
        "    print(\"Total epoch loss = \" + str(sum(losses)/(i+1)) + \"; Total training r = \" + str(evaluate(preds, all_ys)))            \n",
        "    return losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxMq9zWYddE6"
      },
      "source": [
        "# Evaluation function\n",
        "\n",
        "def eval_model(model, criterion, evalloader):  \n",
        "    # set the model to evaluation mode\n",
        "    model.eval()  \n",
        "    all_ys = []    \n",
        "    total_epoch_loss = 0    \n",
        "    preds = []\n",
        "    # disable gradient calculation for evaluation\n",
        "    with torch.no_grad():\n",
        "        for i, (x1, x2, y) in enumerate(evalloader):\n",
        "            # Do a forward pass and compute the loss (see function above)             \n",
        "            ## TO COMPLETE\n",
        "            # (1) Forward        \n",
        "            out = model(x1.to(device), x2.to(device))\n",
        "            loss = criterion(out, y.to(device))\n",
        "            \n",
        "            total_epoch_loss += loss.item()            \n",
        "            preds.extend(out.cpu().detach().numpy())\n",
        "            all_ys.extend(y.cpu().detach().numpy())\n",
        "    \n",
        "    return total_epoch_loss/(i+1), evaluate(preds, all_ys) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhJ_kOOcdgCh"
      },
      "source": [
        "# A function which will help you execute experiments rapidly - with a early_stopping option when necessary. \n",
        "def experiment(model, opt, criterion, num_epochs = 5, early_stopping = True, evaluate_on_test=False):\n",
        "  '''\n",
        "  model: the SiameseBILSTM model\n",
        "  opt: the optimizer\n",
        "  criterion: the loss function\n",
        "  if early stopping is set to True, training will stop if the validation loss starts going up. This prevents overfitting\n",
        "  evaluate_on_test: bool. If True, the function performs a final evaluation on the test set'''\n",
        "  train_losses = []\n",
        "  dev_losses = []\n",
        "  best_dev_loss = 100. \n",
        "  if early_stopping: \n",
        "      best_dev_loss = 100. \n",
        "  print(\"Beginning training...\")\n",
        "  # Run an epoch (one full pass of the whole dataset)\n",
        "  for e in range(num_epochs):\n",
        "      print(\"Epoch \" + str(e+1) + \":\")\n",
        "      train_losses += train_epoch(model, opt, criterion, training_dataloader)\n",
        "      dev_loss, dev_r = eval_model(model, criterion, dev_dataloader)        \n",
        "      dev_losses.append(dev_loss)\n",
        "      print(\"Epoch \" + str(e+1) + \" : Validation loss = \" + str(dev_loss) + \"; Validation r = \" + str(dev_r))\n",
        "      if early_stopping:\n",
        "          if dev_loss < best_dev_loss:\n",
        "              best_dev_loss = dev_loss\n",
        "          else:\n",
        "              print(\"Early stopping.\") # if validation loss started going up... stop the training to prevent overfitting\n",
        "              break  \n",
        "  if evaluate_on_test:\n",
        "    test_loss, test_r = eval_model(model, criterion, test_dataloader)\n",
        "    print(\"Epoch \" + str(e+1) + \" : Test loss = \" + str(test_loss) + \"; Test r = \" + str(test_r))    \n",
        "  return train_losses, best_dev_loss, dev_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzZCSb6ew3U8"
      },
      "source": [
        "### (3.4-3.5) Train and evaluate\n",
        "\n",
        "3.4 and 3.5: train different configurations of the model using the experiment() function, evaluate them on dev and evaluate the best one on the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import et récupération des résultats précédents**"
      ],
      "metadata": {
        "id": "arRf77XWcFCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from itertools import product\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "def saveResultsPickle(results):\n",
        "    drive.mount('/content/drive/')\n",
        "    if not os.path.exists('/content/drive/MyDrive/CollabNlpProject/results.pkl'):\n",
        "        os.mkdir('/content/drive/MyDrive/CollabNlpProject')\n",
        "    with open('/content/drive/MyDrive/CollabNlpProject/results.pkl', 'wb') as results_file:\n",
        "        pickle.dump(results, results_file)\n",
        "    drive.flush_and_unmount()\n",
        "\n",
        "\n",
        "def loadResultsPickle():\n",
        "    drive.mount('/content/drive/')\n",
        "    results = pickle.load( open('/content/drive/MyDrive/CollabNlpProject/results.pkl', 'rb' ) )\n",
        "    drive.flush_and_unmount()\n",
        "    return results"
      ],
      "metadata": {
        "id": "FeXtbfAUfb-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fonction de création des modèles et de recherche**"
      ],
      "metadata": {
        "id": "CnDVrbxai9hX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model with the desired parameters\n",
        "def instantiateModel(embedding_dim, hidden_dim, num_lstm_layers):\n",
        "  \n",
        "    model = SiameseBILSTM(embedding_dim, len(training_word2idx), hidden_dim=hidden_dim, num_lstm_layers=num_lstm_layers)\n",
        "    model.to(device)\n",
        "\n",
        "    # Create an optimizer\n",
        "    opt = optim.Adam(model.parameters(), lr=0.0025, betas=(0.9, 0.999))\n",
        "\n",
        "    # The criterion is MSE since we have a regression problem\n",
        "    criterion = MSELoss()\n",
        "    return model, opt, criterion"
      ],
      "metadata": {
        "id": "gQ1VsNq7ZpKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alimentation de results avec les résultats obtenus à partir des hyperparamètres. Si results contient déjà un test des hyperparamètres, celui-ci n'est pas réalisé.\n",
        "def search_HP(results, configurationsHP):\n",
        "    for configuration in configurationsHP:\n",
        "        if configuration not in results:\n",
        "            embedding_dim, hidden_dim, num_lstm_layers = configuration\n",
        "            print(\"Training BiLSTM model with following layers hyperparameters \\n\", \"embedding_dim : {} hidden_dim : {} num_lstm_layers : {} \".format(embedding_dim, hidden_dim, num_lstm_layers))\n",
        "            model, opt, criterion = instantiateModel(embedding_dim, hidden_dim, num_lstm_layers)\n",
        "            _, _, dev_losses = experiment(model, opt, criterion, num_epochs = 5, early_stopping = False, evaluate_on_test=False)\n",
        "            results[configuration] = dev_losses\n",
        "    return results"
      ],
      "metadata": {
        "id": "BabKOJj3cTrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpétation des résultats**"
      ],
      "metadata": {
        "id": "VERvapIW0P6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def best_dev_loss_col(row):\n",
        "    return (row[row == row['best_dev_loss']].index)[0]\n",
        "\n",
        "def loss_decrease_perc(row, max_epoch):\n",
        "    l = list(row.iloc[:max_epoch])\n",
        "    l_res = []\n",
        "    for i in range(len(l) - 1):\n",
        "        l_res.append((l[i] - l[i + 1])/ l[i])\n",
        "    return l_res\n",
        "\n",
        "\n",
        "def score_models(df_results):\n",
        "\n",
        "    \"\"\"\n",
        "    Attribue un score à chaque modèle testé\n",
        "    Args :\n",
        "      - df_results : la DataFrame des résultats\n",
        "    Returns :\n",
        "      - df_results : la DataFrame des résultats avec les scores de chaque modèle\n",
        "    \"\"\"\n",
        "    max_epoch = df_results['max_epoch'].max()\n",
        "    ### Performance observée de la région\n",
        "    df_results['score_perf'] = df_results['best_dev_loss']\n",
        "\n",
        "    ### Performance potentielle de la région\n",
        "    last_col_decrease_perc = 'loss_decrease_perc_' + str(max_epoch - 1)\n",
        "    df_results['score_pot'] = - df_results[last_col_decrease_perc] * df_results['best_dev_loss']\n",
        "\n",
        "    ### Performance de la région : observée + potentielle\n",
        "    df_results[\"score\"] = 1 / (df_results['score_perf'] + df_results['score_pot'])\n",
        "\n",
        "    return df_results\n",
        "\n",
        "def softmax(x, t = 1):\n",
        "    \"\"\"\n",
        "    Fonction Softmax avec température\n",
        "    \"\"\"\n",
        "    x_max = x.max()\n",
        "    num = np.exp((x - x_max) / t)\n",
        "    den = num.sum()\n",
        "    return num / den\n",
        "\n",
        "def resampling_models(df_results, temperature = 1):\n",
        "    \"\"\"\n",
        "    Convertit en probabilités d'être sélectionné le score de df_results\n",
        "    Args :\n",
        "      - df_results : la DataFrame des résultats, contenant une colonne score\n",
        "    Returns :\n",
        "      - df_results : la DataFrame des résultats contenant une colonne proba_selection (forte proba -> beaucoup de modèles similaires)\n",
        "    \"\"\"\n",
        "    ### Softmax avec température pour calibrer le resampling\n",
        "    # df_results['proba_selection'] = softmax(score), temperature)\n",
        "\n",
        "    ### Rank selection , n'utilise pas la température\n",
        "    df_results['proba_selection'] = 1 / df_results[\"model_rank\"] / (1 / df_results[\"model_rank\"]).sum()\n",
        "\n",
        "    return df_results\n",
        "\n",
        "\n",
        "def create_df_res(results, temperature = 1):\n",
        "\n",
        "    \"\"\"\n",
        "    Récupère results et transforme en une dataframe avec ajout d'informations permettant d'interpréter les résultats et choisir les prochaines configurations à tester\n",
        "    Args :\n",
        "      - results : format dict\n",
        "    Returns :\n",
        "      - DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    #### Initialisation de la df ####\n",
        "    df_results = pd.DataFrame.from_dict(results, orient='index')\n",
        "    max_epoch = len(df_results.columns)\n",
        "\n",
        "    #### Ajout d'informations ####\n",
        "    df_results[\"max_epoch\"] = max_epoch - 1\n",
        "    df_results[\"best_dev_loss\"] = df_results.apply(min, axis = 1)\n",
        "    df_results[\"best_dev_loss_epoch\"] = df_results.apply(lambda row: best_dev_loss_col(row), axis=1)\n",
        "    df_results['lastEpochImproved'] = (df_results[\"best_dev_loss_epoch\"] == df_results['max_epoch'])\n",
        "    df_results[['embedding_dim', 'hidden_dim', 'num_lstm_layers']] = pd.DataFrame(df_results.index.tolist(), index=df_results.index)\n",
        "    df_results = df_results.reset_index(drop = True)\n",
        "    \n",
        "    df_results['loss_decrease_perc'] = df_results.apply(lambda row : loss_decrease_perc(row, max_epoch), axis = 1)\n",
        "    df_results = pd.concat([df_results, pd.DataFrame(df_results.loss_decrease_perc.values.tolist()).add_prefix('loss_decrease_perc_')], axis = 1)\n",
        "    df_results = df_results.drop(labels = 'loss_decrease_perc', axis = 1)\n",
        "\n",
        "    ### Ajout de la partie interprétation des résultats\n",
        "    df_results = score_models(df_results)\n",
        "    df_results[\"model_rank\"] = df_results['score'].rank(method='dense', ascending = False)\n",
        "    df_results = resampling_models(df_results, temperature)\n",
        "\n",
        "\n",
        "    return df_results"
      ],
      "metadata": {
        "id": "iVTf9swmjfka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_next_config(df_results):\n",
        "    \"\"\"\n",
        "    Interprète les résultats pour proposer 1 nouvelle configuration d'hyperparamètres à tester.\n",
        "    Introduction d'une part aléatoire en choisissant un modèle inspiré de la génération précédente + bruit gaussien\n",
        "    Args :\n",
        "      - df_results : la DataFrame des résultats, avec une probabilité pour chaque modèle d'être sélectionné (proba_selection)\n",
        "    Returns :\n",
        "      - configuration : une configuration à tester pour les hyperparamètres : embedding_dim, hidden_dim, num_lstm_layers\n",
        "    \"\"\"\n",
        "    # Retrouver les paramètres d'un modèle en fonction de proba sélection\n",
        "    rand = random.random()\n",
        "    idx = (df_results['proba_selection'].sort_values(ascending=False).cumsum()[(df_results['proba_selection'].sort_values(ascending=False).cumsum())>rand]).index[0]\n",
        "    embedding_dim,\thidden_dim,\tnum_lstm_layers = df_results.loc[idx, ['embedding_dim',\t'hidden_dim',\t'num_lstm_layers']]\n",
        "    # Appliquer un bruit aux paramètres\n",
        "    bruit_dim = 25\n",
        "    bruit_layers = 0.5\n",
        "\n",
        "    embedding_dim_gauss = round(random.gauss(embedding_dim, bruit_dim))\n",
        "    hidden_dim_gauss = round(random.gauss(hidden_dim, bruit_dim))\n",
        "    num_lstm_layers_gauss = round(random.gauss(num_lstm_layers, bruit_layers))\n",
        "\n",
        "    # Corriger les bords (nombre de dimensions > 100)\n",
        "    if embedding_dim_gauss < 100:\n",
        "        embedding_dim_gauss = 100\n",
        "    if hidden_dim_gauss < 100:\n",
        "        hidden_dim_gauss = 100\n",
        "    if num_lstm_layers_gauss < 1:\n",
        "        num_lstm_layers_gauss = 1\n",
        "    \n",
        "    configuration = (embedding_dim_gauss,\thidden_dim_gauss,\tnum_lstm_layers_gauss)\n",
        "\n",
        "    return configuration, rand"
      ],
      "metadata": {
        "id": "B4XqXvZlZRGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_next_configs(df_results, n = 1):\n",
        "    \"\"\"\n",
        "    Interprète les résultats pour proposer de nouvelles configurations d'hyperparamètres à tester.\n",
        "    Introduction d'une part aléatoire en choisissant un modèle inspiré de la génération précédente + bruit gaussien\n",
        "    Args :\n",
        "      - df_results : la DataFrame des résultats, avec une probabilité pour chaque modèle d'être sélectionné\n",
        "      - n : le nombre de nouvelles configurations à tester\n",
        "    Returns :\n",
        "      - configurationsHP : liste de n configurations à tester pour les hyperparamètres : embedding_dim, hidden_dim, num_lstm_layers\n",
        "    \"\"\"\n",
        "    configurationsHP = []\n",
        "    for i in range(n):\n",
        "        config, random_number = choose_next_config(df_results)\n",
        "        configurationsHP.append(config)\n",
        "    return configurationsHP"
      ],
      "metadata": {
        "id": "NvWqgNjlgQMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_model(df_results):\n",
        "    best_dev_loss_BM, embedding_dim_BM,\thidden_dim_BM,\tnum_lstm_layers_BM = (df_results[df_results[\"score\"] == df_results[\"score\"].max()])[['best_dev_loss', 'embedding_dim',\t'hidden_dim',\t'num_lstm_layers']].iloc[0]\n",
        "    return best_dev_loss_BM, embedding_dim_BM,\thidden_dim_BM,\tnum_lstm_layers_BM"
      ],
      "metadata": {
        "id": "M4b7axeigzuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iteration\n",
        "def add_generation(results, n = 1, temperature = 1):\n",
        "    \"\"\"\n",
        "    Ajoute n nouveaux modèles au test\n",
        "    Args :\n",
        "      - results : le dictionnaire des résultats\n",
        "      - n : le nombre de nouvelles configurations à tester\n",
        "    Returns :\n",
        "      - results : le dictionnaire des résultats, avec n configurations en plus\n",
        "    \"\"\"\n",
        "    df_results = create_df_res(results, temperature)\n",
        "    configurationsHP = find_next_configs(df_results, n)\n",
        "    results = search_HP(results, configurationsHP)\n",
        "    saveResultsPickle(results)\n",
        "    return results"
      ],
      "metadata": {
        "id": "oNmSLsFk0ze9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MAIN : Recherche des Hyperparamètres**"
      ],
      "metadata": {
        "id": "dI9X5H2CjD6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Récupérer les résultats des précédentes sessions\n",
        "results = loadResultsPickle()\n",
        "\n",
        "# Créer le dictionnaire results si results n'existe pas\n",
        "try:\n",
        "  results\n",
        "except:\n",
        "  results = {}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0zOlTM6bPiW",
        "outputId": "afdaec27-e679-4c22-e1e6-c3d3473086b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation de la recherche par un gridsearch\n",
        "\n",
        "# Gridsearch\n",
        "l_embedding_dim = [300, 350] ## TO CHOOSE\n",
        "l_hidden_dim = [200, 300] ## TO CHOOSE\n",
        "l_num_lstm_layers = [2, 3] ## TO CHOOSE\n",
        "\n",
        "configurationsHP = list(product(l_embedding_dim, l_hidden_dim, l_num_lstm_layers))\n",
        "\n",
        "results = search_HP(results, configurationsHP)\n",
        "\n",
        "saveResultsPickle(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-MHDjP1eR-_",
        "outputId": "c889cecf-a579-4fa3-d0ee-09864b45ab4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 300 hidden_dim : 200 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2681, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.39456732441572145\n",
            "Epoch 1 : Validation loss = 0.2854425506045421; Validation r = 0.4993750778078896\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1208, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7314992669425124\n",
            "Epoch 2 : Validation loss = 0.2201611239463091; Validation r = 0.5322153803302242\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8969139742004016\n",
            "Epoch 3 : Validation loss = 0.22601835193733374; Validation r = 0.5279490031392022\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0284, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9417431974045809\n",
            "Epoch 4 : Validation loss = 0.2129658553749323; Validation r = 0.5465870716200584\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0198, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.95871374145213\n",
            "Epoch 5 : Validation loss = 0.2278967640052239; Validation r = 0.5305613752254742\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 300 hidden_dim : 200 num_lstm_layers : 3 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.3102, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.2828159405398378\n",
            "Epoch 1 : Validation loss = 0.29596188068389895; Validation r = 0.37079890142884825\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1656, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.593114563737553\n",
            "Epoch 2 : Validation loss = 0.254919312770168; Validation r = 0.4050797420135921\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.1027, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7700524539448312\n",
            "Epoch 3 : Validation loss = 0.24872070476412772; Validation r = 0.4345867331998825\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0611, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8747504731098101\n",
            "Epoch 4 : Validation loss = 0.23461008953551452; Validation r = 0.45391787283982526\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0357, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9296146404948422\n",
            "Epoch 5 : Validation loss = 0.24350314252078534; Validation r = 0.43169278528428007\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 300 hidden_dim : 300 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2762, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.35507610458854183\n",
            "Epoch 1 : Validation loss = 0.26492861409982044; Validation r = 0.4159342971579066\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1399, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6761944630360276\n",
            "Epoch 2 : Validation loss = 0.22417408414185047; Validation r = 0.4871366063985066\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0612, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.873064912337876\n",
            "Epoch 3 : Validation loss = 0.22817292883992196; Validation r = 0.49619555541164734\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0294, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9401327394859641\n",
            "Epoch 4 : Validation loss = 0.21496886077026525; Validation r = 0.5071384500902342\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0189, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.960600774333945\n",
            "Epoch 5 : Validation loss = 0.2181947318216165; Validation r = 0.5195918732092646\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 300 hidden_dim : 300 num_lstm_layers : 3 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.3055, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.26100039332139413\n",
            "Epoch 1 : Validation loss = 0.2702350233991941; Validation r = 0.3631972773116532\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1876, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.5103135871956233\n",
            "Epoch 2 : Validation loss = 0.2286623261248072; Validation r = 0.44487512902627546\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.1289, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6918825343223034\n",
            "Epoch 3 : Validation loss = 0.24931265314420065; Validation r = 0.44237211689844014\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0799, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8290377881708114\n",
            "Epoch 4 : Validation loss = 0.20866262689232826; Validation r = 0.4954285498805176\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9067596389755329\n",
            "Epoch 5 : Validation loss = 0.21133285909891128; Validation r = 0.5019307265932461\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 350 hidden_dim : 200 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2653, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3962913226225533\n",
            "Epoch 1 : Validation loss = 0.25763387779394786; Validation r = 0.44442783328894175\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1168, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7435248720224357\n",
            "Epoch 2 : Validation loss = 0.22384292582670848; Validation r = 0.524379711932207\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8966553196263266\n",
            "Epoch 3 : Validation loss = 0.24518288957575957; Validation r = 0.4933278813687416\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0262, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9459678298612912\n",
            "Epoch 4 : Validation loss = 0.21960178247342507; Validation r = 0.5293210049767325\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0209, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9559848670677601\n",
            "Epoch 5 : Validation loss = 0.22368845902383327; Validation r = 0.5239148839726865\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 350 hidden_dim : 200 num_lstm_layers : 3 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.3051, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.30044926009440714\n",
            "Epoch 1 : Validation loss = 0.28959607928991316; Validation r = 0.34760569719779616\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1759, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.5613288891248918\n",
            "Epoch 2 : Validation loss = 0.24028382872541745; Validation r = 0.4412754635378923\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.1032, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7674259999244782\n",
            "Epoch 3 : Validation loss = 0.2352814355244239; Validation r = 0.48243837001538237\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0574, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8810672226101879\n",
            "Epoch 4 : Validation loss = 0.22989612501114606; Validation r = 0.48918053630046765\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0345, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9297200925482365\n",
            "Epoch 5 : Validation loss = 0.24781136823197206; Validation r = 0.46444880728538995\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 350 hidden_dim : 300 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2750, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.36117572471144466\n",
            "Epoch 1 : Validation loss = 0.2548770925650994; Validation r = 0.45740212113583917\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1322, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7002105995561149\n",
            "Epoch 2 : Validation loss = 0.2597449116408825; Validation r = 0.46072993017343783\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0576, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8828669936627664\n",
            "Epoch 3 : Validation loss = 0.23072000574320556; Validation r = 0.5020311689143143\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0295, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9393341391356829\n",
            "Epoch 4 : Validation loss = 0.24017165414988995; Validation r = 0.4957151425426391\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0212, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9557266278519361\n",
            "Epoch 5 : Validation loss = 0.24136403643836576; Validation r = 0.4936883968505145\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 350 hidden_dim : 300 num_lstm_layers : 3 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.3218, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.2329416566138596\n",
            "Epoch 1 : Validation loss = 0.3060849942266941; Validation r = 0.3338045223672318\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1945, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.4935384377272915\n",
            "Epoch 2 : Validation loss = 0.2692043217519919; Validation r = 0.3846668871843205\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.1340, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6768273029690421\n",
            "Epoch 3 : Validation loss = 0.23490023277699948; Validation r = 0.4360343814908696\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0852, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8139288349150029\n",
            "Epoch 4 : Validation loss = 0.24020061617096264; Validation r = 0.4295727797288692\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8945163775234817\n",
            "Epoch 5 : Validation loss = 0.240892443805933; Validation r = 0.4424818133802063\n",
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recherche par générations\n",
        "ngen = 10\n",
        "n_child = 5\n",
        "\n",
        "for i in range (ngen):\n",
        "  print('\\n\\n\\n\\n')\n",
        "  print(\"Génération : \" , i+1)\n",
        "  print('\\n\\n\\n\\n')\n",
        "  add_generation(results, n = n_child, temperature = 1)\n",
        "  df_results = create_df_res(results, temperature = 1)\n",
        "  display(df_results)\n",
        "  print(get_best_model(df_results))"
      ],
      "metadata": {
        "id": "3mCSfm9eo2KM",
        "outputId": "cb75bc84-d96d-4373-da8d-233607a483b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Génération :  1\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 219 hidden_dim : 272 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2666, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3832700197193243\n",
            "Epoch 1 : Validation loss = 0.25840575446685154; Validation r = 0.44323237235833607\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1407, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6701799381044322\n",
            "Epoch 2 : Validation loss = 0.22660944772263367; Validation r = 0.49125004536291095\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0694, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.855527916776942\n",
            "Epoch 3 : Validation loss = 0.23161034000416597; Validation r = 0.5040367705124568\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0345, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9314515470231542\n",
            "Epoch 4 : Validation loss = 0.23464466153333585; Validation r = 0.506377958219445\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0221, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.955189070869708\n",
            "Epoch 5 : Validation loss = 0.22169588382045427; Validation r = 0.5020668191205839\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 264 hidden_dim : 309 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2647, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.374787431331596\n",
            "Epoch 1 : Validation loss = 0.2737849639107784; Validation r = 0.408201797491049\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1418, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6677114032257837\n",
            "Epoch 2 : Validation loss = 0.23442301588753858; Validation r = 0.4872209818433038\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0683, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8598748091277866\n",
            "Epoch 3 : Validation loss = 0.2140541604409615; Validation r = 0.5165117481367179\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0332, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9338261673229045\n",
            "Epoch 4 : Validation loss = 0.22670264629026254; Validation r = 0.4959492754887112\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0214, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9564452049977937\n",
            "Epoch 5 : Validation loss = 0.21848488561809062; Validation r = 0.5063415166532064\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 201 hidden_dim : 100 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2650, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.41225485067281936\n",
            "Epoch 1 : Validation loss = 0.2811741527169943; Validation r = 0.45276016620867554\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1187, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7467657500367867\n",
            "Epoch 2 : Validation loss = 0.2584731108198563; Validation r = 0.46972320803784984\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0579, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8810784308435298\n",
            "Epoch 3 : Validation loss = 0.24239047169685363; Validation r = 0.4958627381638623\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0321, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9335717582390763\n",
            "Epoch 4 : Validation loss = 0.2319969286521276; Validation r = 0.5085626604465053\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0215, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9554193712981739\n",
            "Epoch 5 : Validation loss = 0.23358673714101313; Validation r = 0.5010736693300855\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 352 hidden_dim : 240 num_lstm_layers : 3 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.3046, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.274564917144906\n",
            "Epoch 1 : Validation loss = 0.2766107104718685; Validation r = 0.35707769532667394\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1786, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.5465017419564501\n",
            "Epoch 2 : Validation loss = 0.2575009301304817; Validation r = 0.39085448416025526\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.1129, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7381150431360217\n",
            "Epoch 3 : Validation loss = 0.2358300251265367; Validation r = 0.44882389519027205\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0659, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8614670617744621\n",
            "Epoch 4 : Validation loss = 0.23586247575779756; Validation r = 0.4512986312907274\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0388, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9218467599481113\n",
            "Epoch 5 : Validation loss = 0.2445858812580506; Validation r = 0.4502543318877948\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 100 hidden_dim : 113 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2739, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.4070170770640456\n",
            "Epoch 1 : Validation loss = 0.29063800734778245; Validation r = 0.41480637816932125\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1463, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6809838685479292\n",
            "Epoch 2 : Validation loss = 0.2698975139607986; Validation r = 0.48601452979826576\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0788, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8365669911997757\n",
            "Epoch 3 : Validation loss = 0.22208364630738894; Validation r = 0.5358665790900106\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9136133462659638\n",
            "Epoch 4 : Validation loss = 0.20996024558941523; Validation r = 0.5380223532994054\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0264, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9473790009443488\n",
            "Epoch 5 : Validation loss = 0.21372744763890902; Validation r = 0.533858482836256\n",
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-96f16f5c-a3f7-4010-9d01-052afa108aaf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>max_epoch</th>\n",
              "      <th>best_dev_loss</th>\n",
              "      <th>best_dev_loss_epoch</th>\n",
              "      <th>lastEpochImproved</th>\n",
              "      <th>embedding_dim</th>\n",
              "      <th>hidden_dim</th>\n",
              "      <th>num_lstm_layers</th>\n",
              "      <th>loss_decrease_perc_0</th>\n",
              "      <th>loss_decrease_perc_1</th>\n",
              "      <th>loss_decrease_perc_2</th>\n",
              "      <th>loss_decrease_perc_3</th>\n",
              "      <th>score_perf</th>\n",
              "      <th>score_pot</th>\n",
              "      <th>score</th>\n",
              "      <th>model_rank</th>\n",
              "      <th>proba_selection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.297629</td>\n",
              "      <td>0.256342</td>\n",
              "      <td>0.224900</td>\n",
              "      <td>0.221605</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.138718</td>\n",
              "      <td>0.122657</td>\n",
              "      <td>0.014653</td>\n",
              "      <td>0.044878</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>-0.009499</td>\n",
              "      <td>4.946564</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.145368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.276555</td>\n",
              "      <td>0.242013</td>\n",
              "      <td>0.211087</td>\n",
              "      <td>0.214948</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.124904</td>\n",
              "      <td>0.127787</td>\n",
              "      <td>-0.018291</td>\n",
              "      <td>0.031870</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>-0.006632</td>\n",
              "      <td>4.963635</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.290735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.296420</td>\n",
              "      <td>0.235832</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.233284</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>175</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.204399</td>\n",
              "      <td>0.034291</td>\n",
              "      <td>-0.024319</td>\n",
              "      <td>0.045231</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>-0.010074</td>\n",
              "      <td>4.702388</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.048456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.270336</td>\n",
              "      <td>0.227319</td>\n",
              "      <td>0.231308</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.223296</td>\n",
              "      <td>4</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>175</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.159127</td>\n",
              "      <td>-0.017549</td>\n",
              "      <td>0.089803</td>\n",
              "      <td>-0.060609</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.012760</td>\n",
              "      <td>4.478359</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.032304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.285443</td>\n",
              "      <td>0.220161</td>\n",
              "      <td>0.226018</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.227897</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.228703</td>\n",
              "      <td>-0.026604</td>\n",
              "      <td>0.057750</td>\n",
              "      <td>-0.070109</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.014931</td>\n",
              "      <td>4.387952</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.026430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.295962</td>\n",
              "      <td>0.254919</td>\n",
              "      <td>0.248721</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>0.243503</td>\n",
              "      <td>4</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>0.138675</td>\n",
              "      <td>0.024316</td>\n",
              "      <td>0.056733</td>\n",
              "      <td>-0.037906</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>0.008893</td>\n",
              "      <td>4.106723</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.019382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.264929</td>\n",
              "      <td>0.224174</td>\n",
              "      <td>0.228173</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>0.218195</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>300</td>\n",
              "      <td>2</td>\n",
              "      <td>0.153832</td>\n",
              "      <td>-0.017838</td>\n",
              "      <td>0.057869</td>\n",
              "      <td>-0.015006</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>0.003226</td>\n",
              "      <td>4.583062</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.036342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.270235</td>\n",
              "      <td>0.228662</td>\n",
              "      <td>0.249313</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>0.211333</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>300</td>\n",
              "      <td>3</td>\n",
              "      <td>0.153839</td>\n",
              "      <td>-0.090309</td>\n",
              "      <td>0.163048</td>\n",
              "      <td>-0.012797</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>0.002670</td>\n",
              "      <td>4.731872</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.058147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.257634</td>\n",
              "      <td>0.223843</td>\n",
              "      <td>0.245183</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>0.223688</td>\n",
              "      <td>4</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.131159</td>\n",
              "      <td>-0.095335</td>\n",
              "      <td>0.104335</td>\n",
              "      <td>-0.018609</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>0.004087</td>\n",
              "      <td>4.470503</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.029074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.289596</td>\n",
              "      <td>0.240284</td>\n",
              "      <td>0.235281</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>0.247811</td>\n",
              "      <td>4</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>0.170279</td>\n",
              "      <td>0.020819</td>\n",
              "      <td>0.022889</td>\n",
              "      <td>-0.077928</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>0.017915</td>\n",
              "      <td>4.035327</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.017102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.254877</td>\n",
              "      <td>0.259745</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>0.240172</td>\n",
              "      <td>0.241364</td>\n",
              "      <td>4</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>300</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.019099</td>\n",
              "      <td>0.111744</td>\n",
              "      <td>-0.040966</td>\n",
              "      <td>-0.004965</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>0.001145</td>\n",
              "      <td>4.312846</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.024228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.306085</td>\n",
              "      <td>0.269204</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>0.240201</td>\n",
              "      <td>0.240892</td>\n",
              "      <td>4</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>300</td>\n",
              "      <td>3</td>\n",
              "      <td>0.120492</td>\n",
              "      <td>0.127428</td>\n",
              "      <td>-0.022564</td>\n",
              "      <td>-0.002880</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>0.000677</td>\n",
              "      <td>4.244900</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.020767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.258406</td>\n",
              "      <td>0.226609</td>\n",
              "      <td>0.231610</td>\n",
              "      <td>0.234645</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>4</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>219</td>\n",
              "      <td>272</td>\n",
              "      <td>2</td>\n",
              "      <td>0.123048</td>\n",
              "      <td>-0.022068</td>\n",
              "      <td>-0.013101</td>\n",
              "      <td>0.055185</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>-0.012234</td>\n",
              "      <td>4.774143</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.072684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.273785</td>\n",
              "      <td>0.234423</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>0.226703</td>\n",
              "      <td>0.218485</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>264</td>\n",
              "      <td>309</td>\n",
              "      <td>2</td>\n",
              "      <td>0.143770</td>\n",
              "      <td>0.086889</td>\n",
              "      <td>-0.059090</td>\n",
              "      <td>0.036249</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>-0.007759</td>\n",
              "      <td>4.847430</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.096912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.281174</td>\n",
              "      <td>0.258473</td>\n",
              "      <td>0.242390</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>0.233587</td>\n",
              "      <td>4</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>201</td>\n",
              "      <td>100</td>\n",
              "      <td>2</td>\n",
              "      <td>0.080737</td>\n",
              "      <td>0.062222</td>\n",
              "      <td>0.042879</td>\n",
              "      <td>-0.006853</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>0.001590</td>\n",
              "      <td>4.281065</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.022364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.276611</td>\n",
              "      <td>0.257501</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>0.235862</td>\n",
              "      <td>0.244586</td>\n",
              "      <td>4</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>352</td>\n",
              "      <td>240</td>\n",
              "      <td>3</td>\n",
              "      <td>0.069085</td>\n",
              "      <td>0.084159</td>\n",
              "      <td>-0.000138</td>\n",
              "      <td>-0.036985</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>0.008722</td>\n",
              "      <td>4.089106</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.018171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.290638</td>\n",
              "      <td>0.269898</td>\n",
              "      <td>0.222084</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>0.213727</td>\n",
              "      <td>4</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>113</td>\n",
              "      <td>2</td>\n",
              "      <td>0.071362</td>\n",
              "      <td>0.177156</td>\n",
              "      <td>0.054589</td>\n",
              "      <td>-0.017942</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>0.003767</td>\n",
              "      <td>4.678856</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.041534</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-96f16f5c-a3f7-4010-9d01-052afa108aaf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-96f16f5c-a3f7-4010-9d01-052afa108aaf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-96f16f5c-a3f7-4010-9d01-052afa108aaf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           0         1         2  ...     score  model_rank  proba_selection\n",
              "0   0.297629  0.256342  0.224900  ...  4.946564         2.0         0.145368\n",
              "1   0.276555  0.242013  0.211087  ...  4.963635         1.0         0.290735\n",
              "2   0.296420  0.235832  0.227745  ...  4.702388         6.0         0.048456\n",
              "3   0.270336  0.227319  0.231308  ...  4.478359         9.0         0.032304\n",
              "4   0.285443  0.220161  0.226018  ...  4.387952        11.0         0.026430\n",
              "5   0.295962  0.254919  0.248721  ...  4.106723        15.0         0.019382\n",
              "6   0.264929  0.224174  0.228173  ...  4.583062         8.0         0.036342\n",
              "7   0.270235  0.228662  0.249313  ...  4.731872         5.0         0.058147\n",
              "8   0.257634  0.223843  0.245183  ...  4.470503        10.0         0.029074\n",
              "9   0.289596  0.240284  0.235281  ...  4.035327        17.0         0.017102\n",
              "10  0.254877  0.259745  0.230720  ...  4.312846        12.0         0.024228\n",
              "11  0.306085  0.269204  0.234900  ...  4.244900        14.0         0.020767\n",
              "12  0.258406  0.226609  0.231610  ...  4.774143         4.0         0.072684\n",
              "13  0.273785  0.234423  0.214054  ...  4.847430         3.0         0.096912\n",
              "14  0.281174  0.258473  0.242390  ...  4.281065        13.0         0.022364\n",
              "15  0.276611  0.257501  0.235830  ...  4.089106        16.0         0.018171\n",
              "16  0.290638  0.269898  0.222084  ...  4.678856         7.0         0.041534\n",
              "\n",
              "[17 rows x 21 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.20809729459385076, 100.0, 275.0, 2.0)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Génération :  2\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 100 hidden_dim : 322 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2773, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3475665096041437\n",
            "Epoch 1 : Validation loss = 0.2628717365364234; Validation r = 0.3822072183532963\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1822, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.5422838374199185\n",
            "Epoch 2 : Validation loss = 0.2275003441919883; Validation r = 0.4522082151606786\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.1193, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7265636177385655\n",
            "Epoch 3 : Validation loss = 0.21443559465308984; Validation r = 0.49380431962160803\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0690, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8596039016257068\n",
            "Epoch 4 : Validation loss = 0.2059827834367752; Validation r = 0.5206649420170463\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0386, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9269545609368078\n",
            "Epoch 5 : Validation loss = 0.207672364761432; Validation r = 0.5098080109746682\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 100 hidden_dim : 145 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2774, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.37964786217985647\n",
            "Epoch 1 : Validation loss = 0.3105399320522944; Validation r = 0.4333172241024896\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1489, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6678252504650984\n",
            "Epoch 2 : Validation loss = 0.24577028850714366; Validation r = 0.4598962468710406\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0874, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8142820916005271\n",
            "Epoch 3 : Validation loss = 0.2284099123130242; Validation r = 0.5008845920172522\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9059923546195576\n",
            "Epoch 4 : Validation loss = 0.2358057872702678; Validation r = 0.4878635394839617\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0277, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9454285592496283\n",
            "Epoch 5 : Validation loss = 0.2223410484691461; Validation r = 0.5123058820259365\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 100 hidden_dim : 254 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2739, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.36487438141712325\n",
            "Epoch 1 : Validation loss = 0.2537873212248087; Validation r = 0.4125396755294525\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1670, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.5935043974783548\n",
            "Epoch 2 : Validation loss = 0.23051248776415983; Validation r = 0.46866201046910044\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.1026, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7726427621809829\n",
            "Epoch 3 : Validation loss = 0.21992157399654388; Validation r = 0.48212799855862026\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0577, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8850158517205702\n",
            "Epoch 4 : Validation loss = 0.2141129565735658; Validation r = 0.49138955631578496\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0323, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9383518899439852\n",
            "Epoch 5 : Validation loss = 0.21842099080483118; Validation r = 0.4895110218750837\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 218 hidden_dim : 110 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2714, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3871412501645864\n",
            "Epoch 1 : Validation loss = 0.2832363481322924; Validation r = 0.4755556051896157\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1226, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7405600446750034\n",
            "Epoch 2 : Validation loss = 0.24003483889003593; Validation r = 0.5073684804994205\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0597, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.878127420316705\n",
            "Epoch 3 : Validation loss = 0.24299960943559806; Validation r = 0.5012344214259115\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0318, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9368958109719392\n",
            "Epoch 4 : Validation loss = 0.22744696866720915; Validation r = 0.5263665752140488\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0198, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.960083535621937\n",
            "Epoch 5 : Validation loss = 0.2207641669859489; Validation r = 0.5361225906761903\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 131 hidden_dim : 292 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2629, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.39625240451760496\n",
            "Epoch 1 : Validation loss = 0.2492206650475661; Validation r = 0.46409850421063764\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1373, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6892913569236735\n",
            "Epoch 2 : Validation loss = 0.23100369734068713; Validation r = 0.47180563880282905\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8487431324321326\n",
            "Epoch 3 : Validation loss = 0.2119209894289573; Validation r = 0.5245523219041702\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0392, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9238492030486958\n",
            "Epoch 4 : Validation loss = 0.21831898937622707; Validation r = 0.520068835228258\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0243, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9534334171842972\n",
            "Epoch 5 : Validation loss = 0.21208286148806413; Validation r = 0.5246806212253927\n",
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-045094e1-dd60-4970-bef6-cf21d7d18f8a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>max_epoch</th>\n",
              "      <th>best_dev_loss</th>\n",
              "      <th>best_dev_loss_epoch</th>\n",
              "      <th>lastEpochImproved</th>\n",
              "      <th>embedding_dim</th>\n",
              "      <th>hidden_dim</th>\n",
              "      <th>num_lstm_layers</th>\n",
              "      <th>loss_decrease_perc_0</th>\n",
              "      <th>loss_decrease_perc_1</th>\n",
              "      <th>loss_decrease_perc_2</th>\n",
              "      <th>loss_decrease_perc_3</th>\n",
              "      <th>score_perf</th>\n",
              "      <th>score_pot</th>\n",
              "      <th>score</th>\n",
              "      <th>model_rank</th>\n",
              "      <th>proba_selection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.297629</td>\n",
              "      <td>0.256342</td>\n",
              "      <td>0.224900</td>\n",
              "      <td>0.221605</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.138718</td>\n",
              "      <td>0.122657</td>\n",
              "      <td>0.014653</td>\n",
              "      <td>0.044878</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>-0.009499</td>\n",
              "      <td>4.946564</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.135471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.276555</td>\n",
              "      <td>0.242013</td>\n",
              "      <td>0.211087</td>\n",
              "      <td>0.214948</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.124904</td>\n",
              "      <td>0.127787</td>\n",
              "      <td>-0.018291</td>\n",
              "      <td>0.031870</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>-0.006632</td>\n",
              "      <td>4.963635</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.270943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.296420</td>\n",
              "      <td>0.235832</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.233284</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>175</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.204399</td>\n",
              "      <td>0.034291</td>\n",
              "      <td>-0.024319</td>\n",
              "      <td>0.045231</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>-0.010074</td>\n",
              "      <td>4.702388</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.030105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.270336</td>\n",
              "      <td>0.227319</td>\n",
              "      <td>0.231308</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.223296</td>\n",
              "      <td>4</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>175</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.159127</td>\n",
              "      <td>-0.017549</td>\n",
              "      <td>0.089803</td>\n",
              "      <td>-0.060609</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.012760</td>\n",
              "      <td>4.478359</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.019353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.285443</td>\n",
              "      <td>0.220161</td>\n",
              "      <td>0.226018</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.227897</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.228703</td>\n",
              "      <td>-0.026604</td>\n",
              "      <td>0.057750</td>\n",
              "      <td>-0.070109</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.014931</td>\n",
              "      <td>4.387952</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.016934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.295962</td>\n",
              "      <td>0.254919</td>\n",
              "      <td>0.248721</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>0.243503</td>\n",
              "      <td>4</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>0.138675</td>\n",
              "      <td>0.024316</td>\n",
              "      <td>0.056733</td>\n",
              "      <td>-0.037906</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>0.008893</td>\n",
              "      <td>4.106723</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.013547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.264929</td>\n",
              "      <td>0.224174</td>\n",
              "      <td>0.228173</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>0.218195</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>300</td>\n",
              "      <td>2</td>\n",
              "      <td>0.153832</td>\n",
              "      <td>-0.017838</td>\n",
              "      <td>0.057869</td>\n",
              "      <td>-0.015006</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>0.003226</td>\n",
              "      <td>4.583062</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.022579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.270235</td>\n",
              "      <td>0.228662</td>\n",
              "      <td>0.249313</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>0.211333</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>300</td>\n",
              "      <td>3</td>\n",
              "      <td>0.153839</td>\n",
              "      <td>-0.090309</td>\n",
              "      <td>0.163048</td>\n",
              "      <td>-0.012797</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>0.002670</td>\n",
              "      <td>4.731872</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.033868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.257634</td>\n",
              "      <td>0.223843</td>\n",
              "      <td>0.245183</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>0.223688</td>\n",
              "      <td>4</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.131159</td>\n",
              "      <td>-0.095335</td>\n",
              "      <td>0.104335</td>\n",
              "      <td>-0.018609</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>0.004087</td>\n",
              "      <td>4.470503</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.018063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.289596</td>\n",
              "      <td>0.240284</td>\n",
              "      <td>0.235281</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>0.247811</td>\n",
              "      <td>4</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>0.170279</td>\n",
              "      <td>0.020819</td>\n",
              "      <td>0.022889</td>\n",
              "      <td>-0.077928</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>0.017915</td>\n",
              "      <td>4.035327</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.012316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.254877</td>\n",
              "      <td>0.259745</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>0.240172</td>\n",
              "      <td>0.241364</td>\n",
              "      <td>4</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>300</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.019099</td>\n",
              "      <td>0.111744</td>\n",
              "      <td>-0.040966</td>\n",
              "      <td>-0.004965</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>0.001145</td>\n",
              "      <td>4.312846</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.015938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.306085</td>\n",
              "      <td>0.269204</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>0.240201</td>\n",
              "      <td>0.240892</td>\n",
              "      <td>4</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>300</td>\n",
              "      <td>3</td>\n",
              "      <td>0.120492</td>\n",
              "      <td>0.127428</td>\n",
              "      <td>-0.022564</td>\n",
              "      <td>-0.002880</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>0.000677</td>\n",
              "      <td>4.244900</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0.014260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.258406</td>\n",
              "      <td>0.226609</td>\n",
              "      <td>0.231610</td>\n",
              "      <td>0.234645</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>4</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>219</td>\n",
              "      <td>272</td>\n",
              "      <td>2</td>\n",
              "      <td>0.123048</td>\n",
              "      <td>-0.022068</td>\n",
              "      <td>-0.013101</td>\n",
              "      <td>0.055185</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>-0.012234</td>\n",
              "      <td>4.774143</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.045157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.273785</td>\n",
              "      <td>0.234423</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>0.226703</td>\n",
              "      <td>0.218485</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>264</td>\n",
              "      <td>309</td>\n",
              "      <td>2</td>\n",
              "      <td>0.143770</td>\n",
              "      <td>0.086889</td>\n",
              "      <td>-0.059090</td>\n",
              "      <td>0.036249</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>-0.007759</td>\n",
              "      <td>4.847430</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.067736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.281174</td>\n",
              "      <td>0.258473</td>\n",
              "      <td>0.242390</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>0.233587</td>\n",
              "      <td>4</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>201</td>\n",
              "      <td>100</td>\n",
              "      <td>2</td>\n",
              "      <td>0.080737</td>\n",
              "      <td>0.062222</td>\n",
              "      <td>0.042879</td>\n",
              "      <td>-0.006853</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>0.001590</td>\n",
              "      <td>4.281065</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0.015052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.276611</td>\n",
              "      <td>0.257501</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>0.235862</td>\n",
              "      <td>0.244586</td>\n",
              "      <td>4</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>352</td>\n",
              "      <td>240</td>\n",
              "      <td>3</td>\n",
              "      <td>0.069085</td>\n",
              "      <td>0.084159</td>\n",
              "      <td>-0.000138</td>\n",
              "      <td>-0.036985</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>0.008722</td>\n",
              "      <td>4.089106</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.012902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.290638</td>\n",
              "      <td>0.269898</td>\n",
              "      <td>0.222084</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>0.213727</td>\n",
              "      <td>4</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>113</td>\n",
              "      <td>2</td>\n",
              "      <td>0.071362</td>\n",
              "      <td>0.177156</td>\n",
              "      <td>0.054589</td>\n",
              "      <td>-0.017942</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>0.003767</td>\n",
              "      <td>4.678856</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.027094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.262872</td>\n",
              "      <td>0.227500</td>\n",
              "      <td>0.214436</td>\n",
              "      <td>0.205983</td>\n",
              "      <td>0.207672</td>\n",
              "      <td>4</td>\n",
              "      <td>0.205983</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>322</td>\n",
              "      <td>2</td>\n",
              "      <td>0.134558</td>\n",
              "      <td>0.057427</td>\n",
              "      <td>0.039419</td>\n",
              "      <td>-0.008203</td>\n",
              "      <td>0.205983</td>\n",
              "      <td>0.001690</td>\n",
              "      <td>4.815277</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.054189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.310540</td>\n",
              "      <td>0.245770</td>\n",
              "      <td>0.228410</td>\n",
              "      <td>0.235806</td>\n",
              "      <td>0.222341</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222341</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>145</td>\n",
              "      <td>2</td>\n",
              "      <td>0.208571</td>\n",
              "      <td>0.070637</td>\n",
              "      <td>-0.032380</td>\n",
              "      <td>0.057101</td>\n",
              "      <td>0.222341</td>\n",
              "      <td>-0.012696</td>\n",
              "      <td>4.769965</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.038706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.253787</td>\n",
              "      <td>0.230512</td>\n",
              "      <td>0.219922</td>\n",
              "      <td>0.214113</td>\n",
              "      <td>0.218421</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214113</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>254</td>\n",
              "      <td>2</td>\n",
              "      <td>0.091710</td>\n",
              "      <td>0.045945</td>\n",
              "      <td>0.026412</td>\n",
              "      <td>-0.020120</td>\n",
              "      <td>0.214113</td>\n",
              "      <td>0.004308</td>\n",
              "      <td>4.578315</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.020842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.283236</td>\n",
              "      <td>0.240035</td>\n",
              "      <td>0.243000</td>\n",
              "      <td>0.227447</td>\n",
              "      <td>0.220764</td>\n",
              "      <td>4</td>\n",
              "      <td>0.220764</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>218</td>\n",
              "      <td>110</td>\n",
              "      <td>1</td>\n",
              "      <td>0.152528</td>\n",
              "      <td>-0.012351</td>\n",
              "      <td>0.064003</td>\n",
              "      <td>0.029382</td>\n",
              "      <td>0.220764</td>\n",
              "      <td>-0.006486</td>\n",
              "      <td>4.666841</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.024631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.249221</td>\n",
              "      <td>0.231004</td>\n",
              "      <td>0.211921</td>\n",
              "      <td>0.218319</td>\n",
              "      <td>0.212083</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211921</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>131</td>\n",
              "      <td>292</td>\n",
              "      <td>1</td>\n",
              "      <td>0.073096</td>\n",
              "      <td>0.082608</td>\n",
              "      <td>-0.030190</td>\n",
              "      <td>0.028564</td>\n",
              "      <td>0.211921</td>\n",
              "      <td>-0.006053</td>\n",
              "      <td>4.857491</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.090314</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-045094e1-dd60-4970-bef6-cf21d7d18f8a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-045094e1-dd60-4970-bef6-cf21d7d18f8a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-045094e1-dd60-4970-bef6-cf21d7d18f8a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           0         1         2  ...     score  model_rank  proba_selection\n",
              "0   0.297629  0.256342  0.224900  ...  4.946564         2.0         0.135471\n",
              "1   0.276555  0.242013  0.211087  ...  4.963635         1.0         0.270943\n",
              "2   0.296420  0.235832  0.227745  ...  4.702388         9.0         0.030105\n",
              "3   0.270336  0.227319  0.231308  ...  4.478359        14.0         0.019353\n",
              "4   0.285443  0.220161  0.226018  ...  4.387952        16.0         0.016934\n",
              "5   0.295962  0.254919  0.248721  ...  4.106723        20.0         0.013547\n",
              "6   0.264929  0.224174  0.228173  ...  4.583062        12.0         0.022579\n",
              "7   0.270235  0.228662  0.249313  ...  4.731872         8.0         0.033868\n",
              "8   0.257634  0.223843  0.245183  ...  4.470503        15.0         0.018063\n",
              "9   0.289596  0.240284  0.235281  ...  4.035327        22.0         0.012316\n",
              "10  0.254877  0.259745  0.230720  ...  4.312846        17.0         0.015938\n",
              "11  0.306085  0.269204  0.234900  ...  4.244900        19.0         0.014260\n",
              "12  0.258406  0.226609  0.231610  ...  4.774143         6.0         0.045157\n",
              "13  0.273785  0.234423  0.214054  ...  4.847430         4.0         0.067736\n",
              "14  0.281174  0.258473  0.242390  ...  4.281065        18.0         0.015052\n",
              "15  0.276611  0.257501  0.235830  ...  4.089106        21.0         0.012902\n",
              "16  0.290638  0.269898  0.222084  ...  4.678856        10.0         0.027094\n",
              "17  0.262872  0.227500  0.214436  ...  4.815277         5.0         0.054189\n",
              "18  0.310540  0.245770  0.228410  ...  4.769965         7.0         0.038706\n",
              "19  0.253787  0.230512  0.219922  ...  4.578315        13.0         0.020842\n",
              "20  0.283236  0.240035  0.243000  ...  4.666841        11.0         0.024631\n",
              "21  0.249221  0.231004  0.211921  ...  4.857491         3.0         0.090314\n",
              "\n",
              "[22 rows x 21 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.20809729459385076, 100.0, 275.0, 2.0)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Génération :  3\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 156 hidden_dim : 276 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2581, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.40397971929670295\n",
            "Epoch 1 : Validation loss = 0.2652598542471727; Validation r = 0.43464817600206046\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1268, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7177572114395768\n",
            "Epoch 2 : Validation loss = 0.22728776646157106; Validation r = 0.4919671233420142\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0616, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8743955710721926\n",
            "Epoch 3 : Validation loss = 0.21776333625117938; Validation r = 0.5118055470470052\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0320, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9383854763286529\n",
            "Epoch 4 : Validation loss = 0.21023509080211322; Validation r = 0.5221906349474947\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0194, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9625967522475757\n",
            "Epoch 5 : Validation loss = 0.20227106225987276; Validation r = 0.5358036179250609\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 100 hidden_dim : 151 num_lstm_layers : 3 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.3154, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3069457350141926\n",
            "Epoch 1 : Validation loss = 0.2794105798006058; Validation r = 0.38327077669912624\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1935, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.5112430161646404\n",
            "Epoch 2 : Validation loss = 0.2421170689165592; Validation r = 0.43220018111447084\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.1343, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6807556044861086\n",
            "Epoch 3 : Validation loss = 0.23179634995758533; Validation r = 0.46553792142905437\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0902, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8048540338484046\n",
            "Epoch 4 : Validation loss = 0.2153128990282615; Validation r = 0.48252372249370484\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8823741262216667\n",
            "Epoch 5 : Validation loss = 0.22157442793250084; Validation r = 0.47644568989179326\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 120 hidden_dim : 122 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2690, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.40480006059508594\n",
            "Epoch 1 : Validation loss = 0.31144098987181984; Validation r = 0.4486515100956053\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1372, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6999446940167692\n",
            "Epoch 2 : Validation loss = 0.22590058942635854; Validation r = 0.5180180778178086\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0703, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8548060760851426\n",
            "Epoch 3 : Validation loss = 0.21739217601716518; Validation r = 0.5270109073372652\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0374, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9256759214127186\n",
            "Epoch 4 : Validation loss = 0.21637343540787696; Validation r = 0.5377947896809697\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0221, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.955152666926228\n",
            "Epoch 5 : Validation loss = 0.21376739976306755; Validation r = 0.5305307274344316\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 103 hidden_dim : 124 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2738, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.39540596074198847\n",
            "Epoch 1 : Validation loss = 0.27785192218919597; Validation r = 0.4510642570241656\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1518, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6602371590003467\n",
            "Epoch 2 : Validation loss = 0.23653430553774038; Validation r = 0.5229032122954473\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0856, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8190440636074385\n",
            "Epoch 3 : Validation loss = 0.22258601064483324; Validation r = 0.521857558024878\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9078125646446206\n",
            "Epoch 4 : Validation loss = 0.21359584132830303; Validation r = 0.5348036970204163\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0267, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9469885313314933\n",
            "Epoch 5 : Validation loss = 0.21947249434888363; Validation r = 0.5399653467652673\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 197 hidden_dim : 134 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2670, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.4078785815116789\n",
            "Epoch 1 : Validation loss = 0.2967773128300905; Validation r = 0.45870841320173295\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1277, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7213112394093905\n",
            "Epoch 2 : Validation loss = 0.2342843733727932; Validation r = 0.5080955247276949\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0614, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8732503265787455\n",
            "Epoch 3 : Validation loss = 0.21908530729512374; Validation r = 0.5318837052363824\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0314, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9356593064240721\n",
            "Epoch 4 : Validation loss = 0.20989956272145113; Validation r = 0.5394222161495305\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0203, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9583822148505049\n",
            "Epoch 5 : Validation loss = 0.21167209918300312; Validation r = 0.5411330892601748\n",
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8cdf1eb5-b8bf-4667-beee-40a9a8c445a6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>max_epoch</th>\n",
              "      <th>best_dev_loss</th>\n",
              "      <th>best_dev_loss_epoch</th>\n",
              "      <th>lastEpochImproved</th>\n",
              "      <th>embedding_dim</th>\n",
              "      <th>hidden_dim</th>\n",
              "      <th>num_lstm_layers</th>\n",
              "      <th>loss_decrease_perc_0</th>\n",
              "      <th>loss_decrease_perc_1</th>\n",
              "      <th>loss_decrease_perc_2</th>\n",
              "      <th>loss_decrease_perc_3</th>\n",
              "      <th>score_perf</th>\n",
              "      <th>score_pot</th>\n",
              "      <th>score</th>\n",
              "      <th>model_rank</th>\n",
              "      <th>proba_selection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.297629</td>\n",
              "      <td>0.256342</td>\n",
              "      <td>0.224900</td>\n",
              "      <td>0.221605</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.138718</td>\n",
              "      <td>0.122657</td>\n",
              "      <td>0.014653</td>\n",
              "      <td>0.044878</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>-0.009499</td>\n",
              "      <td>4.946564</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.085658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.276555</td>\n",
              "      <td>0.242013</td>\n",
              "      <td>0.211087</td>\n",
              "      <td>0.214948</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.124904</td>\n",
              "      <td>0.127787</td>\n",
              "      <td>-0.018291</td>\n",
              "      <td>0.031870</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>-0.006632</td>\n",
              "      <td>4.963635</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.128487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.296420</td>\n",
              "      <td>0.235832</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.233284</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>175</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.204399</td>\n",
              "      <td>0.034291</td>\n",
              "      <td>-0.024319</td>\n",
              "      <td>0.045231</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>-0.010074</td>\n",
              "      <td>4.702388</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.021414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.270336</td>\n",
              "      <td>0.227319</td>\n",
              "      <td>0.231308</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.223296</td>\n",
              "      <td>4</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>175</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.159127</td>\n",
              "      <td>-0.017549</td>\n",
              "      <td>0.089803</td>\n",
              "      <td>-0.060609</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.012760</td>\n",
              "      <td>4.478359</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0.013525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.285443</td>\n",
              "      <td>0.220161</td>\n",
              "      <td>0.226018</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.227897</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.228703</td>\n",
              "      <td>-0.026604</td>\n",
              "      <td>0.057750</td>\n",
              "      <td>-0.070109</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.014931</td>\n",
              "      <td>4.387952</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.012237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.295962</td>\n",
              "      <td>0.254919</td>\n",
              "      <td>0.248721</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>0.243503</td>\n",
              "      <td>4</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>0.138675</td>\n",
              "      <td>0.024316</td>\n",
              "      <td>0.056733</td>\n",
              "      <td>-0.037906</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>0.008893</td>\n",
              "      <td>4.106723</td>\n",
              "      <td>25.0</td>\n",
              "      <td>0.010279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.264929</td>\n",
              "      <td>0.224174</td>\n",
              "      <td>0.228173</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>0.218195</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>300</td>\n",
              "      <td>2</td>\n",
              "      <td>0.153832</td>\n",
              "      <td>-0.017838</td>\n",
              "      <td>0.057869</td>\n",
              "      <td>-0.015006</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>0.003226</td>\n",
              "      <td>4.583062</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.017132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.270235</td>\n",
              "      <td>0.228662</td>\n",
              "      <td>0.249313</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>0.211333</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>300</td>\n",
              "      <td>3</td>\n",
              "      <td>0.153839</td>\n",
              "      <td>-0.090309</td>\n",
              "      <td>0.163048</td>\n",
              "      <td>-0.012797</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>0.002670</td>\n",
              "      <td>4.731872</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.025697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.257634</td>\n",
              "      <td>0.223843</td>\n",
              "      <td>0.245183</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>0.223688</td>\n",
              "      <td>4</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.131159</td>\n",
              "      <td>-0.095335</td>\n",
              "      <td>0.104335</td>\n",
              "      <td>-0.018609</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>0.004087</td>\n",
              "      <td>4.470503</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.012849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.289596</td>\n",
              "      <td>0.240284</td>\n",
              "      <td>0.235281</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>0.247811</td>\n",
              "      <td>4</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>0.170279</td>\n",
              "      <td>0.020819</td>\n",
              "      <td>0.022889</td>\n",
              "      <td>-0.077928</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>0.017915</td>\n",
              "      <td>4.035327</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.009518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.254877</td>\n",
              "      <td>0.259745</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>0.240172</td>\n",
              "      <td>0.241364</td>\n",
              "      <td>4</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>300</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.019099</td>\n",
              "      <td>0.111744</td>\n",
              "      <td>-0.040966</td>\n",
              "      <td>-0.004965</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>0.001145</td>\n",
              "      <td>4.312846</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.011681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.306085</td>\n",
              "      <td>0.269204</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>0.240201</td>\n",
              "      <td>0.240892</td>\n",
              "      <td>4</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>300</td>\n",
              "      <td>3</td>\n",
              "      <td>0.120492</td>\n",
              "      <td>0.127428</td>\n",
              "      <td>-0.022564</td>\n",
              "      <td>-0.002880</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>0.000677</td>\n",
              "      <td>4.244900</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.010707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.258406</td>\n",
              "      <td>0.226609</td>\n",
              "      <td>0.231610</td>\n",
              "      <td>0.234645</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>4</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>219</td>\n",
              "      <td>272</td>\n",
              "      <td>2</td>\n",
              "      <td>0.123048</td>\n",
              "      <td>-0.022068</td>\n",
              "      <td>-0.013101</td>\n",
              "      <td>0.055185</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>-0.012234</td>\n",
              "      <td>4.774143</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.036710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.273785</td>\n",
              "      <td>0.234423</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>0.226703</td>\n",
              "      <td>0.218485</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>264</td>\n",
              "      <td>309</td>\n",
              "      <td>2</td>\n",
              "      <td>0.143770</td>\n",
              "      <td>0.086889</td>\n",
              "      <td>-0.059090</td>\n",
              "      <td>0.036249</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>-0.007759</td>\n",
              "      <td>4.847430</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.051395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.281174</td>\n",
              "      <td>0.258473</td>\n",
              "      <td>0.242390</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>0.233587</td>\n",
              "      <td>4</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>201</td>\n",
              "      <td>100</td>\n",
              "      <td>2</td>\n",
              "      <td>0.080737</td>\n",
              "      <td>0.062222</td>\n",
              "      <td>0.042879</td>\n",
              "      <td>-0.006853</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>0.001590</td>\n",
              "      <td>4.281065</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.011173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.276611</td>\n",
              "      <td>0.257501</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>0.235862</td>\n",
              "      <td>0.244586</td>\n",
              "      <td>4</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>352</td>\n",
              "      <td>240</td>\n",
              "      <td>3</td>\n",
              "      <td>0.069085</td>\n",
              "      <td>0.084159</td>\n",
              "      <td>-0.000138</td>\n",
              "      <td>-0.036985</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>0.008722</td>\n",
              "      <td>4.089106</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.009884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.290638</td>\n",
              "      <td>0.269898</td>\n",
              "      <td>0.222084</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>0.213727</td>\n",
              "      <td>4</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>113</td>\n",
              "      <td>2</td>\n",
              "      <td>0.071362</td>\n",
              "      <td>0.177156</td>\n",
              "      <td>0.054589</td>\n",
              "      <td>-0.017942</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>0.003767</td>\n",
              "      <td>4.678856</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.019767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.262872</td>\n",
              "      <td>0.227500</td>\n",
              "      <td>0.214436</td>\n",
              "      <td>0.205983</td>\n",
              "      <td>0.207672</td>\n",
              "      <td>4</td>\n",
              "      <td>0.205983</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>322</td>\n",
              "      <td>2</td>\n",
              "      <td>0.134558</td>\n",
              "      <td>0.057427</td>\n",
              "      <td>0.039419</td>\n",
              "      <td>-0.008203</td>\n",
              "      <td>0.205983</td>\n",
              "      <td>0.001690</td>\n",
              "      <td>4.815277</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.042829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.310540</td>\n",
              "      <td>0.245770</td>\n",
              "      <td>0.228410</td>\n",
              "      <td>0.235806</td>\n",
              "      <td>0.222341</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222341</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>145</td>\n",
              "      <td>2</td>\n",
              "      <td>0.208571</td>\n",
              "      <td>0.070637</td>\n",
              "      <td>-0.032380</td>\n",
              "      <td>0.057101</td>\n",
              "      <td>0.222341</td>\n",
              "      <td>-0.012696</td>\n",
              "      <td>4.769965</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.032122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.253787</td>\n",
              "      <td>0.230512</td>\n",
              "      <td>0.219922</td>\n",
              "      <td>0.214113</td>\n",
              "      <td>0.218421</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214113</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>254</td>\n",
              "      <td>2</td>\n",
              "      <td>0.091710</td>\n",
              "      <td>0.045945</td>\n",
              "      <td>0.026412</td>\n",
              "      <td>-0.020120</td>\n",
              "      <td>0.214113</td>\n",
              "      <td>0.004308</td>\n",
              "      <td>4.578315</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.016061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.283236</td>\n",
              "      <td>0.240035</td>\n",
              "      <td>0.243000</td>\n",
              "      <td>0.227447</td>\n",
              "      <td>0.220764</td>\n",
              "      <td>4</td>\n",
              "      <td>0.220764</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>218</td>\n",
              "      <td>110</td>\n",
              "      <td>1</td>\n",
              "      <td>0.152528</td>\n",
              "      <td>-0.012351</td>\n",
              "      <td>0.064003</td>\n",
              "      <td>0.029382</td>\n",
              "      <td>0.220764</td>\n",
              "      <td>-0.006486</td>\n",
              "      <td>4.666841</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.018355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.249221</td>\n",
              "      <td>0.231004</td>\n",
              "      <td>0.211921</td>\n",
              "      <td>0.218319</td>\n",
              "      <td>0.212083</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211921</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>131</td>\n",
              "      <td>292</td>\n",
              "      <td>1</td>\n",
              "      <td>0.073096</td>\n",
              "      <td>0.082608</td>\n",
              "      <td>-0.030190</td>\n",
              "      <td>0.028564</td>\n",
              "      <td>0.211921</td>\n",
              "      <td>-0.006053</td>\n",
              "      <td>4.857491</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.064243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.265260</td>\n",
              "      <td>0.227288</td>\n",
              "      <td>0.217763</td>\n",
              "      <td>0.210235</td>\n",
              "      <td>0.202271</td>\n",
              "      <td>4</td>\n",
              "      <td>0.202271</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>156</td>\n",
              "      <td>276</td>\n",
              "      <td>1</td>\n",
              "      <td>0.143151</td>\n",
              "      <td>0.041905</td>\n",
              "      <td>0.034571</td>\n",
              "      <td>0.037882</td>\n",
              "      <td>0.202271</td>\n",
              "      <td>-0.007662</td>\n",
              "      <td>5.138516</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.256973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.279411</td>\n",
              "      <td>0.242117</td>\n",
              "      <td>0.231796</td>\n",
              "      <td>0.215313</td>\n",
              "      <td>0.221574</td>\n",
              "      <td>4</td>\n",
              "      <td>0.215313</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>151</td>\n",
              "      <td>3</td>\n",
              "      <td>0.133472</td>\n",
              "      <td>0.042627</td>\n",
              "      <td>0.071112</td>\n",
              "      <td>-0.029081</td>\n",
              "      <td>0.215313</td>\n",
              "      <td>0.006262</td>\n",
              "      <td>4.513156</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0.014276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.311441</td>\n",
              "      <td>0.225901</td>\n",
              "      <td>0.217392</td>\n",
              "      <td>0.216373</td>\n",
              "      <td>0.213767</td>\n",
              "      <td>4</td>\n",
              "      <td>0.213767</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>120</td>\n",
              "      <td>122</td>\n",
              "      <td>2</td>\n",
              "      <td>0.274660</td>\n",
              "      <td>0.037664</td>\n",
              "      <td>0.004686</td>\n",
              "      <td>0.012044</td>\n",
              "      <td>0.213767</td>\n",
              "      <td>-0.002575</td>\n",
              "      <td>4.735011</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.028553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.277852</td>\n",
              "      <td>0.236534</td>\n",
              "      <td>0.222586</td>\n",
              "      <td>0.213596</td>\n",
              "      <td>0.219472</td>\n",
              "      <td>4</td>\n",
              "      <td>0.213596</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>103</td>\n",
              "      <td>124</td>\n",
              "      <td>2</td>\n",
              "      <td>0.148704</td>\n",
              "      <td>0.058969</td>\n",
              "      <td>0.040390</td>\n",
              "      <td>-0.027513</td>\n",
              "      <td>0.213596</td>\n",
              "      <td>0.005877</td>\n",
              "      <td>4.556380</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.015116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.296777</td>\n",
              "      <td>0.234284</td>\n",
              "      <td>0.219085</td>\n",
              "      <td>0.209900</td>\n",
              "      <td>0.211672</td>\n",
              "      <td>4</td>\n",
              "      <td>0.209900</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>197</td>\n",
              "      <td>134</td>\n",
              "      <td>2</td>\n",
              "      <td>0.210572</td>\n",
              "      <td>0.064874</td>\n",
              "      <td>0.041928</td>\n",
              "      <td>-0.008445</td>\n",
              "      <td>0.209900</td>\n",
              "      <td>0.001773</td>\n",
              "      <td>4.724288</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.023361</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8cdf1eb5-b8bf-4667-beee-40a9a8c445a6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8cdf1eb5-b8bf-4667-beee-40a9a8c445a6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8cdf1eb5-b8bf-4667-beee-40a9a8c445a6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           0         1         2  ...     score  model_rank  proba_selection\n",
              "0   0.297629  0.256342  0.224900  ...  4.946564         3.0         0.085658\n",
              "1   0.276555  0.242013  0.211087  ...  4.963635         2.0         0.128487\n",
              "2   0.296420  0.235832  0.227745  ...  4.702388        12.0         0.021414\n",
              "3   0.270336  0.227319  0.231308  ...  4.478359        19.0         0.013525\n",
              "4   0.285443  0.220161  0.226018  ...  4.387952        21.0         0.012237\n",
              "5   0.295962  0.254919  0.248721  ...  4.106723        25.0         0.010279\n",
              "6   0.264929  0.224174  0.228173  ...  4.583062        15.0         0.017132\n",
              "7   0.270235  0.228662  0.249313  ...  4.731872        10.0         0.025697\n",
              "8   0.257634  0.223843  0.245183  ...  4.470503        20.0         0.012849\n",
              "9   0.289596  0.240284  0.235281  ...  4.035327        27.0         0.009518\n",
              "10  0.254877  0.259745  0.230720  ...  4.312846        22.0         0.011681\n",
              "11  0.306085  0.269204  0.234900  ...  4.244900        24.0         0.010707\n",
              "12  0.258406  0.226609  0.231610  ...  4.774143         7.0         0.036710\n",
              "13  0.273785  0.234423  0.214054  ...  4.847430         5.0         0.051395\n",
              "14  0.281174  0.258473  0.242390  ...  4.281065        23.0         0.011173\n",
              "15  0.276611  0.257501  0.235830  ...  4.089106        26.0         0.009884\n",
              "16  0.290638  0.269898  0.222084  ...  4.678856        13.0         0.019767\n",
              "17  0.262872  0.227500  0.214436  ...  4.815277         6.0         0.042829\n",
              "18  0.310540  0.245770  0.228410  ...  4.769965         8.0         0.032122\n",
              "19  0.253787  0.230512  0.219922  ...  4.578315        16.0         0.016061\n",
              "20  0.283236  0.240035  0.243000  ...  4.666841        14.0         0.018355\n",
              "21  0.249221  0.231004  0.211921  ...  4.857491         4.0         0.064243\n",
              "22  0.265260  0.227288  0.217763  ...  5.138516         1.0         0.256973\n",
              "23  0.279411  0.242117  0.231796  ...  4.513156        18.0         0.014276\n",
              "24  0.311441  0.225901  0.217392  ...  4.735011         9.0         0.028553\n",
              "25  0.277852  0.236534  0.222586  ...  4.556380        17.0         0.015116\n",
              "26  0.296777  0.234284  0.219085  ...  4.724288        11.0         0.023361\n",
              "\n",
              "[27 rows x 21 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.20227106225987276, 156.0, 276.0, 1.0)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Génération :  4\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 127 hidden_dim : 305 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2577, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3982117707984813\n",
            "Epoch 1 : Validation loss = 0.2660891609887282; Validation r = 0.4662123823343509\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1399, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6803462442381027\n",
            "Epoch 2 : Validation loss = 0.23626097925007344; Validation r = 0.5054025898917963\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0760, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8406523645653968\n",
            "Epoch 3 : Validation loss = 0.20505609860022864; Validation r = 0.5371527511995647\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0399, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9219252445605643\n",
            "Epoch 4 : Validation loss = 0.20483125199874241; Validation r = 0.5384570583559232\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0242, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9537422709488039\n",
            "Epoch 5 : Validation loss = 0.20065437903006872; Validation r = 0.5487739202732909\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 204 hidden_dim : 292 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2590, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.39868361101599586\n",
            "Epoch 1 : Validation loss = 0.2900462806224823; Validation r = 0.46520643336020173\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1200, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7320657075939465\n",
            "Epoch 2 : Validation loss = 0.22533843989173571; Validation r = 0.5033478884680688\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0552, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.887837028570913\n",
            "Epoch 3 : Validation loss = 0.20673739730070034; Validation r = 0.5367388456042265\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0280, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9459270624022726\n",
            "Epoch 4 : Validation loss = 0.20727530059715113; Validation r = 0.5365869817940053\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0165, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.967417799745651\n",
            "Epoch 5 : Validation loss = 0.2100036167850097; Validation r = 0.5354860617162316\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 100 hidden_dim : 157 num_lstm_layers : 3 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.3180, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.29408677054179533\n",
            "Epoch 1 : Validation loss = 0.3017664153128862; Validation r = 0.3622793185389204\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.2032, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.48318369229192853\n",
            "Epoch 2 : Validation loss = 0.2628402624279261; Validation r = 0.3820185944250392\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.1448, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.645252892335585\n",
            "Epoch 3 : Validation loss = 0.22636340043197076; Validation r = 0.46985987826389736\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0951, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7895589427350006\n",
            "Epoch 4 : Validation loss = 0.21854293197393418; Validation r = 0.4659369034260393\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0630, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8712106879075405\n",
            "Epoch 5 : Validation loss = 0.22467501933375994; Validation r = 0.4797088221759034\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 149 hidden_dim : 295 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2608, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3991491418835468\n",
            "Epoch 1 : Validation loss = 0.24425809097786744; Validation r = 0.47032888128765105\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1283, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.713191716415813\n",
            "Epoch 2 : Validation loss = 0.215384724487861; Validation r = 0.53398994124313\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0664, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8637213556379351\n",
            "Epoch 3 : Validation loss = 0.2135872036218643; Validation r = 0.5276629109692141\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0353, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9315085350496672\n",
            "Epoch 4 : Validation loss = 0.21251018047332765; Validation r = 0.5257021127302045\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0213, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.959075485186172\n",
            "Epoch 5 : Validation loss = 0.21107790035506088; Validation r = 0.5267706573481877\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 178 hidden_dim : 324 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2608, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.391499611098234\n",
            "Epoch 1 : Validation loss = 0.27612260915338993; Validation r = 0.4415906546180084\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1291, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.711224956449074\n",
            "Epoch 2 : Validation loss = 0.23215723956624668; Validation r = 0.5253003901340146\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0616, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.875111745609097\n",
            "Epoch 3 : Validation loss = 0.21306399988631408; Validation r = 0.5213613253457758\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0291, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9441347999610606\n",
            "Epoch 4 : Validation loss = 0.21300748251378537; Validation r = 0.527626081688315\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0176, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9658370632457243\n",
            "Epoch 5 : Validation loss = 0.20698337840537231; Validation r = 0.534160084838538\n",
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-241a5748-08c6-4fbe-9c97-3f8acb541b24\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>max_epoch</th>\n",
              "      <th>best_dev_loss</th>\n",
              "      <th>best_dev_loss_epoch</th>\n",
              "      <th>lastEpochImproved</th>\n",
              "      <th>embedding_dim</th>\n",
              "      <th>hidden_dim</th>\n",
              "      <th>num_lstm_layers</th>\n",
              "      <th>loss_decrease_perc_0</th>\n",
              "      <th>loss_decrease_perc_1</th>\n",
              "      <th>loss_decrease_perc_2</th>\n",
              "      <th>loss_decrease_perc_3</th>\n",
              "      <th>score_perf</th>\n",
              "      <th>score_pot</th>\n",
              "      <th>score</th>\n",
              "      <th>model_rank</th>\n",
              "      <th>proba_selection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.297629</td>\n",
              "      <td>0.256342</td>\n",
              "      <td>0.224900</td>\n",
              "      <td>0.221605</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.138718</td>\n",
              "      <td>0.122657</td>\n",
              "      <td>0.014653</td>\n",
              "      <td>0.044878</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>-0.009499</td>\n",
              "      <td>4.946564</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.049279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.276555</td>\n",
              "      <td>0.242013</td>\n",
              "      <td>0.211087</td>\n",
              "      <td>0.214948</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.124904</td>\n",
              "      <td>0.127787</td>\n",
              "      <td>-0.018291</td>\n",
              "      <td>0.031870</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>-0.006632</td>\n",
              "      <td>4.963635</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.061599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.296420</td>\n",
              "      <td>0.235832</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.233284</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>175</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.204399</td>\n",
              "      <td>0.034291</td>\n",
              "      <td>-0.024319</td>\n",
              "      <td>0.045231</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>-0.010074</td>\n",
              "      <td>4.702388</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.015400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.270336</td>\n",
              "      <td>0.227319</td>\n",
              "      <td>0.231308</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.223296</td>\n",
              "      <td>4</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>175</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.159127</td>\n",
              "      <td>-0.017549</td>\n",
              "      <td>0.089803</td>\n",
              "      <td>-0.060609</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.012760</td>\n",
              "      <td>4.478359</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.010713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.285443</td>\n",
              "      <td>0.220161</td>\n",
              "      <td>0.226018</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.227897</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.228703</td>\n",
              "      <td>-0.026604</td>\n",
              "      <td>0.057750</td>\n",
              "      <td>-0.070109</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.014931</td>\n",
              "      <td>4.387952</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.009477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.295962</td>\n",
              "      <td>0.254919</td>\n",
              "      <td>0.248721</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>0.243503</td>\n",
              "      <td>4</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>0.138675</td>\n",
              "      <td>0.024316</td>\n",
              "      <td>0.056733</td>\n",
              "      <td>-0.037906</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>0.008893</td>\n",
              "      <td>4.106723</td>\n",
              "      <td>30.0</td>\n",
              "      <td>0.008213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.264929</td>\n",
              "      <td>0.224174</td>\n",
              "      <td>0.228173</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>0.218195</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>300</td>\n",
              "      <td>2</td>\n",
              "      <td>0.153832</td>\n",
              "      <td>-0.017838</td>\n",
              "      <td>0.057869</td>\n",
              "      <td>-0.015006</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>0.003226</td>\n",
              "      <td>4.583062</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0.012968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.270235</td>\n",
              "      <td>0.228662</td>\n",
              "      <td>0.249313</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>0.211333</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>300</td>\n",
              "      <td>3</td>\n",
              "      <td>0.153839</td>\n",
              "      <td>-0.090309</td>\n",
              "      <td>0.163048</td>\n",
              "      <td>-0.012797</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>0.002670</td>\n",
              "      <td>4.731872</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.017600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.257634</td>\n",
              "      <td>0.223843</td>\n",
              "      <td>0.245183</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>0.223688</td>\n",
              "      <td>4</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.131159</td>\n",
              "      <td>-0.095335</td>\n",
              "      <td>0.104335</td>\n",
              "      <td>-0.018609</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>0.004087</td>\n",
              "      <td>4.470503</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.010267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.289596</td>\n",
              "      <td>0.240284</td>\n",
              "      <td>0.235281</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>0.247811</td>\n",
              "      <td>4</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>0.170279</td>\n",
              "      <td>0.020819</td>\n",
              "      <td>0.022889</td>\n",
              "      <td>-0.077928</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>0.017915</td>\n",
              "      <td>4.035327</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.007700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.254877</td>\n",
              "      <td>0.259745</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>0.240172</td>\n",
              "      <td>0.241364</td>\n",
              "      <td>4</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>300</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.019099</td>\n",
              "      <td>0.111744</td>\n",
              "      <td>-0.040966</td>\n",
              "      <td>-0.004965</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>0.001145</td>\n",
              "      <td>4.312846</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.009126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.306085</td>\n",
              "      <td>0.269204</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>0.240201</td>\n",
              "      <td>0.240892</td>\n",
              "      <td>4</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>300</td>\n",
              "      <td>3</td>\n",
              "      <td>0.120492</td>\n",
              "      <td>0.127428</td>\n",
              "      <td>-0.022564</td>\n",
              "      <td>-0.002880</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>0.000677</td>\n",
              "      <td>4.244900</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0.008496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.258406</td>\n",
              "      <td>0.226609</td>\n",
              "      <td>0.231610</td>\n",
              "      <td>0.234645</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>4</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>219</td>\n",
              "      <td>272</td>\n",
              "      <td>2</td>\n",
              "      <td>0.123048</td>\n",
              "      <td>-0.022068</td>\n",
              "      <td>-0.013101</td>\n",
              "      <td>0.055185</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>-0.012234</td>\n",
              "      <td>4.774143</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.024640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.273785</td>\n",
              "      <td>0.234423</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>0.226703</td>\n",
              "      <td>0.218485</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>264</td>\n",
              "      <td>309</td>\n",
              "      <td>2</td>\n",
              "      <td>0.143770</td>\n",
              "      <td>0.086889</td>\n",
              "      <td>-0.059090</td>\n",
              "      <td>0.036249</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>-0.007759</td>\n",
              "      <td>4.847430</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.035200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.281174</td>\n",
              "      <td>0.258473</td>\n",
              "      <td>0.242390</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>0.233587</td>\n",
              "      <td>4</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>201</td>\n",
              "      <td>100</td>\n",
              "      <td>2</td>\n",
              "      <td>0.080737</td>\n",
              "      <td>0.062222</td>\n",
              "      <td>0.042879</td>\n",
              "      <td>-0.006853</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>0.001590</td>\n",
              "      <td>4.281065</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0.008800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.276611</td>\n",
              "      <td>0.257501</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>0.235862</td>\n",
              "      <td>0.244586</td>\n",
              "      <td>4</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>352</td>\n",
              "      <td>240</td>\n",
              "      <td>3</td>\n",
              "      <td>0.069085</td>\n",
              "      <td>0.084159</td>\n",
              "      <td>-0.000138</td>\n",
              "      <td>-0.036985</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>0.008722</td>\n",
              "      <td>4.089106</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.007948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.290638</td>\n",
              "      <td>0.269898</td>\n",
              "      <td>0.222084</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>0.213727</td>\n",
              "      <td>4</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>113</td>\n",
              "      <td>2</td>\n",
              "      <td>0.071362</td>\n",
              "      <td>0.177156</td>\n",
              "      <td>0.054589</td>\n",
              "      <td>-0.017942</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>0.003767</td>\n",
              "      <td>4.678856</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.014494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.262872</td>\n",
              "      <td>0.227500</td>\n",
              "      <td>0.214436</td>\n",
              "      <td>0.205983</td>\n",
              "      <td>0.207672</td>\n",
              "      <td>4</td>\n",
              "      <td>0.205983</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>322</td>\n",
              "      <td>2</td>\n",
              "      <td>0.134558</td>\n",
              "      <td>0.057427</td>\n",
              "      <td>0.039419</td>\n",
              "      <td>-0.008203</td>\n",
              "      <td>0.205983</td>\n",
              "      <td>0.001690</td>\n",
              "      <td>4.815277</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.030800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.310540</td>\n",
              "      <td>0.245770</td>\n",
              "      <td>0.228410</td>\n",
              "      <td>0.235806</td>\n",
              "      <td>0.222341</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222341</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>145</td>\n",
              "      <td>2</td>\n",
              "      <td>0.208571</td>\n",
              "      <td>0.070637</td>\n",
              "      <td>-0.032380</td>\n",
              "      <td>0.057101</td>\n",
              "      <td>0.222341</td>\n",
              "      <td>-0.012696</td>\n",
              "      <td>4.769965</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.022400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.253787</td>\n",
              "      <td>0.230512</td>\n",
              "      <td>0.219922</td>\n",
              "      <td>0.214113</td>\n",
              "      <td>0.218421</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214113</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>254</td>\n",
              "      <td>2</td>\n",
              "      <td>0.091710</td>\n",
              "      <td>0.045945</td>\n",
              "      <td>0.026412</td>\n",
              "      <td>-0.020120</td>\n",
              "      <td>0.214113</td>\n",
              "      <td>0.004308</td>\n",
              "      <td>4.578315</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.012320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.283236</td>\n",
              "      <td>0.240035</td>\n",
              "      <td>0.243000</td>\n",
              "      <td>0.227447</td>\n",
              "      <td>0.220764</td>\n",
              "      <td>4</td>\n",
              "      <td>0.220764</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>218</td>\n",
              "      <td>110</td>\n",
              "      <td>1</td>\n",
              "      <td>0.152528</td>\n",
              "      <td>-0.012351</td>\n",
              "      <td>0.064003</td>\n",
              "      <td>0.029382</td>\n",
              "      <td>0.220764</td>\n",
              "      <td>-0.006486</td>\n",
              "      <td>4.666841</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0.013689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.249221</td>\n",
              "      <td>0.231004</td>\n",
              "      <td>0.211921</td>\n",
              "      <td>0.218319</td>\n",
              "      <td>0.212083</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211921</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>131</td>\n",
              "      <td>292</td>\n",
              "      <td>1</td>\n",
              "      <td>0.073096</td>\n",
              "      <td>0.082608</td>\n",
              "      <td>-0.030190</td>\n",
              "      <td>0.028564</td>\n",
              "      <td>0.211921</td>\n",
              "      <td>-0.006053</td>\n",
              "      <td>4.857491</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.041066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.265260</td>\n",
              "      <td>0.227288</td>\n",
              "      <td>0.217763</td>\n",
              "      <td>0.210235</td>\n",
              "      <td>0.202271</td>\n",
              "      <td>4</td>\n",
              "      <td>0.202271</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>156</td>\n",
              "      <td>276</td>\n",
              "      <td>1</td>\n",
              "      <td>0.143151</td>\n",
              "      <td>0.041905</td>\n",
              "      <td>0.034571</td>\n",
              "      <td>0.037882</td>\n",
              "      <td>0.202271</td>\n",
              "      <td>-0.007662</td>\n",
              "      <td>5.138516</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.246397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.279411</td>\n",
              "      <td>0.242117</td>\n",
              "      <td>0.231796</td>\n",
              "      <td>0.215313</td>\n",
              "      <td>0.221574</td>\n",
              "      <td>4</td>\n",
              "      <td>0.215313</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>151</td>\n",
              "      <td>3</td>\n",
              "      <td>0.133472</td>\n",
              "      <td>0.042627</td>\n",
              "      <td>0.071112</td>\n",
              "      <td>-0.029081</td>\n",
              "      <td>0.215313</td>\n",
              "      <td>0.006262</td>\n",
              "      <td>4.513156</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.011200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.311441</td>\n",
              "      <td>0.225901</td>\n",
              "      <td>0.217392</td>\n",
              "      <td>0.216373</td>\n",
              "      <td>0.213767</td>\n",
              "      <td>4</td>\n",
              "      <td>0.213767</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>120</td>\n",
              "      <td>122</td>\n",
              "      <td>2</td>\n",
              "      <td>0.274660</td>\n",
              "      <td>0.037664</td>\n",
              "      <td>0.004686</td>\n",
              "      <td>0.012044</td>\n",
              "      <td>0.213767</td>\n",
              "      <td>-0.002575</td>\n",
              "      <td>4.735011</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.018954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.277852</td>\n",
              "      <td>0.236534</td>\n",
              "      <td>0.222586</td>\n",
              "      <td>0.213596</td>\n",
              "      <td>0.219472</td>\n",
              "      <td>4</td>\n",
              "      <td>0.213596</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>103</td>\n",
              "      <td>124</td>\n",
              "      <td>2</td>\n",
              "      <td>0.148704</td>\n",
              "      <td>0.058969</td>\n",
              "      <td>0.040390</td>\n",
              "      <td>-0.027513</td>\n",
              "      <td>0.213596</td>\n",
              "      <td>0.005877</td>\n",
              "      <td>4.556380</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.011733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.296777</td>\n",
              "      <td>0.234284</td>\n",
              "      <td>0.219085</td>\n",
              "      <td>0.209900</td>\n",
              "      <td>0.211672</td>\n",
              "      <td>4</td>\n",
              "      <td>0.209900</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>197</td>\n",
              "      <td>134</td>\n",
              "      <td>2</td>\n",
              "      <td>0.210572</td>\n",
              "      <td>0.064874</td>\n",
              "      <td>0.041928</td>\n",
              "      <td>-0.008445</td>\n",
              "      <td>0.209900</td>\n",
              "      <td>0.001773</td>\n",
              "      <td>4.724288</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.016426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.266089</td>\n",
              "      <td>0.236261</td>\n",
              "      <td>0.205056</td>\n",
              "      <td>0.204831</td>\n",
              "      <td>0.200654</td>\n",
              "      <td>4</td>\n",
              "      <td>0.200654</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>127</td>\n",
              "      <td>305</td>\n",
              "      <td>1</td>\n",
              "      <td>0.112098</td>\n",
              "      <td>0.132078</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>0.020392</td>\n",
              "      <td>0.200654</td>\n",
              "      <td>-0.004092</td>\n",
              "      <td>5.087436</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.123198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.290046</td>\n",
              "      <td>0.225338</td>\n",
              "      <td>0.206737</td>\n",
              "      <td>0.207275</td>\n",
              "      <td>0.210004</td>\n",
              "      <td>4</td>\n",
              "      <td>0.206737</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>204</td>\n",
              "      <td>292</td>\n",
              "      <td>1</td>\n",
              "      <td>0.223095</td>\n",
              "      <td>0.082547</td>\n",
              "      <td>-0.002602</td>\n",
              "      <td>-0.013163</td>\n",
              "      <td>0.206737</td>\n",
              "      <td>0.002721</td>\n",
              "      <td>4.774212</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.027377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.301766</td>\n",
              "      <td>0.262840</td>\n",
              "      <td>0.226363</td>\n",
              "      <td>0.218543</td>\n",
              "      <td>0.224675</td>\n",
              "      <td>4</td>\n",
              "      <td>0.218543</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>157</td>\n",
              "      <td>3</td>\n",
              "      <td>0.128994</td>\n",
              "      <td>0.138780</td>\n",
              "      <td>0.034548</td>\n",
              "      <td>-0.028059</td>\n",
              "      <td>0.218543</td>\n",
              "      <td>0.006132</td>\n",
              "      <td>4.450873</td>\n",
              "      <td>25.0</td>\n",
              "      <td>0.009856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.244258</td>\n",
              "      <td>0.215385</td>\n",
              "      <td>0.213587</td>\n",
              "      <td>0.212510</td>\n",
              "      <td>0.211078</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211078</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>149</td>\n",
              "      <td>295</td>\n",
              "      <td>1</td>\n",
              "      <td>0.118208</td>\n",
              "      <td>0.008346</td>\n",
              "      <td>0.005043</td>\n",
              "      <td>0.006740</td>\n",
              "      <td>0.211078</td>\n",
              "      <td>-0.001423</td>\n",
              "      <td>4.769735</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.020533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.276123</td>\n",
              "      <td>0.232157</td>\n",
              "      <td>0.213064</td>\n",
              "      <td>0.213007</td>\n",
              "      <td>0.206983</td>\n",
              "      <td>4</td>\n",
              "      <td>0.206983</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>178</td>\n",
              "      <td>324</td>\n",
              "      <td>1</td>\n",
              "      <td>0.159224</td>\n",
              "      <td>0.082243</td>\n",
              "      <td>0.000265</td>\n",
              "      <td>0.028281</td>\n",
              "      <td>0.206983</td>\n",
              "      <td>-0.005854</td>\n",
              "      <td>4.971918</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.082132</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-241a5748-08c6-4fbe-9c97-3f8acb541b24')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-241a5748-08c6-4fbe-9c97-3f8acb541b24 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-241a5748-08c6-4fbe-9c97-3f8acb541b24');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           0         1         2  ...     score  model_rank  proba_selection\n",
              "0   0.297629  0.256342  0.224900  ...  4.946564         5.0         0.049279\n",
              "1   0.276555  0.242013  0.211087  ...  4.963635         4.0         0.061599\n",
              "2   0.296420  0.235832  0.227745  ...  4.702388        16.0         0.015400\n",
              "3   0.270336  0.227319  0.231308  ...  4.478359        23.0         0.010713\n",
              "4   0.285443  0.220161  0.226018  ...  4.387952        26.0         0.009477\n",
              "5   0.295962  0.254919  0.248721  ...  4.106723        30.0         0.008213\n",
              "6   0.264929  0.224174  0.228173  ...  4.583062        19.0         0.012968\n",
              "7   0.270235  0.228662  0.249313  ...  4.731872        14.0         0.017600\n",
              "8   0.257634  0.223843  0.245183  ...  4.470503        24.0         0.010267\n",
              "9   0.289596  0.240284  0.235281  ...  4.035327        32.0         0.007700\n",
              "10  0.254877  0.259745  0.230720  ...  4.312846        27.0         0.009126\n",
              "11  0.306085  0.269204  0.234900  ...  4.244900        29.0         0.008496\n",
              "12  0.258406  0.226609  0.231610  ...  4.774143        10.0         0.024640\n",
              "13  0.273785  0.234423  0.214054  ...  4.847430         7.0         0.035200\n",
              "14  0.281174  0.258473  0.242390  ...  4.281065        28.0         0.008800\n",
              "15  0.276611  0.257501  0.235830  ...  4.089106        31.0         0.007948\n",
              "16  0.290638  0.269898  0.222084  ...  4.678856        17.0         0.014494\n",
              "17  0.262872  0.227500  0.214436  ...  4.815277         8.0         0.030800\n",
              "18  0.310540  0.245770  0.228410  ...  4.769965        11.0         0.022400\n",
              "19  0.253787  0.230512  0.219922  ...  4.578315        20.0         0.012320\n",
              "20  0.283236  0.240035  0.243000  ...  4.666841        18.0         0.013689\n",
              "21  0.249221  0.231004  0.211921  ...  4.857491         6.0         0.041066\n",
              "22  0.265260  0.227288  0.217763  ...  5.138516         1.0         0.246397\n",
              "23  0.279411  0.242117  0.231796  ...  4.513156        22.0         0.011200\n",
              "24  0.311441  0.225901  0.217392  ...  4.735011        13.0         0.018954\n",
              "25  0.277852  0.236534  0.222586  ...  4.556380        21.0         0.011733\n",
              "26  0.296777  0.234284  0.219085  ...  4.724288        15.0         0.016426\n",
              "27  0.266089  0.236261  0.205056  ...  5.087436         2.0         0.123198\n",
              "28  0.290046  0.225338  0.206737  ...  4.774212         9.0         0.027377\n",
              "29  0.301766  0.262840  0.226363  ...  4.450873        25.0         0.009856\n",
              "30  0.244258  0.215385  0.213587  ...  4.769735        12.0         0.020533\n",
              "31  0.276123  0.232157  0.213064  ...  4.971918         3.0         0.082132\n",
              "\n",
              "[32 rows x 21 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.20227106225987276, 156.0, 276.0, 1.0)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Génération :  5\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 163 hidden_dim : 290 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2568, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.4089149024867468\n",
            "Epoch 1 : Validation loss = 0.27222839221358297; Validation r = 0.4479971672981183\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1277, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7153470150478117\n",
            "Epoch 2 : Validation loss = 0.22216321863234043; Validation r = 0.511806725870529\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0625, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8727463954038582\n",
            "Epoch 3 : Validation loss = 0.21754651684314014; Validation r = 0.518883407828747\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0313, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9394100238301899\n",
            "Epoch 4 : Validation loss = 0.21329657025635243; Validation r = 0.5224954784297835\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0188, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9636366958742565\n",
            "Epoch 5 : Validation loss = 0.20755440214027962; Validation r = 0.5343268347771236\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 156 hidden_dim : 144 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2748, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.38849098072388066\n",
            "Epoch 1 : Validation loss = 0.30660243208209675; Validation r = 0.4396979298160235\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1372, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6958684557090608\n",
            "Epoch 2 : Validation loss = 0.23345136704544225; Validation r = 0.4988253558511289\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0697, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8542608601798266\n",
            "Epoch 3 : Validation loss = 0.22791602537035943; Validation r = 0.5418386753923705\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0374, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9241941467125317\n",
            "Epoch 4 : Validation loss = 0.22253301938374836; Validation r = 0.5340812803995745\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0222, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9548220518553306\n",
            "Epoch 5 : Validation loss = 0.22127001148959; Validation r = 0.5386518599294069\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 374 hidden_dim : 159 num_lstm_layers : 3 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2999, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3323848838940562\n",
            "Epoch 1 : Validation loss = 0.27340847390393413; Validation r = 0.4066783296300163\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1544, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6344520277115183\n",
            "Epoch 2 : Validation loss = 0.2534168849388758; Validation r = 0.4600996536190225\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0865, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8134860371091996\n",
            "Epoch 3 : Validation loss = 0.25469139056901136; Validation r = 0.4367416735476039\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9109105870525568\n",
            "Epoch 4 : Validation loss = 0.23505640191336472; Validation r = 0.47143806730880766\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0263, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9462609569488758\n",
            "Epoch 5 : Validation loss = 0.2352645014723142; Validation r = 0.4742952598578621\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 100 hidden_dim : 157 num_lstm_layers : 4 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.3725, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.19269679534086695\n",
            "Epoch 1 : Validation loss = 0.3618798142919938; Validation r = 0.2669673546993676\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.2756, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.2749750432628566\n",
            "Epoch 2 : Validation loss = 0.2932211545606454; Validation r = 0.2777377806960955\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.2502, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3472290918405094\n",
            "Epoch 3 : Validation loss = 0.29366381590565044; Validation r = 0.2783867438156727\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.2132, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.42669838689887934\n",
            "Epoch 4 : Validation loss = 0.296553664530317; Validation r = 0.28888495864314223\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.1781, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.536776427212548\n",
            "Epoch 5 : Validation loss = 0.25209809752802054; Validation r = 0.3399500984944379\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 100 hidden_dim : 337 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2626, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.38013479680116624\n",
            "Epoch 1 : Validation loss = 0.3120992814501127; Validation r = 0.4007159953798299\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1523, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.642254673952733\n",
            "Epoch 2 : Validation loss = 0.22889821690817674; Validation r = 0.48617648357989357\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0880, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8120981454018524\n",
            "Epoch 3 : Validation loss = 0.2166830734660228; Validation r = 0.4998289530175223\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9069042004764425\n",
            "Epoch 4 : Validation loss = 0.20889642213781676; Validation r = 0.5140063270955622\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0290, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9464819038666651\n",
            "Epoch 5 : Validation loss = 0.20424115719894567; Validation r = 0.5205650986803462\n",
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a1807c46-7a6b-4449-a465-133f24d2bbb9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>max_epoch</th>\n",
              "      <th>best_dev_loss</th>\n",
              "      <th>best_dev_loss_epoch</th>\n",
              "      <th>lastEpochImproved</th>\n",
              "      <th>embedding_dim</th>\n",
              "      <th>hidden_dim</th>\n",
              "      <th>num_lstm_layers</th>\n",
              "      <th>loss_decrease_perc_0</th>\n",
              "      <th>loss_decrease_perc_1</th>\n",
              "      <th>loss_decrease_perc_2</th>\n",
              "      <th>loss_decrease_perc_3</th>\n",
              "      <th>score_perf</th>\n",
              "      <th>score_pot</th>\n",
              "      <th>score</th>\n",
              "      <th>model_rank</th>\n",
              "      <th>proba_selection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.297629</td>\n",
              "      <td>0.256342</td>\n",
              "      <td>0.224900</td>\n",
              "      <td>0.221605</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.138718</td>\n",
              "      <td>0.122657</td>\n",
              "      <td>0.014653</td>\n",
              "      <td>0.044878</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>-0.009499</td>\n",
              "      <td>4.946564</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.034001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.276555</td>\n",
              "      <td>0.242013</td>\n",
              "      <td>0.211087</td>\n",
              "      <td>0.214948</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.124904</td>\n",
              "      <td>0.127787</td>\n",
              "      <td>-0.018291</td>\n",
              "      <td>0.031870</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>-0.006632</td>\n",
              "      <td>4.963635</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.047601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.296420</td>\n",
              "      <td>0.235832</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.233284</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>175</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.204399</td>\n",
              "      <td>0.034291</td>\n",
              "      <td>-0.024319</td>\n",
              "      <td>0.045231</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>-0.010074</td>\n",
              "      <td>4.702388</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0.013223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.270336</td>\n",
              "      <td>0.227319</td>\n",
              "      <td>0.231308</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.223296</td>\n",
              "      <td>4</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>175</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.159127</td>\n",
              "      <td>-0.017549</td>\n",
              "      <td>0.089803</td>\n",
              "      <td>-0.060609</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.012760</td>\n",
              "      <td>4.478359</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.008815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.285443</td>\n",
              "      <td>0.220161</td>\n",
              "      <td>0.226018</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.227897</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.228703</td>\n",
              "      <td>-0.026604</td>\n",
              "      <td>0.057750</td>\n",
              "      <td>-0.070109</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.014931</td>\n",
              "      <td>4.387952</td>\n",
              "      <td>30.0</td>\n",
              "      <td>0.007934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.295962</td>\n",
              "      <td>0.254919</td>\n",
              "      <td>0.248721</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>0.243503</td>\n",
              "      <td>4</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>0.138675</td>\n",
              "      <td>0.024316</td>\n",
              "      <td>0.056733</td>\n",
              "      <td>-0.037906</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>0.008893</td>\n",
              "      <td>4.106723</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0.006800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.264929</td>\n",
              "      <td>0.224174</td>\n",
              "      <td>0.228173</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>0.218195</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>300</td>\n",
              "      <td>2</td>\n",
              "      <td>0.153832</td>\n",
              "      <td>-0.017838</td>\n",
              "      <td>0.057869</td>\n",
              "      <td>-0.015006</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>0.003226</td>\n",
              "      <td>4.583062</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.010818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.270235</td>\n",
              "      <td>0.228662</td>\n",
              "      <td>0.249313</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>0.211333</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>300</td>\n",
              "      <td>3</td>\n",
              "      <td>0.153839</td>\n",
              "      <td>-0.090309</td>\n",
              "      <td>0.163048</td>\n",
              "      <td>-0.012797</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>0.002670</td>\n",
              "      <td>4.731872</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.014875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.257634</td>\n",
              "      <td>0.223843</td>\n",
              "      <td>0.245183</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>0.223688</td>\n",
              "      <td>4</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.131159</td>\n",
              "      <td>-0.095335</td>\n",
              "      <td>0.104335</td>\n",
              "      <td>-0.018609</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>0.004087</td>\n",
              "      <td>4.470503</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0.008500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.289596</td>\n",
              "      <td>0.240284</td>\n",
              "      <td>0.235281</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>0.247811</td>\n",
              "      <td>4</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>0.170279</td>\n",
              "      <td>0.020819</td>\n",
              "      <td>0.022889</td>\n",
              "      <td>-0.077928</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>0.017915</td>\n",
              "      <td>4.035327</td>\n",
              "      <td>37.0</td>\n",
              "      <td>0.006433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.254877</td>\n",
              "      <td>0.259745</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>0.240172</td>\n",
              "      <td>0.241364</td>\n",
              "      <td>4</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>300</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.019099</td>\n",
              "      <td>0.111744</td>\n",
              "      <td>-0.040966</td>\n",
              "      <td>-0.004965</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>0.001145</td>\n",
              "      <td>4.312846</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.007678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.306085</td>\n",
              "      <td>0.269204</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>0.240201</td>\n",
              "      <td>0.240892</td>\n",
              "      <td>4</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>300</td>\n",
              "      <td>3</td>\n",
              "      <td>0.120492</td>\n",
              "      <td>0.127428</td>\n",
              "      <td>-0.022564</td>\n",
              "      <td>-0.002880</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>0.000677</td>\n",
              "      <td>4.244900</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.007000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.258406</td>\n",
              "      <td>0.226609</td>\n",
              "      <td>0.231610</td>\n",
              "      <td>0.234645</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>4</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>219</td>\n",
              "      <td>272</td>\n",
              "      <td>2</td>\n",
              "      <td>0.123048</td>\n",
              "      <td>-0.022068</td>\n",
              "      <td>-0.013101</td>\n",
              "      <td>0.055185</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>-0.012234</td>\n",
              "      <td>4.774143</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.019834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.273785</td>\n",
              "      <td>0.234423</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>0.226703</td>\n",
              "      <td>0.218485</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>264</td>\n",
              "      <td>309</td>\n",
              "      <td>2</td>\n",
              "      <td>0.143770</td>\n",
              "      <td>0.086889</td>\n",
              "      <td>-0.059090</td>\n",
              "      <td>0.036249</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>-0.007759</td>\n",
              "      <td>4.847430</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.026445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.281174</td>\n",
              "      <td>0.258473</td>\n",
              "      <td>0.242390</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>0.233587</td>\n",
              "      <td>4</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>201</td>\n",
              "      <td>100</td>\n",
              "      <td>2</td>\n",
              "      <td>0.080737</td>\n",
              "      <td>0.062222</td>\n",
              "      <td>0.042879</td>\n",
              "      <td>-0.006853</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>0.001590</td>\n",
              "      <td>4.281065</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.007438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.276611</td>\n",
              "      <td>0.257501</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>0.235862</td>\n",
              "      <td>0.244586</td>\n",
              "      <td>4</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>352</td>\n",
              "      <td>240</td>\n",
              "      <td>3</td>\n",
              "      <td>0.069085</td>\n",
              "      <td>0.084159</td>\n",
              "      <td>-0.000138</td>\n",
              "      <td>-0.036985</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>0.008722</td>\n",
              "      <td>4.089106</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.006611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.290638</td>\n",
              "      <td>0.269898</td>\n",
              "      <td>0.222084</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>0.213727</td>\n",
              "      <td>4</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>113</td>\n",
              "      <td>2</td>\n",
              "      <td>0.071362</td>\n",
              "      <td>0.177156</td>\n",
              "      <td>0.054589</td>\n",
              "      <td>-0.017942</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>0.003767</td>\n",
              "      <td>4.678856</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0.012527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.262872</td>\n",
              "      <td>0.227500</td>\n",
              "      <td>0.214436</td>\n",
              "      <td>0.205983</td>\n",
              "      <td>0.207672</td>\n",
              "      <td>4</td>\n",
              "      <td>0.205983</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>322</td>\n",
              "      <td>2</td>\n",
              "      <td>0.134558</td>\n",
              "      <td>0.057427</td>\n",
              "      <td>0.039419</td>\n",
              "      <td>-0.008203</td>\n",
              "      <td>0.205983</td>\n",
              "      <td>0.001690</td>\n",
              "      <td>4.815277</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.023801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.310540</td>\n",
              "      <td>0.245770</td>\n",
              "      <td>0.228410</td>\n",
              "      <td>0.235806</td>\n",
              "      <td>0.222341</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222341</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>145</td>\n",
              "      <td>2</td>\n",
              "      <td>0.208571</td>\n",
              "      <td>0.070637</td>\n",
              "      <td>-0.032380</td>\n",
              "      <td>0.057101</td>\n",
              "      <td>0.222341</td>\n",
              "      <td>-0.012696</td>\n",
              "      <td>4.769965</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.018308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.253787</td>\n",
              "      <td>0.230512</td>\n",
              "      <td>0.219922</td>\n",
              "      <td>0.214113</td>\n",
              "      <td>0.218421</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214113</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>254</td>\n",
              "      <td>2</td>\n",
              "      <td>0.091710</td>\n",
              "      <td>0.045945</td>\n",
              "      <td>0.026412</td>\n",
              "      <td>-0.020120</td>\n",
              "      <td>0.214113</td>\n",
              "      <td>0.004308</td>\n",
              "      <td>4.578315</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.010348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.283236</td>\n",
              "      <td>0.240035</td>\n",
              "      <td>0.243000</td>\n",
              "      <td>0.227447</td>\n",
              "      <td>0.220764</td>\n",
              "      <td>4</td>\n",
              "      <td>0.220764</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>218</td>\n",
              "      <td>110</td>\n",
              "      <td>1</td>\n",
              "      <td>0.152528</td>\n",
              "      <td>-0.012351</td>\n",
              "      <td>0.064003</td>\n",
              "      <td>0.029382</td>\n",
              "      <td>0.220764</td>\n",
              "      <td>-0.006486</td>\n",
              "      <td>4.666841</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.011900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.249221</td>\n",
              "      <td>0.231004</td>\n",
              "      <td>0.211921</td>\n",
              "      <td>0.218319</td>\n",
              "      <td>0.212083</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211921</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>131</td>\n",
              "      <td>292</td>\n",
              "      <td>1</td>\n",
              "      <td>0.073096</td>\n",
              "      <td>0.082608</td>\n",
              "      <td>-0.030190</td>\n",
              "      <td>0.028564</td>\n",
              "      <td>0.211921</td>\n",
              "      <td>-0.006053</td>\n",
              "      <td>4.857491</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.029751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.265260</td>\n",
              "      <td>0.227288</td>\n",
              "      <td>0.217763</td>\n",
              "      <td>0.210235</td>\n",
              "      <td>0.202271</td>\n",
              "      <td>4</td>\n",
              "      <td>0.202271</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>156</td>\n",
              "      <td>276</td>\n",
              "      <td>1</td>\n",
              "      <td>0.143151</td>\n",
              "      <td>0.041905</td>\n",
              "      <td>0.034571</td>\n",
              "      <td>0.037882</td>\n",
              "      <td>0.202271</td>\n",
              "      <td>-0.007662</td>\n",
              "      <td>5.138516</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.238005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.279411</td>\n",
              "      <td>0.242117</td>\n",
              "      <td>0.231796</td>\n",
              "      <td>0.215313</td>\n",
              "      <td>0.221574</td>\n",
              "      <td>4</td>\n",
              "      <td>0.215313</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>151</td>\n",
              "      <td>3</td>\n",
              "      <td>0.133472</td>\n",
              "      <td>0.042627</td>\n",
              "      <td>0.071112</td>\n",
              "      <td>-0.029081</td>\n",
              "      <td>0.215313</td>\n",
              "      <td>0.006262</td>\n",
              "      <td>4.513156</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.009154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.311441</td>\n",
              "      <td>0.225901</td>\n",
              "      <td>0.217392</td>\n",
              "      <td>0.216373</td>\n",
              "      <td>0.213767</td>\n",
              "      <td>4</td>\n",
              "      <td>0.213767</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>120</td>\n",
              "      <td>122</td>\n",
              "      <td>2</td>\n",
              "      <td>0.274660</td>\n",
              "      <td>0.037664</td>\n",
              "      <td>0.004686</td>\n",
              "      <td>0.012044</td>\n",
              "      <td>0.213767</td>\n",
              "      <td>-0.002575</td>\n",
              "      <td>4.735011</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.015867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.277852</td>\n",
              "      <td>0.236534</td>\n",
              "      <td>0.222586</td>\n",
              "      <td>0.213596</td>\n",
              "      <td>0.219472</td>\n",
              "      <td>4</td>\n",
              "      <td>0.213596</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>103</td>\n",
              "      <td>124</td>\n",
              "      <td>2</td>\n",
              "      <td>0.148704</td>\n",
              "      <td>0.058969</td>\n",
              "      <td>0.040390</td>\n",
              "      <td>-0.027513</td>\n",
              "      <td>0.213596</td>\n",
              "      <td>0.005877</td>\n",
              "      <td>4.556380</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.009917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.296777</td>\n",
              "      <td>0.234284</td>\n",
              "      <td>0.219085</td>\n",
              "      <td>0.209900</td>\n",
              "      <td>0.211672</td>\n",
              "      <td>4</td>\n",
              "      <td>0.209900</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>197</td>\n",
              "      <td>134</td>\n",
              "      <td>2</td>\n",
              "      <td>0.210572</td>\n",
              "      <td>0.064874</td>\n",
              "      <td>0.041928</td>\n",
              "      <td>-0.008445</td>\n",
              "      <td>0.209900</td>\n",
              "      <td>0.001773</td>\n",
              "      <td>4.724288</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.014000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.266089</td>\n",
              "      <td>0.236261</td>\n",
              "      <td>0.205056</td>\n",
              "      <td>0.204831</td>\n",
              "      <td>0.200654</td>\n",
              "      <td>4</td>\n",
              "      <td>0.200654</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>127</td>\n",
              "      <td>305</td>\n",
              "      <td>1</td>\n",
              "      <td>0.112098</td>\n",
              "      <td>0.132078</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>0.020392</td>\n",
              "      <td>0.200654</td>\n",
              "      <td>-0.004092</td>\n",
              "      <td>5.087436</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.119003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.290046</td>\n",
              "      <td>0.225338</td>\n",
              "      <td>0.206737</td>\n",
              "      <td>0.207275</td>\n",
              "      <td>0.210004</td>\n",
              "      <td>4</td>\n",
              "      <td>0.206737</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>204</td>\n",
              "      <td>292</td>\n",
              "      <td>1</td>\n",
              "      <td>0.223095</td>\n",
              "      <td>0.082547</td>\n",
              "      <td>-0.002602</td>\n",
              "      <td>-0.013163</td>\n",
              "      <td>0.206737</td>\n",
              "      <td>0.002721</td>\n",
              "      <td>4.774212</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.021637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.301766</td>\n",
              "      <td>0.262840</td>\n",
              "      <td>0.226363</td>\n",
              "      <td>0.218543</td>\n",
              "      <td>0.224675</td>\n",
              "      <td>4</td>\n",
              "      <td>0.218543</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>157</td>\n",
              "      <td>3</td>\n",
              "      <td>0.128994</td>\n",
              "      <td>0.138780</td>\n",
              "      <td>0.034548</td>\n",
              "      <td>-0.028059</td>\n",
              "      <td>0.218543</td>\n",
              "      <td>0.006132</td>\n",
              "      <td>4.450873</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0.008207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.244258</td>\n",
              "      <td>0.215385</td>\n",
              "      <td>0.213587</td>\n",
              "      <td>0.212510</td>\n",
              "      <td>0.211078</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211078</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>149</td>\n",
              "      <td>295</td>\n",
              "      <td>1</td>\n",
              "      <td>0.118208</td>\n",
              "      <td>0.008346</td>\n",
              "      <td>0.005043</td>\n",
              "      <td>0.006740</td>\n",
              "      <td>0.211078</td>\n",
              "      <td>-0.001423</td>\n",
              "      <td>4.769735</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.017000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.276123</td>\n",
              "      <td>0.232157</td>\n",
              "      <td>0.213064</td>\n",
              "      <td>0.213007</td>\n",
              "      <td>0.206983</td>\n",
              "      <td>4</td>\n",
              "      <td>0.206983</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>178</td>\n",
              "      <td>324</td>\n",
              "      <td>1</td>\n",
              "      <td>0.159224</td>\n",
              "      <td>0.082243</td>\n",
              "      <td>0.000265</td>\n",
              "      <td>0.028281</td>\n",
              "      <td>0.206983</td>\n",
              "      <td>-0.005854</td>\n",
              "      <td>4.971918</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.059501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.272228</td>\n",
              "      <td>0.222163</td>\n",
              "      <td>0.217547</td>\n",
              "      <td>0.213297</td>\n",
              "      <td>0.207554</td>\n",
              "      <td>4</td>\n",
              "      <td>0.207554</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>163</td>\n",
              "      <td>290</td>\n",
              "      <td>1</td>\n",
              "      <td>0.183909</td>\n",
              "      <td>0.020781</td>\n",
              "      <td>0.019536</td>\n",
              "      <td>0.026921</td>\n",
              "      <td>0.207554</td>\n",
              "      <td>-0.005588</td>\n",
              "      <td>4.951308</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.039668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.306602</td>\n",
              "      <td>0.233451</td>\n",
              "      <td>0.227916</td>\n",
              "      <td>0.222533</td>\n",
              "      <td>0.221270</td>\n",
              "      <td>4</td>\n",
              "      <td>0.221270</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>156</td>\n",
              "      <td>144</td>\n",
              "      <td>2</td>\n",
              "      <td>0.238586</td>\n",
              "      <td>0.023711</td>\n",
              "      <td>0.023618</td>\n",
              "      <td>0.005676</td>\n",
              "      <td>0.221270</td>\n",
              "      <td>-0.001256</td>\n",
              "      <td>4.545162</td>\n",
              "      <td>25.0</td>\n",
              "      <td>0.009520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.273408</td>\n",
              "      <td>0.253417</td>\n",
              "      <td>0.254691</td>\n",
              "      <td>0.235056</td>\n",
              "      <td>0.235265</td>\n",
              "      <td>4</td>\n",
              "      <td>0.235056</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>374</td>\n",
              "      <td>159</td>\n",
              "      <td>3</td>\n",
              "      <td>0.073120</td>\n",
              "      <td>-0.005029</td>\n",
              "      <td>0.077093</td>\n",
              "      <td>-0.000885</td>\n",
              "      <td>0.235056</td>\n",
              "      <td>0.000208</td>\n",
              "      <td>4.250535</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.007212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.361880</td>\n",
              "      <td>0.293221</td>\n",
              "      <td>0.293664</td>\n",
              "      <td>0.296554</td>\n",
              "      <td>0.252098</td>\n",
              "      <td>4</td>\n",
              "      <td>0.252098</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>157</td>\n",
              "      <td>4</td>\n",
              "      <td>0.189728</td>\n",
              "      <td>-0.001510</td>\n",
              "      <td>-0.009841</td>\n",
              "      <td>0.149907</td>\n",
              "      <td>0.252098</td>\n",
              "      <td>-0.037791</td>\n",
              "      <td>4.666209</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.011334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.312099</td>\n",
              "      <td>0.228898</td>\n",
              "      <td>0.216683</td>\n",
              "      <td>0.208896</td>\n",
              "      <td>0.204241</td>\n",
              "      <td>4</td>\n",
              "      <td>0.204241</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>337</td>\n",
              "      <td>1</td>\n",
              "      <td>0.266585</td>\n",
              "      <td>0.053365</td>\n",
              "      <td>0.035936</td>\n",
              "      <td>0.022285</td>\n",
              "      <td>0.204241</td>\n",
              "      <td>-0.004552</td>\n",
              "      <td>5.007771</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.079335</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a1807c46-7a6b-4449-a465-133f24d2bbb9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a1807c46-7a6b-4449-a465-133f24d2bbb9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a1807c46-7a6b-4449-a465-133f24d2bbb9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           0         1         2  ...     score  model_rank  proba_selection\n",
              "0   0.297629  0.256342  0.224900  ...  4.946564         7.0         0.034001\n",
              "1   0.276555  0.242013  0.211087  ...  4.963635         5.0         0.047601\n",
              "2   0.296420  0.235832  0.227745  ...  4.702388        18.0         0.013223\n",
              "3   0.270336  0.227319  0.231308  ...  4.478359        27.0         0.008815\n",
              "4   0.285443  0.220161  0.226018  ...  4.387952        30.0         0.007934\n",
              "5   0.295962  0.254919  0.248721  ...  4.106723        35.0         0.006800\n",
              "6   0.264929  0.224174  0.228173  ...  4.583062        22.0         0.010818\n",
              "7   0.270235  0.228662  0.249313  ...  4.731872        16.0         0.014875\n",
              "8   0.257634  0.223843  0.245183  ...  4.470503        28.0         0.008500\n",
              "9   0.289596  0.240284  0.235281  ...  4.035327        37.0         0.006433\n",
              "10  0.254877  0.259745  0.230720  ...  4.312846        31.0         0.007678\n",
              "11  0.306085  0.269204  0.234900  ...  4.244900        34.0         0.007000\n",
              "12  0.258406  0.226609  0.231610  ...  4.774143        12.0         0.019834\n",
              "13  0.273785  0.234423  0.214054  ...  4.847430         9.0         0.026445\n",
              "14  0.281174  0.258473  0.242390  ...  4.281065        32.0         0.007438\n",
              "15  0.276611  0.257501  0.235830  ...  4.089106        36.0         0.006611\n",
              "16  0.290638  0.269898  0.222084  ...  4.678856        19.0         0.012527\n",
              "17  0.262872  0.227500  0.214436  ...  4.815277        10.0         0.023801\n",
              "18  0.310540  0.245770  0.228410  ...  4.769965        13.0         0.018308\n",
              "19  0.253787  0.230512  0.219922  ...  4.578315        23.0         0.010348\n",
              "20  0.283236  0.240035  0.243000  ...  4.666841        20.0         0.011900\n",
              "21  0.249221  0.231004  0.211921  ...  4.857491         8.0         0.029751\n",
              "22  0.265260  0.227288  0.217763  ...  5.138516         1.0         0.238005\n",
              "23  0.279411  0.242117  0.231796  ...  4.513156        26.0         0.009154\n",
              "24  0.311441  0.225901  0.217392  ...  4.735011        15.0         0.015867\n",
              "25  0.277852  0.236534  0.222586  ...  4.556380        24.0         0.009917\n",
              "26  0.296777  0.234284  0.219085  ...  4.724288        17.0         0.014000\n",
              "27  0.266089  0.236261  0.205056  ...  5.087436         2.0         0.119003\n",
              "28  0.290046  0.225338  0.206737  ...  4.774212        11.0         0.021637\n",
              "29  0.301766  0.262840  0.226363  ...  4.450873        29.0         0.008207\n",
              "30  0.244258  0.215385  0.213587  ...  4.769735        14.0         0.017000\n",
              "31  0.276123  0.232157  0.213064  ...  4.971918         4.0         0.059501\n",
              "32  0.272228  0.222163  0.217547  ...  4.951308         6.0         0.039668\n",
              "33  0.306602  0.233451  0.227916  ...  4.545162        25.0         0.009520\n",
              "34  0.273408  0.253417  0.254691  ...  4.250535        33.0         0.007212\n",
              "35  0.361880  0.293221  0.293664  ...  4.666209        21.0         0.011334\n",
              "36  0.312099  0.228898  0.216683  ...  5.007771         3.0         0.079335\n",
              "\n",
              "[37 rows x 21 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.20227106225987276, 156.0, 276.0, 1.0)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Génération :  6\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 193 hidden_dim : 292 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2528, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.4130912689550269\n",
            "Epoch 1 : Validation loss = 0.24679851531982422; Validation r = 0.48401481701010624\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1203, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7350201039036036\n",
            "Epoch 2 : Validation loss = 0.2251344058662653; Validation r = 0.5032242690863407\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8823753646177349\n",
            "Epoch 3 : Validation loss = 0.22339904910574357; Validation r = 0.5027042384645277\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0298, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9417235047814836\n",
            "Epoch 4 : Validation loss = 0.22086976369222006; Validation r = 0.4962894288735675\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0185, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9635568940829637\n",
            "Epoch 5 : Validation loss = 0.21795566026121377; Validation r = 0.5175302627737549\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 125 hidden_dim : 121 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2725, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.4054772562110543\n",
            "Epoch 1 : Validation loss = 0.2829727727919817; Validation r = 0.4622872547188728\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1406, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6941554918865843\n",
            "Epoch 2 : Validation loss = 0.23244674541056157; Validation r = 0.5008894949857221\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0753, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8442892738799926\n",
            "Epoch 3 : Validation loss = 0.2318665179113547; Validation r = 0.4909417595113421\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9184195413588558\n",
            "Epoch 4 : Validation loss = 0.22102266227205594; Validation r = 0.5224269728178915\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0250, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9493351035555635\n",
            "Epoch 5 : Validation loss = 0.21179871732989947; Validation r = 0.5324112466602933\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 309 hidden_dim : 289 num_lstm_layers : 3 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.3285, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.21658019632373443\n",
            "Epoch 1 : Validation loss = 0.33140876243511835; Validation r = 0.25407372193128874\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.2166, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.42977388554789125\n",
            "Epoch 2 : Validation loss = 0.27398982693751656; Validation r = 0.36171171766246385\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.1546, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6164300271152544\n",
            "Epoch 3 : Validation loss = 0.23516544674833614; Validation r = 0.3861543160637083\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0973, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7807712787654836\n",
            "Epoch 4 : Validation loss = 0.2430130382378896; Validation r = 0.3972098532587425\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0608, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8721634064740647\n",
            "Epoch 5 : Validation loss = 0.2531210930397113; Validation r = 0.3857645251635365\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 115 hidden_dim : 262 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2572, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.4021178778334626\n",
            "Epoch 1 : Validation loss = 0.2602294017871221; Validation r = 0.44276518907226414\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1421, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6737785279512061\n",
            "Epoch 2 : Validation loss = 0.24501002319157122; Validation r = 0.4929795580424259\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0769, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8399897301351611\n",
            "Epoch 3 : Validation loss = 0.21892987365523975; Validation r = 0.49560776413544544\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9187574461553522\n",
            "Epoch 4 : Validation loss = 0.21245600084463756; Validation r = 0.502907202996639\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0241, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9547662232251157\n",
            "Epoch 5 : Validation loss = 0.21303306048115095; Validation r = 0.5033891279586984\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 181 hidden_dim : 275 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2633, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3951612070708705\n",
            "Epoch 1 : Validation loss = 0.2629928134381771; Validation r = 0.45332868299390006\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1238, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7238132504238188\n",
            "Epoch 2 : Validation loss = 0.22872131032248338; Validation r = 0.5240079537523316\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0574, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8855256689659403\n",
            "Epoch 3 : Validation loss = 0.21279166030387084; Validation r = 0.5286738071879387\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0287, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9440022432349967\n",
            "Epoch 4 : Validation loss = 0.21131756752729416; Validation r = 0.5314714457761143\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0179, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9653241568981653\n",
            "Epoch 5 : Validation loss = 0.20362447556108237; Validation r = 0.5580179832840358\n",
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c2e57a18-6c2f-422c-adaf-1602507958bd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>max_epoch</th>\n",
              "      <th>best_dev_loss</th>\n",
              "      <th>best_dev_loss_epoch</th>\n",
              "      <th>lastEpochImproved</th>\n",
              "      <th>embedding_dim</th>\n",
              "      <th>hidden_dim</th>\n",
              "      <th>num_lstm_layers</th>\n",
              "      <th>loss_decrease_perc_0</th>\n",
              "      <th>loss_decrease_perc_1</th>\n",
              "      <th>loss_decrease_perc_2</th>\n",
              "      <th>loss_decrease_perc_3</th>\n",
              "      <th>score_perf</th>\n",
              "      <th>score_pot</th>\n",
              "      <th>score</th>\n",
              "      <th>model_rank</th>\n",
              "      <th>proba_selection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.297629</td>\n",
              "      <td>0.256342</td>\n",
              "      <td>0.224900</td>\n",
              "      <td>0.221605</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.138718</td>\n",
              "      <td>0.122657</td>\n",
              "      <td>0.014653</td>\n",
              "      <td>0.044878</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>-0.009499</td>\n",
              "      <td>4.946564</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.028890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.276555</td>\n",
              "      <td>0.242013</td>\n",
              "      <td>0.211087</td>\n",
              "      <td>0.214948</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.124904</td>\n",
              "      <td>0.127787</td>\n",
              "      <td>-0.018291</td>\n",
              "      <td>0.031870</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>-0.006632</td>\n",
              "      <td>4.963635</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.038520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.296420</td>\n",
              "      <td>0.235832</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.233284</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>175</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.204399</td>\n",
              "      <td>0.034291</td>\n",
              "      <td>-0.024319</td>\n",
              "      <td>0.045231</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>-0.010074</td>\n",
              "      <td>4.702388</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.011556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.270336</td>\n",
              "      <td>0.227319</td>\n",
              "      <td>0.231308</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.223296</td>\n",
              "      <td>4</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>175</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.159127</td>\n",
              "      <td>-0.017549</td>\n",
              "      <td>0.089803</td>\n",
              "      <td>-0.060609</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.012760</td>\n",
              "      <td>4.478359</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.007456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.285443</td>\n",
              "      <td>0.220161</td>\n",
              "      <td>0.226018</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.227897</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.228703</td>\n",
              "      <td>-0.026604</td>\n",
              "      <td>0.057750</td>\n",
              "      <td>-0.070109</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.014931</td>\n",
              "      <td>4.387952</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.006798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.295962</td>\n",
              "      <td>0.254919</td>\n",
              "      <td>0.248721</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>0.243503</td>\n",
              "      <td>4</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>0.138675</td>\n",
              "      <td>0.024316</td>\n",
              "      <td>0.056733</td>\n",
              "      <td>-0.037906</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>0.008893</td>\n",
              "      <td>4.106723</td>\n",
              "      <td>39.0</td>\n",
              "      <td>0.005926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.264929</td>\n",
              "      <td>0.224174</td>\n",
              "      <td>0.228173</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>0.218195</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>300</td>\n",
              "      <td>2</td>\n",
              "      <td>0.153832</td>\n",
              "      <td>-0.017838</td>\n",
              "      <td>0.057869</td>\n",
              "      <td>-0.015006</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>0.003226</td>\n",
              "      <td>4.583062</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.008889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.270235</td>\n",
              "      <td>0.228662</td>\n",
              "      <td>0.249313</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>0.211333</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>300</td>\n",
              "      <td>3</td>\n",
              "      <td>0.153839</td>\n",
              "      <td>-0.090309</td>\n",
              "      <td>0.163048</td>\n",
              "      <td>-0.012797</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>0.002670</td>\n",
              "      <td>4.731872</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0.012840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.257634</td>\n",
              "      <td>0.223843</td>\n",
              "      <td>0.245183</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>0.223688</td>\n",
              "      <td>4</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.131159</td>\n",
              "      <td>-0.095335</td>\n",
              "      <td>0.104335</td>\n",
              "      <td>-0.018609</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>0.004087</td>\n",
              "      <td>4.470503</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.007223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.289596</td>\n",
              "      <td>0.240284</td>\n",
              "      <td>0.235281</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>0.247811</td>\n",
              "      <td>4</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>0.170279</td>\n",
              "      <td>0.020819</td>\n",
              "      <td>0.022889</td>\n",
              "      <td>-0.077928</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>0.017915</td>\n",
              "      <td>4.035327</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0.005503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.254877</td>\n",
              "      <td>0.259745</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>0.240172</td>\n",
              "      <td>0.241364</td>\n",
              "      <td>4</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>300</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.019099</td>\n",
              "      <td>0.111744</td>\n",
              "      <td>-0.040966</td>\n",
              "      <td>-0.004965</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>0.001145</td>\n",
              "      <td>4.312846</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0.006603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.306085</td>\n",
              "      <td>0.269204</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>0.240201</td>\n",
              "      <td>0.240892</td>\n",
              "      <td>4</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>300</td>\n",
              "      <td>3</td>\n",
              "      <td>0.120492</td>\n",
              "      <td>0.127428</td>\n",
              "      <td>-0.022564</td>\n",
              "      <td>-0.002880</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>0.000677</td>\n",
              "      <td>4.244900</td>\n",
              "      <td>38.0</td>\n",
              "      <td>0.006082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.258406</td>\n",
              "      <td>0.226609</td>\n",
              "      <td>0.231610</td>\n",
              "      <td>0.234645</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>4</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>219</td>\n",
              "      <td>272</td>\n",
              "      <td>2</td>\n",
              "      <td>0.123048</td>\n",
              "      <td>-0.022068</td>\n",
              "      <td>-0.013101</td>\n",
              "      <td>0.055185</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>-0.012234</td>\n",
              "      <td>4.774143</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.016509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.273785</td>\n",
              "      <td>0.234423</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>0.226703</td>\n",
              "      <td>0.218485</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>264</td>\n",
              "      <td>309</td>\n",
              "      <td>2</td>\n",
              "      <td>0.143770</td>\n",
              "      <td>0.086889</td>\n",
              "      <td>-0.059090</td>\n",
              "      <td>0.036249</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>-0.007759</td>\n",
              "      <td>4.847430</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.021011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.281174</td>\n",
              "      <td>0.258473</td>\n",
              "      <td>0.242390</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>0.233587</td>\n",
              "      <td>4</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>201</td>\n",
              "      <td>100</td>\n",
              "      <td>2</td>\n",
              "      <td>0.080737</td>\n",
              "      <td>0.062222</td>\n",
              "      <td>0.042879</td>\n",
              "      <td>-0.006853</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>0.001590</td>\n",
              "      <td>4.281065</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.006420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.276611</td>\n",
              "      <td>0.257501</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>0.235862</td>\n",
              "      <td>0.244586</td>\n",
              "      <td>4</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>352</td>\n",
              "      <td>240</td>\n",
              "      <td>3</td>\n",
              "      <td>0.069085</td>\n",
              "      <td>0.084159</td>\n",
              "      <td>-0.000138</td>\n",
              "      <td>-0.036985</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>0.008722</td>\n",
              "      <td>4.089106</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.005778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.290638</td>\n",
              "      <td>0.269898</td>\n",
              "      <td>0.222084</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>0.213727</td>\n",
              "      <td>4</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>113</td>\n",
              "      <td>2</td>\n",
              "      <td>0.071362</td>\n",
              "      <td>0.177156</td>\n",
              "      <td>0.054589</td>\n",
              "      <td>-0.017942</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>0.003767</td>\n",
              "      <td>4.678856</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.010505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.262872</td>\n",
              "      <td>0.227500</td>\n",
              "      <td>0.214436</td>\n",
              "      <td>0.205983</td>\n",
              "      <td>0.207672</td>\n",
              "      <td>4</td>\n",
              "      <td>0.205983</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>322</td>\n",
              "      <td>2</td>\n",
              "      <td>0.134558</td>\n",
              "      <td>0.057427</td>\n",
              "      <td>0.039419</td>\n",
              "      <td>-0.008203</td>\n",
              "      <td>0.205983</td>\n",
              "      <td>0.001690</td>\n",
              "      <td>4.815277</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.019260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.310540</td>\n",
              "      <td>0.245770</td>\n",
              "      <td>0.228410</td>\n",
              "      <td>0.235806</td>\n",
              "      <td>0.222341</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222341</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>145</td>\n",
              "      <td>2</td>\n",
              "      <td>0.208571</td>\n",
              "      <td>0.070637</td>\n",
              "      <td>-0.032380</td>\n",
              "      <td>0.057101</td>\n",
              "      <td>0.222341</td>\n",
              "      <td>-0.012696</td>\n",
              "      <td>4.769965</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.015408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.253787</td>\n",
              "      <td>0.230512</td>\n",
              "      <td>0.219922</td>\n",
              "      <td>0.214113</td>\n",
              "      <td>0.218421</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214113</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>254</td>\n",
              "      <td>2</td>\n",
              "      <td>0.091710</td>\n",
              "      <td>0.045945</td>\n",
              "      <td>0.026412</td>\n",
              "      <td>-0.020120</td>\n",
              "      <td>0.214113</td>\n",
              "      <td>0.004308</td>\n",
              "      <td>4.578315</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.008560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.283236</td>\n",
              "      <td>0.240035</td>\n",
              "      <td>0.243000</td>\n",
              "      <td>0.227447</td>\n",
              "      <td>0.220764</td>\n",
              "      <td>4</td>\n",
              "      <td>0.220764</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>218</td>\n",
              "      <td>110</td>\n",
              "      <td>1</td>\n",
              "      <td>0.152528</td>\n",
              "      <td>-0.012351</td>\n",
              "      <td>0.064003</td>\n",
              "      <td>0.029382</td>\n",
              "      <td>0.220764</td>\n",
              "      <td>-0.006486</td>\n",
              "      <td>4.666841</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.010049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.249221</td>\n",
              "      <td>0.231004</td>\n",
              "      <td>0.211921</td>\n",
              "      <td>0.218319</td>\n",
              "      <td>0.212083</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211921</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>131</td>\n",
              "      <td>292</td>\n",
              "      <td>1</td>\n",
              "      <td>0.073096</td>\n",
              "      <td>0.082608</td>\n",
              "      <td>-0.030190</td>\n",
              "      <td>0.028564</td>\n",
              "      <td>0.211921</td>\n",
              "      <td>-0.006053</td>\n",
              "      <td>4.857491</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.023112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.265260</td>\n",
              "      <td>0.227288</td>\n",
              "      <td>0.217763</td>\n",
              "      <td>0.210235</td>\n",
              "      <td>0.202271</td>\n",
              "      <td>4</td>\n",
              "      <td>0.202271</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>156</td>\n",
              "      <td>276</td>\n",
              "      <td>1</td>\n",
              "      <td>0.143151</td>\n",
              "      <td>0.041905</td>\n",
              "      <td>0.034571</td>\n",
              "      <td>0.037882</td>\n",
              "      <td>0.202271</td>\n",
              "      <td>-0.007662</td>\n",
              "      <td>5.138516</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.231121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.279411</td>\n",
              "      <td>0.242117</td>\n",
              "      <td>0.231796</td>\n",
              "      <td>0.215313</td>\n",
              "      <td>0.221574</td>\n",
              "      <td>4</td>\n",
              "      <td>0.215313</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>151</td>\n",
              "      <td>3</td>\n",
              "      <td>0.133472</td>\n",
              "      <td>0.042627</td>\n",
              "      <td>0.071112</td>\n",
              "      <td>-0.029081</td>\n",
              "      <td>0.215313</td>\n",
              "      <td>0.006262</td>\n",
              "      <td>4.513156</td>\n",
              "      <td>30.0</td>\n",
              "      <td>0.007704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.311441</td>\n",
              "      <td>0.225901</td>\n",
              "      <td>0.217392</td>\n",
              "      <td>0.216373</td>\n",
              "      <td>0.213767</td>\n",
              "      <td>4</td>\n",
              "      <td>0.213767</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>120</td>\n",
              "      <td>122</td>\n",
              "      <td>2</td>\n",
              "      <td>0.274660</td>\n",
              "      <td>0.037664</td>\n",
              "      <td>0.004686</td>\n",
              "      <td>0.012044</td>\n",
              "      <td>0.213767</td>\n",
              "      <td>-0.002575</td>\n",
              "      <td>4.735011</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.013595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.277852</td>\n",
              "      <td>0.236534</td>\n",
              "      <td>0.222586</td>\n",
              "      <td>0.213596</td>\n",
              "      <td>0.219472</td>\n",
              "      <td>4</td>\n",
              "      <td>0.213596</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>103</td>\n",
              "      <td>124</td>\n",
              "      <td>2</td>\n",
              "      <td>0.148704</td>\n",
              "      <td>0.058969</td>\n",
              "      <td>0.040390</td>\n",
              "      <td>-0.027513</td>\n",
              "      <td>0.213596</td>\n",
              "      <td>0.005877</td>\n",
              "      <td>4.556380</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0.008254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.296777</td>\n",
              "      <td>0.234284</td>\n",
              "      <td>0.219085</td>\n",
              "      <td>0.209900</td>\n",
              "      <td>0.211672</td>\n",
              "      <td>4</td>\n",
              "      <td>0.209900</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>197</td>\n",
              "      <td>134</td>\n",
              "      <td>2</td>\n",
              "      <td>0.210572</td>\n",
              "      <td>0.064874</td>\n",
              "      <td>0.041928</td>\n",
              "      <td>-0.008445</td>\n",
              "      <td>0.209900</td>\n",
              "      <td>0.001773</td>\n",
              "      <td>4.724288</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0.012164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.266089</td>\n",
              "      <td>0.236261</td>\n",
              "      <td>0.205056</td>\n",
              "      <td>0.204831</td>\n",
              "      <td>0.200654</td>\n",
              "      <td>4</td>\n",
              "      <td>0.200654</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>127</td>\n",
              "      <td>305</td>\n",
              "      <td>1</td>\n",
              "      <td>0.112098</td>\n",
              "      <td>0.132078</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>0.020392</td>\n",
              "      <td>0.200654</td>\n",
              "      <td>-0.004092</td>\n",
              "      <td>5.087436</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.077040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.290046</td>\n",
              "      <td>0.225338</td>\n",
              "      <td>0.206737</td>\n",
              "      <td>0.207275</td>\n",
              "      <td>0.210004</td>\n",
              "      <td>4</td>\n",
              "      <td>0.206737</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>204</td>\n",
              "      <td>292</td>\n",
              "      <td>1</td>\n",
              "      <td>0.223095</td>\n",
              "      <td>0.082547</td>\n",
              "      <td>-0.002602</td>\n",
              "      <td>-0.013163</td>\n",
              "      <td>0.206737</td>\n",
              "      <td>0.002721</td>\n",
              "      <td>4.774212</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.017779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.301766</td>\n",
              "      <td>0.262840</td>\n",
              "      <td>0.226363</td>\n",
              "      <td>0.218543</td>\n",
              "      <td>0.224675</td>\n",
              "      <td>4</td>\n",
              "      <td>0.218543</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>157</td>\n",
              "      <td>3</td>\n",
              "      <td>0.128994</td>\n",
              "      <td>0.138780</td>\n",
              "      <td>0.034548</td>\n",
              "      <td>-0.028059</td>\n",
              "      <td>0.218543</td>\n",
              "      <td>0.006132</td>\n",
              "      <td>4.450873</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.007004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.244258</td>\n",
              "      <td>0.215385</td>\n",
              "      <td>0.213587</td>\n",
              "      <td>0.212510</td>\n",
              "      <td>0.211078</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211078</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>149</td>\n",
              "      <td>295</td>\n",
              "      <td>1</td>\n",
              "      <td>0.118208</td>\n",
              "      <td>0.008346</td>\n",
              "      <td>0.005043</td>\n",
              "      <td>0.006740</td>\n",
              "      <td>0.211078</td>\n",
              "      <td>-0.001423</td>\n",
              "      <td>4.769735</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.014445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.276123</td>\n",
              "      <td>0.232157</td>\n",
              "      <td>0.213064</td>\n",
              "      <td>0.213007</td>\n",
              "      <td>0.206983</td>\n",
              "      <td>4</td>\n",
              "      <td>0.206983</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>178</td>\n",
              "      <td>324</td>\n",
              "      <td>1</td>\n",
              "      <td>0.159224</td>\n",
              "      <td>0.082243</td>\n",
              "      <td>0.000265</td>\n",
              "      <td>0.028281</td>\n",
              "      <td>0.206983</td>\n",
              "      <td>-0.005854</td>\n",
              "      <td>4.971918</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.046224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.272228</td>\n",
              "      <td>0.222163</td>\n",
              "      <td>0.217547</td>\n",
              "      <td>0.213297</td>\n",
              "      <td>0.207554</td>\n",
              "      <td>4</td>\n",
              "      <td>0.207554</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>163</td>\n",
              "      <td>290</td>\n",
              "      <td>1</td>\n",
              "      <td>0.183909</td>\n",
              "      <td>0.020781</td>\n",
              "      <td>0.019536</td>\n",
              "      <td>0.026921</td>\n",
              "      <td>0.207554</td>\n",
              "      <td>-0.005588</td>\n",
              "      <td>4.951308</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.033017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.306602</td>\n",
              "      <td>0.233451</td>\n",
              "      <td>0.227916</td>\n",
              "      <td>0.222533</td>\n",
              "      <td>0.221270</td>\n",
              "      <td>4</td>\n",
              "      <td>0.221270</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>156</td>\n",
              "      <td>144</td>\n",
              "      <td>2</td>\n",
              "      <td>0.238586</td>\n",
              "      <td>0.023711</td>\n",
              "      <td>0.023618</td>\n",
              "      <td>0.005676</td>\n",
              "      <td>0.221270</td>\n",
              "      <td>-0.001256</td>\n",
              "      <td>4.545162</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0.007970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.273408</td>\n",
              "      <td>0.253417</td>\n",
              "      <td>0.254691</td>\n",
              "      <td>0.235056</td>\n",
              "      <td>0.235265</td>\n",
              "      <td>4</td>\n",
              "      <td>0.235056</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>374</td>\n",
              "      <td>159</td>\n",
              "      <td>3</td>\n",
              "      <td>0.073120</td>\n",
              "      <td>-0.005029</td>\n",
              "      <td>0.077093</td>\n",
              "      <td>-0.000885</td>\n",
              "      <td>0.235056</td>\n",
              "      <td>0.000208</td>\n",
              "      <td>4.250535</td>\n",
              "      <td>37.0</td>\n",
              "      <td>0.006247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.361880</td>\n",
              "      <td>0.293221</td>\n",
              "      <td>0.293664</td>\n",
              "      <td>0.296554</td>\n",
              "      <td>0.252098</td>\n",
              "      <td>4</td>\n",
              "      <td>0.252098</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>157</td>\n",
              "      <td>4</td>\n",
              "      <td>0.189728</td>\n",
              "      <td>-0.001510</td>\n",
              "      <td>-0.009841</td>\n",
              "      <td>0.149907</td>\n",
              "      <td>0.252098</td>\n",
              "      <td>-0.037791</td>\n",
              "      <td>4.666209</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.009630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.312099</td>\n",
              "      <td>0.228898</td>\n",
              "      <td>0.216683</td>\n",
              "      <td>0.208896</td>\n",
              "      <td>0.204241</td>\n",
              "      <td>4</td>\n",
              "      <td>0.204241</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>337</td>\n",
              "      <td>1</td>\n",
              "      <td>0.266585</td>\n",
              "      <td>0.053365</td>\n",
              "      <td>0.035936</td>\n",
              "      <td>0.022285</td>\n",
              "      <td>0.204241</td>\n",
              "      <td>-0.004552</td>\n",
              "      <td>5.007771</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.057780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.246799</td>\n",
              "      <td>0.225134</td>\n",
              "      <td>0.223399</td>\n",
              "      <td>0.220870</td>\n",
              "      <td>0.217956</td>\n",
              "      <td>4</td>\n",
              "      <td>0.217956</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>193</td>\n",
              "      <td>292</td>\n",
              "      <td>1</td>\n",
              "      <td>0.087781</td>\n",
              "      <td>0.007708</td>\n",
              "      <td>0.011322</td>\n",
              "      <td>0.013194</td>\n",
              "      <td>0.217956</td>\n",
              "      <td>-0.002876</td>\n",
              "      <td>4.649433</td>\n",
              "      <td>25.0</td>\n",
              "      <td>0.009245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.282973</td>\n",
              "      <td>0.232447</td>\n",
              "      <td>0.231867</td>\n",
              "      <td>0.221023</td>\n",
              "      <td>0.211799</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211799</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>125</td>\n",
              "      <td>121</td>\n",
              "      <td>2</td>\n",
              "      <td>0.178554</td>\n",
              "      <td>0.002496</td>\n",
              "      <td>0.046768</td>\n",
              "      <td>0.041733</td>\n",
              "      <td>0.211799</td>\n",
              "      <td>-0.008839</td>\n",
              "      <td>4.927086</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.025680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.331409</td>\n",
              "      <td>0.273990</td>\n",
              "      <td>0.235165</td>\n",
              "      <td>0.243013</td>\n",
              "      <td>0.253121</td>\n",
              "      <td>4</td>\n",
              "      <td>0.235165</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>309</td>\n",
              "      <td>289</td>\n",
              "      <td>3</td>\n",
              "      <td>0.173257</td>\n",
              "      <td>0.141700</td>\n",
              "      <td>-0.033371</td>\n",
              "      <td>-0.041595</td>\n",
              "      <td>0.235165</td>\n",
              "      <td>0.009782</td>\n",
              "      <td>4.082514</td>\n",
              "      <td>41.0</td>\n",
              "      <td>0.005637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.260229</td>\n",
              "      <td>0.245010</td>\n",
              "      <td>0.218930</td>\n",
              "      <td>0.212456</td>\n",
              "      <td>0.213033</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212456</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>115</td>\n",
              "      <td>262</td>\n",
              "      <td>1</td>\n",
              "      <td>0.058484</td>\n",
              "      <td>0.106445</td>\n",
              "      <td>0.029571</td>\n",
              "      <td>-0.002716</td>\n",
              "      <td>0.212456</td>\n",
              "      <td>0.000577</td>\n",
              "      <td>4.694107</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.011006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.262993</td>\n",
              "      <td>0.228721</td>\n",
              "      <td>0.212792</td>\n",
              "      <td>0.211318</td>\n",
              "      <td>0.203624</td>\n",
              "      <td>4</td>\n",
              "      <td>0.203624</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>181</td>\n",
              "      <td>275</td>\n",
              "      <td>1</td>\n",
              "      <td>0.130313</td>\n",
              "      <td>0.069647</td>\n",
              "      <td>0.006927</td>\n",
              "      <td>0.036405</td>\n",
              "      <td>0.203624</td>\n",
              "      <td>-0.007413</td>\n",
              "      <td>5.096542</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.115560</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c2e57a18-6c2f-422c-adaf-1602507958bd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c2e57a18-6c2f-422c-adaf-1602507958bd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c2e57a18-6c2f-422c-adaf-1602507958bd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           0         1         2  ...     score  model_rank  proba_selection\n",
              "0   0.297629  0.256342  0.224900  ...  4.946564         8.0         0.028890\n",
              "1   0.276555  0.242013  0.211087  ...  4.963635         6.0         0.038520\n",
              "2   0.296420  0.235832  0.227745  ...  4.702388        20.0         0.011556\n",
              "3   0.270336  0.227319  0.231308  ...  4.478359        31.0         0.007456\n",
              "4   0.285443  0.220161  0.226018  ...  4.387952        34.0         0.006798\n",
              "5   0.295962  0.254919  0.248721  ...  4.106723        39.0         0.005926\n",
              "6   0.264929  0.224174  0.228173  ...  4.583062        26.0         0.008889\n",
              "7   0.270235  0.228662  0.249313  ...  4.731872        18.0         0.012840\n",
              "8   0.257634  0.223843  0.245183  ...  4.470503        32.0         0.007223\n",
              "9   0.289596  0.240284  0.235281  ...  4.035327        42.0         0.005503\n",
              "10  0.254877  0.259745  0.230720  ...  4.312846        35.0         0.006603\n",
              "11  0.306085  0.269204  0.234900  ...  4.244900        38.0         0.006082\n",
              "12  0.258406  0.226609  0.231610  ...  4.774143        14.0         0.016509\n",
              "13  0.273785  0.234423  0.214054  ...  4.847430        11.0         0.021011\n",
              "14  0.281174  0.258473  0.242390  ...  4.281065        36.0         0.006420\n",
              "15  0.276611  0.257501  0.235830  ...  4.089106        40.0         0.005778\n",
              "16  0.290638  0.269898  0.222084  ...  4.678856        22.0         0.010505\n",
              "17  0.262872  0.227500  0.214436  ...  4.815277        12.0         0.019260\n",
              "18  0.310540  0.245770  0.228410  ...  4.769965        15.0         0.015408\n",
              "19  0.253787  0.230512  0.219922  ...  4.578315        27.0         0.008560\n",
              "20  0.283236  0.240035  0.243000  ...  4.666841        23.0         0.010049\n",
              "21  0.249221  0.231004  0.211921  ...  4.857491        10.0         0.023112\n",
              "22  0.265260  0.227288  0.217763  ...  5.138516         1.0         0.231121\n",
              "23  0.279411  0.242117  0.231796  ...  4.513156        30.0         0.007704\n",
              "24  0.311441  0.225901  0.217392  ...  4.735011        17.0         0.013595\n",
              "25  0.277852  0.236534  0.222586  ...  4.556380        28.0         0.008254\n",
              "26  0.296777  0.234284  0.219085  ...  4.724288        19.0         0.012164\n",
              "27  0.266089  0.236261  0.205056  ...  5.087436         3.0         0.077040\n",
              "28  0.290046  0.225338  0.206737  ...  4.774212        13.0         0.017779\n",
              "29  0.301766  0.262840  0.226363  ...  4.450873        33.0         0.007004\n",
              "30  0.244258  0.215385  0.213587  ...  4.769735        16.0         0.014445\n",
              "31  0.276123  0.232157  0.213064  ...  4.971918         5.0         0.046224\n",
              "32  0.272228  0.222163  0.217547  ...  4.951308         7.0         0.033017\n",
              "33  0.306602  0.233451  0.227916  ...  4.545162        29.0         0.007970\n",
              "34  0.273408  0.253417  0.254691  ...  4.250535        37.0         0.006247\n",
              "35  0.361880  0.293221  0.293664  ...  4.666209        24.0         0.009630\n",
              "36  0.312099  0.228898  0.216683  ...  5.007771         4.0         0.057780\n",
              "37  0.246799  0.225134  0.223399  ...  4.649433        25.0         0.009245\n",
              "38  0.282973  0.232447  0.231867  ...  4.927086         9.0         0.025680\n",
              "39  0.331409  0.273990  0.235165  ...  4.082514        41.0         0.005637\n",
              "40  0.260229  0.245010  0.218930  ...  4.694107        21.0         0.011006\n",
              "41  0.262993  0.228721  0.212792  ...  5.096542         2.0         0.115560\n",
              "\n",
              "[42 rows x 21 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.20227106225987276, 156.0, 276.0, 1.0)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Génération :  7\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 205 hidden_dim : 314 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2596, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.39777703354567806\n",
            "Epoch 1 : Validation loss = 0.2525144452850024; Validation r = 0.45886098692536353\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1217, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7248060522076563\n",
            "Epoch 2 : Validation loss = 0.2570667453110218; Validation r = 0.49884754442439255\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0555, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8875778644390142\n",
            "Epoch 3 : Validation loss = 0.20951807511349518; Validation r = 0.53677289520967\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0265, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9484229363061932\n",
            "Epoch 4 : Validation loss = 0.20779798043270906; Validation r = 0.534957331130121\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0168, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9672999440500258\n",
            "Epoch 5 : Validation loss = 0.20943936333060265; Validation r = 0.5336731377372398\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 165 hidden_dim : 232 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2607, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.4032601722911662\n",
            "Epoch 1 : Validation loss = 0.26010130072633425; Validation r = 0.5071999843308428\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1246, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7268991058228628\n",
            "Epoch 2 : Validation loss = 0.2380629984041055; Validation r = 0.5083229502394968\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0617, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8747269646508514\n",
            "Epoch 3 : Validation loss = 0.22483584148188432; Validation r = 0.5294281550334118\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0319, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9379695637060004\n",
            "Epoch 4 : Validation loss = 0.2145110490421454; Validation r = 0.5383478331703435\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0189, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9632906528600341\n",
            "Epoch 5 : Validation loss = 0.21213119166592756; Validation r = 0.5471571643033851\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 100 hidden_dim : 118 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2747, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.4029079524474295\n",
            "Epoch 1 : Validation loss = 0.2804919682443142; Validation r = 0.4412630364397605\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1501, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6700184836149977\n",
            "Epoch 2 : Validation loss = 0.26323088618616264; Validation r = 0.4523081570207242\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0823, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8279585599217302\n",
            "Epoch 3 : Validation loss = 0.23657259990771612; Validation r = 0.4922718150075421\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9087288206967475\n",
            "Epoch 4 : Validation loss = 0.2240919541567564; Validation r = 0.5175484409426303\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0275, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9453918461704499\n",
            "Epoch 5 : Validation loss = 0.2269128209600846; Validation r = 0.49844174073442576\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 164 hidden_dim : 263 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2658, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.39074406100266523\n",
            "Epoch 1 : Validation loss = 0.26528354523082576; Validation r = 0.4783472843275212\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1265, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7234600866366989\n",
            "Epoch 2 : Validation loss = 0.24398348691562813; Validation r = 0.52185919690833\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0625, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8727081254453563\n",
            "Epoch 3 : Validation loss = 0.2104679697503646; Validation r = 0.5404401414034067\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0318, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9379437619124136\n",
            "Epoch 4 : Validation loss = 0.2080450624227524; Validation r = 0.5375675251639668\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0185, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9638305568887833\n",
            "Epoch 5 : Validation loss = 0.20945859532803296; Validation r = 0.551452595999903\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 100 hidden_dim : 352 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2773, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.33256819356850614\n",
            "Epoch 1 : Validation loss = 0.24684515309830507; Validation r = 0.38033684284845587\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1766, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.5519537557625438\n",
            "Epoch 2 : Validation loss = 0.2313760702808698; Validation r = 0.4349224146332081\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.1179, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.729214537620462\n",
            "Epoch 3 : Validation loss = 0.2103305019438267; Validation r = 0.498440089274181\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0672, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8639228495593118\n",
            "Epoch 4 : Validation loss = 0.2095654532313347; Validation r = 0.49260624381195717\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0375, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9295064804496231\n",
            "Epoch 5 : Validation loss = 0.21328602060675622; Validation r = 0.502366272360832\n",
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b8a30b43-8b59-41db-ab95-ce1970c0a6a5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>max_epoch</th>\n",
              "      <th>best_dev_loss</th>\n",
              "      <th>best_dev_loss_epoch</th>\n",
              "      <th>lastEpochImproved</th>\n",
              "      <th>embedding_dim</th>\n",
              "      <th>hidden_dim</th>\n",
              "      <th>num_lstm_layers</th>\n",
              "      <th>loss_decrease_perc_0</th>\n",
              "      <th>loss_decrease_perc_1</th>\n",
              "      <th>loss_decrease_perc_2</th>\n",
              "      <th>loss_decrease_perc_3</th>\n",
              "      <th>score_perf</th>\n",
              "      <th>score_pot</th>\n",
              "      <th>score</th>\n",
              "      <th>model_rank</th>\n",
              "      <th>proba_selection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.297629</td>\n",
              "      <td>0.256342</td>\n",
              "      <td>0.224900</td>\n",
              "      <td>0.221605</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.138718</td>\n",
              "      <td>0.122657</td>\n",
              "      <td>0.014653</td>\n",
              "      <td>0.044878</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>-0.009499</td>\n",
              "      <td>4.946564</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.028166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.276555</td>\n",
              "      <td>0.242013</td>\n",
              "      <td>0.211087</td>\n",
              "      <td>0.214948</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.124904</td>\n",
              "      <td>0.127787</td>\n",
              "      <td>-0.018291</td>\n",
              "      <td>0.031870</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>-0.006632</td>\n",
              "      <td>4.963635</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.037555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.296420</td>\n",
              "      <td>0.235832</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.233284</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>175</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.204399</td>\n",
              "      <td>0.034291</td>\n",
              "      <td>-0.024319</td>\n",
              "      <td>0.045231</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>-0.010074</td>\n",
              "      <td>4.702388</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.009797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.270336</td>\n",
              "      <td>0.227319</td>\n",
              "      <td>0.231308</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.223296</td>\n",
              "      <td>4</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>175</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.159127</td>\n",
              "      <td>-0.017549</td>\n",
              "      <td>0.089803</td>\n",
              "      <td>-0.060609</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.012760</td>\n",
              "      <td>4.478359</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0.006438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.285443</td>\n",
              "      <td>0.220161</td>\n",
              "      <td>0.226018</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.227897</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.228703</td>\n",
              "      <td>-0.026604</td>\n",
              "      <td>0.057750</td>\n",
              "      <td>-0.070109</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.014931</td>\n",
              "      <td>4.387952</td>\n",
              "      <td>39.0</td>\n",
              "      <td>0.005778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.295962</td>\n",
              "      <td>0.254919</td>\n",
              "      <td>0.248721</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>0.243503</td>\n",
              "      <td>4</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>0.138675</td>\n",
              "      <td>0.024316</td>\n",
              "      <td>0.056733</td>\n",
              "      <td>-0.037906</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>0.008893</td>\n",
              "      <td>4.106723</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.005121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.264929</td>\n",
              "      <td>0.224174</td>\n",
              "      <td>0.228173</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>0.218195</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>300</td>\n",
              "      <td>2</td>\n",
              "      <td>0.153832</td>\n",
              "      <td>-0.017838</td>\n",
              "      <td>0.057869</td>\n",
              "      <td>-0.015006</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>0.003226</td>\n",
              "      <td>4.583062</td>\n",
              "      <td>30.0</td>\n",
              "      <td>0.007511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.270235</td>\n",
              "      <td>0.228662</td>\n",
              "      <td>0.249313</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>0.211333</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>300</td>\n",
              "      <td>3</td>\n",
              "      <td>0.153839</td>\n",
              "      <td>-0.090309</td>\n",
              "      <td>0.163048</td>\n",
              "      <td>-0.012797</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>0.002670</td>\n",
              "      <td>4.731872</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.010730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.257634</td>\n",
              "      <td>0.223843</td>\n",
              "      <td>0.245183</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>0.223688</td>\n",
              "      <td>4</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.131159</td>\n",
              "      <td>-0.095335</td>\n",
              "      <td>0.104335</td>\n",
              "      <td>-0.018609</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>0.004087</td>\n",
              "      <td>4.470503</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.006259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.289596</td>\n",
              "      <td>0.240284</td>\n",
              "      <td>0.235281</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>0.247811</td>\n",
              "      <td>4</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>0.170279</td>\n",
              "      <td>0.020819</td>\n",
              "      <td>0.022889</td>\n",
              "      <td>-0.077928</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>0.017915</td>\n",
              "      <td>4.035327</td>\n",
              "      <td>47.0</td>\n",
              "      <td>0.004794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.254877</td>\n",
              "      <td>0.259745</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>0.240172</td>\n",
              "      <td>0.241364</td>\n",
              "      <td>4</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>300</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.019099</td>\n",
              "      <td>0.111744</td>\n",
              "      <td>-0.040966</td>\n",
              "      <td>-0.004965</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>0.001145</td>\n",
              "      <td>4.312846</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.005633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.306085</td>\n",
              "      <td>0.269204</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>0.240201</td>\n",
              "      <td>0.240892</td>\n",
              "      <td>4</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>300</td>\n",
              "      <td>3</td>\n",
              "      <td>0.120492</td>\n",
              "      <td>0.127428</td>\n",
              "      <td>-0.022564</td>\n",
              "      <td>-0.002880</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>0.000677</td>\n",
              "      <td>4.244900</td>\n",
              "      <td>43.0</td>\n",
              "      <td>0.005240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.258406</td>\n",
              "      <td>0.226609</td>\n",
              "      <td>0.231610</td>\n",
              "      <td>0.234645</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>4</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>219</td>\n",
              "      <td>272</td>\n",
              "      <td>2</td>\n",
              "      <td>0.123048</td>\n",
              "      <td>-0.022068</td>\n",
              "      <td>-0.013101</td>\n",
              "      <td>0.055185</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>-0.012234</td>\n",
              "      <td>4.774143</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.014083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.273785</td>\n",
              "      <td>0.234423</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>0.226703</td>\n",
              "      <td>0.218485</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>264</td>\n",
              "      <td>309</td>\n",
              "      <td>2</td>\n",
              "      <td>0.143770</td>\n",
              "      <td>0.086889</td>\n",
              "      <td>-0.059090</td>\n",
              "      <td>0.036249</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>-0.007759</td>\n",
              "      <td>4.847430</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.020484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.281174</td>\n",
              "      <td>0.258473</td>\n",
              "      <td>0.242390</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>0.233587</td>\n",
              "      <td>4</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>201</td>\n",
              "      <td>100</td>\n",
              "      <td>2</td>\n",
              "      <td>0.080737</td>\n",
              "      <td>0.062222</td>\n",
              "      <td>0.042879</td>\n",
              "      <td>-0.006853</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>0.001590</td>\n",
              "      <td>4.281065</td>\n",
              "      <td>41.0</td>\n",
              "      <td>0.005496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.276611</td>\n",
              "      <td>0.257501</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>0.235862</td>\n",
              "      <td>0.244586</td>\n",
              "      <td>4</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>352</td>\n",
              "      <td>240</td>\n",
              "      <td>3</td>\n",
              "      <td>0.069085</td>\n",
              "      <td>0.084159</td>\n",
              "      <td>-0.000138</td>\n",
              "      <td>-0.036985</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>0.008722</td>\n",
              "      <td>4.089106</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.005007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.290638</td>\n",
              "      <td>0.269898</td>\n",
              "      <td>0.222084</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>0.213727</td>\n",
              "      <td>4</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>113</td>\n",
              "      <td>2</td>\n",
              "      <td>0.071362</td>\n",
              "      <td>0.177156</td>\n",
              "      <td>0.054589</td>\n",
              "      <td>-0.017942</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>0.003767</td>\n",
              "      <td>4.678856</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.008666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.262872</td>\n",
              "      <td>0.227500</td>\n",
              "      <td>0.214436</td>\n",
              "      <td>0.205983</td>\n",
              "      <td>0.207672</td>\n",
              "      <td>4</td>\n",
              "      <td>0.205983</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>322</td>\n",
              "      <td>2</td>\n",
              "      <td>0.134558</td>\n",
              "      <td>0.057427</td>\n",
              "      <td>0.039419</td>\n",
              "      <td>-0.008203</td>\n",
              "      <td>0.205983</td>\n",
              "      <td>0.001690</td>\n",
              "      <td>4.815277</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.018777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.310540</td>\n",
              "      <td>0.245770</td>\n",
              "      <td>0.228410</td>\n",
              "      <td>0.235806</td>\n",
              "      <td>0.222341</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222341</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>145</td>\n",
              "      <td>2</td>\n",
              "      <td>0.208571</td>\n",
              "      <td>0.070637</td>\n",
              "      <td>-0.032380</td>\n",
              "      <td>0.057101</td>\n",
              "      <td>0.222341</td>\n",
              "      <td>-0.012696</td>\n",
              "      <td>4.769965</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.013255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.253787</td>\n",
              "      <td>0.230512</td>\n",
              "      <td>0.219922</td>\n",
              "      <td>0.214113</td>\n",
              "      <td>0.218421</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214113</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>254</td>\n",
              "      <td>2</td>\n",
              "      <td>0.091710</td>\n",
              "      <td>0.045945</td>\n",
              "      <td>0.026412</td>\n",
              "      <td>-0.020120</td>\n",
              "      <td>0.214113</td>\n",
              "      <td>0.004308</td>\n",
              "      <td>4.578315</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.007269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.283236</td>\n",
              "      <td>0.240035</td>\n",
              "      <td>0.243000</td>\n",
              "      <td>0.227447</td>\n",
              "      <td>0.220764</td>\n",
              "      <td>4</td>\n",
              "      <td>0.220764</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>218</td>\n",
              "      <td>110</td>\n",
              "      <td>1</td>\n",
              "      <td>0.152528</td>\n",
              "      <td>-0.012351</td>\n",
              "      <td>0.064003</td>\n",
              "      <td>0.029382</td>\n",
              "      <td>0.220764</td>\n",
              "      <td>-0.006486</td>\n",
              "      <td>4.666841</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.008346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.249221</td>\n",
              "      <td>0.231004</td>\n",
              "      <td>0.211921</td>\n",
              "      <td>0.218319</td>\n",
              "      <td>0.212083</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211921</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>131</td>\n",
              "      <td>292</td>\n",
              "      <td>1</td>\n",
              "      <td>0.073096</td>\n",
              "      <td>0.082608</td>\n",
              "      <td>-0.030190</td>\n",
              "      <td>0.028564</td>\n",
              "      <td>0.211921</td>\n",
              "      <td>-0.006053</td>\n",
              "      <td>4.857491</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.022533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.265260</td>\n",
              "      <td>0.227288</td>\n",
              "      <td>0.217763</td>\n",
              "      <td>0.210235</td>\n",
              "      <td>0.202271</td>\n",
              "      <td>4</td>\n",
              "      <td>0.202271</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>156</td>\n",
              "      <td>276</td>\n",
              "      <td>1</td>\n",
              "      <td>0.143151</td>\n",
              "      <td>0.041905</td>\n",
              "      <td>0.034571</td>\n",
              "      <td>0.037882</td>\n",
              "      <td>0.202271</td>\n",
              "      <td>-0.007662</td>\n",
              "      <td>5.138516</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.225329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.279411</td>\n",
              "      <td>0.242117</td>\n",
              "      <td>0.231796</td>\n",
              "      <td>0.215313</td>\n",
              "      <td>0.221574</td>\n",
              "      <td>4</td>\n",
              "      <td>0.215313</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>151</td>\n",
              "      <td>3</td>\n",
              "      <td>0.133472</td>\n",
              "      <td>0.042627</td>\n",
              "      <td>0.071112</td>\n",
              "      <td>-0.029081</td>\n",
              "      <td>0.215313</td>\n",
              "      <td>0.006262</td>\n",
              "      <td>4.513156</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.006627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.311441</td>\n",
              "      <td>0.225901</td>\n",
              "      <td>0.217392</td>\n",
              "      <td>0.216373</td>\n",
              "      <td>0.213767</td>\n",
              "      <td>4</td>\n",
              "      <td>0.213767</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>120</td>\n",
              "      <td>122</td>\n",
              "      <td>2</td>\n",
              "      <td>0.274660</td>\n",
              "      <td>0.037664</td>\n",
              "      <td>0.004686</td>\n",
              "      <td>0.012044</td>\n",
              "      <td>0.213767</td>\n",
              "      <td>-0.002575</td>\n",
              "      <td>4.735011</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.011266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.277852</td>\n",
              "      <td>0.236534</td>\n",
              "      <td>0.222586</td>\n",
              "      <td>0.213596</td>\n",
              "      <td>0.219472</td>\n",
              "      <td>4</td>\n",
              "      <td>0.213596</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>103</td>\n",
              "      <td>124</td>\n",
              "      <td>2</td>\n",
              "      <td>0.148704</td>\n",
              "      <td>0.058969</td>\n",
              "      <td>0.040390</td>\n",
              "      <td>-0.027513</td>\n",
              "      <td>0.213596</td>\n",
              "      <td>0.005877</td>\n",
              "      <td>4.556380</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.007042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.296777</td>\n",
              "      <td>0.234284</td>\n",
              "      <td>0.219085</td>\n",
              "      <td>0.209900</td>\n",
              "      <td>0.211672</td>\n",
              "      <td>4</td>\n",
              "      <td>0.209900</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>197</td>\n",
              "      <td>134</td>\n",
              "      <td>2</td>\n",
              "      <td>0.210572</td>\n",
              "      <td>0.064874</td>\n",
              "      <td>0.041928</td>\n",
              "      <td>-0.008445</td>\n",
              "      <td>0.209900</td>\n",
              "      <td>0.001773</td>\n",
              "      <td>4.724288</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.010242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.266089</td>\n",
              "      <td>0.236261</td>\n",
              "      <td>0.205056</td>\n",
              "      <td>0.204831</td>\n",
              "      <td>0.200654</td>\n",
              "      <td>4</td>\n",
              "      <td>0.200654</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>127</td>\n",
              "      <td>305</td>\n",
              "      <td>1</td>\n",
              "      <td>0.112098</td>\n",
              "      <td>0.132078</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>0.020392</td>\n",
              "      <td>0.200654</td>\n",
              "      <td>-0.004092</td>\n",
              "      <td>5.087436</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.075110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.290046</td>\n",
              "      <td>0.225338</td>\n",
              "      <td>0.206737</td>\n",
              "      <td>0.207275</td>\n",
              "      <td>0.210004</td>\n",
              "      <td>4</td>\n",
              "      <td>0.206737</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>204</td>\n",
              "      <td>292</td>\n",
              "      <td>1</td>\n",
              "      <td>0.223095</td>\n",
              "      <td>0.082547</td>\n",
              "      <td>-0.002602</td>\n",
              "      <td>-0.013163</td>\n",
              "      <td>0.206737</td>\n",
              "      <td>0.002721</td>\n",
              "      <td>4.774212</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.015022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.301766</td>\n",
              "      <td>0.262840</td>\n",
              "      <td>0.226363</td>\n",
              "      <td>0.218543</td>\n",
              "      <td>0.224675</td>\n",
              "      <td>4</td>\n",
              "      <td>0.218543</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>157</td>\n",
              "      <td>3</td>\n",
              "      <td>0.128994</td>\n",
              "      <td>0.138780</td>\n",
              "      <td>0.034548</td>\n",
              "      <td>-0.028059</td>\n",
              "      <td>0.218543</td>\n",
              "      <td>0.006132</td>\n",
              "      <td>4.450873</td>\n",
              "      <td>37.0</td>\n",
              "      <td>0.006090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.244258</td>\n",
              "      <td>0.215385</td>\n",
              "      <td>0.213587</td>\n",
              "      <td>0.212510</td>\n",
              "      <td>0.211078</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211078</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>149</td>\n",
              "      <td>295</td>\n",
              "      <td>1</td>\n",
              "      <td>0.118208</td>\n",
              "      <td>0.008346</td>\n",
              "      <td>0.005043</td>\n",
              "      <td>0.006740</td>\n",
              "      <td>0.211078</td>\n",
              "      <td>-0.001423</td>\n",
              "      <td>4.769735</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0.012518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.276123</td>\n",
              "      <td>0.232157</td>\n",
              "      <td>0.213064</td>\n",
              "      <td>0.213007</td>\n",
              "      <td>0.206983</td>\n",
              "      <td>4</td>\n",
              "      <td>0.206983</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>178</td>\n",
              "      <td>324</td>\n",
              "      <td>1</td>\n",
              "      <td>0.159224</td>\n",
              "      <td>0.082243</td>\n",
              "      <td>0.000265</td>\n",
              "      <td>0.028281</td>\n",
              "      <td>0.206983</td>\n",
              "      <td>-0.005854</td>\n",
              "      <td>4.971918</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.045066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.272228</td>\n",
              "      <td>0.222163</td>\n",
              "      <td>0.217547</td>\n",
              "      <td>0.213297</td>\n",
              "      <td>0.207554</td>\n",
              "      <td>4</td>\n",
              "      <td>0.207554</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>163</td>\n",
              "      <td>290</td>\n",
              "      <td>1</td>\n",
              "      <td>0.183909</td>\n",
              "      <td>0.020781</td>\n",
              "      <td>0.019536</td>\n",
              "      <td>0.026921</td>\n",
              "      <td>0.207554</td>\n",
              "      <td>-0.005588</td>\n",
              "      <td>4.951308</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.032190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.306602</td>\n",
              "      <td>0.233451</td>\n",
              "      <td>0.227916</td>\n",
              "      <td>0.222533</td>\n",
              "      <td>0.221270</td>\n",
              "      <td>4</td>\n",
              "      <td>0.221270</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>156</td>\n",
              "      <td>144</td>\n",
              "      <td>2</td>\n",
              "      <td>0.238586</td>\n",
              "      <td>0.023711</td>\n",
              "      <td>0.023618</td>\n",
              "      <td>0.005676</td>\n",
              "      <td>0.221270</td>\n",
              "      <td>-0.001256</td>\n",
              "      <td>4.545162</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.006828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.273408</td>\n",
              "      <td>0.253417</td>\n",
              "      <td>0.254691</td>\n",
              "      <td>0.235056</td>\n",
              "      <td>0.235265</td>\n",
              "      <td>4</td>\n",
              "      <td>0.235056</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>374</td>\n",
              "      <td>159</td>\n",
              "      <td>3</td>\n",
              "      <td>0.073120</td>\n",
              "      <td>-0.005029</td>\n",
              "      <td>0.077093</td>\n",
              "      <td>-0.000885</td>\n",
              "      <td>0.235056</td>\n",
              "      <td>0.000208</td>\n",
              "      <td>4.250535</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0.005365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.361880</td>\n",
              "      <td>0.293221</td>\n",
              "      <td>0.293664</td>\n",
              "      <td>0.296554</td>\n",
              "      <td>0.252098</td>\n",
              "      <td>4</td>\n",
              "      <td>0.252098</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>157</td>\n",
              "      <td>4</td>\n",
              "      <td>0.189728</td>\n",
              "      <td>-0.001510</td>\n",
              "      <td>-0.009841</td>\n",
              "      <td>0.149907</td>\n",
              "      <td>0.252098</td>\n",
              "      <td>-0.037791</td>\n",
              "      <td>4.666209</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0.008047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.312099</td>\n",
              "      <td>0.228898</td>\n",
              "      <td>0.216683</td>\n",
              "      <td>0.208896</td>\n",
              "      <td>0.204241</td>\n",
              "      <td>4</td>\n",
              "      <td>0.204241</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>337</td>\n",
              "      <td>1</td>\n",
              "      <td>0.266585</td>\n",
              "      <td>0.053365</td>\n",
              "      <td>0.035936</td>\n",
              "      <td>0.022285</td>\n",
              "      <td>0.204241</td>\n",
              "      <td>-0.004552</td>\n",
              "      <td>5.007771</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.056332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.246799</td>\n",
              "      <td>0.225134</td>\n",
              "      <td>0.223399</td>\n",
              "      <td>0.220870</td>\n",
              "      <td>0.217956</td>\n",
              "      <td>4</td>\n",
              "      <td>0.217956</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>193</td>\n",
              "      <td>292</td>\n",
              "      <td>1</td>\n",
              "      <td>0.087781</td>\n",
              "      <td>0.007708</td>\n",
              "      <td>0.011322</td>\n",
              "      <td>0.013194</td>\n",
              "      <td>0.217956</td>\n",
              "      <td>-0.002876</td>\n",
              "      <td>4.649433</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0.007770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.282973</td>\n",
              "      <td>0.232447</td>\n",
              "      <td>0.231867</td>\n",
              "      <td>0.221023</td>\n",
              "      <td>0.211799</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211799</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>125</td>\n",
              "      <td>121</td>\n",
              "      <td>2</td>\n",
              "      <td>0.178554</td>\n",
              "      <td>0.002496</td>\n",
              "      <td>0.046768</td>\n",
              "      <td>0.041733</td>\n",
              "      <td>0.211799</td>\n",
              "      <td>-0.008839</td>\n",
              "      <td>4.927086</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.025037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.331409</td>\n",
              "      <td>0.273990</td>\n",
              "      <td>0.235165</td>\n",
              "      <td>0.243013</td>\n",
              "      <td>0.253121</td>\n",
              "      <td>4</td>\n",
              "      <td>0.235165</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>309</td>\n",
              "      <td>289</td>\n",
              "      <td>3</td>\n",
              "      <td>0.173257</td>\n",
              "      <td>0.141700</td>\n",
              "      <td>-0.033371</td>\n",
              "      <td>-0.041595</td>\n",
              "      <td>0.235165</td>\n",
              "      <td>0.009782</td>\n",
              "      <td>4.082514</td>\n",
              "      <td>46.0</td>\n",
              "      <td>0.004898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.260229</td>\n",
              "      <td>0.245010</td>\n",
              "      <td>0.218930</td>\n",
              "      <td>0.212456</td>\n",
              "      <td>0.213033</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212456</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>115</td>\n",
              "      <td>262</td>\n",
              "      <td>1</td>\n",
              "      <td>0.058484</td>\n",
              "      <td>0.106445</td>\n",
              "      <td>0.029571</td>\n",
              "      <td>-0.002716</td>\n",
              "      <td>0.212456</td>\n",
              "      <td>0.000577</td>\n",
              "      <td>4.694107</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.009389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.262993</td>\n",
              "      <td>0.228721</td>\n",
              "      <td>0.212792</td>\n",
              "      <td>0.211318</td>\n",
              "      <td>0.203624</td>\n",
              "      <td>4</td>\n",
              "      <td>0.203624</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>181</td>\n",
              "      <td>275</td>\n",
              "      <td>1</td>\n",
              "      <td>0.130313</td>\n",
              "      <td>0.069647</td>\n",
              "      <td>0.006927</td>\n",
              "      <td>0.036405</td>\n",
              "      <td>0.203624</td>\n",
              "      <td>-0.007413</td>\n",
              "      <td>5.096542</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.112664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.252514</td>\n",
              "      <td>0.257067</td>\n",
              "      <td>0.209518</td>\n",
              "      <td>0.207798</td>\n",
              "      <td>0.209439</td>\n",
              "      <td>4</td>\n",
              "      <td>0.207798</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>205</td>\n",
              "      <td>314</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.018028</td>\n",
              "      <td>0.184966</td>\n",
              "      <td>0.008210</td>\n",
              "      <td>-0.007899</td>\n",
              "      <td>0.207798</td>\n",
              "      <td>0.001641</td>\n",
              "      <td>4.774652</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.017333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.260101</td>\n",
              "      <td>0.238063</td>\n",
              "      <td>0.224836</td>\n",
              "      <td>0.214511</td>\n",
              "      <td>0.212131</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212131</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>165</td>\n",
              "      <td>232</td>\n",
              "      <td>1</td>\n",
              "      <td>0.084730</td>\n",
              "      <td>0.055562</td>\n",
              "      <td>0.045921</td>\n",
              "      <td>0.011094</td>\n",
              "      <td>0.212131</td>\n",
              "      <td>-0.002353</td>\n",
              "      <td>4.766950</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0.011859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.280492</td>\n",
              "      <td>0.263231</td>\n",
              "      <td>0.236573</td>\n",
              "      <td>0.224092</td>\n",
              "      <td>0.226913</td>\n",
              "      <td>4</td>\n",
              "      <td>0.224092</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>118</td>\n",
              "      <td>2</td>\n",
              "      <td>0.061539</td>\n",
              "      <td>0.101273</td>\n",
              "      <td>0.052756</td>\n",
              "      <td>-0.012588</td>\n",
              "      <td>0.224092</td>\n",
              "      <td>0.002821</td>\n",
              "      <td>4.406979</td>\n",
              "      <td>38.0</td>\n",
              "      <td>0.005930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.265284</td>\n",
              "      <td>0.243983</td>\n",
              "      <td>0.210468</td>\n",
              "      <td>0.208045</td>\n",
              "      <td>0.209459</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208045</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>164</td>\n",
              "      <td>263</td>\n",
              "      <td>1</td>\n",
              "      <td>0.080292</td>\n",
              "      <td>0.137368</td>\n",
              "      <td>0.011512</td>\n",
              "      <td>-0.006794</td>\n",
              "      <td>0.208045</td>\n",
              "      <td>0.001414</td>\n",
              "      <td>4.774213</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.016095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.246845</td>\n",
              "      <td>0.231376</td>\n",
              "      <td>0.210331</td>\n",
              "      <td>0.209565</td>\n",
              "      <td>0.213286</td>\n",
              "      <td>4</td>\n",
              "      <td>0.209565</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>352</td>\n",
              "      <td>2</td>\n",
              "      <td>0.062667</td>\n",
              "      <td>0.090958</td>\n",
              "      <td>0.003637</td>\n",
              "      <td>-0.017754</td>\n",
              "      <td>0.209565</td>\n",
              "      <td>0.003721</td>\n",
              "      <td>4.688540</td>\n",
              "      <td>25.0</td>\n",
              "      <td>0.009013</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b8a30b43-8b59-41db-ab95-ce1970c0a6a5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b8a30b43-8b59-41db-ab95-ce1970c0a6a5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b8a30b43-8b59-41db-ab95-ce1970c0a6a5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           0         1         2  ...     score  model_rank  proba_selection\n",
              "0   0.297629  0.256342  0.224900  ...  4.946564         8.0         0.028166\n",
              "1   0.276555  0.242013  0.211087  ...  4.963635         6.0         0.037555\n",
              "2   0.296420  0.235832  0.227745  ...  4.702388        23.0         0.009797\n",
              "3   0.270336  0.227319  0.231308  ...  4.478359        35.0         0.006438\n",
              "4   0.285443  0.220161  0.226018  ...  4.387952        39.0         0.005778\n",
              "5   0.295962  0.254919  0.248721  ...  4.106723        44.0         0.005121\n",
              "6   0.264929  0.224174  0.228173  ...  4.583062        30.0         0.007511\n",
              "7   0.270235  0.228662  0.249313  ...  4.731872        21.0         0.010730\n",
              "8   0.257634  0.223843  0.245183  ...  4.470503        36.0         0.006259\n",
              "9   0.289596  0.240284  0.235281  ...  4.035327        47.0         0.004794\n",
              "10  0.254877  0.259745  0.230720  ...  4.312846        40.0         0.005633\n",
              "11  0.306085  0.269204  0.234900  ...  4.244900        43.0         0.005240\n",
              "12  0.258406  0.226609  0.231610  ...  4.774143        16.0         0.014083\n",
              "13  0.273785  0.234423  0.214054  ...  4.847430        11.0         0.020484\n",
              "14  0.281174  0.258473  0.242390  ...  4.281065        41.0         0.005496\n",
              "15  0.276611  0.257501  0.235830  ...  4.089106        45.0         0.005007\n",
              "16  0.290638  0.269898  0.222084  ...  4.678856        26.0         0.008666\n",
              "17  0.262872  0.227500  0.214436  ...  4.815277        12.0         0.018777\n",
              "18  0.310540  0.245770  0.228410  ...  4.769965        17.0         0.013255\n",
              "19  0.253787  0.230512  0.219922  ...  4.578315        31.0         0.007269\n",
              "20  0.283236  0.240035  0.243000  ...  4.666841        27.0         0.008346\n",
              "21  0.249221  0.231004  0.211921  ...  4.857491        10.0         0.022533\n",
              "22  0.265260  0.227288  0.217763  ...  5.138516         1.0         0.225329\n",
              "23  0.279411  0.242117  0.231796  ...  4.513156        34.0         0.006627\n",
              "24  0.311441  0.225901  0.217392  ...  4.735011        20.0         0.011266\n",
              "25  0.277852  0.236534  0.222586  ...  4.556380        32.0         0.007042\n",
              "26  0.296777  0.234284  0.219085  ...  4.724288        22.0         0.010242\n",
              "27  0.266089  0.236261  0.205056  ...  5.087436         3.0         0.075110\n",
              "28  0.290046  0.225338  0.206737  ...  4.774212        15.0         0.015022\n",
              "29  0.301766  0.262840  0.226363  ...  4.450873        37.0         0.006090\n",
              "30  0.244258  0.215385  0.213587  ...  4.769735        18.0         0.012518\n",
              "31  0.276123  0.232157  0.213064  ...  4.971918         5.0         0.045066\n",
              "32  0.272228  0.222163  0.217547  ...  4.951308         7.0         0.032190\n",
              "33  0.306602  0.233451  0.227916  ...  4.545162        33.0         0.006828\n",
              "34  0.273408  0.253417  0.254691  ...  4.250535        42.0         0.005365\n",
              "35  0.361880  0.293221  0.293664  ...  4.666209        28.0         0.008047\n",
              "36  0.312099  0.228898  0.216683  ...  5.007771         4.0         0.056332\n",
              "37  0.246799  0.225134  0.223399  ...  4.649433        29.0         0.007770\n",
              "38  0.282973  0.232447  0.231867  ...  4.927086         9.0         0.025037\n",
              "39  0.331409  0.273990  0.235165  ...  4.082514        46.0         0.004898\n",
              "40  0.260229  0.245010  0.218930  ...  4.694107        24.0         0.009389\n",
              "41  0.262993  0.228721  0.212792  ...  5.096542         2.0         0.112664\n",
              "42  0.252514  0.257067  0.209518  ...  4.774652        13.0         0.017333\n",
              "43  0.260101  0.238063  0.224836  ...  4.766950        19.0         0.011859\n",
              "44  0.280492  0.263231  0.236573  ...  4.406979        38.0         0.005930\n",
              "45  0.265284  0.243983  0.210468  ...  4.774213        14.0         0.016095\n",
              "46  0.246845  0.231376  0.210331  ...  4.688540        25.0         0.009013\n",
              "\n",
              "[47 rows x 21 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.20227106225987276, 156.0, 276.0, 1.0)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Génération :  8\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 224 hidden_dim : 314 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2678, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.37494513939764473\n",
            "Epoch 1 : Validation loss = 0.27041639300684134; Validation r = 0.42361089903347954\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1435, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6601888256416567\n",
            "Epoch 2 : Validation loss = 0.23205114292601745; Validation r = 0.48717164699318255\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0728, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8496568581633168\n",
            "Epoch 3 : Validation loss = 0.2189780080070098; Validation r = 0.5082404284215796\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0367, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9276535296780558\n",
            "Epoch 4 : Validation loss = 0.21504682352145513; Validation r = 0.5055028479843775\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0212, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9571668915002651\n",
            "Epoch 5 : Validation loss = 0.21822081332405407; Validation r = 0.5168568525932148\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 100 hidden_dim : 287 num_lstm_layers : 3 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.3262, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.23234444965111\n",
            "Epoch 1 : Validation loss = 0.2896426728616158; Validation r = 0.28555566847478164\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.2419, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3456004630320078\n",
            "Epoch 2 : Validation loss = 0.27352926035722097; Validation r = 0.2901320129472528\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.2053, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.4360932531380869\n",
            "Epoch 3 : Validation loss = 0.25801325030624866; Validation r = 0.3660505815912191\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.1593, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.5899213719409167\n",
            "Epoch 4 : Validation loss = 0.24857099714378517; Validation r = 0.36689465722137704\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.1242, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7021834324304789\n",
            "Epoch 5 : Validation loss = 0.24204989795883497; Validation r = 0.40713429903658443\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 119 hidden_dim : 263 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2826, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3452722974401425\n",
            "Epoch 1 : Validation loss = 0.2657428249716759; Validation r = 0.4212546433490142\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1584, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6184571292474806\n",
            "Epoch 2 : Validation loss = 0.2124649129807949; Validation r = 0.5278409811489819\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0931, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7981611380671579\n",
            "Epoch 3 : Validation loss = 0.20806183976431689; Validation r = 0.5089952835109703\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0518, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8973997766322996\n",
            "Epoch 4 : Validation loss = 0.21218426637351512; Validation r = 0.51229236978228\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0285, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.945475613246182\n",
            "Epoch 5 : Validation loss = 0.2155198390285174; Validation r = 0.4997072675938088\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 117 hidden_dim : 279 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2769, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3584495289055683\n",
            "Epoch 1 : Validation loss = 0.2786273763825496; Validation r = 0.40619566711546623\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1591, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6181858930434768\n",
            "Epoch 2 : Validation loss = 0.25386946834623814; Validation r = 0.45326665226455587\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0975, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7872888187386514\n",
            "Epoch 3 : Validation loss = 0.20568056007226307; Validation r = 0.5182075837752222\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0523, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8987960205604986\n",
            "Epoch 4 : Validation loss = 0.2096092672397693; Validation r = 0.4971149539517889\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0293, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.943628902408872\n",
            "Epoch 5 : Validation loss = 0.21819750977059205; Validation r = 0.498854789071613\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 154 hidden_dim : 260 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2702, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3712491101060499\n",
            "Epoch 1 : Validation loss = 0.2548874123642842; Validation r = 0.4025184153693504\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1457, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.65710444553791\n",
            "Epoch 2 : Validation loss = 0.22964394291241963; Validation r = 0.4930749959263917\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0785, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8349248127390221\n",
            "Epoch 3 : Validation loss = 0.21924259904772042; Validation r = 0.5022318602585827\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0421, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9166154331905713\n",
            "Epoch 4 : Validation loss = 0.2087271402279536; Validation r = 0.5304139857263771\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0241, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9518261522837943\n",
            "Epoch 5 : Validation loss = 0.21104614349702994; Validation r = 0.5168079383290852\n",
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4b3c8311-9b58-415f-8ea4-da62170979ae\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>max_epoch</th>\n",
              "      <th>best_dev_loss</th>\n",
              "      <th>best_dev_loss_epoch</th>\n",
              "      <th>lastEpochImproved</th>\n",
              "      <th>embedding_dim</th>\n",
              "      <th>hidden_dim</th>\n",
              "      <th>num_lstm_layers</th>\n",
              "      <th>loss_decrease_perc_0</th>\n",
              "      <th>loss_decrease_perc_1</th>\n",
              "      <th>loss_decrease_perc_2</th>\n",
              "      <th>loss_decrease_perc_3</th>\n",
              "      <th>score_perf</th>\n",
              "      <th>score_pot</th>\n",
              "      <th>score</th>\n",
              "      <th>model_rank</th>\n",
              "      <th>proba_selection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.297629</td>\n",
              "      <td>0.256342</td>\n",
              "      <td>0.224900</td>\n",
              "      <td>0.221605</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.138718</td>\n",
              "      <td>0.122657</td>\n",
              "      <td>0.014653</td>\n",
              "      <td>0.044878</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>-0.009499</td>\n",
              "      <td>4.946564</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.027545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.276555</td>\n",
              "      <td>0.242013</td>\n",
              "      <td>0.211087</td>\n",
              "      <td>0.214948</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.124904</td>\n",
              "      <td>0.127787</td>\n",
              "      <td>-0.018291</td>\n",
              "      <td>0.031870</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>-0.006632</td>\n",
              "      <td>4.963635</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.036727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.296420</td>\n",
              "      <td>0.235832</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.233284</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>175</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.204399</td>\n",
              "      <td>0.034291</td>\n",
              "      <td>-0.024319</td>\n",
              "      <td>0.045231</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>-0.010074</td>\n",
              "      <td>4.702388</td>\n",
              "      <td>25.0</td>\n",
              "      <td>0.008814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.270336</td>\n",
              "      <td>0.227319</td>\n",
              "      <td>0.231308</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.223296</td>\n",
              "      <td>4</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>175</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.159127</td>\n",
              "      <td>-0.017549</td>\n",
              "      <td>0.089803</td>\n",
              "      <td>-0.060609</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.012760</td>\n",
              "      <td>4.478359</td>\n",
              "      <td>39.0</td>\n",
              "      <td>0.005650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.285443</td>\n",
              "      <td>0.220161</td>\n",
              "      <td>0.226018</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.227897</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.228703</td>\n",
              "      <td>-0.026604</td>\n",
              "      <td>0.057750</td>\n",
              "      <td>-0.070109</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.014931</td>\n",
              "      <td>4.387952</td>\n",
              "      <td>43.0</td>\n",
              "      <td>0.005125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.295962</td>\n",
              "      <td>0.254919</td>\n",
              "      <td>0.248721</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>0.243503</td>\n",
              "      <td>4</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>0.138675</td>\n",
              "      <td>0.024316</td>\n",
              "      <td>0.056733</td>\n",
              "      <td>-0.037906</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>0.008893</td>\n",
              "      <td>4.106723</td>\n",
              "      <td>49.0</td>\n",
              "      <td>0.004497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.264929</td>\n",
              "      <td>0.224174</td>\n",
              "      <td>0.228173</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>0.218195</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>300</td>\n",
              "      <td>2</td>\n",
              "      <td>0.153832</td>\n",
              "      <td>-0.017838</td>\n",
              "      <td>0.057869</td>\n",
              "      <td>-0.015006</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>0.003226</td>\n",
              "      <td>4.583062</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.006678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.270235</td>\n",
              "      <td>0.228662</td>\n",
              "      <td>0.249313</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>0.211333</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>300</td>\n",
              "      <td>3</td>\n",
              "      <td>0.153839</td>\n",
              "      <td>-0.090309</td>\n",
              "      <td>0.163048</td>\n",
              "      <td>-0.012797</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>0.002670</td>\n",
              "      <td>4.731872</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.009581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.257634</td>\n",
              "      <td>0.223843</td>\n",
              "      <td>0.245183</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>0.223688</td>\n",
              "      <td>4</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.131159</td>\n",
              "      <td>-0.095335</td>\n",
              "      <td>0.104335</td>\n",
              "      <td>-0.018609</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>0.004087</td>\n",
              "      <td>4.470503</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.005509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.289596</td>\n",
              "      <td>0.240284</td>\n",
              "      <td>0.235281</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>0.247811</td>\n",
              "      <td>4</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>0.170279</td>\n",
              "      <td>0.020819</td>\n",
              "      <td>0.022889</td>\n",
              "      <td>-0.077928</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>0.017915</td>\n",
              "      <td>4.035327</td>\n",
              "      <td>52.0</td>\n",
              "      <td>0.004238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.254877</td>\n",
              "      <td>0.259745</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>0.240172</td>\n",
              "      <td>0.241364</td>\n",
              "      <td>4</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>300</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.019099</td>\n",
              "      <td>0.111744</td>\n",
              "      <td>-0.040966</td>\n",
              "      <td>-0.004965</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>0.001145</td>\n",
              "      <td>4.312846</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.005008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.306085</td>\n",
              "      <td>0.269204</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>0.240201</td>\n",
              "      <td>0.240892</td>\n",
              "      <td>4</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>300</td>\n",
              "      <td>3</td>\n",
              "      <td>0.120492</td>\n",
              "      <td>0.127428</td>\n",
              "      <td>-0.022564</td>\n",
              "      <td>-0.002880</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>0.000677</td>\n",
              "      <td>4.244900</td>\n",
              "      <td>47.0</td>\n",
              "      <td>0.004688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.258406</td>\n",
              "      <td>0.226609</td>\n",
              "      <td>0.231610</td>\n",
              "      <td>0.234645</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>4</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>219</td>\n",
              "      <td>272</td>\n",
              "      <td>2</td>\n",
              "      <td>0.123048</td>\n",
              "      <td>-0.022068</td>\n",
              "      <td>-0.013101</td>\n",
              "      <td>0.055185</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>-0.012234</td>\n",
              "      <td>4.774143</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.013772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.273785</td>\n",
              "      <td>0.234423</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>0.226703</td>\n",
              "      <td>0.218485</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>264</td>\n",
              "      <td>309</td>\n",
              "      <td>2</td>\n",
              "      <td>0.143770</td>\n",
              "      <td>0.086889</td>\n",
              "      <td>-0.059090</td>\n",
              "      <td>0.036249</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>-0.007759</td>\n",
              "      <td>4.847430</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.020033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.281174</td>\n",
              "      <td>0.258473</td>\n",
              "      <td>0.242390</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>0.233587</td>\n",
              "      <td>4</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>201</td>\n",
              "      <td>100</td>\n",
              "      <td>2</td>\n",
              "      <td>0.080737</td>\n",
              "      <td>0.062222</td>\n",
              "      <td>0.042879</td>\n",
              "      <td>-0.006853</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>0.001590</td>\n",
              "      <td>4.281065</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.004897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.276611</td>\n",
              "      <td>0.257501</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>0.235862</td>\n",
              "      <td>0.244586</td>\n",
              "      <td>4</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>352</td>\n",
              "      <td>240</td>\n",
              "      <td>3</td>\n",
              "      <td>0.069085</td>\n",
              "      <td>0.084159</td>\n",
              "      <td>-0.000138</td>\n",
              "      <td>-0.036985</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>0.008722</td>\n",
              "      <td>4.089106</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.004407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.290638</td>\n",
              "      <td>0.269898</td>\n",
              "      <td>0.222084</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>0.213727</td>\n",
              "      <td>4</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>113</td>\n",
              "      <td>2</td>\n",
              "      <td>0.071362</td>\n",
              "      <td>0.177156</td>\n",
              "      <td>0.054589</td>\n",
              "      <td>-0.017942</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>0.003767</td>\n",
              "      <td>4.678856</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0.007870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.262872</td>\n",
              "      <td>0.227500</td>\n",
              "      <td>0.214436</td>\n",
              "      <td>0.205983</td>\n",
              "      <td>0.207672</td>\n",
              "      <td>4</td>\n",
              "      <td>0.205983</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>322</td>\n",
              "      <td>2</td>\n",
              "      <td>0.134558</td>\n",
              "      <td>0.057427</td>\n",
              "      <td>0.039419</td>\n",
              "      <td>-0.008203</td>\n",
              "      <td>0.205983</td>\n",
              "      <td>0.001690</td>\n",
              "      <td>4.815277</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.018363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.310540</td>\n",
              "      <td>0.245770</td>\n",
              "      <td>0.228410</td>\n",
              "      <td>0.235806</td>\n",
              "      <td>0.222341</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222341</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>145</td>\n",
              "      <td>2</td>\n",
              "      <td>0.208571</td>\n",
              "      <td>0.070637</td>\n",
              "      <td>-0.032380</td>\n",
              "      <td>0.057101</td>\n",
              "      <td>0.222341</td>\n",
              "      <td>-0.012696</td>\n",
              "      <td>4.769965</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.012962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.253787</td>\n",
              "      <td>0.230512</td>\n",
              "      <td>0.219922</td>\n",
              "      <td>0.214113</td>\n",
              "      <td>0.218421</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214113</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>254</td>\n",
              "      <td>2</td>\n",
              "      <td>0.091710</td>\n",
              "      <td>0.045945</td>\n",
              "      <td>0.026412</td>\n",
              "      <td>-0.020120</td>\n",
              "      <td>0.214113</td>\n",
              "      <td>0.004308</td>\n",
              "      <td>4.578315</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0.006296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.283236</td>\n",
              "      <td>0.240035</td>\n",
              "      <td>0.243000</td>\n",
              "      <td>0.227447</td>\n",
              "      <td>0.220764</td>\n",
              "      <td>4</td>\n",
              "      <td>0.220764</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>218</td>\n",
              "      <td>110</td>\n",
              "      <td>1</td>\n",
              "      <td>0.152528</td>\n",
              "      <td>-0.012351</td>\n",
              "      <td>0.064003</td>\n",
              "      <td>0.029382</td>\n",
              "      <td>0.220764</td>\n",
              "      <td>-0.006486</td>\n",
              "      <td>4.666841</td>\n",
              "      <td>30.0</td>\n",
              "      <td>0.007345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.249221</td>\n",
              "      <td>0.231004</td>\n",
              "      <td>0.211921</td>\n",
              "      <td>0.218319</td>\n",
              "      <td>0.212083</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211921</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>131</td>\n",
              "      <td>292</td>\n",
              "      <td>1</td>\n",
              "      <td>0.073096</td>\n",
              "      <td>0.082608</td>\n",
              "      <td>-0.030190</td>\n",
              "      <td>0.028564</td>\n",
              "      <td>0.211921</td>\n",
              "      <td>-0.006053</td>\n",
              "      <td>4.857491</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.022036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.265260</td>\n",
              "      <td>0.227288</td>\n",
              "      <td>0.217763</td>\n",
              "      <td>0.210235</td>\n",
              "      <td>0.202271</td>\n",
              "      <td>4</td>\n",
              "      <td>0.202271</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>156</td>\n",
              "      <td>276</td>\n",
              "      <td>1</td>\n",
              "      <td>0.143151</td>\n",
              "      <td>0.041905</td>\n",
              "      <td>0.034571</td>\n",
              "      <td>0.037882</td>\n",
              "      <td>0.202271</td>\n",
              "      <td>-0.007662</td>\n",
              "      <td>5.138516</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.220359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.279411</td>\n",
              "      <td>0.242117</td>\n",
              "      <td>0.231796</td>\n",
              "      <td>0.215313</td>\n",
              "      <td>0.221574</td>\n",
              "      <td>4</td>\n",
              "      <td>0.215313</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>151</td>\n",
              "      <td>3</td>\n",
              "      <td>0.133472</td>\n",
              "      <td>0.042627</td>\n",
              "      <td>0.071112</td>\n",
              "      <td>-0.029081</td>\n",
              "      <td>0.215313</td>\n",
              "      <td>0.006262</td>\n",
              "      <td>4.513156</td>\n",
              "      <td>38.0</td>\n",
              "      <td>0.005799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.311441</td>\n",
              "      <td>0.225901</td>\n",
              "      <td>0.217392</td>\n",
              "      <td>0.216373</td>\n",
              "      <td>0.213767</td>\n",
              "      <td>4</td>\n",
              "      <td>0.213767</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>120</td>\n",
              "      <td>122</td>\n",
              "      <td>2</td>\n",
              "      <td>0.274660</td>\n",
              "      <td>0.037664</td>\n",
              "      <td>0.004686</td>\n",
              "      <td>0.012044</td>\n",
              "      <td>0.213767</td>\n",
              "      <td>-0.002575</td>\n",
              "      <td>4.735011</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.010493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.277852</td>\n",
              "      <td>0.236534</td>\n",
              "      <td>0.222586</td>\n",
              "      <td>0.213596</td>\n",
              "      <td>0.219472</td>\n",
              "      <td>4</td>\n",
              "      <td>0.213596</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>103</td>\n",
              "      <td>124</td>\n",
              "      <td>2</td>\n",
              "      <td>0.148704</td>\n",
              "      <td>0.058969</td>\n",
              "      <td>0.040390</td>\n",
              "      <td>-0.027513</td>\n",
              "      <td>0.213596</td>\n",
              "      <td>0.005877</td>\n",
              "      <td>4.556380</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.006121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.296777</td>\n",
              "      <td>0.234284</td>\n",
              "      <td>0.219085</td>\n",
              "      <td>0.209900</td>\n",
              "      <td>0.211672</td>\n",
              "      <td>4</td>\n",
              "      <td>0.209900</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>197</td>\n",
              "      <td>134</td>\n",
              "      <td>2</td>\n",
              "      <td>0.210572</td>\n",
              "      <td>0.064874</td>\n",
              "      <td>0.041928</td>\n",
              "      <td>-0.008445</td>\n",
              "      <td>0.209900</td>\n",
              "      <td>0.001773</td>\n",
              "      <td>4.724288</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.009182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.266089</td>\n",
              "      <td>0.236261</td>\n",
              "      <td>0.205056</td>\n",
              "      <td>0.204831</td>\n",
              "      <td>0.200654</td>\n",
              "      <td>4</td>\n",
              "      <td>0.200654</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>127</td>\n",
              "      <td>305</td>\n",
              "      <td>1</td>\n",
              "      <td>0.112098</td>\n",
              "      <td>0.132078</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>0.020392</td>\n",
              "      <td>0.200654</td>\n",
              "      <td>-0.004092</td>\n",
              "      <td>5.087436</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.073453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.290046</td>\n",
              "      <td>0.225338</td>\n",
              "      <td>0.206737</td>\n",
              "      <td>0.207275</td>\n",
              "      <td>0.210004</td>\n",
              "      <td>4</td>\n",
              "      <td>0.206737</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>204</td>\n",
              "      <td>292</td>\n",
              "      <td>1</td>\n",
              "      <td>0.223095</td>\n",
              "      <td>0.082547</td>\n",
              "      <td>-0.002602</td>\n",
              "      <td>-0.013163</td>\n",
              "      <td>0.206737</td>\n",
              "      <td>0.002721</td>\n",
              "      <td>4.774212</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.014691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.301766</td>\n",
              "      <td>0.262840</td>\n",
              "      <td>0.226363</td>\n",
              "      <td>0.218543</td>\n",
              "      <td>0.224675</td>\n",
              "      <td>4</td>\n",
              "      <td>0.218543</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>157</td>\n",
              "      <td>3</td>\n",
              "      <td>0.128994</td>\n",
              "      <td>0.138780</td>\n",
              "      <td>0.034548</td>\n",
              "      <td>-0.028059</td>\n",
              "      <td>0.218543</td>\n",
              "      <td>0.006132</td>\n",
              "      <td>4.450873</td>\n",
              "      <td>41.0</td>\n",
              "      <td>0.005375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.244258</td>\n",
              "      <td>0.215385</td>\n",
              "      <td>0.213587</td>\n",
              "      <td>0.212510</td>\n",
              "      <td>0.211078</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211078</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>149</td>\n",
              "      <td>295</td>\n",
              "      <td>1</td>\n",
              "      <td>0.118208</td>\n",
              "      <td>0.008346</td>\n",
              "      <td>0.005043</td>\n",
              "      <td>0.006740</td>\n",
              "      <td>0.211078</td>\n",
              "      <td>-0.001423</td>\n",
              "      <td>4.769735</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0.012242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.276123</td>\n",
              "      <td>0.232157</td>\n",
              "      <td>0.213064</td>\n",
              "      <td>0.213007</td>\n",
              "      <td>0.206983</td>\n",
              "      <td>4</td>\n",
              "      <td>0.206983</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>178</td>\n",
              "      <td>324</td>\n",
              "      <td>1</td>\n",
              "      <td>0.159224</td>\n",
              "      <td>0.082243</td>\n",
              "      <td>0.000265</td>\n",
              "      <td>0.028281</td>\n",
              "      <td>0.206983</td>\n",
              "      <td>-0.005854</td>\n",
              "      <td>4.971918</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.044072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.272228</td>\n",
              "      <td>0.222163</td>\n",
              "      <td>0.217547</td>\n",
              "      <td>0.213297</td>\n",
              "      <td>0.207554</td>\n",
              "      <td>4</td>\n",
              "      <td>0.207554</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>163</td>\n",
              "      <td>290</td>\n",
              "      <td>1</td>\n",
              "      <td>0.183909</td>\n",
              "      <td>0.020781</td>\n",
              "      <td>0.019536</td>\n",
              "      <td>0.026921</td>\n",
              "      <td>0.207554</td>\n",
              "      <td>-0.005588</td>\n",
              "      <td>4.951308</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.031480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.306602</td>\n",
              "      <td>0.233451</td>\n",
              "      <td>0.227916</td>\n",
              "      <td>0.222533</td>\n",
              "      <td>0.221270</td>\n",
              "      <td>4</td>\n",
              "      <td>0.221270</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>156</td>\n",
              "      <td>144</td>\n",
              "      <td>2</td>\n",
              "      <td>0.238586</td>\n",
              "      <td>0.023711</td>\n",
              "      <td>0.023618</td>\n",
              "      <td>0.005676</td>\n",
              "      <td>0.221270</td>\n",
              "      <td>-0.001256</td>\n",
              "      <td>4.545162</td>\n",
              "      <td>37.0</td>\n",
              "      <td>0.005956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.273408</td>\n",
              "      <td>0.253417</td>\n",
              "      <td>0.254691</td>\n",
              "      <td>0.235056</td>\n",
              "      <td>0.235265</td>\n",
              "      <td>4</td>\n",
              "      <td>0.235056</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>374</td>\n",
              "      <td>159</td>\n",
              "      <td>3</td>\n",
              "      <td>0.073120</td>\n",
              "      <td>-0.005029</td>\n",
              "      <td>0.077093</td>\n",
              "      <td>-0.000885</td>\n",
              "      <td>0.235056</td>\n",
              "      <td>0.000208</td>\n",
              "      <td>4.250535</td>\n",
              "      <td>46.0</td>\n",
              "      <td>0.004790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.361880</td>\n",
              "      <td>0.293221</td>\n",
              "      <td>0.293664</td>\n",
              "      <td>0.296554</td>\n",
              "      <td>0.252098</td>\n",
              "      <td>4</td>\n",
              "      <td>0.252098</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>157</td>\n",
              "      <td>4</td>\n",
              "      <td>0.189728</td>\n",
              "      <td>-0.001510</td>\n",
              "      <td>-0.009841</td>\n",
              "      <td>0.149907</td>\n",
              "      <td>0.252098</td>\n",
              "      <td>-0.037791</td>\n",
              "      <td>4.666209</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.007108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.312099</td>\n",
              "      <td>0.228898</td>\n",
              "      <td>0.216683</td>\n",
              "      <td>0.208896</td>\n",
              "      <td>0.204241</td>\n",
              "      <td>4</td>\n",
              "      <td>0.204241</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>337</td>\n",
              "      <td>1</td>\n",
              "      <td>0.266585</td>\n",
              "      <td>0.053365</td>\n",
              "      <td>0.035936</td>\n",
              "      <td>0.022285</td>\n",
              "      <td>0.204241</td>\n",
              "      <td>-0.004552</td>\n",
              "      <td>5.007771</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.055090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.246799</td>\n",
              "      <td>0.225134</td>\n",
              "      <td>0.223399</td>\n",
              "      <td>0.220870</td>\n",
              "      <td>0.217956</td>\n",
              "      <td>4</td>\n",
              "      <td>0.217956</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>193</td>\n",
              "      <td>292</td>\n",
              "      <td>1</td>\n",
              "      <td>0.087781</td>\n",
              "      <td>0.007708</td>\n",
              "      <td>0.011322</td>\n",
              "      <td>0.013194</td>\n",
              "      <td>0.217956</td>\n",
              "      <td>-0.002876</td>\n",
              "      <td>4.649433</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.006886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.282973</td>\n",
              "      <td>0.232447</td>\n",
              "      <td>0.231867</td>\n",
              "      <td>0.221023</td>\n",
              "      <td>0.211799</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211799</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>125</td>\n",
              "      <td>121</td>\n",
              "      <td>2</td>\n",
              "      <td>0.178554</td>\n",
              "      <td>0.002496</td>\n",
              "      <td>0.046768</td>\n",
              "      <td>0.041733</td>\n",
              "      <td>0.211799</td>\n",
              "      <td>-0.008839</td>\n",
              "      <td>4.927086</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.024484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.331409</td>\n",
              "      <td>0.273990</td>\n",
              "      <td>0.235165</td>\n",
              "      <td>0.243013</td>\n",
              "      <td>0.253121</td>\n",
              "      <td>4</td>\n",
              "      <td>0.235165</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>309</td>\n",
              "      <td>289</td>\n",
              "      <td>3</td>\n",
              "      <td>0.173257</td>\n",
              "      <td>0.141700</td>\n",
              "      <td>-0.033371</td>\n",
              "      <td>-0.041595</td>\n",
              "      <td>0.235165</td>\n",
              "      <td>0.009782</td>\n",
              "      <td>4.082514</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0.004321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.260229</td>\n",
              "      <td>0.245010</td>\n",
              "      <td>0.218930</td>\n",
              "      <td>0.212456</td>\n",
              "      <td>0.213033</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212456</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>115</td>\n",
              "      <td>262</td>\n",
              "      <td>1</td>\n",
              "      <td>0.058484</td>\n",
              "      <td>0.106445</td>\n",
              "      <td>0.029571</td>\n",
              "      <td>-0.002716</td>\n",
              "      <td>0.212456</td>\n",
              "      <td>0.000577</td>\n",
              "      <td>4.694107</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.008475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.262993</td>\n",
              "      <td>0.228721</td>\n",
              "      <td>0.212792</td>\n",
              "      <td>0.211318</td>\n",
              "      <td>0.203624</td>\n",
              "      <td>4</td>\n",
              "      <td>0.203624</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>181</td>\n",
              "      <td>275</td>\n",
              "      <td>1</td>\n",
              "      <td>0.130313</td>\n",
              "      <td>0.069647</td>\n",
              "      <td>0.006927</td>\n",
              "      <td>0.036405</td>\n",
              "      <td>0.203624</td>\n",
              "      <td>-0.007413</td>\n",
              "      <td>5.096542</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.110180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.252514</td>\n",
              "      <td>0.257067</td>\n",
              "      <td>0.209518</td>\n",
              "      <td>0.207798</td>\n",
              "      <td>0.209439</td>\n",
              "      <td>4</td>\n",
              "      <td>0.207798</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>205</td>\n",
              "      <td>314</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.018028</td>\n",
              "      <td>0.184966</td>\n",
              "      <td>0.008210</td>\n",
              "      <td>-0.007899</td>\n",
              "      <td>0.207798</td>\n",
              "      <td>0.001641</td>\n",
              "      <td>4.774652</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.016951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.260101</td>\n",
              "      <td>0.238063</td>\n",
              "      <td>0.224836</td>\n",
              "      <td>0.214511</td>\n",
              "      <td>0.212131</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212131</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>165</td>\n",
              "      <td>232</td>\n",
              "      <td>1</td>\n",
              "      <td>0.084730</td>\n",
              "      <td>0.055562</td>\n",
              "      <td>0.045921</td>\n",
              "      <td>0.011094</td>\n",
              "      <td>0.212131</td>\n",
              "      <td>-0.002353</td>\n",
              "      <td>4.766950</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0.011598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.280492</td>\n",
              "      <td>0.263231</td>\n",
              "      <td>0.236573</td>\n",
              "      <td>0.224092</td>\n",
              "      <td>0.226913</td>\n",
              "      <td>4</td>\n",
              "      <td>0.224092</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>118</td>\n",
              "      <td>2</td>\n",
              "      <td>0.061539</td>\n",
              "      <td>0.101273</td>\n",
              "      <td>0.052756</td>\n",
              "      <td>-0.012588</td>\n",
              "      <td>0.224092</td>\n",
              "      <td>0.002821</td>\n",
              "      <td>4.406979</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0.005247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.265284</td>\n",
              "      <td>0.243983</td>\n",
              "      <td>0.210468</td>\n",
              "      <td>0.208045</td>\n",
              "      <td>0.209459</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208045</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>164</td>\n",
              "      <td>263</td>\n",
              "      <td>1</td>\n",
              "      <td>0.080292</td>\n",
              "      <td>0.137368</td>\n",
              "      <td>0.011512</td>\n",
              "      <td>-0.006794</td>\n",
              "      <td>0.208045</td>\n",
              "      <td>0.001414</td>\n",
              "      <td>4.774213</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.015740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.246845</td>\n",
              "      <td>0.231376</td>\n",
              "      <td>0.210331</td>\n",
              "      <td>0.209565</td>\n",
              "      <td>0.213286</td>\n",
              "      <td>4</td>\n",
              "      <td>0.209565</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>352</td>\n",
              "      <td>2</td>\n",
              "      <td>0.062667</td>\n",
              "      <td>0.090958</td>\n",
              "      <td>0.003637</td>\n",
              "      <td>-0.017754</td>\n",
              "      <td>0.209565</td>\n",
              "      <td>0.003721</td>\n",
              "      <td>4.688540</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.008161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.270416</td>\n",
              "      <td>0.232051</td>\n",
              "      <td>0.218978</td>\n",
              "      <td>0.215047</td>\n",
              "      <td>0.218221</td>\n",
              "      <td>4</td>\n",
              "      <td>0.215047</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>224</td>\n",
              "      <td>314</td>\n",
              "      <td>2</td>\n",
              "      <td>0.141875</td>\n",
              "      <td>0.056337</td>\n",
              "      <td>0.017952</td>\n",
              "      <td>-0.014760</td>\n",
              "      <td>0.215047</td>\n",
              "      <td>0.003174</td>\n",
              "      <td>4.582514</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.006481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.289643</td>\n",
              "      <td>0.273529</td>\n",
              "      <td>0.258013</td>\n",
              "      <td>0.248571</td>\n",
              "      <td>0.242050</td>\n",
              "      <td>4</td>\n",
              "      <td>0.242050</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>287</td>\n",
              "      <td>3</td>\n",
              "      <td>0.055632</td>\n",
              "      <td>0.056725</td>\n",
              "      <td>0.036596</td>\n",
              "      <td>0.026234</td>\n",
              "      <td>0.242050</td>\n",
              "      <td>-0.006350</td>\n",
              "      <td>4.242684</td>\n",
              "      <td>48.0</td>\n",
              "      <td>0.004591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.265743</td>\n",
              "      <td>0.212465</td>\n",
              "      <td>0.208062</td>\n",
              "      <td>0.212184</td>\n",
              "      <td>0.215520</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208062</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>119</td>\n",
              "      <td>263</td>\n",
              "      <td>2</td>\n",
              "      <td>0.200487</td>\n",
              "      <td>0.020724</td>\n",
              "      <td>-0.019813</td>\n",
              "      <td>-0.015720</td>\n",
              "      <td>0.208062</td>\n",
              "      <td>0.003271</td>\n",
              "      <td>4.731877</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.010016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>0.278627</td>\n",
              "      <td>0.253869</td>\n",
              "      <td>0.205681</td>\n",
              "      <td>0.209609</td>\n",
              "      <td>0.218198</td>\n",
              "      <td>4</td>\n",
              "      <td>0.205681</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>117</td>\n",
              "      <td>279</td>\n",
              "      <td>2</td>\n",
              "      <td>0.088857</td>\n",
              "      <td>0.189818</td>\n",
              "      <td>-0.019101</td>\n",
              "      <td>-0.040973</td>\n",
              "      <td>0.205681</td>\n",
              "      <td>0.008427</td>\n",
              "      <td>4.670544</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0.007599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>0.254887</td>\n",
              "      <td>0.229644</td>\n",
              "      <td>0.219243</td>\n",
              "      <td>0.208727</td>\n",
              "      <td>0.211046</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208727</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>154</td>\n",
              "      <td>260</td>\n",
              "      <td>2</td>\n",
              "      <td>0.099038</td>\n",
              "      <td>0.045293</td>\n",
              "      <td>0.047963</td>\n",
              "      <td>-0.011110</td>\n",
              "      <td>0.208727</td>\n",
              "      <td>0.002319</td>\n",
              "      <td>4.738300</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.011018</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4b3c8311-9b58-415f-8ea4-da62170979ae')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4b3c8311-9b58-415f-8ea4-da62170979ae button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4b3c8311-9b58-415f-8ea4-da62170979ae');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           0         1         2  ...     score  model_rank  proba_selection\n",
              "0   0.297629  0.256342  0.224900  ...  4.946564         8.0         0.027545\n",
              "1   0.276555  0.242013  0.211087  ...  4.963635         6.0         0.036727\n",
              "2   0.296420  0.235832  0.227745  ...  4.702388        25.0         0.008814\n",
              "3   0.270336  0.227319  0.231308  ...  4.478359        39.0         0.005650\n",
              "4   0.285443  0.220161  0.226018  ...  4.387952        43.0         0.005125\n",
              "5   0.295962  0.254919  0.248721  ...  4.106723        49.0         0.004497\n",
              "6   0.264929  0.224174  0.228173  ...  4.583062        33.0         0.006678\n",
              "7   0.270235  0.228662  0.249313  ...  4.731872        23.0         0.009581\n",
              "8   0.257634  0.223843  0.245183  ...  4.470503        40.0         0.005509\n",
              "9   0.289596  0.240284  0.235281  ...  4.035327        52.0         0.004238\n",
              "10  0.254877  0.259745  0.230720  ...  4.312846        44.0         0.005008\n",
              "11  0.306085  0.269204  0.234900  ...  4.244900        47.0         0.004688\n",
              "12  0.258406  0.226609  0.231610  ...  4.774143        16.0         0.013772\n",
              "13  0.273785  0.234423  0.214054  ...  4.847430        11.0         0.020033\n",
              "14  0.281174  0.258473  0.242390  ...  4.281065        45.0         0.004897\n",
              "15  0.276611  0.257501  0.235830  ...  4.089106        50.0         0.004407\n",
              "16  0.290638  0.269898  0.222084  ...  4.678856        28.0         0.007870\n",
              "17  0.262872  0.227500  0.214436  ...  4.815277        12.0         0.018363\n",
              "18  0.310540  0.245770  0.228410  ...  4.769965        17.0         0.012962\n",
              "19  0.253787  0.230512  0.219922  ...  4.578315        35.0         0.006296\n",
              "20  0.283236  0.240035  0.243000  ...  4.666841        30.0         0.007345\n",
              "21  0.249221  0.231004  0.211921  ...  4.857491        10.0         0.022036\n",
              "22  0.265260  0.227288  0.217763  ...  5.138516         1.0         0.220359\n",
              "23  0.279411  0.242117  0.231796  ...  4.513156        38.0         0.005799\n",
              "24  0.311441  0.225901  0.217392  ...  4.735011        21.0         0.010493\n",
              "25  0.277852  0.236534  0.222586  ...  4.556380        36.0         0.006121\n",
              "26  0.296777  0.234284  0.219085  ...  4.724288        24.0         0.009182\n",
              "27  0.266089  0.236261  0.205056  ...  5.087436         3.0         0.073453\n",
              "28  0.290046  0.225338  0.206737  ...  4.774212        15.0         0.014691\n",
              "29  0.301766  0.262840  0.226363  ...  4.450873        41.0         0.005375\n",
              "30  0.244258  0.215385  0.213587  ...  4.769735        18.0         0.012242\n",
              "31  0.276123  0.232157  0.213064  ...  4.971918         5.0         0.044072\n",
              "32  0.272228  0.222163  0.217547  ...  4.951308         7.0         0.031480\n",
              "33  0.306602  0.233451  0.227916  ...  4.545162        37.0         0.005956\n",
              "34  0.273408  0.253417  0.254691  ...  4.250535        46.0         0.004790\n",
              "35  0.361880  0.293221  0.293664  ...  4.666209        31.0         0.007108\n",
              "36  0.312099  0.228898  0.216683  ...  5.007771         4.0         0.055090\n",
              "37  0.246799  0.225134  0.223399  ...  4.649433        32.0         0.006886\n",
              "38  0.282973  0.232447  0.231867  ...  4.927086         9.0         0.024484\n",
              "39  0.331409  0.273990  0.235165  ...  4.082514        51.0         0.004321\n",
              "40  0.260229  0.245010  0.218930  ...  4.694107        26.0         0.008475\n",
              "41  0.262993  0.228721  0.212792  ...  5.096542         2.0         0.110180\n",
              "42  0.252514  0.257067  0.209518  ...  4.774652        13.0         0.016951\n",
              "43  0.260101  0.238063  0.224836  ...  4.766950        19.0         0.011598\n",
              "44  0.280492  0.263231  0.236573  ...  4.406979        42.0         0.005247\n",
              "45  0.265284  0.243983  0.210468  ...  4.774213        14.0         0.015740\n",
              "46  0.246845  0.231376  0.210331  ...  4.688540        27.0         0.008161\n",
              "47  0.270416  0.232051  0.218978  ...  4.582514        34.0         0.006481\n",
              "48  0.289643  0.273529  0.258013  ...  4.242684        48.0         0.004591\n",
              "49  0.265743  0.212465  0.208062  ...  4.731877        22.0         0.010016\n",
              "50  0.278627  0.253869  0.205681  ...  4.670544        29.0         0.007599\n",
              "51  0.254887  0.229644  0.219243  ...  4.738300        20.0         0.011018\n",
              "\n",
              "[52 rows x 21 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.20227106225987276, 156.0, 276.0, 1.0)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Génération :  9\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 203 hidden_dim : 253 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2562, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.4161412575883786\n",
            "Epoch 1 : Validation loss = 0.2513181349883477; Validation r = 0.49069592559570663\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1175, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.743811331845937\n",
            "Epoch 2 : Validation loss = 0.223805841182669; Validation r = 0.5347751175643278\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0554, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8870774889780308\n",
            "Epoch 3 : Validation loss = 0.23333106450736524; Validation r = 0.5159870846635792\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0290, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9428754732177682\n",
            "Epoch 4 : Validation loss = 0.20519668521980444; Validation r = 0.564141400916479\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0173, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9653513951076239\n",
            "Epoch 5 : Validation loss = 0.2130409968396028; Validation r = 0.5522525138809377\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 170 hidden_dim : 226 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2736, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3766855441235767\n",
            "Epoch 1 : Validation loss = 0.2606076958278815; Validation r = 0.43739983765579143\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1435, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6685062924798206\n",
            "Epoch 2 : Validation loss = 0.22895483349760373; Validation r = 0.503278780736796\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0759, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8420357014968277\n",
            "Epoch 3 : Validation loss = 0.22748195814589658; Validation r = 0.5096957251782601\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0372, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9257483030794322\n",
            "Epoch 4 : Validation loss = 0.22900426375369232; Validation r = 0.5034936323132129\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0226, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9545596900778976\n",
            "Epoch 5 : Validation loss = 0.2241375649968783; Validation r = 0.5193724892154983\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 206 hidden_dim : 290 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2564, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.4031771759997664\n",
            "Epoch 1 : Validation loss = 0.25180822759866717; Validation r = 0.47395250202278755\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1161, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7468374963440302\n",
            "Epoch 2 : Validation loss = 0.22741509477297464; Validation r = 0.520437728973112\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0532, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8927676675757935\n",
            "Epoch 3 : Validation loss = 0.2115006217112144; Validation r = 0.5192025225608905\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0260, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9496061040974587\n",
            "Epoch 4 : Validation loss = 0.21019591515262923; Validation r = 0.5442075667420951\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0158, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9687216295189892\n",
            "Epoch 5 : Validation loss = 0.20826446873446305; Validation r = 0.5332088817056377\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 178 hidden_dim : 252 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2605, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3952667738185024\n",
            "Epoch 1 : Validation loss = 0.2806661701450745; Validation r = 0.4715653800200575\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1220, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7318298859377743\n",
            "Epoch 2 : Validation loss = 0.21485224328935146; Validation r = 0.5366092904577682\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0604, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8770235182296037\n",
            "Epoch 3 : Validation loss = 0.21203697708745797; Validation r = 0.5294657742298854\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0310, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9399796387736992\n",
            "Epoch 4 : Validation loss = 0.20463506902257603; Validation r = 0.5375168811968508\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0179, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9648073333483829\n",
            "Epoch 5 : Validation loss = 0.20491367690265178; Validation r = 0.5468773571780657\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 230 hidden_dim : 262 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2761, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3668005683777\n",
            "Epoch 1 : Validation loss = 0.30357363261282444; Validation r = 0.3770929565175282\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1459, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6618135489878214\n",
            "Epoch 2 : Validation loss = 0.23141014551122982; Validation r = 0.48247197340394615\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0701, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8535295799491659\n",
            "Epoch 3 : Validation loss = 0.21656799887617428; Validation r = 0.5326061182782003\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0341, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9308133282053555\n",
            "Epoch 4 : Validation loss = 0.2152064194281896; Validation r = 0.5237117889182801\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0208, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9574511304645282\n",
            "Epoch 5 : Validation loss = 0.2213604496171077; Validation r = 0.5193205939978678\n",
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-cfb4dcd6-88a1-4769-9c85-d13d7e37e4e7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>max_epoch</th>\n",
              "      <th>best_dev_loss</th>\n",
              "      <th>best_dev_loss_epoch</th>\n",
              "      <th>lastEpochImproved</th>\n",
              "      <th>embedding_dim</th>\n",
              "      <th>hidden_dim</th>\n",
              "      <th>num_lstm_layers</th>\n",
              "      <th>loss_decrease_perc_0</th>\n",
              "      <th>loss_decrease_perc_1</th>\n",
              "      <th>loss_decrease_perc_2</th>\n",
              "      <th>loss_decrease_perc_3</th>\n",
              "      <th>score_perf</th>\n",
              "      <th>score_pot</th>\n",
              "      <th>score</th>\n",
              "      <th>model_rank</th>\n",
              "      <th>proba_selection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.297629</td>\n",
              "      <td>0.256342</td>\n",
              "      <td>0.224900</td>\n",
              "      <td>0.221605</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.138718</td>\n",
              "      <td>0.122657</td>\n",
              "      <td>0.014653</td>\n",
              "      <td>0.044878</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>-0.009499</td>\n",
              "      <td>4.946564</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.027004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.276555</td>\n",
              "      <td>0.242013</td>\n",
              "      <td>0.211087</td>\n",
              "      <td>0.214948</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.124904</td>\n",
              "      <td>0.127787</td>\n",
              "      <td>-0.018291</td>\n",
              "      <td>0.031870</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>-0.006632</td>\n",
              "      <td>4.963635</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.036005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.296420</td>\n",
              "      <td>0.235832</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.233284</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>175</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.204399</td>\n",
              "      <td>0.034291</td>\n",
              "      <td>-0.024319</td>\n",
              "      <td>0.045231</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>-0.010074</td>\n",
              "      <td>4.702388</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.008001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.270336</td>\n",
              "      <td>0.227319</td>\n",
              "      <td>0.231308</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.223296</td>\n",
              "      <td>4</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>175</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.159127</td>\n",
              "      <td>-0.017549</td>\n",
              "      <td>0.089803</td>\n",
              "      <td>-0.060609</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.012760</td>\n",
              "      <td>4.478359</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.004910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.285443</td>\n",
              "      <td>0.220161</td>\n",
              "      <td>0.226018</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.227897</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.228703</td>\n",
              "      <td>-0.026604</td>\n",
              "      <td>0.057750</td>\n",
              "      <td>-0.070109</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.014931</td>\n",
              "      <td>4.387952</td>\n",
              "      <td>48.0</td>\n",
              "      <td>0.004501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.295962</td>\n",
              "      <td>0.254919</td>\n",
              "      <td>0.248721</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>0.243503</td>\n",
              "      <td>4</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>0.138675</td>\n",
              "      <td>0.024316</td>\n",
              "      <td>0.056733</td>\n",
              "      <td>-0.037906</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>0.008893</td>\n",
              "      <td>4.106723</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.004001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.264929</td>\n",
              "      <td>0.224174</td>\n",
              "      <td>0.228173</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>0.218195</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>300</td>\n",
              "      <td>2</td>\n",
              "      <td>0.153832</td>\n",
              "      <td>-0.017838</td>\n",
              "      <td>0.057869</td>\n",
              "      <td>-0.015006</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>0.003226</td>\n",
              "      <td>4.583062</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.006001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.270235</td>\n",
              "      <td>0.228662</td>\n",
              "      <td>0.249313</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>0.211333</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>300</td>\n",
              "      <td>3</td>\n",
              "      <td>0.153839</td>\n",
              "      <td>-0.090309</td>\n",
              "      <td>0.163048</td>\n",
              "      <td>-0.012797</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>0.002670</td>\n",
              "      <td>4.731872</td>\n",
              "      <td>25.0</td>\n",
              "      <td>0.008641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.257634</td>\n",
              "      <td>0.223843</td>\n",
              "      <td>0.245183</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>0.223688</td>\n",
              "      <td>4</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.131159</td>\n",
              "      <td>-0.095335</td>\n",
              "      <td>0.104335</td>\n",
              "      <td>-0.018609</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>0.004087</td>\n",
              "      <td>4.470503</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.004801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.289596</td>\n",
              "      <td>0.240284</td>\n",
              "      <td>0.235281</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>0.247811</td>\n",
              "      <td>4</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>0.170279</td>\n",
              "      <td>0.020819</td>\n",
              "      <td>0.022889</td>\n",
              "      <td>-0.077928</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>0.017915</td>\n",
              "      <td>4.035327</td>\n",
              "      <td>57.0</td>\n",
              "      <td>0.003790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.254877</td>\n",
              "      <td>0.259745</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>0.240172</td>\n",
              "      <td>0.241364</td>\n",
              "      <td>4</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>300</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.019099</td>\n",
              "      <td>0.111744</td>\n",
              "      <td>-0.040966</td>\n",
              "      <td>-0.004965</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>0.001145</td>\n",
              "      <td>4.312846</td>\n",
              "      <td>49.0</td>\n",
              "      <td>0.004409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.306085</td>\n",
              "      <td>0.269204</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>0.240201</td>\n",
              "      <td>0.240892</td>\n",
              "      <td>4</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>300</td>\n",
              "      <td>3</td>\n",
              "      <td>0.120492</td>\n",
              "      <td>0.127428</td>\n",
              "      <td>-0.022564</td>\n",
              "      <td>-0.002880</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>0.000677</td>\n",
              "      <td>4.244900</td>\n",
              "      <td>52.0</td>\n",
              "      <td>0.004154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.258406</td>\n",
              "      <td>0.226609</td>\n",
              "      <td>0.231610</td>\n",
              "      <td>0.234645</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>4</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>219</td>\n",
              "      <td>272</td>\n",
              "      <td>2</td>\n",
              "      <td>0.123048</td>\n",
              "      <td>-0.022068</td>\n",
              "      <td>-0.013101</td>\n",
              "      <td>0.055185</td>\n",
              "      <td>0.221696</td>\n",
              "      <td>-0.012234</td>\n",
              "      <td>4.774143</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0.012002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.273785</td>\n",
              "      <td>0.234423</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>0.226703</td>\n",
              "      <td>0.218485</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>264</td>\n",
              "      <td>309</td>\n",
              "      <td>2</td>\n",
              "      <td>0.143770</td>\n",
              "      <td>0.086889</td>\n",
              "      <td>-0.059090</td>\n",
              "      <td>0.036249</td>\n",
              "      <td>0.214054</td>\n",
              "      <td>-0.007759</td>\n",
              "      <td>4.847430</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.018002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.281174</td>\n",
              "      <td>0.258473</td>\n",
              "      <td>0.242390</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>0.233587</td>\n",
              "      <td>4</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>201</td>\n",
              "      <td>100</td>\n",
              "      <td>2</td>\n",
              "      <td>0.080737</td>\n",
              "      <td>0.062222</td>\n",
              "      <td>0.042879</td>\n",
              "      <td>-0.006853</td>\n",
              "      <td>0.231997</td>\n",
              "      <td>0.001590</td>\n",
              "      <td>4.281065</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.004321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.276611</td>\n",
              "      <td>0.257501</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>0.235862</td>\n",
              "      <td>0.244586</td>\n",
              "      <td>4</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>352</td>\n",
              "      <td>240</td>\n",
              "      <td>3</td>\n",
              "      <td>0.069085</td>\n",
              "      <td>0.084159</td>\n",
              "      <td>-0.000138</td>\n",
              "      <td>-0.036985</td>\n",
              "      <td>0.235830</td>\n",
              "      <td>0.008722</td>\n",
              "      <td>4.089106</td>\n",
              "      <td>55.0</td>\n",
              "      <td>0.003928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.290638</td>\n",
              "      <td>0.269898</td>\n",
              "      <td>0.222084</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>0.213727</td>\n",
              "      <td>4</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>113</td>\n",
              "      <td>2</td>\n",
              "      <td>0.071362</td>\n",
              "      <td>0.177156</td>\n",
              "      <td>0.054589</td>\n",
              "      <td>-0.017942</td>\n",
              "      <td>0.209960</td>\n",
              "      <td>0.003767</td>\n",
              "      <td>4.678856</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.006969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.262872</td>\n",
              "      <td>0.227500</td>\n",
              "      <td>0.214436</td>\n",
              "      <td>0.205983</td>\n",
              "      <td>0.207672</td>\n",
              "      <td>4</td>\n",
              "      <td>0.205983</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>322</td>\n",
              "      <td>2</td>\n",
              "      <td>0.134558</td>\n",
              "      <td>0.057427</td>\n",
              "      <td>0.039419</td>\n",
              "      <td>-0.008203</td>\n",
              "      <td>0.205983</td>\n",
              "      <td>0.001690</td>\n",
              "      <td>4.815277</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.015431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.310540</td>\n",
              "      <td>0.245770</td>\n",
              "      <td>0.228410</td>\n",
              "      <td>0.235806</td>\n",
              "      <td>0.222341</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222341</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>145</td>\n",
              "      <td>2</td>\n",
              "      <td>0.208571</td>\n",
              "      <td>0.070637</td>\n",
              "      <td>-0.032380</td>\n",
              "      <td>0.057101</td>\n",
              "      <td>0.222341</td>\n",
              "      <td>-0.012696</td>\n",
              "      <td>4.769965</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0.011370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.253787</td>\n",
              "      <td>0.230512</td>\n",
              "      <td>0.219922</td>\n",
              "      <td>0.214113</td>\n",
              "      <td>0.218421</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214113</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>254</td>\n",
              "      <td>2</td>\n",
              "      <td>0.091710</td>\n",
              "      <td>0.045945</td>\n",
              "      <td>0.026412</td>\n",
              "      <td>-0.020120</td>\n",
              "      <td>0.214113</td>\n",
              "      <td>0.004308</td>\n",
              "      <td>4.578315</td>\n",
              "      <td>38.0</td>\n",
              "      <td>0.005685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.283236</td>\n",
              "      <td>0.240035</td>\n",
              "      <td>0.243000</td>\n",
              "      <td>0.227447</td>\n",
              "      <td>0.220764</td>\n",
              "      <td>4</td>\n",
              "      <td>0.220764</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>218</td>\n",
              "      <td>110</td>\n",
              "      <td>1</td>\n",
              "      <td>0.152528</td>\n",
              "      <td>-0.012351</td>\n",
              "      <td>0.064003</td>\n",
              "      <td>0.029382</td>\n",
              "      <td>0.220764</td>\n",
              "      <td>-0.006486</td>\n",
              "      <td>4.666841</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.006546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.249221</td>\n",
              "      <td>0.231004</td>\n",
              "      <td>0.211921</td>\n",
              "      <td>0.218319</td>\n",
              "      <td>0.212083</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211921</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>131</td>\n",
              "      <td>292</td>\n",
              "      <td>1</td>\n",
              "      <td>0.073096</td>\n",
              "      <td>0.082608</td>\n",
              "      <td>-0.030190</td>\n",
              "      <td>0.028564</td>\n",
              "      <td>0.211921</td>\n",
              "      <td>-0.006053</td>\n",
              "      <td>4.857491</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.019639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.265260</td>\n",
              "      <td>0.227288</td>\n",
              "      <td>0.217763</td>\n",
              "      <td>0.210235</td>\n",
              "      <td>0.202271</td>\n",
              "      <td>4</td>\n",
              "      <td>0.202271</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>156</td>\n",
              "      <td>276</td>\n",
              "      <td>1</td>\n",
              "      <td>0.143151</td>\n",
              "      <td>0.041905</td>\n",
              "      <td>0.034571</td>\n",
              "      <td>0.037882</td>\n",
              "      <td>0.202271</td>\n",
              "      <td>-0.007662</td>\n",
              "      <td>5.138516</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.216029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.279411</td>\n",
              "      <td>0.242117</td>\n",
              "      <td>0.231796</td>\n",
              "      <td>0.215313</td>\n",
              "      <td>0.221574</td>\n",
              "      <td>4</td>\n",
              "      <td>0.215313</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>151</td>\n",
              "      <td>3</td>\n",
              "      <td>0.133472</td>\n",
              "      <td>0.042627</td>\n",
              "      <td>0.071112</td>\n",
              "      <td>-0.029081</td>\n",
              "      <td>0.215313</td>\n",
              "      <td>0.006262</td>\n",
              "      <td>4.513156</td>\n",
              "      <td>43.0</td>\n",
              "      <td>0.005024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.311441</td>\n",
              "      <td>0.225901</td>\n",
              "      <td>0.217392</td>\n",
              "      <td>0.216373</td>\n",
              "      <td>0.213767</td>\n",
              "      <td>4</td>\n",
              "      <td>0.213767</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>120</td>\n",
              "      <td>122</td>\n",
              "      <td>2</td>\n",
              "      <td>0.274660</td>\n",
              "      <td>0.037664</td>\n",
              "      <td>0.004686</td>\n",
              "      <td>0.012044</td>\n",
              "      <td>0.213767</td>\n",
              "      <td>-0.002575</td>\n",
              "      <td>4.735011</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.009393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.277852</td>\n",
              "      <td>0.236534</td>\n",
              "      <td>0.222586</td>\n",
              "      <td>0.213596</td>\n",
              "      <td>0.219472</td>\n",
              "      <td>4</td>\n",
              "      <td>0.213596</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>103</td>\n",
              "      <td>124</td>\n",
              "      <td>2</td>\n",
              "      <td>0.148704</td>\n",
              "      <td>0.058969</td>\n",
              "      <td>0.040390</td>\n",
              "      <td>-0.027513</td>\n",
              "      <td>0.213596</td>\n",
              "      <td>0.005877</td>\n",
              "      <td>4.556380</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.005401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.296777</td>\n",
              "      <td>0.234284</td>\n",
              "      <td>0.219085</td>\n",
              "      <td>0.209900</td>\n",
              "      <td>0.211672</td>\n",
              "      <td>4</td>\n",
              "      <td>0.209900</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>197</td>\n",
              "      <td>134</td>\n",
              "      <td>2</td>\n",
              "      <td>0.210572</td>\n",
              "      <td>0.064874</td>\n",
              "      <td>0.041928</td>\n",
              "      <td>-0.008445</td>\n",
              "      <td>0.209900</td>\n",
              "      <td>0.001773</td>\n",
              "      <td>4.724288</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.008309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.266089</td>\n",
              "      <td>0.236261</td>\n",
              "      <td>0.205056</td>\n",
              "      <td>0.204831</td>\n",
              "      <td>0.200654</td>\n",
              "      <td>4</td>\n",
              "      <td>0.200654</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>127</td>\n",
              "      <td>305</td>\n",
              "      <td>1</td>\n",
              "      <td>0.112098</td>\n",
              "      <td>0.132078</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>0.020392</td>\n",
              "      <td>0.200654</td>\n",
              "      <td>-0.004092</td>\n",
              "      <td>5.087436</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.072010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.290046</td>\n",
              "      <td>0.225338</td>\n",
              "      <td>0.206737</td>\n",
              "      <td>0.207275</td>\n",
              "      <td>0.210004</td>\n",
              "      <td>4</td>\n",
              "      <td>0.206737</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>204</td>\n",
              "      <td>292</td>\n",
              "      <td>1</td>\n",
              "      <td>0.223095</td>\n",
              "      <td>0.082547</td>\n",
              "      <td>-0.002602</td>\n",
              "      <td>-0.013163</td>\n",
              "      <td>0.206737</td>\n",
              "      <td>0.002721</td>\n",
              "      <td>4.774212</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.012708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.301766</td>\n",
              "      <td>0.262840</td>\n",
              "      <td>0.226363</td>\n",
              "      <td>0.218543</td>\n",
              "      <td>0.224675</td>\n",
              "      <td>4</td>\n",
              "      <td>0.218543</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>157</td>\n",
              "      <td>3</td>\n",
              "      <td>0.128994</td>\n",
              "      <td>0.138780</td>\n",
              "      <td>0.034548</td>\n",
              "      <td>-0.028059</td>\n",
              "      <td>0.218543</td>\n",
              "      <td>0.006132</td>\n",
              "      <td>4.450873</td>\n",
              "      <td>46.0</td>\n",
              "      <td>0.004696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.244258</td>\n",
              "      <td>0.215385</td>\n",
              "      <td>0.213587</td>\n",
              "      <td>0.212510</td>\n",
              "      <td>0.211078</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211078</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>149</td>\n",
              "      <td>295</td>\n",
              "      <td>1</td>\n",
              "      <td>0.118208</td>\n",
              "      <td>0.008346</td>\n",
              "      <td>0.005043</td>\n",
              "      <td>0.006740</td>\n",
              "      <td>0.211078</td>\n",
              "      <td>-0.001423</td>\n",
              "      <td>4.769735</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.010801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.276123</td>\n",
              "      <td>0.232157</td>\n",
              "      <td>0.213064</td>\n",
              "      <td>0.213007</td>\n",
              "      <td>0.206983</td>\n",
              "      <td>4</td>\n",
              "      <td>0.206983</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>178</td>\n",
              "      <td>324</td>\n",
              "      <td>1</td>\n",
              "      <td>0.159224</td>\n",
              "      <td>0.082243</td>\n",
              "      <td>0.000265</td>\n",
              "      <td>0.028281</td>\n",
              "      <td>0.206983</td>\n",
              "      <td>-0.005854</td>\n",
              "      <td>4.971918</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.043206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.272228</td>\n",
              "      <td>0.222163</td>\n",
              "      <td>0.217547</td>\n",
              "      <td>0.213297</td>\n",
              "      <td>0.207554</td>\n",
              "      <td>4</td>\n",
              "      <td>0.207554</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>163</td>\n",
              "      <td>290</td>\n",
              "      <td>1</td>\n",
              "      <td>0.183909</td>\n",
              "      <td>0.020781</td>\n",
              "      <td>0.019536</td>\n",
              "      <td>0.026921</td>\n",
              "      <td>0.207554</td>\n",
              "      <td>-0.005588</td>\n",
              "      <td>4.951308</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.030861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.306602</td>\n",
              "      <td>0.233451</td>\n",
              "      <td>0.227916</td>\n",
              "      <td>0.222533</td>\n",
              "      <td>0.221270</td>\n",
              "      <td>4</td>\n",
              "      <td>0.221270</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>156</td>\n",
              "      <td>144</td>\n",
              "      <td>2</td>\n",
              "      <td>0.238586</td>\n",
              "      <td>0.023711</td>\n",
              "      <td>0.023618</td>\n",
              "      <td>0.005676</td>\n",
              "      <td>0.221270</td>\n",
              "      <td>-0.001256</td>\n",
              "      <td>4.545162</td>\n",
              "      <td>41.0</td>\n",
              "      <td>0.005269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.273408</td>\n",
              "      <td>0.253417</td>\n",
              "      <td>0.254691</td>\n",
              "      <td>0.235056</td>\n",
              "      <td>0.235265</td>\n",
              "      <td>4</td>\n",
              "      <td>0.235056</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>374</td>\n",
              "      <td>159</td>\n",
              "      <td>3</td>\n",
              "      <td>0.073120</td>\n",
              "      <td>-0.005029</td>\n",
              "      <td>0.077093</td>\n",
              "      <td>-0.000885</td>\n",
              "      <td>0.235056</td>\n",
              "      <td>0.000208</td>\n",
              "      <td>4.250535</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0.004236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.361880</td>\n",
              "      <td>0.293221</td>\n",
              "      <td>0.293664</td>\n",
              "      <td>0.296554</td>\n",
              "      <td>0.252098</td>\n",
              "      <td>4</td>\n",
              "      <td>0.252098</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>157</td>\n",
              "      <td>4</td>\n",
              "      <td>0.189728</td>\n",
              "      <td>-0.001510</td>\n",
              "      <td>-0.009841</td>\n",
              "      <td>0.149907</td>\n",
              "      <td>0.252098</td>\n",
              "      <td>-0.037791</td>\n",
              "      <td>4.666209</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.006354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.312099</td>\n",
              "      <td>0.228898</td>\n",
              "      <td>0.216683</td>\n",
              "      <td>0.208896</td>\n",
              "      <td>0.204241</td>\n",
              "      <td>4</td>\n",
              "      <td>0.204241</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>337</td>\n",
              "      <td>1</td>\n",
              "      <td>0.266585</td>\n",
              "      <td>0.053365</td>\n",
              "      <td>0.035936</td>\n",
              "      <td>0.022285</td>\n",
              "      <td>0.204241</td>\n",
              "      <td>-0.004552</td>\n",
              "      <td>5.007771</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.054007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.246799</td>\n",
              "      <td>0.225134</td>\n",
              "      <td>0.223399</td>\n",
              "      <td>0.220870</td>\n",
              "      <td>0.217956</td>\n",
              "      <td>4</td>\n",
              "      <td>0.217956</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>193</td>\n",
              "      <td>292</td>\n",
              "      <td>1</td>\n",
              "      <td>0.087781</td>\n",
              "      <td>0.007708</td>\n",
              "      <td>0.011322</td>\n",
              "      <td>0.013194</td>\n",
              "      <td>0.217956</td>\n",
              "      <td>-0.002876</td>\n",
              "      <td>4.649433</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0.006172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.282973</td>\n",
              "      <td>0.232447</td>\n",
              "      <td>0.231867</td>\n",
              "      <td>0.221023</td>\n",
              "      <td>0.211799</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211799</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>125</td>\n",
              "      <td>121</td>\n",
              "      <td>2</td>\n",
              "      <td>0.178554</td>\n",
              "      <td>0.002496</td>\n",
              "      <td>0.046768</td>\n",
              "      <td>0.041733</td>\n",
              "      <td>0.211799</td>\n",
              "      <td>-0.008839</td>\n",
              "      <td>4.927086</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.024003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.331409</td>\n",
              "      <td>0.273990</td>\n",
              "      <td>0.235165</td>\n",
              "      <td>0.243013</td>\n",
              "      <td>0.253121</td>\n",
              "      <td>4</td>\n",
              "      <td>0.235165</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>309</td>\n",
              "      <td>289</td>\n",
              "      <td>3</td>\n",
              "      <td>0.173257</td>\n",
              "      <td>0.141700</td>\n",
              "      <td>-0.033371</td>\n",
              "      <td>-0.041595</td>\n",
              "      <td>0.235165</td>\n",
              "      <td>0.009782</td>\n",
              "      <td>4.082514</td>\n",
              "      <td>56.0</td>\n",
              "      <td>0.003858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.260229</td>\n",
              "      <td>0.245010</td>\n",
              "      <td>0.218930</td>\n",
              "      <td>0.212456</td>\n",
              "      <td>0.213033</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212456</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>115</td>\n",
              "      <td>262</td>\n",
              "      <td>1</td>\n",
              "      <td>0.058484</td>\n",
              "      <td>0.106445</td>\n",
              "      <td>0.029571</td>\n",
              "      <td>-0.002716</td>\n",
              "      <td>0.212456</td>\n",
              "      <td>0.000577</td>\n",
              "      <td>4.694107</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0.007715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.262993</td>\n",
              "      <td>0.228721</td>\n",
              "      <td>0.212792</td>\n",
              "      <td>0.211318</td>\n",
              "      <td>0.203624</td>\n",
              "      <td>4</td>\n",
              "      <td>0.203624</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>181</td>\n",
              "      <td>275</td>\n",
              "      <td>1</td>\n",
              "      <td>0.130313</td>\n",
              "      <td>0.069647</td>\n",
              "      <td>0.006927</td>\n",
              "      <td>0.036405</td>\n",
              "      <td>0.203624</td>\n",
              "      <td>-0.007413</td>\n",
              "      <td>5.096542</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.108014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.252514</td>\n",
              "      <td>0.257067</td>\n",
              "      <td>0.209518</td>\n",
              "      <td>0.207798</td>\n",
              "      <td>0.209439</td>\n",
              "      <td>4</td>\n",
              "      <td>0.207798</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>205</td>\n",
              "      <td>314</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.018028</td>\n",
              "      <td>0.184966</td>\n",
              "      <td>0.008210</td>\n",
              "      <td>-0.007899</td>\n",
              "      <td>0.207798</td>\n",
              "      <td>0.001641</td>\n",
              "      <td>4.774652</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.014402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.260101</td>\n",
              "      <td>0.238063</td>\n",
              "      <td>0.224836</td>\n",
              "      <td>0.214511</td>\n",
              "      <td>0.212131</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212131</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>165</td>\n",
              "      <td>232</td>\n",
              "      <td>1</td>\n",
              "      <td>0.084730</td>\n",
              "      <td>0.055562</td>\n",
              "      <td>0.045921</td>\n",
              "      <td>0.011094</td>\n",
              "      <td>0.212131</td>\n",
              "      <td>-0.002353</td>\n",
              "      <td>4.766950</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.010287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.280492</td>\n",
              "      <td>0.263231</td>\n",
              "      <td>0.236573</td>\n",
              "      <td>0.224092</td>\n",
              "      <td>0.226913</td>\n",
              "      <td>4</td>\n",
              "      <td>0.224092</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>118</td>\n",
              "      <td>2</td>\n",
              "      <td>0.061539</td>\n",
              "      <td>0.101273</td>\n",
              "      <td>0.052756</td>\n",
              "      <td>-0.012588</td>\n",
              "      <td>0.224092</td>\n",
              "      <td>0.002821</td>\n",
              "      <td>4.406979</td>\n",
              "      <td>47.0</td>\n",
              "      <td>0.004596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.265284</td>\n",
              "      <td>0.243983</td>\n",
              "      <td>0.210468</td>\n",
              "      <td>0.208045</td>\n",
              "      <td>0.209459</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208045</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>164</td>\n",
              "      <td>263</td>\n",
              "      <td>1</td>\n",
              "      <td>0.080292</td>\n",
              "      <td>0.137368</td>\n",
              "      <td>0.011512</td>\n",
              "      <td>-0.006794</td>\n",
              "      <td>0.208045</td>\n",
              "      <td>0.001414</td>\n",
              "      <td>4.774213</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.013502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.246845</td>\n",
              "      <td>0.231376</td>\n",
              "      <td>0.210331</td>\n",
              "      <td>0.209565</td>\n",
              "      <td>0.213286</td>\n",
              "      <td>4</td>\n",
              "      <td>0.209565</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>100</td>\n",
              "      <td>352</td>\n",
              "      <td>2</td>\n",
              "      <td>0.062667</td>\n",
              "      <td>0.090958</td>\n",
              "      <td>0.003637</td>\n",
              "      <td>-0.017754</td>\n",
              "      <td>0.209565</td>\n",
              "      <td>0.003721</td>\n",
              "      <td>4.688540</td>\n",
              "      <td>30.0</td>\n",
              "      <td>0.007201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.270416</td>\n",
              "      <td>0.232051</td>\n",
              "      <td>0.218978</td>\n",
              "      <td>0.215047</td>\n",
              "      <td>0.218221</td>\n",
              "      <td>4</td>\n",
              "      <td>0.215047</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>224</td>\n",
              "      <td>314</td>\n",
              "      <td>2</td>\n",
              "      <td>0.141875</td>\n",
              "      <td>0.056337</td>\n",
              "      <td>0.017952</td>\n",
              "      <td>-0.014760</td>\n",
              "      <td>0.215047</td>\n",
              "      <td>0.003174</td>\n",
              "      <td>4.582514</td>\n",
              "      <td>37.0</td>\n",
              "      <td>0.005839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.289643</td>\n",
              "      <td>0.273529</td>\n",
              "      <td>0.258013</td>\n",
              "      <td>0.248571</td>\n",
              "      <td>0.242050</td>\n",
              "      <td>4</td>\n",
              "      <td>0.242050</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>287</td>\n",
              "      <td>3</td>\n",
              "      <td>0.055632</td>\n",
              "      <td>0.056725</td>\n",
              "      <td>0.036596</td>\n",
              "      <td>0.026234</td>\n",
              "      <td>0.242050</td>\n",
              "      <td>-0.006350</td>\n",
              "      <td>4.242684</td>\n",
              "      <td>53.0</td>\n",
              "      <td>0.004076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.265743</td>\n",
              "      <td>0.212465</td>\n",
              "      <td>0.208062</td>\n",
              "      <td>0.212184</td>\n",
              "      <td>0.215520</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208062</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>119</td>\n",
              "      <td>263</td>\n",
              "      <td>2</td>\n",
              "      <td>0.200487</td>\n",
              "      <td>0.020724</td>\n",
              "      <td>-0.019813</td>\n",
              "      <td>-0.015720</td>\n",
              "      <td>0.208062</td>\n",
              "      <td>0.003271</td>\n",
              "      <td>4.731877</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.009001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>0.278627</td>\n",
              "      <td>0.253869</td>\n",
              "      <td>0.205681</td>\n",
              "      <td>0.209609</td>\n",
              "      <td>0.218198</td>\n",
              "      <td>4</td>\n",
              "      <td>0.205681</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>117</td>\n",
              "      <td>279</td>\n",
              "      <td>2</td>\n",
              "      <td>0.088857</td>\n",
              "      <td>0.189818</td>\n",
              "      <td>-0.019101</td>\n",
              "      <td>-0.040973</td>\n",
              "      <td>0.205681</td>\n",
              "      <td>0.008427</td>\n",
              "      <td>4.670544</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.006751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>0.254887</td>\n",
              "      <td>0.229644</td>\n",
              "      <td>0.219243</td>\n",
              "      <td>0.208727</td>\n",
              "      <td>0.211046</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208727</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>154</td>\n",
              "      <td>260</td>\n",
              "      <td>2</td>\n",
              "      <td>0.099038</td>\n",
              "      <td>0.045293</td>\n",
              "      <td>0.047963</td>\n",
              "      <td>-0.011110</td>\n",
              "      <td>0.208727</td>\n",
              "      <td>0.002319</td>\n",
              "      <td>4.738300</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.009819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>0.251318</td>\n",
              "      <td>0.223806</td>\n",
              "      <td>0.233331</td>\n",
              "      <td>0.205197</td>\n",
              "      <td>0.213041</td>\n",
              "      <td>4</td>\n",
              "      <td>0.205197</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>203</td>\n",
              "      <td>253</td>\n",
              "      <td>1</td>\n",
              "      <td>0.109472</td>\n",
              "      <td>-0.042560</td>\n",
              "      <td>0.120577</td>\n",
              "      <td>-0.038228</td>\n",
              "      <td>0.205197</td>\n",
              "      <td>0.007844</td>\n",
              "      <td>4.693932</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0.007449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>0.260608</td>\n",
              "      <td>0.228955</td>\n",
              "      <td>0.227482</td>\n",
              "      <td>0.229004</td>\n",
              "      <td>0.224138</td>\n",
              "      <td>4</td>\n",
              "      <td>0.224138</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>170</td>\n",
              "      <td>226</td>\n",
              "      <td>2</td>\n",
              "      <td>0.121458</td>\n",
              "      <td>0.006433</td>\n",
              "      <td>-0.006692</td>\n",
              "      <td>0.021252</td>\n",
              "      <td>0.224138</td>\n",
              "      <td>-0.004763</td>\n",
              "      <td>4.558419</td>\n",
              "      <td>39.0</td>\n",
              "      <td>0.005539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>0.251808</td>\n",
              "      <td>0.227415</td>\n",
              "      <td>0.211501</td>\n",
              "      <td>0.210196</td>\n",
              "      <td>0.208264</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208264</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>206</td>\n",
              "      <td>290</td>\n",
              "      <td>1</td>\n",
              "      <td>0.096872</td>\n",
              "      <td>0.069980</td>\n",
              "      <td>0.006169</td>\n",
              "      <td>0.009189</td>\n",
              "      <td>0.208264</td>\n",
              "      <td>-0.001914</td>\n",
              "      <td>4.846117</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.016618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>0.280666</td>\n",
              "      <td>0.214852</td>\n",
              "      <td>0.212037</td>\n",
              "      <td>0.204635</td>\n",
              "      <td>0.204914</td>\n",
              "      <td>4</td>\n",
              "      <td>0.204635</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>178</td>\n",
              "      <td>252</td>\n",
              "      <td>1</td>\n",
              "      <td>0.234492</td>\n",
              "      <td>0.013103</td>\n",
              "      <td>0.034909</td>\n",
              "      <td>-0.001361</td>\n",
              "      <td>0.204635</td>\n",
              "      <td>0.000279</td>\n",
              "      <td>4.880104</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.021603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>0.303574</td>\n",
              "      <td>0.231410</td>\n",
              "      <td>0.216568</td>\n",
              "      <td>0.215206</td>\n",
              "      <td>0.221360</td>\n",
              "      <td>4</td>\n",
              "      <td>0.215206</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>230</td>\n",
              "      <td>262</td>\n",
              "      <td>2</td>\n",
              "      <td>0.237713</td>\n",
              "      <td>0.064138</td>\n",
              "      <td>0.006287</td>\n",
              "      <td>-0.028596</td>\n",
              "      <td>0.215206</td>\n",
              "      <td>0.006154</td>\n",
              "      <td>4.517519</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0.005144</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cfb4dcd6-88a1-4769-9c85-d13d7e37e4e7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cfb4dcd6-88a1-4769-9c85-d13d7e37e4e7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cfb4dcd6-88a1-4769-9c85-d13d7e37e4e7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           0         1         2  ...     score  model_rank  proba_selection\n",
              "0   0.297629  0.256342  0.224900  ...  4.946564         8.0         0.027004\n",
              "1   0.276555  0.242013  0.211087  ...  4.963635         6.0         0.036005\n",
              "2   0.296420  0.235832  0.227745  ...  4.702388        27.0         0.008001\n",
              "3   0.270336  0.227319  0.231308  ...  4.478359        44.0         0.004910\n",
              "4   0.285443  0.220161  0.226018  ...  4.387952        48.0         0.004501\n",
              "5   0.295962  0.254919  0.248721  ...  4.106723        54.0         0.004001\n",
              "6   0.264929  0.224174  0.228173  ...  4.583062        36.0         0.006001\n",
              "7   0.270235  0.228662  0.249313  ...  4.731872        25.0         0.008641\n",
              "8   0.257634  0.223843  0.245183  ...  4.470503        45.0         0.004801\n",
              "9   0.289596  0.240284  0.235281  ...  4.035327        57.0         0.003790\n",
              "10  0.254877  0.259745  0.230720  ...  4.312846        49.0         0.004409\n",
              "11  0.306085  0.269204  0.234900  ...  4.244900        52.0         0.004154\n",
              "12  0.258406  0.226609  0.231610  ...  4.774143        18.0         0.012002\n",
              "13  0.273785  0.234423  0.214054  ...  4.847430        12.0         0.018002\n",
              "14  0.281174  0.258473  0.242390  ...  4.281065        50.0         0.004321\n",
              "15  0.276611  0.257501  0.235830  ...  4.089106        55.0         0.003928\n",
              "16  0.290638  0.269898  0.222084  ...  4.678856        31.0         0.006969\n",
              "17  0.262872  0.227500  0.214436  ...  4.815277        14.0         0.015431\n",
              "18  0.310540  0.245770  0.228410  ...  4.769965        19.0         0.011370\n",
              "19  0.253787  0.230512  0.219922  ...  4.578315        38.0         0.005685\n",
              "20  0.283236  0.240035  0.243000  ...  4.666841        33.0         0.006546\n",
              "21  0.249221  0.231004  0.211921  ...  4.857491        11.0         0.019639\n",
              "22  0.265260  0.227288  0.217763  ...  5.138516         1.0         0.216029\n",
              "23  0.279411  0.242117  0.231796  ...  4.513156        43.0         0.005024\n",
              "24  0.311441  0.225901  0.217392  ...  4.735011        23.0         0.009393\n",
              "25  0.277852  0.236534  0.222586  ...  4.556380        40.0         0.005401\n",
              "26  0.296777  0.234284  0.219085  ...  4.724288        26.0         0.008309\n",
              "27  0.266089  0.236261  0.205056  ...  5.087436         3.0         0.072010\n",
              "28  0.290046  0.225338  0.206737  ...  4.774212        17.0         0.012708\n",
              "29  0.301766  0.262840  0.226363  ...  4.450873        46.0         0.004696\n",
              "30  0.244258  0.215385  0.213587  ...  4.769735        20.0         0.010801\n",
              "31  0.276123  0.232157  0.213064  ...  4.971918         5.0         0.043206\n",
              "32  0.272228  0.222163  0.217547  ...  4.951308         7.0         0.030861\n",
              "33  0.306602  0.233451  0.227916  ...  4.545162        41.0         0.005269\n",
              "34  0.273408  0.253417  0.254691  ...  4.250535        51.0         0.004236\n",
              "35  0.361880  0.293221  0.293664  ...  4.666209        34.0         0.006354\n",
              "36  0.312099  0.228898  0.216683  ...  5.007771         4.0         0.054007\n",
              "37  0.246799  0.225134  0.223399  ...  4.649433        35.0         0.006172\n",
              "38  0.282973  0.232447  0.231867  ...  4.927086         9.0         0.024003\n",
              "39  0.331409  0.273990  0.235165  ...  4.082514        56.0         0.003858\n",
              "40  0.260229  0.245010  0.218930  ...  4.694107        28.0         0.007715\n",
              "41  0.262993  0.228721  0.212792  ...  5.096542         2.0         0.108014\n",
              "42  0.252514  0.257067  0.209518  ...  4.774652        15.0         0.014402\n",
              "43  0.260101  0.238063  0.224836  ...  4.766950        21.0         0.010287\n",
              "44  0.280492  0.263231  0.236573  ...  4.406979        47.0         0.004596\n",
              "45  0.265284  0.243983  0.210468  ...  4.774213        16.0         0.013502\n",
              "46  0.246845  0.231376  0.210331  ...  4.688540        30.0         0.007201\n",
              "47  0.270416  0.232051  0.218978  ...  4.582514        37.0         0.005839\n",
              "48  0.289643  0.273529  0.258013  ...  4.242684        53.0         0.004076\n",
              "49  0.265743  0.212465  0.208062  ...  4.731877        24.0         0.009001\n",
              "50  0.278627  0.253869  0.205681  ...  4.670544        32.0         0.006751\n",
              "51  0.254887  0.229644  0.219243  ...  4.738300        22.0         0.009819\n",
              "52  0.251318  0.223806  0.233331  ...  4.693932        29.0         0.007449\n",
              "53  0.260608  0.228955  0.227482  ...  4.558419        39.0         0.005539\n",
              "54  0.251808  0.227415  0.211501  ...  4.846117        13.0         0.016618\n",
              "55  0.280666  0.214852  0.212037  ...  4.880104        10.0         0.021603\n",
              "56  0.303574  0.231410  0.216568  ...  4.517519        42.0         0.005144\n",
              "\n",
              "[57 rows x 21 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.20227106225987276, 156.0, 276.0, 1.0)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Génération :  10\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 110 hidden_dim : 322 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2775, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3337654449892544\n",
            "Epoch 1 : Validation loss = 0.2550368655472994; Validation r = 0.3850226979252692\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1682, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.5792004982792159\n",
            "Epoch 2 : Validation loss = 0.2275001335889101; Validation r = 0.49523892471121955\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.1008, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7767293166255752\n",
            "Epoch 3 : Validation loss = 0.2214143839975198; Validation r = 0.4924066741277902\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0578, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8857967425124332\n",
            "Epoch 4 : Validation loss = 0.21065408779929082; Validation r = 0.4936917869494211\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0336, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9350315862169596\n",
            "Epoch 5 : Validation loss = 0.21271081461260716; Validation r = 0.5041088274156197\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 171 hidden_dim : 277 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2783, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.35793750940119995\n",
            "Epoch 1 : Validation loss = 0.270681211600701; Validation r = 0.40978721196411894\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1537, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6318265268469155\n",
            "Epoch 2 : Validation loss = 0.22900813507537046; Validation r = 0.4841860146695137\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0798, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8310426194306754\n",
            "Epoch 3 : Validation loss = 0.2117907737692197; Validation r = 0.5230764843022665\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0393, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9225805807643599\n",
            "Epoch 4 : Validation loss = 0.2183250430971384; Validation r = 0.49754124851456843\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0228, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9542810585346956\n",
            "Epoch 5 : Validation loss = 0.2150384485721588; Validation r = 0.5137127666670505\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 170 hidden_dim : 310 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2702, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.39025139852798324\n",
            "Epoch 1 : Validation loss = 0.27816337073842684; Validation r = 0.40147315690214663\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1377, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6895197660206874\n",
            "Epoch 2 : Validation loss = 0.21945203815897305; Validation r = 0.5064858432053545\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0683, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8589567299958899\n",
            "Epoch 3 : Validation loss = 0.21088090191284817; Validation r = 0.5238304572557593\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0339, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9345530420916921\n",
            "Epoch 4 : Validation loss = 0.20827058317760627; Validation r = 0.5306801160362415\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0189, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9639788843098996\n",
            "Epoch 5 : Validation loss = 0.20293748726447422; Validation r = 0.5399093763902976\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 139 hidden_dim : 351 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2604, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.39094017890690386\n",
            "Epoch 1 : Validation loss = 0.26020020581781866; Validation r = 0.42629059553851517\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1402, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6754348444879298\n",
            "Epoch 2 : Validation loss = 0.22097573752204577; Validation r = 0.4922517464038572\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0762, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8424506979648374\n",
            "Epoch 3 : Validation loss = 0.21338194099565347; Validation r = 0.5245776972114409\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.915647658147042\n",
            "Epoch 4 : Validation loss = 0.21372663242121537; Validation r = 0.5234544040801146\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0229, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9571935050056462\n",
            "Epoch 5 : Validation loss = 0.2117124514033397; Validation r = 0.5266555839657927\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 113 hidden_dim : 297 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2645, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.38619703607361955\n",
            "Epoch 1 : Validation loss = 0.25748519226908684; Validation r = 0.4280614962089419\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1433, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6708347854427028\n",
            "Epoch 2 : Validation loss = 0.2200489035497109; Validation r = 0.4923332805709139\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0842, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8225807939494889\n",
            "Epoch 3 : Validation loss = 0.21600240282714367; Validation r = 0.5075546437956531\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9144062574831759\n",
            "Epoch 4 : Validation loss = 0.208729733278354; Validation r = 0.5193513166742753\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0252, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9521622078351555\n",
            "Epoch 5 : Validation loss = 0.21193097593883672; Validation r = 0.5112342230763798\n",
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-148449ed-5dae-463a-a9d3-dcaed960a11f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>max_epoch</th>\n",
              "      <th>best_dev_loss</th>\n",
              "      <th>best_dev_loss_epoch</th>\n",
              "      <th>lastEpochImproved</th>\n",
              "      <th>embedding_dim</th>\n",
              "      <th>hidden_dim</th>\n",
              "      <th>num_lstm_layers</th>\n",
              "      <th>loss_decrease_perc_0</th>\n",
              "      <th>loss_decrease_perc_1</th>\n",
              "      <th>loss_decrease_perc_2</th>\n",
              "      <th>loss_decrease_perc_3</th>\n",
              "      <th>score_perf</th>\n",
              "      <th>score_pot</th>\n",
              "      <th>score</th>\n",
              "      <th>model_rank</th>\n",
              "      <th>proba_selection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.297629</td>\n",
              "      <td>0.256342</td>\n",
              "      <td>0.224900</td>\n",
              "      <td>0.221605</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.138718</td>\n",
              "      <td>0.122657</td>\n",
              "      <td>0.014653</td>\n",
              "      <td>0.044878</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>-0.009499</td>\n",
              "      <td>4.946564</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.023578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.276555</td>\n",
              "      <td>0.242013</td>\n",
              "      <td>0.211087</td>\n",
              "      <td>0.214948</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.124904</td>\n",
              "      <td>0.127787</td>\n",
              "      <td>-0.018291</td>\n",
              "      <td>0.031870</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>-0.006632</td>\n",
              "      <td>4.963635</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.030315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.296420</td>\n",
              "      <td>0.235832</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.233284</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>175</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.204399</td>\n",
              "      <td>0.034291</td>\n",
              "      <td>-0.024319</td>\n",
              "      <td>0.045231</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>-0.010074</td>\n",
              "      <td>4.702388</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.006845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.270336</td>\n",
              "      <td>0.227319</td>\n",
              "      <td>0.231308</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.223296</td>\n",
              "      <td>4</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>175</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.159127</td>\n",
              "      <td>-0.017549</td>\n",
              "      <td>0.089803</td>\n",
              "      <td>-0.060609</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.012760</td>\n",
              "      <td>4.478359</td>\n",
              "      <td>49.0</td>\n",
              "      <td>0.004331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.285443</td>\n",
              "      <td>0.220161</td>\n",
              "      <td>0.226018</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.227897</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.228703</td>\n",
              "      <td>-0.026604</td>\n",
              "      <td>0.057750</td>\n",
              "      <td>-0.070109</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.014931</td>\n",
              "      <td>4.387952</td>\n",
              "      <td>53.0</td>\n",
              "      <td>0.004004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>0.255037</td>\n",
              "      <td>0.227500</td>\n",
              "      <td>0.221414</td>\n",
              "      <td>0.210654</td>\n",
              "      <td>0.212711</td>\n",
              "      <td>4</td>\n",
              "      <td>0.210654</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>110</td>\n",
              "      <td>322</td>\n",
              "      <td>2</td>\n",
              "      <td>0.107972</td>\n",
              "      <td>0.026751</td>\n",
              "      <td>0.048598</td>\n",
              "      <td>-0.009764</td>\n",
              "      <td>0.210654</td>\n",
              "      <td>0.002057</td>\n",
              "      <td>4.701218</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.006631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>0.270681</td>\n",
              "      <td>0.229008</td>\n",
              "      <td>0.211791</td>\n",
              "      <td>0.218325</td>\n",
              "      <td>0.215038</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211791</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>171</td>\n",
              "      <td>277</td>\n",
              "      <td>2</td>\n",
              "      <td>0.153956</td>\n",
              "      <td>0.075182</td>\n",
              "      <td>-0.030852</td>\n",
              "      <td>0.015054</td>\n",
              "      <td>0.211791</td>\n",
              "      <td>-0.003188</td>\n",
              "      <td>4.793805</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.013263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>0.278163</td>\n",
              "      <td>0.219452</td>\n",
              "      <td>0.210881</td>\n",
              "      <td>0.208271</td>\n",
              "      <td>0.202937</td>\n",
              "      <td>4</td>\n",
              "      <td>0.202937</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>170</td>\n",
              "      <td>310</td>\n",
              "      <td>1</td>\n",
              "      <td>0.211068</td>\n",
              "      <td>0.039057</td>\n",
              "      <td>0.012378</td>\n",
              "      <td>0.025607</td>\n",
              "      <td>0.202937</td>\n",
              "      <td>-0.005197</td>\n",
              "      <td>5.057121</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.053052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>0.260200</td>\n",
              "      <td>0.220976</td>\n",
              "      <td>0.213382</td>\n",
              "      <td>0.213727</td>\n",
              "      <td>0.211712</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211712</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>139</td>\n",
              "      <td>351</td>\n",
              "      <td>1</td>\n",
              "      <td>0.150747</td>\n",
              "      <td>0.034365</td>\n",
              "      <td>-0.001615</td>\n",
              "      <td>0.009424</td>\n",
              "      <td>0.211712</td>\n",
              "      <td>-0.001995</td>\n",
              "      <td>4.768325</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.009226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>0.257485</td>\n",
              "      <td>0.220049</td>\n",
              "      <td>0.216002</td>\n",
              "      <td>0.208730</td>\n",
              "      <td>0.211931</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208730</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>113</td>\n",
              "      <td>297</td>\n",
              "      <td>1</td>\n",
              "      <td>0.145392</td>\n",
              "      <td>0.018389</td>\n",
              "      <td>0.033669</td>\n",
              "      <td>-0.015337</td>\n",
              "      <td>0.208730</td>\n",
              "      <td>0.003201</td>\n",
              "      <td>4.718517</td>\n",
              "      <td>30.0</td>\n",
              "      <td>0.007074</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>62 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-148449ed-5dae-463a-a9d3-dcaed960a11f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-148449ed-5dae-463a-a9d3-dcaed960a11f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-148449ed-5dae-463a-a9d3-dcaed960a11f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           0         1         2  ...     score  model_rank  proba_selection\n",
              "0   0.297629  0.256342  0.224900  ...  4.946564         9.0         0.023578\n",
              "1   0.276555  0.242013  0.211087  ...  4.963635         7.0         0.030315\n",
              "2   0.296420  0.235832  0.227745  ...  4.702388        31.0         0.006845\n",
              "3   0.270336  0.227319  0.231308  ...  4.478359        49.0         0.004331\n",
              "4   0.285443  0.220161  0.226018  ...  4.387952        53.0         0.004004\n",
              "..       ...       ...       ...  ...       ...         ...              ...\n",
              "57  0.255037  0.227500  0.221414  ...  4.701218        32.0         0.006631\n",
              "58  0.270681  0.229008  0.211791  ...  4.793805        16.0         0.013263\n",
              "59  0.278163  0.219452  0.210881  ...  5.057121         4.0         0.053052\n",
              "60  0.260200  0.220976  0.213382  ...  4.768325        23.0         0.009226\n",
              "61  0.257485  0.220049  0.216002  ...  4.718517        30.0         0.007074\n",
              "\n",
              "[62 rows x 21 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.20227106225987276, 156.0, 276.0, 1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recherche par générations\n",
        "ngen = 10\n",
        "n_child = 5\n",
        "\n",
        "for i in range (ngen):\n",
        "  print('\\n\\n\\n\\n')\n",
        "  print(\"Génération : \" , i+1)\n",
        "  print('\\n\\n\\n\\n')\n",
        "  add_generation(results, n = n_child, temperature = 1)\n",
        "  df_results = create_df_res(results, temperature = 1)\n",
        "  display(df_results)\n",
        "  print(get_best_model(df_results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tSwauHGa7zQI",
        "outputId": "ed138c6f-dcce-444f-faa9-2ef7f77b86c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Génération :  1\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 153 hidden_dim : 319 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2647, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.395600032191333\n",
            "Epoch 1 : Validation loss = 0.24230645758410294; Validation r = 0.45907069685181134\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1339, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7006618564089536\n",
            "Epoch 2 : Validation loss = 0.23873397732774418; Validation r = 0.4998645680030075\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0727, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.848403307286989\n",
            "Epoch 3 : Validation loss = 0.21664260687927406; Validation r = 0.5338677609810439\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0380, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9253893619992047\n",
            "Epoch 4 : Validation loss = 0.2119001696507136; Validation r = 0.5306911744281876\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0220, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9573556818193395\n",
            "Epoch 5 : Validation loss = 0.20914379184444745; Validation r = 0.541187131989875\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 358 hidden_dim : 161 num_lstm_layers : 3 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.3042, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3230019659939066\n",
            "Epoch 1 : Validation loss = 0.29750266795357067; Validation r = 0.4118386523175625\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1619, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6152999502028442\n",
            "Epoch 2 : Validation loss = 0.2403592107196649; Validation r = 0.47726714103937623\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0884, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8098690409025721\n",
            "Epoch 3 : Validation loss = 0.23410521944363913; Validation r = 0.5060757253845408\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9017112050129541\n",
            "Epoch 4 : Validation loss = 0.23298537495235602; Validation r = 0.500708911619324\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0296, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.940071951603469\n",
            "Epoch 5 : Validation loss = 0.22008728937556346; Validation r = 0.5144800140816864\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 160 hidden_dim : 310 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2580, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.39645048963168755\n",
            "Epoch 1 : Validation loss = 0.29972489376862843; Validation r = 0.4381256644030055\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1326, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7030456179093751\n",
            "Epoch 2 : Validation loss = 0.23494867086410523; Validation r = 0.5040439731314438\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0740, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.846293778807803\n",
            "Epoch 3 : Validation loss = 0.21732154923180738; Validation r = 0.506400416548116\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0385, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9248102645989744\n",
            "Epoch 4 : Validation loss = 0.21713331378996373; Validation r = 0.5231650184366169\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0218, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9581763457644416\n",
            "Epoch 5 : Validation loss = 0.21149426822861037; Validation r = 0.5201479793103368\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 172 hidden_dim : 272 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2749, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.35799649972203107\n",
            "Epoch 1 : Validation loss = 0.2492136010279258; Validation r = 0.45670286674308636\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1563, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.626812275745027\n",
            "Epoch 2 : Validation loss = 0.22591965335110822; Validation r = 0.46731747241547533\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0845, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8203336748546219\n",
            "Epoch 3 : Validation loss = 0.21043944731354713; Validation r = 0.5132946120182931\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0406, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9186189167794075\n",
            "Epoch 4 : Validation loss = 0.20894971924523512; Validation r = 0.5233396780956231\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0215, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9572195983050965\n",
            "Epoch 5 : Validation loss = 0.20617497625450293; Validation r = 0.5293224963996087\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 127 hidden_dim : 347 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2846, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.31676011274393717\n",
            "Epoch 1 : Validation loss = 0.2638820317884286; Validation r = 0.3866748118140889\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1871, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.5174710163252813\n",
            "Epoch 2 : Validation loss = 0.22598667964339256; Validation r = 0.4357683024803636\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.1186, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7231607514635384\n",
            "Epoch 3 : Validation loss = 0.22052957105139892; Validation r = 0.4656826817909633\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0695, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8577425275750905\n",
            "Epoch 4 : Validation loss = 0.21451786682009696; Validation r = 0.4895359131428968\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0386, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9260979663342944\n",
            "Epoch 5 : Validation loss = 0.21720547303557397; Validation r = 0.4764060842713174\n",
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e6c23b36-cd33-4d4a-91fe-ab322b668b85\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>max_epoch</th>\n",
              "      <th>best_dev_loss</th>\n",
              "      <th>best_dev_loss_epoch</th>\n",
              "      <th>lastEpochImproved</th>\n",
              "      <th>embedding_dim</th>\n",
              "      <th>hidden_dim</th>\n",
              "      <th>num_lstm_layers</th>\n",
              "      <th>loss_decrease_perc_0</th>\n",
              "      <th>loss_decrease_perc_1</th>\n",
              "      <th>loss_decrease_perc_2</th>\n",
              "      <th>loss_decrease_perc_3</th>\n",
              "      <th>score_perf</th>\n",
              "      <th>score_pot</th>\n",
              "      <th>score</th>\n",
              "      <th>model_rank</th>\n",
              "      <th>proba_selection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.297629</td>\n",
              "      <td>0.256342</td>\n",
              "      <td>0.224900</td>\n",
              "      <td>0.221605</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.138718</td>\n",
              "      <td>0.122657</td>\n",
              "      <td>0.014653</td>\n",
              "      <td>0.044878</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>-0.009499</td>\n",
              "      <td>4.946564</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.023200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.276555</td>\n",
              "      <td>0.242013</td>\n",
              "      <td>0.211087</td>\n",
              "      <td>0.214948</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.124904</td>\n",
              "      <td>0.127787</td>\n",
              "      <td>-0.018291</td>\n",
              "      <td>0.031870</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>-0.006632</td>\n",
              "      <td>4.963635</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.029828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.296420</td>\n",
              "      <td>0.235832</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.233284</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>175</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.204399</td>\n",
              "      <td>0.034291</td>\n",
              "      <td>-0.024319</td>\n",
              "      <td>0.045231</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>-0.010074</td>\n",
              "      <td>4.702388</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0.005966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.270336</td>\n",
              "      <td>0.227319</td>\n",
              "      <td>0.231308</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.223296</td>\n",
              "      <td>4</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>175</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.159127</td>\n",
              "      <td>-0.017549</td>\n",
              "      <td>0.089803</td>\n",
              "      <td>-0.060609</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.012760</td>\n",
              "      <td>4.478359</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.003867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.285443</td>\n",
              "      <td>0.220161</td>\n",
              "      <td>0.226018</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.227897</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.228703</td>\n",
              "      <td>-0.026604</td>\n",
              "      <td>0.057750</td>\n",
              "      <td>-0.070109</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.014931</td>\n",
              "      <td>4.387952</td>\n",
              "      <td>58.0</td>\n",
              "      <td>0.003600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>0.242306</td>\n",
              "      <td>0.238734</td>\n",
              "      <td>0.216643</td>\n",
              "      <td>0.211900</td>\n",
              "      <td>0.209144</td>\n",
              "      <td>4</td>\n",
              "      <td>0.209144</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>153</td>\n",
              "      <td>319</td>\n",
              "      <td>1</td>\n",
              "      <td>0.014744</td>\n",
              "      <td>0.092536</td>\n",
              "      <td>0.021891</td>\n",
              "      <td>0.013008</td>\n",
              "      <td>0.209144</td>\n",
              "      <td>-0.002721</td>\n",
              "      <td>4.844415</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.012282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>0.297503</td>\n",
              "      <td>0.240359</td>\n",
              "      <td>0.234105</td>\n",
              "      <td>0.232985</td>\n",
              "      <td>0.220087</td>\n",
              "      <td>4</td>\n",
              "      <td>0.220087</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>358</td>\n",
              "      <td>161</td>\n",
              "      <td>3</td>\n",
              "      <td>0.192077</td>\n",
              "      <td>0.026019</td>\n",
              "      <td>0.004784</td>\n",
              "      <td>0.055360</td>\n",
              "      <td>0.220087</td>\n",
              "      <td>-0.012184</td>\n",
              "      <td>4.809930</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0.010989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>0.299725</td>\n",
              "      <td>0.234949</td>\n",
              "      <td>0.217322</td>\n",
              "      <td>0.217133</td>\n",
              "      <td>0.211494</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211494</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>160</td>\n",
              "      <td>310</td>\n",
              "      <td>1</td>\n",
              "      <td>0.216119</td>\n",
              "      <td>0.075025</td>\n",
              "      <td>0.000866</td>\n",
              "      <td>0.025970</td>\n",
              "      <td>0.211494</td>\n",
              "      <td>-0.005493</td>\n",
              "      <td>4.854330</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.014914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>0.249214</td>\n",
              "      <td>0.225920</td>\n",
              "      <td>0.210439</td>\n",
              "      <td>0.208950</td>\n",
              "      <td>0.206175</td>\n",
              "      <td>4</td>\n",
              "      <td>0.206175</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>172</td>\n",
              "      <td>272</td>\n",
              "      <td>2</td>\n",
              "      <td>0.093470</td>\n",
              "      <td>0.068521</td>\n",
              "      <td>0.007079</td>\n",
              "      <td>0.013279</td>\n",
              "      <td>0.206175</td>\n",
              "      <td>-0.002738</td>\n",
              "      <td>4.915525</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.018981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>0.263882</td>\n",
              "      <td>0.225987</td>\n",
              "      <td>0.220530</td>\n",
              "      <td>0.214518</td>\n",
              "      <td>0.217205</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214518</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>127</td>\n",
              "      <td>347</td>\n",
              "      <td>2</td>\n",
              "      <td>0.143607</td>\n",
              "      <td>0.024148</td>\n",
              "      <td>0.027260</td>\n",
              "      <td>-0.012529</td>\n",
              "      <td>0.214518</td>\n",
              "      <td>0.002688</td>\n",
              "      <td>4.603936</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.004640</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>67 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e6c23b36-cd33-4d4a-91fe-ab322b668b85')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e6c23b36-cd33-4d4a-91fe-ab322b668b85 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e6c23b36-cd33-4d4a-91fe-ab322b668b85');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           0         1         2  ...     score  model_rank  proba_selection\n",
              "0   0.297629  0.256342  0.224900  ...  4.946564         9.0         0.023200\n",
              "1   0.276555  0.242013  0.211087  ...  4.963635         7.0         0.029828\n",
              "2   0.296420  0.235832  0.227745  ...  4.702388        35.0         0.005966\n",
              "3   0.270336  0.227319  0.231308  ...  4.478359        54.0         0.003867\n",
              "4   0.285443  0.220161  0.226018  ...  4.387952        58.0         0.003600\n",
              "..       ...       ...       ...  ...       ...         ...              ...\n",
              "62  0.242306  0.238734  0.216643  ...  4.844415        17.0         0.012282\n",
              "63  0.297503  0.240359  0.234105  ...  4.809930        19.0         0.010989\n",
              "64  0.299725  0.234949  0.217322  ...  4.854330        14.0         0.014914\n",
              "65  0.249214  0.225920  0.210439  ...  4.915525        11.0         0.018981\n",
              "66  0.263882  0.225987  0.220530  ...  4.603936        45.0         0.004640\n",
              "\n",
              "[67 rows x 21 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.20227106225987276, 156.0, 276.0, 1.0)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Génération :  2\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 136 hidden_dim : 100 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2762, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.39008527733331166\n",
            "Epoch 1 : Validation loss = 0.28879579020043217; Validation r = 0.4485596996973028\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1331, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7162984697914434\n",
            "Epoch 2 : Validation loss = 0.274250461657842; Validation r = 0.4731627724017081\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0695, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8568706194632593\n",
            "Epoch 3 : Validation loss = 0.2355596954623858; Validation r = 0.5035666140382372\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0365, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9265869857379012\n",
            "Epoch 4 : Validation loss = 0.23468053036679823; Validation r = 0.4975646725963123\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0243, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9511413564010659\n",
            "Epoch 5 : Validation loss = 0.21988778275748094; Validation r = 0.5080117717254279\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 123 hidden_dim : 269 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2834, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3409532610790093\n",
            "Epoch 1 : Validation loss = 0.2740355602155129; Validation r = 0.3778672214002768\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1641, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6016188169149649\n",
            "Epoch 2 : Validation loss = 0.2349780172109604; Validation r = 0.4604487103719477\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0955, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7902929683855234\n",
            "Epoch 3 : Validation loss = 0.22173213412364323; Validation r = 0.48204221912079176\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8997450498642857\n",
            "Epoch 4 : Validation loss = 0.2171759081383546; Validation r = 0.5036959256630468\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0282, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9450281743685248\n",
            "Epoch 5 : Validation loss = 0.22058882241447766; Validation r = 0.4913119247073145\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 128 hidden_dim : 313 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2613, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.4008705673343956\n",
            "Epoch 1 : Validation loss = 0.25132216041286787; Validation r = 0.4583664461836188\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1382, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6859352330350047\n",
            "Epoch 2 : Validation loss = 0.24637225543459257; Validation r = 0.4727681905267522\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0745, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.845976713626906\n",
            "Epoch 3 : Validation loss = 0.21970975808799267; Validation r = 0.5221239846838452\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9170952885427844\n",
            "Epoch 4 : Validation loss = 0.2176044670244058; Validation r = 0.5230563607252179\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0249, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9522419225604977\n",
            "Epoch 5 : Validation loss = 0.21351609639823438; Validation r = 0.5314043909762448\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 142 hidden_dim : 355 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2587, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3834830476288485\n",
            "Epoch 1 : Validation loss = 0.242514772216479; Validation r = 0.4335940180912329\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1371, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6832358084952443\n",
            "Epoch 2 : Validation loss = 0.21708797812461852; Validation r = 0.5185262060823395\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0718, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8525728575552176\n",
            "Epoch 3 : Validation loss = 0.2122993631909291; Validation r = 0.5130657548669669\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0363, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9303444168512526\n",
            "Epoch 4 : Validation loss = 0.2050083227455616; Validation r = 0.5201617334940024\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0211, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9602736095504668\n",
            "Epoch 5 : Validation loss = 0.20698169929285845; Validation r = 0.5292488870370206\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 191 hidden_dim : 279 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2597, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.4035781181933928\n",
            "Epoch 1 : Validation loss = 0.27689453462759656; Validation r = 0.41643534884898725\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1214, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7297458253087988\n",
            "Epoch 2 : Validation loss = 0.231067934508125; Validation r = 0.5125956066778509\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0638, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.869078371647558\n",
            "Epoch 3 : Validation loss = 0.21028775634864966; Validation r = 0.52459805966572\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0311, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.93841643960286\n",
            "Epoch 4 : Validation loss = 0.20970359270771344; Validation r = 0.5232512351000942\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0177, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9651497063628681\n",
            "Epoch 5 : Validation loss = 0.20170069709420205; Validation r = 0.532226336259317\n",
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-bda407c1-1888-41c1-a99e-da62e82d9043\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>max_epoch</th>\n",
              "      <th>best_dev_loss</th>\n",
              "      <th>best_dev_loss_epoch</th>\n",
              "      <th>lastEpochImproved</th>\n",
              "      <th>embedding_dim</th>\n",
              "      <th>hidden_dim</th>\n",
              "      <th>num_lstm_layers</th>\n",
              "      <th>loss_decrease_perc_0</th>\n",
              "      <th>loss_decrease_perc_1</th>\n",
              "      <th>loss_decrease_perc_2</th>\n",
              "      <th>loss_decrease_perc_3</th>\n",
              "      <th>score_perf</th>\n",
              "      <th>score_pot</th>\n",
              "      <th>score</th>\n",
              "      <th>model_rank</th>\n",
              "      <th>proba_selection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.297629</td>\n",
              "      <td>0.256342</td>\n",
              "      <td>0.224900</td>\n",
              "      <td>0.221605</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.138718</td>\n",
              "      <td>0.122657</td>\n",
              "      <td>0.014653</td>\n",
              "      <td>0.044878</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>-0.009499</td>\n",
              "      <td>4.946564</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.020573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.276555</td>\n",
              "      <td>0.242013</td>\n",
              "      <td>0.211087</td>\n",
              "      <td>0.214948</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.124904</td>\n",
              "      <td>0.127787</td>\n",
              "      <td>-0.018291</td>\n",
              "      <td>0.031870</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>-0.006632</td>\n",
              "      <td>4.963635</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.025716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.296420</td>\n",
              "      <td>0.235832</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.233284</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>175</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.204399</td>\n",
              "      <td>0.034291</td>\n",
              "      <td>-0.024319</td>\n",
              "      <td>0.045231</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>-0.010074</td>\n",
              "      <td>4.702388</td>\n",
              "      <td>39.0</td>\n",
              "      <td>0.005275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.270336</td>\n",
              "      <td>0.227319</td>\n",
              "      <td>0.231308</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.223296</td>\n",
              "      <td>4</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>175</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.159127</td>\n",
              "      <td>-0.017549</td>\n",
              "      <td>0.089803</td>\n",
              "      <td>-0.060609</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.012760</td>\n",
              "      <td>4.478359</td>\n",
              "      <td>59.0</td>\n",
              "      <td>0.003487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.285443</td>\n",
              "      <td>0.220161</td>\n",
              "      <td>0.226018</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.227897</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.228703</td>\n",
              "      <td>-0.026604</td>\n",
              "      <td>0.057750</td>\n",
              "      <td>-0.070109</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.014931</td>\n",
              "      <td>4.387952</td>\n",
              "      <td>63.0</td>\n",
              "      <td>0.003266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>0.288796</td>\n",
              "      <td>0.274250</td>\n",
              "      <td>0.235560</td>\n",
              "      <td>0.234681</td>\n",
              "      <td>0.219888</td>\n",
              "      <td>4</td>\n",
              "      <td>0.219888</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>136</td>\n",
              "      <td>100</td>\n",
              "      <td>2</td>\n",
              "      <td>0.050365</td>\n",
              "      <td>0.141078</td>\n",
              "      <td>0.003732</td>\n",
              "      <td>0.063034</td>\n",
              "      <td>0.219888</td>\n",
              "      <td>-0.013860</td>\n",
              "      <td>4.853722</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.012858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>0.274036</td>\n",
              "      <td>0.234978</td>\n",
              "      <td>0.221732</td>\n",
              "      <td>0.217176</td>\n",
              "      <td>0.220589</td>\n",
              "      <td>4</td>\n",
              "      <td>0.217176</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>123</td>\n",
              "      <td>269</td>\n",
              "      <td>2</td>\n",
              "      <td>0.142527</td>\n",
              "      <td>0.056371</td>\n",
              "      <td>0.020548</td>\n",
              "      <td>-0.015715</td>\n",
              "      <td>0.217176</td>\n",
              "      <td>0.003413</td>\n",
              "      <td>4.533321</td>\n",
              "      <td>56.0</td>\n",
              "      <td>0.003674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>0.251322</td>\n",
              "      <td>0.246372</td>\n",
              "      <td>0.219710</td>\n",
              "      <td>0.217604</td>\n",
              "      <td>0.213516</td>\n",
              "      <td>4</td>\n",
              "      <td>0.213516</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>128</td>\n",
              "      <td>313</td>\n",
              "      <td>1</td>\n",
              "      <td>0.019695</td>\n",
              "      <td>0.108220</td>\n",
              "      <td>0.009582</td>\n",
              "      <td>0.018788</td>\n",
              "      <td>0.213516</td>\n",
              "      <td>-0.004012</td>\n",
              "      <td>4.773166</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0.007347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>0.242515</td>\n",
              "      <td>0.217088</td>\n",
              "      <td>0.212299</td>\n",
              "      <td>0.205008</td>\n",
              "      <td>0.206982</td>\n",
              "      <td>4</td>\n",
              "      <td>0.205008</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>142</td>\n",
              "      <td>355</td>\n",
              "      <td>1</td>\n",
              "      <td>0.104846</td>\n",
              "      <td>0.022058</td>\n",
              "      <td>0.034343</td>\n",
              "      <td>-0.009626</td>\n",
              "      <td>0.205008</td>\n",
              "      <td>0.001973</td>\n",
              "      <td>4.831345</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.010286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>0.276895</td>\n",
              "      <td>0.231068</td>\n",
              "      <td>0.210288</td>\n",
              "      <td>0.209704</td>\n",
              "      <td>0.201701</td>\n",
              "      <td>4</td>\n",
              "      <td>0.201701</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>191</td>\n",
              "      <td>279</td>\n",
              "      <td>1</td>\n",
              "      <td>0.165502</td>\n",
              "      <td>0.089931</td>\n",
              "      <td>0.002778</td>\n",
              "      <td>0.038163</td>\n",
              "      <td>0.201701</td>\n",
              "      <td>-0.007697</td>\n",
              "      <td>5.154554</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.205727</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>72 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bda407c1-1888-41c1-a99e-da62e82d9043')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bda407c1-1888-41c1-a99e-da62e82d9043 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bda407c1-1888-41c1-a99e-da62e82d9043');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           0         1         2  ...     score  model_rank  proba_selection\n",
              "0   0.297629  0.256342  0.224900  ...  4.946564        10.0         0.020573\n",
              "1   0.276555  0.242013  0.211087  ...  4.963635         8.0         0.025716\n",
              "2   0.296420  0.235832  0.227745  ...  4.702388        39.0         0.005275\n",
              "3   0.270336  0.227319  0.231308  ...  4.478359        59.0         0.003487\n",
              "4   0.285443  0.220161  0.226018  ...  4.387952        63.0         0.003266\n",
              "..       ...       ...       ...  ...       ...         ...              ...\n",
              "67  0.288796  0.274250  0.235560  ...  4.853722        16.0         0.012858\n",
              "68  0.274036  0.234978  0.221732  ...  4.533321        56.0         0.003674\n",
              "69  0.251322  0.246372  0.219710  ...  4.773166        28.0         0.007347\n",
              "70  0.242515  0.217088  0.212299  ...  4.831345        20.0         0.010286\n",
              "71  0.276895  0.231068  0.210288  ...  5.154554         1.0         0.205727\n",
              "\n",
              "[72 rows x 21 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.20170069709420205, 191.0, 279.0, 1.0)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Génération :  3\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 163 hidden_dim : 332 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2745, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3541676065949392\n",
            "Epoch 1 : Validation loss = 0.26446775210400425; Validation r = 0.40644775539561323\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1550, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6252927797571427\n",
            "Epoch 2 : Validation loss = 0.23654346019029618; Validation r = 0.47932917718611334\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0834, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8216739494342498\n",
            "Epoch 3 : Validation loss = 0.23534732510646184; Validation r = 0.4442670658083801\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9139763916696915\n",
            "Epoch 4 : Validation loss = 0.22427771960695583; Validation r = 0.4712047242913411\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0245, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9517027917787293\n",
            "Epoch 5 : Validation loss = 0.22685343908766906; Validation r = 0.46545135543973376\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 143 hidden_dim : 313 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2602, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3994314882615019\n",
            "Epoch 1 : Validation loss = 0.26634110795954863; Validation r = 0.44852645486013293\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1373, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6892120216844684\n",
            "Epoch 2 : Validation loss = 0.22407528795301915; Validation r = 0.5122163648245098\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0700, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.856844477530374\n",
            "Epoch 3 : Validation loss = 0.21971743454535803; Validation r = 0.5270738794167701\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0354, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9315257076698734\n",
            "Epoch 4 : Validation loss = 0.2046367307504018; Validation r = 0.5448175924666554\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0202, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9615624652710693\n",
            "Epoch 5 : Validation loss = 0.20444491518040497; Validation r = 0.5466006774499523\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 201 hidden_dim : 278 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2612, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.40006656004562396\n",
            "Epoch 1 : Validation loss = 0.2509156096726656; Validation r = 0.47442386104021395\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1189, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7366187501507158\n",
            "Epoch 2 : Validation loss = 0.22587257611254852; Validation r = 0.5154679948807313\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0566, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8848082042761989\n",
            "Epoch 3 : Validation loss = 0.22383901563783487; Validation r = 0.5137941169611597\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0297, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9412340550077676\n",
            "Epoch 4 : Validation loss = 0.21183149702847004; Validation r = 0.5340684797248489\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0181, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9643320551979283\n",
            "Epoch 5 : Validation loss = 0.21457833275198937; Validation r = 0.5341240303364724\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 207 hidden_dim : 285 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2537, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.4215705138450688\n",
            "Epoch 1 : Validation loss = 0.24127171225845814; Validation r = 0.5080624395664636\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1149, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7533244447890369\n",
            "Epoch 2 : Validation loss = 0.2151905690630277; Validation r = 0.5294059507380741\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0555, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8885187682525576\n",
            "Epoch 3 : Validation loss = 0.2089590910822153; Validation r = 0.5407955410189568\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0277, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9457107858789973\n",
            "Epoch 4 : Validation loss = 0.20320635065436363; Validation r = 0.5525769855388054\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0166, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9670782869382982\n",
            "Epoch 5 : Validation loss = 0.19901037688056628; Validation r = 0.5555757063837226\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 212 hidden_dim : 297 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2564, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.4092495223816605\n",
            "Epoch 1 : Validation loss = 0.24310885444283487; Validation r = 0.5160395997131976\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1143, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7503333673569068\n",
            "Epoch 2 : Validation loss = 0.2181070141494274; Validation r = 0.5327414118094043\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0530, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8938126130614713\n",
            "Epoch 3 : Validation loss = 0.21978303119540216; Validation r = 0.522213378482381\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0269, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9471498040480358\n",
            "Epoch 4 : Validation loss = 0.2105062160640955; Validation r = 0.53398102066345\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0163, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.967549071636822\n",
            "Epoch 5 : Validation loss = 0.20867482001582782; Validation r = 0.5441706910663726\n",
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a1290747-d7f0-4f88-9ae4-b1c381defb4d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>max_epoch</th>\n",
              "      <th>best_dev_loss</th>\n",
              "      <th>best_dev_loss_epoch</th>\n",
              "      <th>lastEpochImproved</th>\n",
              "      <th>embedding_dim</th>\n",
              "      <th>hidden_dim</th>\n",
              "      <th>num_lstm_layers</th>\n",
              "      <th>loss_decrease_perc_0</th>\n",
              "      <th>loss_decrease_perc_1</th>\n",
              "      <th>loss_decrease_perc_2</th>\n",
              "      <th>loss_decrease_perc_3</th>\n",
              "      <th>score_perf</th>\n",
              "      <th>score_pot</th>\n",
              "      <th>score</th>\n",
              "      <th>model_rank</th>\n",
              "      <th>proba_selection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.297629</td>\n",
              "      <td>0.256342</td>\n",
              "      <td>0.224900</td>\n",
              "      <td>0.221605</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.138718</td>\n",
              "      <td>0.122657</td>\n",
              "      <td>0.014653</td>\n",
              "      <td>0.044878</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>-0.009499</td>\n",
              "      <td>4.946564</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.018449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.276555</td>\n",
              "      <td>0.242013</td>\n",
              "      <td>0.211087</td>\n",
              "      <td>0.214948</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.124904</td>\n",
              "      <td>0.127787</td>\n",
              "      <td>-0.018291</td>\n",
              "      <td>0.031870</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>-0.006632</td>\n",
              "      <td>4.963635</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.022549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.296420</td>\n",
              "      <td>0.235832</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.233284</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>175</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.204399</td>\n",
              "      <td>0.034291</td>\n",
              "      <td>-0.024319</td>\n",
              "      <td>0.045231</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>-0.010074</td>\n",
              "      <td>4.702388</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0.004832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.270336</td>\n",
              "      <td>0.227319</td>\n",
              "      <td>0.231308</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.223296</td>\n",
              "      <td>4</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>175</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.159127</td>\n",
              "      <td>-0.017549</td>\n",
              "      <td>0.089803</td>\n",
              "      <td>-0.060609</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.012760</td>\n",
              "      <td>4.478359</td>\n",
              "      <td>63.0</td>\n",
              "      <td>0.003221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.285443</td>\n",
              "      <td>0.220161</td>\n",
              "      <td>0.226018</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.227897</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.228703</td>\n",
              "      <td>-0.026604</td>\n",
              "      <td>0.057750</td>\n",
              "      <td>-0.070109</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.014931</td>\n",
              "      <td>4.387952</td>\n",
              "      <td>68.0</td>\n",
              "      <td>0.002984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>0.264468</td>\n",
              "      <td>0.236543</td>\n",
              "      <td>0.235347</td>\n",
              "      <td>0.224278</td>\n",
              "      <td>0.226853</td>\n",
              "      <td>4</td>\n",
              "      <td>0.224278</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>163</td>\n",
              "      <td>332</td>\n",
              "      <td>2</td>\n",
              "      <td>0.105587</td>\n",
              "      <td>0.005057</td>\n",
              "      <td>0.047035</td>\n",
              "      <td>-0.011485</td>\n",
              "      <td>0.224278</td>\n",
              "      <td>0.002576</td>\n",
              "      <td>4.408132</td>\n",
              "      <td>66.0</td>\n",
              "      <td>0.003075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>0.266341</td>\n",
              "      <td>0.224075</td>\n",
              "      <td>0.219717</td>\n",
              "      <td>0.204637</td>\n",
              "      <td>0.204445</td>\n",
              "      <td>4</td>\n",
              "      <td>0.204445</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>143</td>\n",
              "      <td>313</td>\n",
              "      <td>1</td>\n",
              "      <td>0.158691</td>\n",
              "      <td>0.019448</td>\n",
              "      <td>0.068637</td>\n",
              "      <td>0.000937</td>\n",
              "      <td>0.204445</td>\n",
              "      <td>-0.000192</td>\n",
              "      <td>4.895882</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.014496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>0.250916</td>\n",
              "      <td>0.225873</td>\n",
              "      <td>0.223839</td>\n",
              "      <td>0.211831</td>\n",
              "      <td>0.214578</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211831</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>201</td>\n",
              "      <td>278</td>\n",
              "      <td>1</td>\n",
              "      <td>0.099807</td>\n",
              "      <td>0.009003</td>\n",
              "      <td>0.053644</td>\n",
              "      <td>-0.012967</td>\n",
              "      <td>0.211831</td>\n",
              "      <td>0.002747</td>\n",
              "      <td>4.660303</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0.003979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>0.241272</td>\n",
              "      <td>0.215191</td>\n",
              "      <td>0.208959</td>\n",
              "      <td>0.203206</td>\n",
              "      <td>0.199010</td>\n",
              "      <td>4</td>\n",
              "      <td>0.199010</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>207</td>\n",
              "      <td>285</td>\n",
              "      <td>1</td>\n",
              "      <td>0.108099</td>\n",
              "      <td>0.028958</td>\n",
              "      <td>0.027530</td>\n",
              "      <td>0.020649</td>\n",
              "      <td>0.199010</td>\n",
              "      <td>-0.004109</td>\n",
              "      <td>5.130809</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.067648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>0.243109</td>\n",
              "      <td>0.218107</td>\n",
              "      <td>0.219783</td>\n",
              "      <td>0.210506</td>\n",
              "      <td>0.208675</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208675</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>212</td>\n",
              "      <td>297</td>\n",
              "      <td>1</td>\n",
              "      <td>0.102842</td>\n",
              "      <td>-0.007684</td>\n",
              "      <td>0.042209</td>\n",
              "      <td>0.008700</td>\n",
              "      <td>0.208675</td>\n",
              "      <td>-0.001815</td>\n",
              "      <td>4.834202</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.009225</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>77 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a1290747-d7f0-4f88-9ae4-b1c381defb4d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a1290747-d7f0-4f88-9ae4-b1c381defb4d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a1290747-d7f0-4f88-9ae4-b1c381defb4d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           0         1         2  ...     score  model_rank  proba_selection\n",
              "0   0.297629  0.256342  0.224900  ...  4.946564        11.0         0.018449\n",
              "1   0.276555  0.242013  0.211087  ...  4.963635         9.0         0.022549\n",
              "2   0.296420  0.235832  0.227745  ...  4.702388        42.0         0.004832\n",
              "3   0.270336  0.227319  0.231308  ...  4.478359        63.0         0.003221\n",
              "4   0.285443  0.220161  0.226018  ...  4.387952        68.0         0.002984\n",
              "..       ...       ...       ...  ...       ...         ...              ...\n",
              "72  0.264468  0.236543  0.235347  ...  4.408132        66.0         0.003075\n",
              "73  0.266341  0.224075  0.219717  ...  4.895882        14.0         0.014496\n",
              "74  0.250916  0.225873  0.223839  ...  4.660303        51.0         0.003979\n",
              "75  0.241272  0.215191  0.208959  ...  5.130809         3.0         0.067648\n",
              "76  0.243109  0.218107  0.219783  ...  4.834202        22.0         0.009225\n",
              "\n",
              "[77 rows x 21 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.20170069709420205, 191.0, 279.0, 1.0)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Génération :  4\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 189 hidden_dim : 302 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2551, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.409553643082115\n",
            "Epoch 1 : Validation loss = 0.2404067068050305; Validation r = 0.5037164687190185\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1253, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.72359471027824\n",
            "Epoch 2 : Validation loss = 0.22757690772414207; Validation r = 0.5154538887880941\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0638, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8681628395956008\n",
            "Epoch 3 : Validation loss = 0.20635322717328866; Validation r = 0.5386373726293388\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0336, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9343833989669944\n",
            "Epoch 4 : Validation loss = 0.19855168784658114; Validation r = 0.5578655388525439\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0195, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9617750412002954\n",
            "Epoch 5 : Validation loss = 0.2012683176745971; Validation r = 0.558122939213014\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 192 hidden_dim : 311 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2714, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3600795191448339\n",
            "Epoch 1 : Validation loss = 0.24910815705855688; Validation r = 0.4429674261940845\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1400, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6690589826509508\n",
            "Epoch 2 : Validation loss = 0.2196543302386999; Validation r = 0.4926643418433962\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0768, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8366867112281996\n",
            "Epoch 3 : Validation loss = 0.20916549041867255; Validation r = 0.5170115006465499\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0406, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9186201980222162\n",
            "Epoch 4 : Validation loss = 0.20794207627574604; Validation r = 0.5144522007951999\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0250, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9488753221503438\n",
            "Epoch 5 : Validation loss = 0.21261268084247906; Validation r = 0.5251225880757681\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 193 hidden_dim : 309 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2607, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3947845233129228\n",
            "Epoch 1 : Validation loss = 0.2673068600396315; Validation r = 0.4527433118682839\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1233, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7278054648272722\n",
            "Epoch 2 : Validation loss = 0.22500612524648508; Validation r = 0.5239771973391011\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0583, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8821628791069865\n",
            "Epoch 3 : Validation loss = 0.22444518767297267; Validation r = 0.525240064146134\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0303, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9413240974185305\n",
            "Epoch 4 : Validation loss = 0.21288724032541115; Validation r = 0.5373233475581644\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0177, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.965472414650616\n",
            "Epoch 5 : Validation loss = 0.20608655015627544; Validation r = 0.54742612429935\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 172 hidden_dim : 294 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2639, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3881268336746643\n",
            "Epoch 1 : Validation loss = 0.2661167912185192; Validation r = 0.4654660791732921\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1246, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7245023750479455\n",
            "Epoch 2 : Validation loss = 0.21304235346615313; Validation r = 0.5228390788876267\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0606, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8774196366539433\n",
            "Epoch 3 : Validation loss = 0.20709700832764308; Validation r = 0.5339943042895441\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0324, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.937522329089714\n",
            "Epoch 4 : Validation loss = 0.2043004925052325; Validation r = 0.5357598054076441\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0188, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9635266870962081\n",
            "Epoch 5 : Validation loss = 0.20723433407644432; Validation r = 0.5397044195746967\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 192 hidden_dim : 292 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2604, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.39693795423436284\n",
            "Epoch 1 : Validation loss = 0.23848611737291017; Validation r = 0.5064400691843722\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1211, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7331059090364559\n",
            "Epoch 2 : Validation loss = 0.21745616855720679; Validation r = 0.5396566138020438\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0588, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.881698865171768\n",
            "Epoch 3 : Validation loss = 0.2103319442520539; Validation r = 0.5408255751714134\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0287, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9448063655975383\n",
            "Epoch 4 : Validation loss = 0.21174761118988197; Validation r = 0.5460488569809699\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0176, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9656940683979538\n",
            "Epoch 5 : Validation loss = 0.19921749066561462; Validation r = 0.56815986237721\n",
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2fc1e7e6-6e60-46e0-9b24-00c4fc9bcdbb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>max_epoch</th>\n",
              "      <th>best_dev_loss</th>\n",
              "      <th>best_dev_loss_epoch</th>\n",
              "      <th>lastEpochImproved</th>\n",
              "      <th>embedding_dim</th>\n",
              "      <th>hidden_dim</th>\n",
              "      <th>num_lstm_layers</th>\n",
              "      <th>loss_decrease_perc_0</th>\n",
              "      <th>loss_decrease_perc_1</th>\n",
              "      <th>loss_decrease_perc_2</th>\n",
              "      <th>loss_decrease_perc_3</th>\n",
              "      <th>score_perf</th>\n",
              "      <th>score_pot</th>\n",
              "      <th>score</th>\n",
              "      <th>model_rank</th>\n",
              "      <th>proba_selection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.297629</td>\n",
              "      <td>0.256342</td>\n",
              "      <td>0.224900</td>\n",
              "      <td>0.221605</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.138718</td>\n",
              "      <td>0.122657</td>\n",
              "      <td>0.014653</td>\n",
              "      <td>0.044878</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>-0.009499</td>\n",
              "      <td>4.946564</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.014314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.276555</td>\n",
              "      <td>0.242013</td>\n",
              "      <td>0.211087</td>\n",
              "      <td>0.214948</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.124904</td>\n",
              "      <td>0.127787</td>\n",
              "      <td>-0.018291</td>\n",
              "      <td>0.031870</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>-0.006632</td>\n",
              "      <td>4.963635</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.016700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.296420</td>\n",
              "      <td>0.235832</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.233284</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>175</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.204399</td>\n",
              "      <td>0.034291</td>\n",
              "      <td>-0.024319</td>\n",
              "      <td>0.045231</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>-0.010074</td>\n",
              "      <td>4.702388</td>\n",
              "      <td>47.0</td>\n",
              "      <td>0.004264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.270336</td>\n",
              "      <td>0.227319</td>\n",
              "      <td>0.231308</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.223296</td>\n",
              "      <td>4</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>175</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.159127</td>\n",
              "      <td>-0.017549</td>\n",
              "      <td>0.089803</td>\n",
              "      <td>-0.060609</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.012760</td>\n",
              "      <td>4.478359</td>\n",
              "      <td>68.0</td>\n",
              "      <td>0.002947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.285443</td>\n",
              "      <td>0.220161</td>\n",
              "      <td>0.226018</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.227897</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.228703</td>\n",
              "      <td>-0.026604</td>\n",
              "      <td>0.057750</td>\n",
              "      <td>-0.070109</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.014931</td>\n",
              "      <td>4.387952</td>\n",
              "      <td>73.0</td>\n",
              "      <td>0.002745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>0.240407</td>\n",
              "      <td>0.227577</td>\n",
              "      <td>0.206353</td>\n",
              "      <td>0.198552</td>\n",
              "      <td>0.201268</td>\n",
              "      <td>4</td>\n",
              "      <td>0.198552</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>189</td>\n",
              "      <td>302</td>\n",
              "      <td>1</td>\n",
              "      <td>0.053367</td>\n",
              "      <td>0.093259</td>\n",
              "      <td>0.037807</td>\n",
              "      <td>-0.013682</td>\n",
              "      <td>0.198552</td>\n",
              "      <td>0.002717</td>\n",
              "      <td>4.968492</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.018218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>0.249108</td>\n",
              "      <td>0.219654</td>\n",
              "      <td>0.209165</td>\n",
              "      <td>0.207942</td>\n",
              "      <td>0.212613</td>\n",
              "      <td>4</td>\n",
              "      <td>0.207942</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>192</td>\n",
              "      <td>311</td>\n",
              "      <td>2</td>\n",
              "      <td>0.118237</td>\n",
              "      <td>0.047752</td>\n",
              "      <td>0.005849</td>\n",
              "      <td>-0.022461</td>\n",
              "      <td>0.207942</td>\n",
              "      <td>0.004671</td>\n",
              "      <td>4.703388</td>\n",
              "      <td>46.0</td>\n",
              "      <td>0.004357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>0.267307</td>\n",
              "      <td>0.225006</td>\n",
              "      <td>0.224445</td>\n",
              "      <td>0.212887</td>\n",
              "      <td>0.206087</td>\n",
              "      <td>4</td>\n",
              "      <td>0.206087</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>193</td>\n",
              "      <td>309</td>\n",
              "      <td>1</td>\n",
              "      <td>0.158248</td>\n",
              "      <td>0.002493</td>\n",
              "      <td>0.051496</td>\n",
              "      <td>0.031945</td>\n",
              "      <td>0.206087</td>\n",
              "      <td>-0.006583</td>\n",
              "      <td>5.012453</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.025050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>0.266117</td>\n",
              "      <td>0.213042</td>\n",
              "      <td>0.207097</td>\n",
              "      <td>0.204300</td>\n",
              "      <td>0.207234</td>\n",
              "      <td>4</td>\n",
              "      <td>0.204300</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>172</td>\n",
              "      <td>294</td>\n",
              "      <td>1</td>\n",
              "      <td>0.199440</td>\n",
              "      <td>0.027907</td>\n",
              "      <td>0.013503</td>\n",
              "      <td>-0.014360</td>\n",
              "      <td>0.204300</td>\n",
              "      <td>0.002934</td>\n",
              "      <td>4.825455</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.007422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>0.238486</td>\n",
              "      <td>0.217456</td>\n",
              "      <td>0.210332</td>\n",
              "      <td>0.211748</td>\n",
              "      <td>0.199217</td>\n",
              "      <td>4</td>\n",
              "      <td>0.199217</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>192</td>\n",
              "      <td>292</td>\n",
              "      <td>1</td>\n",
              "      <td>0.088181</td>\n",
              "      <td>0.032762</td>\n",
              "      <td>-0.006731</td>\n",
              "      <td>0.059175</td>\n",
              "      <td>0.199217</td>\n",
              "      <td>-0.011789</td>\n",
              "      <td>5.335358</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.200400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>82 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2fc1e7e6-6e60-46e0-9b24-00c4fc9bcdbb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2fc1e7e6-6e60-46e0-9b24-00c4fc9bcdbb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2fc1e7e6-6e60-46e0-9b24-00c4fc9bcdbb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           0         1         2  ...     score  model_rank  proba_selection\n",
              "0   0.297629  0.256342  0.224900  ...  4.946564        14.0         0.014314\n",
              "1   0.276555  0.242013  0.211087  ...  4.963635        12.0         0.016700\n",
              "2   0.296420  0.235832  0.227745  ...  4.702388        47.0         0.004264\n",
              "3   0.270336  0.227319  0.231308  ...  4.478359        68.0         0.002947\n",
              "4   0.285443  0.220161  0.226018  ...  4.387952        73.0         0.002745\n",
              "..       ...       ...       ...  ...       ...         ...              ...\n",
              "77  0.240407  0.227577  0.206353  ...  4.968492        11.0         0.018218\n",
              "78  0.249108  0.219654  0.209165  ...  4.703388        46.0         0.004357\n",
              "79  0.267307  0.225006  0.224445  ...  5.012453         8.0         0.025050\n",
              "80  0.266117  0.213042  0.207097  ...  4.825455        27.0         0.007422\n",
              "81  0.238486  0.217456  0.210332  ...  5.335358         1.0         0.200400\n",
              "\n",
              "[82 rows x 21 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.19921749066561462, 192.0, 292.0, 1.0)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Génération :  5\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 320 hidden_dim : 252 num_lstm_layers : 4 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.3768, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.15221321758914425\n",
            "Epoch 1 : Validation loss = 0.3306534136335055; Validation r = 0.1687249516940301\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.2557, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.30596734084765576\n",
            "Epoch 2 : Validation loss = 0.2970009570320447; Validation r = 0.27920755805392033\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.2023, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.46229957122655346\n",
            "Epoch 3 : Validation loss = 0.29125378963847953; Validation r = 0.31333581414840306\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.1555, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6085715341916828\n",
            "Epoch 4 : Validation loss = 0.27465559418002766; Validation r = 0.31149303113259913\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.1156, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7277447901532492\n",
            "Epoch 5 : Validation loss = 0.26475891744097074; Validation r = 0.3509486603387233\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 212 hidden_dim : 306 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2639, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3780585713609276\n",
            "Epoch 1 : Validation loss = 0.24851397424936295; Validation r = 0.4764289787218385\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1229, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7233189126064314\n",
            "Epoch 2 : Validation loss = 0.224059621989727; Validation r = 0.5122035977800184\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0552, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8877472955746448\n",
            "Epoch 3 : Validation loss = 0.2052085926135381; Validation r = 0.5381427716592376\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0274, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9463354625927788\n",
            "Epoch 4 : Validation loss = 0.21055394212404888; Validation r = 0.5447813193100709\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0169, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9665578431846102\n",
            "Epoch 5 : Validation loss = 0.20773400124162436; Validation r = 0.55559611527526\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 109 hidden_dim : 358 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2615, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.38272575663403186\n",
            "Epoch 1 : Validation loss = 0.29531778022646904; Validation r = 0.4060051798631573\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1550, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6313605439686618\n",
            "Epoch 2 : Validation loss = 0.23600566014647484; Validation r = 0.4905360793558976\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0870, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8139138049506819\n",
            "Epoch 3 : Validation loss = 0.20983847292761007; Validation r = 0.5142761740130349\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0481, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9059827669745645\n",
            "Epoch 4 : Validation loss = 0.20433315175275008; Validation r = 0.540599167137004\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0282, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9473495267013189\n",
            "Epoch 5 : Validation loss = 0.199769949230055; Validation r = 0.5389729653833312\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 140 hidden_dim : 230 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2803, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.35924376127881436\n",
            "Epoch 1 : Validation loss = 0.25924425944685936; Validation r = 0.46783316119412305\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1615, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6170636743903122\n",
            "Epoch 2 : Validation loss = 0.25944443146387736; Validation r = 0.4809179133854726\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0923, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8018377460563546\n",
            "Epoch 3 : Validation loss = 0.20442807599902152; Validation r = 0.5512852531714002\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.903636622476719\n",
            "Epoch 4 : Validation loss = 0.21300558832784494; Validation r = 0.5378119931280004\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0255, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9506111472445274\n",
            "Epoch 5 : Validation loss = 0.2052325171728929; Validation r = 0.5497754869078724\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 117 hidden_dim : 352 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2711, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3494832301930312\n",
            "Epoch 1 : Validation loss = 0.25511469890673955; Validation r = 0.4342036948976952\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1780, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.554565624620649\n",
            "Epoch 2 : Validation loss = 0.2336658359815677; Validation r = 0.42162833603820016\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.1204, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7226917322615855\n",
            "Epoch 3 : Validation loss = 0.21240138846139114; Validation r = 0.4871042896504657\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0688, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8589372154406897\n",
            "Epoch 4 : Validation loss = 0.21247839393715065; Validation r = 0.4776294220106314\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0372, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9281577674244694\n",
            "Epoch 5 : Validation loss = 0.22120830081403256; Validation r = 0.47108967842772476\n",
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c56e77d2-1fad-41dc-a573-ff0e3b33b6bc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>max_epoch</th>\n",
              "      <th>best_dev_loss</th>\n",
              "      <th>best_dev_loss_epoch</th>\n",
              "      <th>lastEpochImproved</th>\n",
              "      <th>embedding_dim</th>\n",
              "      <th>hidden_dim</th>\n",
              "      <th>num_lstm_layers</th>\n",
              "      <th>loss_decrease_perc_0</th>\n",
              "      <th>loss_decrease_perc_1</th>\n",
              "      <th>loss_decrease_perc_2</th>\n",
              "      <th>loss_decrease_perc_3</th>\n",
              "      <th>score_perf</th>\n",
              "      <th>score_pot</th>\n",
              "      <th>score</th>\n",
              "      <th>model_rank</th>\n",
              "      <th>proba_selection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.297629</td>\n",
              "      <td>0.256342</td>\n",
              "      <td>0.224900</td>\n",
              "      <td>0.221605</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.138718</td>\n",
              "      <td>0.122657</td>\n",
              "      <td>0.014653</td>\n",
              "      <td>0.044878</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>-0.009499</td>\n",
              "      <td>4.946564</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.012379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.276555</td>\n",
              "      <td>0.242013</td>\n",
              "      <td>0.211087</td>\n",
              "      <td>0.214948</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.124904</td>\n",
              "      <td>0.127787</td>\n",
              "      <td>-0.018291</td>\n",
              "      <td>0.031870</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>-0.006632</td>\n",
              "      <td>4.963635</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.014147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.296420</td>\n",
              "      <td>0.235832</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.233284</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>175</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.204399</td>\n",
              "      <td>0.034291</td>\n",
              "      <td>-0.024319</td>\n",
              "      <td>0.045231</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>-0.010074</td>\n",
              "      <td>4.702388</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.003961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.270336</td>\n",
              "      <td>0.227319</td>\n",
              "      <td>0.231308</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.223296</td>\n",
              "      <td>4</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>175</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.159127</td>\n",
              "      <td>-0.017549</td>\n",
              "      <td>0.089803</td>\n",
              "      <td>-0.060609</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.012760</td>\n",
              "      <td>4.478359</td>\n",
              "      <td>72.0</td>\n",
              "      <td>0.002751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.285443</td>\n",
              "      <td>0.220161</td>\n",
              "      <td>0.226018</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.227897</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.228703</td>\n",
              "      <td>-0.026604</td>\n",
              "      <td>0.057750</td>\n",
              "      <td>-0.070109</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.014931</td>\n",
              "      <td>4.387952</td>\n",
              "      <td>77.0</td>\n",
              "      <td>0.002572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>0.330653</td>\n",
              "      <td>0.297001</td>\n",
              "      <td>0.291254</td>\n",
              "      <td>0.274656</td>\n",
              "      <td>0.264759</td>\n",
              "      <td>4</td>\n",
              "      <td>0.264759</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>320</td>\n",
              "      <td>252</td>\n",
              "      <td>4</td>\n",
              "      <td>0.101776</td>\n",
              "      <td>0.019351</td>\n",
              "      <td>0.056989</td>\n",
              "      <td>0.036033</td>\n",
              "      <td>0.264759</td>\n",
              "      <td>-0.009540</td>\n",
              "      <td>3.918206</td>\n",
              "      <td>87.0</td>\n",
              "      <td>0.002277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>0.248514</td>\n",
              "      <td>0.224060</td>\n",
              "      <td>0.205209</td>\n",
              "      <td>0.210554</td>\n",
              "      <td>0.207734</td>\n",
              "      <td>4</td>\n",
              "      <td>0.205209</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>212</td>\n",
              "      <td>306</td>\n",
              "      <td>1</td>\n",
              "      <td>0.098402</td>\n",
              "      <td>0.084134</td>\n",
              "      <td>-0.026048</td>\n",
              "      <td>0.013393</td>\n",
              "      <td>0.205209</td>\n",
              "      <td>-0.002748</td>\n",
              "      <td>4.939241</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.011651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>0.295318</td>\n",
              "      <td>0.236006</td>\n",
              "      <td>0.209838</td>\n",
              "      <td>0.204333</td>\n",
              "      <td>0.199770</td>\n",
              "      <td>4</td>\n",
              "      <td>0.199770</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>109</td>\n",
              "      <td>358</td>\n",
              "      <td>1</td>\n",
              "      <td>0.200842</td>\n",
              "      <td>0.110875</td>\n",
              "      <td>0.026236</td>\n",
              "      <td>0.022332</td>\n",
              "      <td>0.199770</td>\n",
              "      <td>-0.004461</td>\n",
              "      <td>5.120101</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.039613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>0.259244</td>\n",
              "      <td>0.259444</td>\n",
              "      <td>0.204428</td>\n",
              "      <td>0.213006</td>\n",
              "      <td>0.205233</td>\n",
              "      <td>4</td>\n",
              "      <td>0.204428</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>140</td>\n",
              "      <td>230</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.000772</td>\n",
              "      <td>0.212054</td>\n",
              "      <td>-0.041959</td>\n",
              "      <td>0.036492</td>\n",
              "      <td>0.204428</td>\n",
              "      <td>-0.007460</td>\n",
              "      <td>5.076966</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.024758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>0.255115</td>\n",
              "      <td>0.233666</td>\n",
              "      <td>0.212401</td>\n",
              "      <td>0.212478</td>\n",
              "      <td>0.221208</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212401</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>117</td>\n",
              "      <td>352</td>\n",
              "      <td>2</td>\n",
              "      <td>0.084075</td>\n",
              "      <td>0.091004</td>\n",
              "      <td>-0.000363</td>\n",
              "      <td>-0.041086</td>\n",
              "      <td>0.212401</td>\n",
              "      <td>0.008727</td>\n",
              "      <td>4.522265</td>\n",
              "      <td>69.0</td>\n",
              "      <td>0.002871</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>87 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c56e77d2-1fad-41dc-a573-ff0e3b33b6bc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c56e77d2-1fad-41dc-a573-ff0e3b33b6bc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c56e77d2-1fad-41dc-a573-ff0e3b33b6bc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           0         1         2  ...     score  model_rank  proba_selection\n",
              "0   0.297629  0.256342  0.224900  ...  4.946564        16.0         0.012379\n",
              "1   0.276555  0.242013  0.211087  ...  4.963635        14.0         0.014147\n",
              "2   0.296420  0.235832  0.227745  ...  4.702388        50.0         0.003961\n",
              "3   0.270336  0.227319  0.231308  ...  4.478359        72.0         0.002751\n",
              "4   0.285443  0.220161  0.226018  ...  4.387952        77.0         0.002572\n",
              "..       ...       ...       ...  ...       ...         ...              ...\n",
              "82  0.330653  0.297001  0.291254  ...  3.918206        87.0         0.002277\n",
              "83  0.248514  0.224060  0.205209  ...  4.939241        17.0         0.011651\n",
              "84  0.295318  0.236006  0.209838  ...  5.120101         5.0         0.039613\n",
              "85  0.259244  0.259444  0.204428  ...  5.076966         8.0         0.024758\n",
              "86  0.255115  0.233666  0.212401  ...  4.522265        69.0         0.002871\n",
              "\n",
              "[87 rows x 21 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.19921749066561462, 192.0, 292.0, 1.0)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Génération :  6\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 145 hidden_dim : 384 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2602, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.39167722041371494\n",
            "Epoch 1 : Validation loss = 0.27820015673836074; Validation r = 0.44897808542573325\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1409, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6759467909094302\n",
            "Epoch 2 : Validation loss = 0.25094484811027845; Validation r = 0.47414453940016316\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0746, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8458360844892183\n",
            "Epoch 3 : Validation loss = 0.2060817045470079; Validation r = 0.5238613264355785\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0391, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9241974720692929\n",
            "Epoch 4 : Validation loss = 0.20880476670960585; Validation r = 0.526643032902877\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0210, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9598801123909166\n",
            "Epoch 5 : Validation loss = 0.20665590601662795; Validation r = 0.5384740346691109\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 126 hidden_dim : 233 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2762, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.37087489586691713\n",
            "Epoch 1 : Validation loss = 0.2574030424157778; Validation r = 0.43410604237503125\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1515, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6447927903194389\n",
            "Epoch 2 : Validation loss = 0.22892751693725585; Validation r = 0.46721357345288367\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0878, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8137734376461278\n",
            "Epoch 3 : Validation loss = 0.21244489923119544; Validation r = 0.5102113420025086\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0496, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9022784690315605\n",
            "Epoch 4 : Validation loss = 0.20898146964609624; Validation r = 0.5172040005534054\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0284, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9446195902261968\n",
            "Epoch 5 : Validation loss = 0.2136496638258298; Validation r = 0.5032692357494009\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 156 hidden_dim : 282 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2574, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.41068511669298213\n",
            "Epoch 1 : Validation loss = 0.2583392007897298; Validation r = 0.45207619860361725\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1293, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.713832537347136\n",
            "Epoch 2 : Validation loss = 0.22472315846631924; Validation r = 0.5135464764512329\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0660, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8669613506158818\n",
            "Epoch 3 : Validation loss = 0.21172988340258597; Validation r = 0.5338898366208061\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0338, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9348854077585429\n",
            "Epoch 4 : Validation loss = 0.20503918242951233; Validation r = 0.5428905345234081\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0193, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9628288614669271\n",
            "Epoch 5 : Validation loss = 0.20345848860840002; Validation r = 0.5474782340091202\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 172 hidden_dim : 270 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2629, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.386447749057233\n",
            "Epoch 1 : Validation loss = 0.24309935818115871; Validation r = 0.49125435981426036\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1248, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7219987488474833\n",
            "Epoch 2 : Validation loss = 0.21602295003831387; Validation r = 0.5436243874058319\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0614, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8743997408201943\n",
            "Epoch 3 : Validation loss = 0.21363084552188713; Validation r = 0.5276996155722848\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0311, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9393656874540625\n",
            "Epoch 4 : Validation loss = 0.21429575967291992; Validation r = 0.5359692730956128\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0187, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9636677379080961\n",
            "Epoch 5 : Validation loss = 0.20901171291867893; Validation r = 0.5405580218624793\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 161 hidden_dim : 258 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2645, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3909180851609918\n",
            "Epoch 1 : Validation loss = 0.23977995552122594; Validation r = 0.48717986179230993\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1231, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7285519172198954\n",
            "Epoch 2 : Validation loss = 0.21907905489206314; Validation r = 0.5098055751430961\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0616, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8744440607654068\n",
            "Epoch 3 : Validation loss = 0.21259943544864654; Validation r = 0.5361736427756107\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0325, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9370794030321602\n",
            "Epoch 4 : Validation loss = 0.20785684883594513; Validation r = 0.5273741935863485\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0187, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9636728029317791\n",
            "Epoch 5 : Validation loss = 0.2024478565901518; Validation r = 0.5415950758216627\n",
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4e582749-5a4b-431b-b690-450ad84e36ca\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>max_epoch</th>\n",
              "      <th>best_dev_loss</th>\n",
              "      <th>best_dev_loss_epoch</th>\n",
              "      <th>lastEpochImproved</th>\n",
              "      <th>embedding_dim</th>\n",
              "      <th>hidden_dim</th>\n",
              "      <th>num_lstm_layers</th>\n",
              "      <th>loss_decrease_perc_0</th>\n",
              "      <th>loss_decrease_perc_1</th>\n",
              "      <th>loss_decrease_perc_2</th>\n",
              "      <th>loss_decrease_perc_3</th>\n",
              "      <th>score_perf</th>\n",
              "      <th>score_pot</th>\n",
              "      <th>score</th>\n",
              "      <th>model_rank</th>\n",
              "      <th>proba_selection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.297629</td>\n",
              "      <td>0.256342</td>\n",
              "      <td>0.224900</td>\n",
              "      <td>0.221605</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.138718</td>\n",
              "      <td>0.122657</td>\n",
              "      <td>0.014653</td>\n",
              "      <td>0.044878</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>-0.009499</td>\n",
              "      <td>4.946564</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0.010884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.276555</td>\n",
              "      <td>0.242013</td>\n",
              "      <td>0.211087</td>\n",
              "      <td>0.214948</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.124904</td>\n",
              "      <td>0.127787</td>\n",
              "      <td>-0.018291</td>\n",
              "      <td>0.031870</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>-0.006632</td>\n",
              "      <td>4.963635</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.013061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.296420</td>\n",
              "      <td>0.235832</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.233284</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>175</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.204399</td>\n",
              "      <td>0.034291</td>\n",
              "      <td>-0.024319</td>\n",
              "      <td>0.045231</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>-0.010074</td>\n",
              "      <td>4.702388</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.003628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.270336</td>\n",
              "      <td>0.227319</td>\n",
              "      <td>0.231308</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.223296</td>\n",
              "      <td>4</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>175</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.159127</td>\n",
              "      <td>-0.017549</td>\n",
              "      <td>0.089803</td>\n",
              "      <td>-0.060609</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.012760</td>\n",
              "      <td>4.478359</td>\n",
              "      <td>77.0</td>\n",
              "      <td>0.002544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.285443</td>\n",
              "      <td>0.220161</td>\n",
              "      <td>0.226018</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.227897</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.228703</td>\n",
              "      <td>-0.026604</td>\n",
              "      <td>0.057750</td>\n",
              "      <td>-0.070109</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.014931</td>\n",
              "      <td>4.387952</td>\n",
              "      <td>82.0</td>\n",
              "      <td>0.002389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>0.278200</td>\n",
              "      <td>0.250945</td>\n",
              "      <td>0.206082</td>\n",
              "      <td>0.208805</td>\n",
              "      <td>0.206656</td>\n",
              "      <td>4</td>\n",
              "      <td>0.206082</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>145</td>\n",
              "      <td>384</td>\n",
              "      <td>1</td>\n",
              "      <td>0.097970</td>\n",
              "      <td>0.178777</td>\n",
              "      <td>-0.013214</td>\n",
              "      <td>0.010291</td>\n",
              "      <td>0.206082</td>\n",
              "      <td>-0.002121</td>\n",
              "      <td>4.902901</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.008518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>0.257403</td>\n",
              "      <td>0.228928</td>\n",
              "      <td>0.212445</td>\n",
              "      <td>0.208981</td>\n",
              "      <td>0.213650</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208981</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>126</td>\n",
              "      <td>233</td>\n",
              "      <td>2</td>\n",
              "      <td>0.110626</td>\n",
              "      <td>0.071999</td>\n",
              "      <td>0.016303</td>\n",
              "      <td>-0.022338</td>\n",
              "      <td>0.208981</td>\n",
              "      <td>0.004668</td>\n",
              "      <td>4.680560</td>\n",
              "      <td>59.0</td>\n",
              "      <td>0.003320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>0.258339</td>\n",
              "      <td>0.224723</td>\n",
              "      <td>0.211730</td>\n",
              "      <td>0.205039</td>\n",
              "      <td>0.203458</td>\n",
              "      <td>4</td>\n",
              "      <td>0.203458</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>156</td>\n",
              "      <td>282</td>\n",
              "      <td>1</td>\n",
              "      <td>0.130124</td>\n",
              "      <td>0.057819</td>\n",
              "      <td>0.031600</td>\n",
              "      <td>0.007709</td>\n",
              "      <td>0.203458</td>\n",
              "      <td>-0.001569</td>\n",
              "      <td>4.953193</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.012244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>0.243099</td>\n",
              "      <td>0.216023</td>\n",
              "      <td>0.213631</td>\n",
              "      <td>0.214296</td>\n",
              "      <td>0.209012</td>\n",
              "      <td>4</td>\n",
              "      <td>0.209012</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>172</td>\n",
              "      <td>270</td>\n",
              "      <td>1</td>\n",
              "      <td>0.111380</td>\n",
              "      <td>0.011073</td>\n",
              "      <td>-0.003112</td>\n",
              "      <td>0.024658</td>\n",
              "      <td>0.209012</td>\n",
              "      <td>-0.005154</td>\n",
              "      <td>4.905376</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.008905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>0.239780</td>\n",
              "      <td>0.219079</td>\n",
              "      <td>0.212599</td>\n",
              "      <td>0.207857</td>\n",
              "      <td>0.202448</td>\n",
              "      <td>4</td>\n",
              "      <td>0.202448</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>161</td>\n",
              "      <td>258</td>\n",
              "      <td>1</td>\n",
              "      <td>0.086333</td>\n",
              "      <td>0.029577</td>\n",
              "      <td>0.022308</td>\n",
              "      <td>0.026023</td>\n",
              "      <td>0.202448</td>\n",
              "      <td>-0.005268</td>\n",
              "      <td>5.071518</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.021768</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>92 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4e582749-5a4b-431b-b690-450ad84e36ca')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4e582749-5a4b-431b-b690-450ad84e36ca button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4e582749-5a4b-431b-b690-450ad84e36ca');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           0         1         2  ...     score  model_rank  proba_selection\n",
              "0   0.297629  0.256342  0.224900  ...  4.946564        18.0         0.010884\n",
              "1   0.276555  0.242013  0.211087  ...  4.963635        15.0         0.013061\n",
              "2   0.296420  0.235832  0.227745  ...  4.702388        54.0         0.003628\n",
              "3   0.270336  0.227319  0.231308  ...  4.478359        77.0         0.002544\n",
              "4   0.285443  0.220161  0.226018  ...  4.387952        82.0         0.002389\n",
              "..       ...       ...       ...  ...       ...         ...              ...\n",
              "87  0.278200  0.250945  0.206082  ...  4.902901        23.0         0.008518\n",
              "88  0.257403  0.228928  0.212445  ...  4.680560        59.0         0.003320\n",
              "89  0.258339  0.224723  0.211730  ...  4.953193        16.0         0.012244\n",
              "90  0.243099  0.216023  0.213631  ...  4.905376        22.0         0.008905\n",
              "91  0.239780  0.219079  0.212599  ...  5.071518         9.0         0.021768\n",
              "\n",
              "[92 rows x 21 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.19921749066561462, 192.0, 292.0, 1.0)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Génération :  7\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 148 hidden_dim : 257 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2732, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3576866758667887\n",
            "Epoch 1 : Validation loss = 0.2558803640305996; Validation r = 0.40283667823010855\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1561, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6271299255032171\n",
            "Epoch 2 : Validation loss = 0.23178788212438425; Validation r = 0.4957600483869138\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0830, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8235177534920276\n",
            "Epoch 3 : Validation loss = 0.21468364583949248; Validation r = 0.4925408681753951\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9196095790801185\n",
            "Epoch 4 : Validation loss = 0.21645565206805864; Validation r = 0.5026409943685474\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0234, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9544368608940463\n",
            "Epoch 5 : Validation loss = 0.20884842065473397; Validation r = 0.5233486466326472\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 198 hidden_dim : 258 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2601, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.4024191493294321\n",
            "Epoch 1 : Validation loss = 0.2589088179171085; Validation r = 0.4969752451560057\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1160, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7494538146605159\n",
            "Epoch 2 : Validation loss = 0.2155729878693819; Validation r = 0.53099507347979\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0555, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8882158311417416\n",
            "Epoch 3 : Validation loss = 0.2134211717794339; Validation r = 0.5355692935657006\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0265, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9482790636700431\n",
            "Epoch 4 : Validation loss = 0.20793151433269183; Validation r = 0.5477618609101481\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0156, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9690849777992586\n",
            "Epoch 5 : Validation loss = 0.20674655971427758; Validation r = 0.5406279510555612\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 198 hidden_dim : 263 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2710, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3727621825358851\n",
            "Epoch 1 : Validation loss = 0.2761141007145246; Validation r = 0.4178228954343931\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1441, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6677526566240404\n",
            "Epoch 2 : Validation loss = 0.2239966863145431; Validation r = 0.49822990173589915\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0710, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8529392987258637\n",
            "Epoch 3 : Validation loss = 0.23317552680770556; Validation r = 0.5113959472000408\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0335, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9339305476842906\n",
            "Epoch 4 : Validation loss = 0.2156120389699936; Validation r = 0.5312544555321073\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0192, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9610718743650007\n",
            "Epoch 5 : Validation loss = 0.2180998039742311; Validation r = 0.523939743268789\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 102 hidden_dim : 100 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2725, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.406903745079585\n",
            "Epoch 1 : Validation loss = 0.30362542656560737; Validation r = 0.41862560259399656\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1413, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6931907044683259\n",
            "Epoch 2 : Validation loss = 0.2372238510598739; Validation r = 0.4959344098802964\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0761, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8405430842122373\n",
            "Epoch 3 : Validation loss = 0.23690313411255678; Validation r = 0.5036007893357706\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.916978759582296\n",
            "Epoch 4 : Validation loss = 0.22131797168403863; Validation r = 0.5213370483904\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0252, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9489786713201269\n",
            "Epoch 5 : Validation loss = 0.230217253540953; Validation r = 0.5134953863300259\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 152 hidden_dim : 100 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2701, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3975979305842594\n",
            "Epoch 1 : Validation loss = 0.2885234291354815; Validation r = 0.4208161698800097\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1281, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7287933095496422\n",
            "Epoch 2 : Validation loss = 0.24785293204089007; Validation r = 0.49502462034401257\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0624, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8723522102015566\n",
            "Epoch 3 : Validation loss = 0.23698708713054656; Validation r = 0.5048237970322965\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0330, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9334314484760121\n",
            "Epoch 4 : Validation loss = 0.21519910482068857; Validation r = 0.5313984863148237\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0213, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9562128883765978\n",
            "Epoch 5 : Validation loss = 0.21705092154443265; Validation r = 0.5305189118789927\n",
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-fd1d553b-8a9f-40e3-8d69-6163d4ecd929\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>max_epoch</th>\n",
              "      <th>best_dev_loss</th>\n",
              "      <th>best_dev_loss_epoch</th>\n",
              "      <th>lastEpochImproved</th>\n",
              "      <th>embedding_dim</th>\n",
              "      <th>hidden_dim</th>\n",
              "      <th>num_lstm_layers</th>\n",
              "      <th>loss_decrease_perc_0</th>\n",
              "      <th>loss_decrease_perc_1</th>\n",
              "      <th>loss_decrease_perc_2</th>\n",
              "      <th>loss_decrease_perc_3</th>\n",
              "      <th>score_perf</th>\n",
              "      <th>score_pot</th>\n",
              "      <th>score</th>\n",
              "      <th>model_rank</th>\n",
              "      <th>proba_selection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.297629</td>\n",
              "      <td>0.256342</td>\n",
              "      <td>0.224900</td>\n",
              "      <td>0.221605</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.138718</td>\n",
              "      <td>0.122657</td>\n",
              "      <td>0.014653</td>\n",
              "      <td>0.044878</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>-0.009499</td>\n",
              "      <td>4.946564</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0.010206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.276555</td>\n",
              "      <td>0.242013</td>\n",
              "      <td>0.211087</td>\n",
              "      <td>0.214948</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.124904</td>\n",
              "      <td>0.127787</td>\n",
              "      <td>-0.018291</td>\n",
              "      <td>0.031870</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>-0.006632</td>\n",
              "      <td>4.963635</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.012927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.296420</td>\n",
              "      <td>0.235832</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.233284</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>175</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.204399</td>\n",
              "      <td>0.034291</td>\n",
              "      <td>-0.024319</td>\n",
              "      <td>0.045231</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>-0.010074</td>\n",
              "      <td>4.702388</td>\n",
              "      <td>56.0</td>\n",
              "      <td>0.003463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.270336</td>\n",
              "      <td>0.227319</td>\n",
              "      <td>0.231308</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.223296</td>\n",
              "      <td>4</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>175</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.159127</td>\n",
              "      <td>-0.017549</td>\n",
              "      <td>0.089803</td>\n",
              "      <td>-0.060609</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.012760</td>\n",
              "      <td>4.478359</td>\n",
              "      <td>81.0</td>\n",
              "      <td>0.002394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.285443</td>\n",
              "      <td>0.220161</td>\n",
              "      <td>0.226018</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.227897</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.228703</td>\n",
              "      <td>-0.026604</td>\n",
              "      <td>0.057750</td>\n",
              "      <td>-0.070109</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.014931</td>\n",
              "      <td>4.387952</td>\n",
              "      <td>86.0</td>\n",
              "      <td>0.002255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>0.255880</td>\n",
              "      <td>0.231788</td>\n",
              "      <td>0.214684</td>\n",
              "      <td>0.216456</td>\n",
              "      <td>0.208848</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208848</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>148</td>\n",
              "      <td>257</td>\n",
              "      <td>2</td>\n",
              "      <td>0.094155</td>\n",
              "      <td>0.073793</td>\n",
              "      <td>-0.008254</td>\n",
              "      <td>0.035145</td>\n",
              "      <td>0.208848</td>\n",
              "      <td>-0.007340</td>\n",
              "      <td>4.962569</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.012119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>0.258909</td>\n",
              "      <td>0.215573</td>\n",
              "      <td>0.213421</td>\n",
              "      <td>0.207932</td>\n",
              "      <td>0.206747</td>\n",
              "      <td>4</td>\n",
              "      <td>0.206747</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>198</td>\n",
              "      <td>258</td>\n",
              "      <td>1</td>\n",
              "      <td>0.167379</td>\n",
              "      <td>0.009982</td>\n",
              "      <td>0.025722</td>\n",
              "      <td>0.005699</td>\n",
              "      <td>0.206747</td>\n",
              "      <td>-0.001178</td>\n",
              "      <td>4.864562</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.007182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>0.276114</td>\n",
              "      <td>0.223997</td>\n",
              "      <td>0.233176</td>\n",
              "      <td>0.215612</td>\n",
              "      <td>0.218100</td>\n",
              "      <td>4</td>\n",
              "      <td>0.215612</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>198</td>\n",
              "      <td>263</td>\n",
              "      <td>2</td>\n",
              "      <td>0.188753</td>\n",
              "      <td>-0.040978</td>\n",
              "      <td>0.075323</td>\n",
              "      <td>-0.011538</td>\n",
              "      <td>0.215612</td>\n",
              "      <td>0.002488</td>\n",
              "      <td>4.585057</td>\n",
              "      <td>70.0</td>\n",
              "      <td>0.002770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.303625</td>\n",
              "      <td>0.237224</td>\n",
              "      <td>0.236903</td>\n",
              "      <td>0.221318</td>\n",
              "      <td>0.230217</td>\n",
              "      <td>4</td>\n",
              "      <td>0.221318</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>102</td>\n",
              "      <td>100</td>\n",
              "      <td>2</td>\n",
              "      <td>0.218696</td>\n",
              "      <td>0.001352</td>\n",
              "      <td>0.065787</td>\n",
              "      <td>-0.040210</td>\n",
              "      <td>0.221318</td>\n",
              "      <td>0.008899</td>\n",
              "      <td>4.343723</td>\n",
              "      <td>87.0</td>\n",
              "      <td>0.002229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.288523</td>\n",
              "      <td>0.247853</td>\n",
              "      <td>0.236987</td>\n",
              "      <td>0.215199</td>\n",
              "      <td>0.217051</td>\n",
              "      <td>4</td>\n",
              "      <td>0.215199</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>152</td>\n",
              "      <td>100</td>\n",
              "      <td>2</td>\n",
              "      <td>0.140961</td>\n",
              "      <td>0.043840</td>\n",
              "      <td>0.091937</td>\n",
              "      <td>-0.008605</td>\n",
              "      <td>0.215199</td>\n",
              "      <td>0.001852</td>\n",
              "      <td>4.607214</td>\n",
              "      <td>68.0</td>\n",
              "      <td>0.002852</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>97 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fd1d553b-8a9f-40e3-8d69-6163d4ecd929')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fd1d553b-8a9f-40e3-8d69-6163d4ecd929 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fd1d553b-8a9f-40e3-8d69-6163d4ecd929');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           0         1         2  ...     score  model_rank  proba_selection\n",
              "0   0.297629  0.256342  0.224900  ...  4.946564        19.0         0.010206\n",
              "1   0.276555  0.242013  0.211087  ...  4.963635        15.0         0.012927\n",
              "2   0.296420  0.235832  0.227745  ...  4.702388        56.0         0.003463\n",
              "3   0.270336  0.227319  0.231308  ...  4.478359        81.0         0.002394\n",
              "4   0.285443  0.220161  0.226018  ...  4.387952        86.0         0.002255\n",
              "..       ...       ...       ...  ...       ...         ...              ...\n",
              "92  0.255880  0.231788  0.214684  ...  4.962569        16.0         0.012119\n",
              "93  0.258909  0.215573  0.213421  ...  4.864562        27.0         0.007182\n",
              "94  0.276114  0.223997  0.233176  ...  4.585057        70.0         0.002770\n",
              "95  0.303625  0.237224  0.236903  ...  4.343723        87.0         0.002229\n",
              "96  0.288523  0.247853  0.236987  ...  4.607214        68.0         0.002852\n",
              "\n",
              "[97 rows x 21 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.19921749066561462, 192.0, 292.0, 1.0)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Génération :  8\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 168 hidden_dim : 229 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2632, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3965369284441186\n",
            "Epoch 1 : Validation loss = 0.24792642071843146; Validation r = 0.4649460877128969\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1248, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7240977629909352\n",
            "Epoch 2 : Validation loss = 0.23048637459675472; Validation r = 0.4975647354018444\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0613, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.875754630432489\n",
            "Epoch 3 : Validation loss = 0.211428115144372; Validation r = 0.5304289589254929\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0312, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9389793825198414\n",
            "Epoch 4 : Validation loss = 0.20869587883353233; Validation r = 0.5285693960374424\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0191, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9624183119947769\n",
            "Epoch 5 : Validation loss = 0.21033458337187766; Validation r = 0.5253485231580352\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 150 hidden_dim : 297 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2606, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.38867629728440195\n",
            "Epoch 1 : Validation loss = 0.2621731586754322; Validation r = 0.4779541480732008\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1300, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7072429472838544\n",
            "Epoch 2 : Validation loss = 0.21828415108223756; Validation r = 0.5199628803159135\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0707, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8546693720934107\n",
            "Epoch 3 : Validation loss = 0.21694591057797272; Validation r = 0.5206730657111445\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0349, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9328857519384497\n",
            "Epoch 4 : Validation loss = 0.20632505975663662; Validation r = 0.5384235665156398\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0199, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9620885874929432\n",
            "Epoch 5 : Validation loss = 0.20590744502842426; Validation r = 0.5244169180479066\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 166 hidden_dim : 269 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2562, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.4142422108084367\n",
            "Epoch 1 : Validation loss = 0.25879473686218263; Validation r = 0.45776928664808914\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1266, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7192213100518716\n",
            "Epoch 2 : Validation loss = 0.22751339351137478; Validation r = 0.5279825973398042\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0632, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8703559900057489\n",
            "Epoch 3 : Validation loss = 0.20577313788235188; Validation r = 0.5361375123548355\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0315, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9390604922976932\n",
            "Epoch 4 : Validation loss = 0.20762059800326824; Validation r = 0.5398449205277487\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0183, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9643705466515404\n",
            "Epoch 5 : Validation loss = 0.2042005990942319; Validation r = 0.5396998864053456\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 208 hidden_dim : 303 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2804, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.350237928495863\n",
            "Epoch 1 : Validation loss = 0.26266055715580783; Validation r = 0.4313965736431403\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1488, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6480580046577264\n",
            "Epoch 2 : Validation loss = 0.22868458032608033; Validation r = 0.4957515665302007\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0805, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8308402669743361\n",
            "Epoch 3 : Validation loss = 0.21279902371267478; Validation r = 0.52214326740474\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0399, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9204608580479098\n",
            "Epoch 4 : Validation loss = 0.21037698034197092; Validation r = 0.5377235846696315\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0222, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9550983595298901\n",
            "Epoch 5 : Validation loss = 0.20907796869675319; Validation r = 0.5386003428749283\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 220 hidden_dim : 297 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2612, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3953308221728322\n",
            "Epoch 1 : Validation loss = 0.24031978162626425; Validation r = 0.4880053430358816\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1196, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7369384536526506\n",
            "Epoch 2 : Validation loss = 0.23800194126864274; Validation r = 0.5336687611766338\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0548, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8888935312122349\n",
            "Epoch 3 : Validation loss = 0.21571053384492794; Validation r = 0.5408893584636408\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0267, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9471848530585801\n",
            "Epoch 4 : Validation loss = 0.20416771322488786; Validation r = 0.5495156299365801\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0167, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9668002486147724\n",
            "Epoch 5 : Validation loss = 0.19738502502441407; Validation r = 0.56642479462416\n",
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4b3eb5bd-192a-4b36-8b59-0c7348c68a48\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>max_epoch</th>\n",
              "      <th>best_dev_loss</th>\n",
              "      <th>best_dev_loss_epoch</th>\n",
              "      <th>lastEpochImproved</th>\n",
              "      <th>embedding_dim</th>\n",
              "      <th>hidden_dim</th>\n",
              "      <th>num_lstm_layers</th>\n",
              "      <th>loss_decrease_perc_0</th>\n",
              "      <th>loss_decrease_perc_1</th>\n",
              "      <th>loss_decrease_perc_2</th>\n",
              "      <th>loss_decrease_perc_3</th>\n",
              "      <th>score_perf</th>\n",
              "      <th>score_pot</th>\n",
              "      <th>score</th>\n",
              "      <th>model_rank</th>\n",
              "      <th>proba_selection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.297629</td>\n",
              "      <td>0.256342</td>\n",
              "      <td>0.224900</td>\n",
              "      <td>0.221605</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.138718</td>\n",
              "      <td>0.122657</td>\n",
              "      <td>0.014653</td>\n",
              "      <td>0.044878</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>-0.009499</td>\n",
              "      <td>4.946564</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.009145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.276555</td>\n",
              "      <td>0.242013</td>\n",
              "      <td>0.211087</td>\n",
              "      <td>0.214948</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.124904</td>\n",
              "      <td>0.127787</td>\n",
              "      <td>-0.018291</td>\n",
              "      <td>0.031870</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>-0.006632</td>\n",
              "      <td>4.963635</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.011297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.296420</td>\n",
              "      <td>0.235832</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.233284</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>175</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.204399</td>\n",
              "      <td>0.034291</td>\n",
              "      <td>-0.024319</td>\n",
              "      <td>0.045231</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>-0.010074</td>\n",
              "      <td>4.702388</td>\n",
              "      <td>61.0</td>\n",
              "      <td>0.003148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.270336</td>\n",
              "      <td>0.227319</td>\n",
              "      <td>0.231308</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.223296</td>\n",
              "      <td>4</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>175</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.159127</td>\n",
              "      <td>-0.017549</td>\n",
              "      <td>0.089803</td>\n",
              "      <td>-0.060609</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.012760</td>\n",
              "      <td>4.478359</td>\n",
              "      <td>86.0</td>\n",
              "      <td>0.002233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.285443</td>\n",
              "      <td>0.220161</td>\n",
              "      <td>0.226018</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.227897</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.228703</td>\n",
              "      <td>-0.026604</td>\n",
              "      <td>0.057750</td>\n",
              "      <td>-0.070109</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.014931</td>\n",
              "      <td>4.387952</td>\n",
              "      <td>91.0</td>\n",
              "      <td>0.002110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.247926</td>\n",
              "      <td>0.230486</td>\n",
              "      <td>0.211428</td>\n",
              "      <td>0.208696</td>\n",
              "      <td>0.210335</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208696</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>168</td>\n",
              "      <td>229</td>\n",
              "      <td>1</td>\n",
              "      <td>0.070344</td>\n",
              "      <td>0.082687</td>\n",
              "      <td>0.012923</td>\n",
              "      <td>-0.007852</td>\n",
              "      <td>0.208696</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>4.754330</td>\n",
              "      <td>53.0</td>\n",
              "      <td>0.003624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.262173</td>\n",
              "      <td>0.218284</td>\n",
              "      <td>0.216946</td>\n",
              "      <td>0.206325</td>\n",
              "      <td>0.205907</td>\n",
              "      <td>4</td>\n",
              "      <td>0.205907</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>150</td>\n",
              "      <td>297</td>\n",
              "      <td>1</td>\n",
              "      <td>0.167405</td>\n",
              "      <td>0.006131</td>\n",
              "      <td>0.048956</td>\n",
              "      <td>0.002024</td>\n",
              "      <td>0.205907</td>\n",
              "      <td>-0.000417</td>\n",
              "      <td>4.866401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0.006622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.258795</td>\n",
              "      <td>0.227513</td>\n",
              "      <td>0.205773</td>\n",
              "      <td>0.207621</td>\n",
              "      <td>0.204201</td>\n",
              "      <td>4</td>\n",
              "      <td>0.204201</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>166</td>\n",
              "      <td>269</td>\n",
              "      <td>1</td>\n",
              "      <td>0.120873</td>\n",
              "      <td>0.095556</td>\n",
              "      <td>-0.008978</td>\n",
              "      <td>0.016472</td>\n",
              "      <td>0.204201</td>\n",
              "      <td>-0.003364</td>\n",
              "      <td>4.979164</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.013718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.262661</td>\n",
              "      <td>0.228685</td>\n",
              "      <td>0.212799</td>\n",
              "      <td>0.210377</td>\n",
              "      <td>0.209078</td>\n",
              "      <td>4</td>\n",
              "      <td>0.209078</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>208</td>\n",
              "      <td>303</td>\n",
              "      <td>2</td>\n",
              "      <td>0.129353</td>\n",
              "      <td>0.069465</td>\n",
              "      <td>0.011382</td>\n",
              "      <td>0.006175</td>\n",
              "      <td>0.209078</td>\n",
              "      <td>-0.001291</td>\n",
              "      <td>4.812621</td>\n",
              "      <td>41.0</td>\n",
              "      <td>0.004684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.240320</td>\n",
              "      <td>0.238002</td>\n",
              "      <td>0.215711</td>\n",
              "      <td>0.204168</td>\n",
              "      <td>0.197385</td>\n",
              "      <td>4</td>\n",
              "      <td>0.197385</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>220</td>\n",
              "      <td>297</td>\n",
              "      <td>1</td>\n",
              "      <td>0.009645</td>\n",
              "      <td>0.093661</td>\n",
              "      <td>0.053511</td>\n",
              "      <td>0.033221</td>\n",
              "      <td>0.197385</td>\n",
              "      <td>-0.006557</td>\n",
              "      <td>5.240330</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.096023</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>102 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4b3eb5bd-192a-4b36-8b59-0c7348c68a48')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4b3eb5bd-192a-4b36-8b59-0c7348c68a48 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4b3eb5bd-192a-4b36-8b59-0c7348c68a48');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            0         1         2  ...     score  model_rank  proba_selection\n",
              "0    0.297629  0.256342  0.224900  ...  4.946564        21.0         0.009145\n",
              "1    0.276555  0.242013  0.211087  ...  4.963635        17.0         0.011297\n",
              "2    0.296420  0.235832  0.227745  ...  4.702388        61.0         0.003148\n",
              "3    0.270336  0.227319  0.231308  ...  4.478359        86.0         0.002233\n",
              "4    0.285443  0.220161  0.226018  ...  4.387952        91.0         0.002110\n",
              "..        ...       ...       ...  ...       ...         ...              ...\n",
              "97   0.247926  0.230486  0.211428  ...  4.754330        53.0         0.003624\n",
              "98   0.262173  0.218284  0.216946  ...  4.866401        29.0         0.006622\n",
              "99   0.258795  0.227513  0.205773  ...  4.979164        14.0         0.013718\n",
              "100  0.262661  0.228685  0.212799  ...  4.812621        41.0         0.004684\n",
              "101  0.240320  0.238002  0.215711  ...  5.240330         2.0         0.096023\n",
              "\n",
              "[102 rows x 21 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.19921749066561462, 192.0, 292.0, 1.0)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Génération :  9\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 188 hidden_dim : 283 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2578, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.406028824475542\n",
            "Epoch 1 : Validation loss = 0.25307872146368027; Validation r = 0.4671416505240667\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1139, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7525796907073441\n",
            "Epoch 2 : Validation loss = 0.2193437887976567; Validation r = 0.5166910261187709\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0555, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8869854954544977\n",
            "Epoch 3 : Validation loss = 0.20823931035896143; Validation r = 0.5393536013991896\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0283, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9445705864596081\n",
            "Epoch 4 : Validation loss = 0.2073696915060282; Validation r = 0.545068578629452\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0165, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9673123623165024\n",
            "Epoch 5 : Validation loss = 0.20340060479938984; Validation r = 0.5394818275410042\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 106 hidden_dim : 262 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2620, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.4025497962168173\n",
            "Epoch 1 : Validation loss = 0.27924860678613184; Validation r = 0.4232375201959402\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1455, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6670670115104472\n",
            "Epoch 2 : Validation loss = 0.23311366488536198; Validation r = 0.5002740826082582\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0816, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8298630014189585\n",
            "Epoch 3 : Validation loss = 0.21275232235590616; Validation r = 0.5170643058473146\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9086711995488682\n",
            "Epoch 4 : Validation loss = 0.21155725742379825; Validation r = 0.512857648704482\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0270, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9490536407054029\n",
            "Epoch 5 : Validation loss = 0.21046788313736517; Validation r = 0.5216702977081533\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 186 hidden_dim : 360 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2574, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.40590547577279457\n",
            "Epoch 1 : Validation loss = 0.23870054632425308; Validation r = 0.5093967060218949\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1191, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7368563349752866\n",
            "Epoch 2 : Validation loss = 0.2363528572022915; Validation r = 0.528851927823399\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0536, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8926307126458413\n",
            "Epoch 3 : Validation loss = 0.20461547387142975; Validation r = 0.551163937429227\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0280, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9454773892739515\n",
            "Epoch 4 : Validation loss = 0.21433154170711835; Validation r = 0.5585015646345125\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0174, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9656843164991228\n",
            "Epoch 5 : Validation loss = 0.20287699823578198; Validation r = 0.5650123600247512\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 104 hidden_dim : 245 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2745, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.3715425598364882\n",
            "Epoch 1 : Validation loss = 0.24893428720533847; Validation r = 0.44968110920026233\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1618, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6145904305627362\n",
            "Epoch 2 : Validation loss = 0.21654084225495657; Validation r = 0.4957275173809672\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0992, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7823116057965649\n",
            "Epoch 3 : Validation loss = 0.22517773310343425; Validation r = 0.4796046462191366\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0547, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8916915183521346\n",
            "Epoch 4 : Validation loss = 0.2145397396137317; Validation r = 0.49945032852992854\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0306, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9415506252095482\n",
            "Epoch 5 : Validation loss = 0.21441638953983783; Validation r = 0.4895035828577562\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 149 hidden_dim : 303 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2532, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.4059248226133554\n",
            "Epoch 1 : Validation loss = 0.29436555976668993; Validation r = 0.4346842615527111\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1300, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7078992404472207\n",
            "Epoch 2 : Validation loss = 0.23524796813726426; Validation r = 0.5083309540256905\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0698, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8563724303323174\n",
            "Epoch 3 : Validation loss = 0.21888290432592233; Validation r = 0.5142630975225192\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0360, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9298222619779771\n",
            "Epoch 4 : Validation loss = 0.21200473066419362; Validation r = 0.5329334909964988\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0202, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9615238218165046\n",
            "Epoch 5 : Validation loss = 0.20434667964776357; Validation r = 0.5392463315992543\n",
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-72386a5e-1818-4fd5-810d-727d2e811830\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>max_epoch</th>\n",
              "      <th>best_dev_loss</th>\n",
              "      <th>best_dev_loss_epoch</th>\n",
              "      <th>lastEpochImproved</th>\n",
              "      <th>embedding_dim</th>\n",
              "      <th>hidden_dim</th>\n",
              "      <th>num_lstm_layers</th>\n",
              "      <th>loss_decrease_perc_0</th>\n",
              "      <th>loss_decrease_perc_1</th>\n",
              "      <th>loss_decrease_perc_2</th>\n",
              "      <th>loss_decrease_perc_3</th>\n",
              "      <th>score_perf</th>\n",
              "      <th>score_pot</th>\n",
              "      <th>score</th>\n",
              "      <th>model_rank</th>\n",
              "      <th>proba_selection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.297629</td>\n",
              "      <td>0.256342</td>\n",
              "      <td>0.224900</td>\n",
              "      <td>0.221605</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.138718</td>\n",
              "      <td>0.122657</td>\n",
              "      <td>0.014653</td>\n",
              "      <td>0.044878</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>-0.009499</td>\n",
              "      <td>4.946564</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.007929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.276555</td>\n",
              "      <td>0.242013</td>\n",
              "      <td>0.211087</td>\n",
              "      <td>0.214948</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.124904</td>\n",
              "      <td>0.127787</td>\n",
              "      <td>-0.018291</td>\n",
              "      <td>0.031870</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>-0.006632</td>\n",
              "      <td>4.963635</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.009515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.296420</td>\n",
              "      <td>0.235832</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.233284</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>175</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.204399</td>\n",
              "      <td>0.034291</td>\n",
              "      <td>-0.024319</td>\n",
              "      <td>0.045231</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>-0.010074</td>\n",
              "      <td>4.702388</td>\n",
              "      <td>65.0</td>\n",
              "      <td>0.002928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.270336</td>\n",
              "      <td>0.227319</td>\n",
              "      <td>0.231308</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.223296</td>\n",
              "      <td>4</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>175</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.159127</td>\n",
              "      <td>-0.017549</td>\n",
              "      <td>0.089803</td>\n",
              "      <td>-0.060609</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.012760</td>\n",
              "      <td>4.478359</td>\n",
              "      <td>91.0</td>\n",
              "      <td>0.002091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.285443</td>\n",
              "      <td>0.220161</td>\n",
              "      <td>0.226018</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.227897</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.228703</td>\n",
              "      <td>-0.026604</td>\n",
              "      <td>0.057750</td>\n",
              "      <td>-0.070109</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.014931</td>\n",
              "      <td>4.387952</td>\n",
              "      <td>96.0</td>\n",
              "      <td>0.001982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.253079</td>\n",
              "      <td>0.219344</td>\n",
              "      <td>0.208239</td>\n",
              "      <td>0.207370</td>\n",
              "      <td>0.203401</td>\n",
              "      <td>4</td>\n",
              "      <td>0.203401</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>188</td>\n",
              "      <td>283</td>\n",
              "      <td>1</td>\n",
              "      <td>0.133298</td>\n",
              "      <td>0.050626</td>\n",
              "      <td>0.004176</td>\n",
              "      <td>0.019140</td>\n",
              "      <td>0.203401</td>\n",
              "      <td>-0.003893</td>\n",
              "      <td>5.012343</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.012687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>0.279249</td>\n",
              "      <td>0.233114</td>\n",
              "      <td>0.212752</td>\n",
              "      <td>0.211557</td>\n",
              "      <td>0.210468</td>\n",
              "      <td>4</td>\n",
              "      <td>0.210468</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>106</td>\n",
              "      <td>262</td>\n",
              "      <td>1</td>\n",
              "      <td>0.165211</td>\n",
              "      <td>0.087345</td>\n",
              "      <td>0.005617</td>\n",
              "      <td>0.005149</td>\n",
              "      <td>0.210468</td>\n",
              "      <td>-0.001084</td>\n",
              "      <td>4.775911</td>\n",
              "      <td>47.0</td>\n",
              "      <td>0.004049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>0.238701</td>\n",
              "      <td>0.236353</td>\n",
              "      <td>0.204615</td>\n",
              "      <td>0.214332</td>\n",
              "      <td>0.202877</td>\n",
              "      <td>4</td>\n",
              "      <td>0.202877</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>186</td>\n",
              "      <td>360</td>\n",
              "      <td>1</td>\n",
              "      <td>0.009835</td>\n",
              "      <td>0.134280</td>\n",
              "      <td>-0.047485</td>\n",
              "      <td>0.053443</td>\n",
              "      <td>0.202877</td>\n",
              "      <td>-0.010842</td>\n",
              "      <td>5.207394</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.063435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>0.248934</td>\n",
              "      <td>0.216541</td>\n",
              "      <td>0.225178</td>\n",
              "      <td>0.214540</td>\n",
              "      <td>0.214416</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214416</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>104</td>\n",
              "      <td>245</td>\n",
              "      <td>2</td>\n",
              "      <td>0.130128</td>\n",
              "      <td>-0.039886</td>\n",
              "      <td>0.047243</td>\n",
              "      <td>0.000575</td>\n",
              "      <td>0.214416</td>\n",
              "      <td>-0.000123</td>\n",
              "      <td>4.666506</td>\n",
              "      <td>74.0</td>\n",
              "      <td>0.002572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>0.294366</td>\n",
              "      <td>0.235248</td>\n",
              "      <td>0.218883</td>\n",
              "      <td>0.212005</td>\n",
              "      <td>0.204347</td>\n",
              "      <td>4</td>\n",
              "      <td>0.204347</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>149</td>\n",
              "      <td>303</td>\n",
              "      <td>1</td>\n",
              "      <td>0.200831</td>\n",
              "      <td>0.069565</td>\n",
              "      <td>0.031424</td>\n",
              "      <td>0.036122</td>\n",
              "      <td>0.204347</td>\n",
              "      <td>-0.007381</td>\n",
              "      <td>5.077038</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.019031</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>107 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-72386a5e-1818-4fd5-810d-727d2e811830')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-72386a5e-1818-4fd5-810d-727d2e811830 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-72386a5e-1818-4fd5-810d-727d2e811830');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            0         1         2  ...     score  model_rank  proba_selection\n",
              "0    0.297629  0.256342  0.224900  ...  4.946564        24.0         0.007929\n",
              "1    0.276555  0.242013  0.211087  ...  4.963635        20.0         0.009515\n",
              "2    0.296420  0.235832  0.227745  ...  4.702388        65.0         0.002928\n",
              "3    0.270336  0.227319  0.231308  ...  4.478359        91.0         0.002091\n",
              "4    0.285443  0.220161  0.226018  ...  4.387952        96.0         0.001982\n",
              "..        ...       ...       ...  ...       ...         ...              ...\n",
              "102  0.253079  0.219344  0.208239  ...  5.012343        15.0         0.012687\n",
              "103  0.279249  0.233114  0.212752  ...  4.775911        47.0         0.004049\n",
              "104  0.238701  0.236353  0.204615  ...  5.207394         3.0         0.063435\n",
              "105  0.248934  0.216541  0.225178  ...  4.666506        74.0         0.002572\n",
              "106  0.294366  0.235248  0.218883  ...  5.077038        10.0         0.019031\n",
              "\n",
              "[107 rows x 21 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.19921749066561462, 192.0, 292.0, 1.0)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Génération :  10\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 176 hidden_dim : 342 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2600, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.39004496306167036\n",
            "Epoch 1 : Validation loss = 0.2717112470418215; Validation r = 0.4211195718977808\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1266, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7132448486396658\n",
            "Epoch 2 : Validation loss = 0.21322166460255781; Validation r = 0.5306635311459292\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0622, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8734539823462687\n",
            "Epoch 3 : Validation loss = 0.21429238617420196; Validation r = 0.5297650938710996\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0312, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9392576924058544\n",
            "Epoch 4 : Validation loss = 0.2050115233908097; Validation r = 0.5492120105462095\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0176, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9659500095954378\n",
            "Epoch 5 : Validation loss = 0.20286062061786653; Validation r = 0.5467663477488764\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 222 hidden_dim : 315 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2569, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.4001710483080242\n",
            "Epoch 1 : Validation loss = 0.24960442508260408; Validation r = 0.481700789523322\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1205, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7309883168152698\n",
            "Epoch 2 : Validation loss = 0.21616728603839874; Validation r = 0.5325020886125995\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0547, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8888428271644614\n",
            "Epoch 3 : Validation loss = 0.2087967677041888; Validation r = 0.5601203898424174\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0265, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9477196666940609\n",
            "Epoch 4 : Validation loss = 0.20625552472968897; Validation r = 0.5667844386637885\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0162, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9676407071103414\n",
            "Epoch 5 : Validation loss = 0.20849232052763303; Validation r = 0.567907589840469\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 151 hidden_dim : 238 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2619, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.39385041684667266\n",
            "Epoch 1 : Validation loss = 0.23613143886129062; Validation r = 0.5077589786468099\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1278, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7163087036093491\n",
            "Epoch 2 : Validation loss = 0.22712377042820056; Validation r = 0.5104225943358154\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0651, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8671640085661476\n",
            "Epoch 3 : Validation loss = 0.2169751441727082; Validation r = 0.5187148464925441\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0330, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9362029443111216\n",
            "Epoch 4 : Validation loss = 0.21695312497516472; Validation r = 0.5175983549204686\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0190, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9637984834625656\n",
            "Epoch 5 : Validation loss = 0.21508009191602467; Validation r = 0.5209653132202251\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 184 hidden_dim : 243 num_lstm_layers : 2 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2710, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.38777216533774267\n",
            "Epoch 1 : Validation loss = 0.24724720753729343; Validation r = 0.4754850698539411\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1472, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.6575321971731098\n",
            "Epoch 2 : Validation loss = 0.23587846159934997; Validation r = 0.46822254794558427\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0768, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.8363625236690043\n",
            "Epoch 3 : Validation loss = 0.22679879007240136; Validation r = 0.48473294727144695\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0391, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9207364930838663\n",
            "Epoch 4 : Validation loss = 0.2206534901012977; Validation r = 0.49284595120263197\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0225, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9537329347585802\n",
            "Epoch 5 : Validation loss = 0.21919162621100743; Validation r = 0.4961269829579239\n",
            "Training BiLSTM model with following layers hyperparameters \n",
            " embedding_dim : 216 hidden_dim : 290 num_lstm_layers : 1 \n",
            "Beginning training...\n",
            "Epoch 1:\n",
            "Total epoch loss = tensor(0.2656, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.38938842222113473\n",
            "Epoch 1 : Validation loss = 0.29468302242457867; Validation r = 0.45166791064873196\n",
            "Epoch 2:\n",
            "Total epoch loss = tensor(0.1174, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.7450850825484194\n",
            "Epoch 2 : Validation loss = 0.22671591316660245; Validation r = 0.5231099176567153\n",
            "Epoch 3:\n",
            "Total epoch loss = tensor(0.0544, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.890712563714486\n",
            "Epoch 3 : Validation loss = 0.21330554777135452; Validation r = 0.5352706936090712\n",
            "Epoch 4:\n",
            "Total epoch loss = tensor(0.0278, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9452697877912553\n",
            "Epoch 4 : Validation loss = 0.208680838222305; Validation r = 0.5486918956104505\n",
            "Epoch 5:\n",
            "Total epoch loss = tensor(0.0174, device='cuda:0', grad_fn=<DivBackward0>); Total training r = 0.9655199403273198\n",
            "Epoch 5 : Validation loss = 0.20691522068033616; Validation r = 0.535018884006287\n",
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f4fd70e5-8b7e-4736-bd9b-03b500e6bf31\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>max_epoch</th>\n",
              "      <th>best_dev_loss</th>\n",
              "      <th>best_dev_loss_epoch</th>\n",
              "      <th>lastEpochImproved</th>\n",
              "      <th>embedding_dim</th>\n",
              "      <th>hidden_dim</th>\n",
              "      <th>num_lstm_layers</th>\n",
              "      <th>loss_decrease_perc_0</th>\n",
              "      <th>loss_decrease_perc_1</th>\n",
              "      <th>loss_decrease_perc_2</th>\n",
              "      <th>loss_decrease_perc_3</th>\n",
              "      <th>score_perf</th>\n",
              "      <th>score_pot</th>\n",
              "      <th>score</th>\n",
              "      <th>model_rank</th>\n",
              "      <th>proba_selection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.297629</td>\n",
              "      <td>0.256342</td>\n",
              "      <td>0.224900</td>\n",
              "      <td>0.221605</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.138718</td>\n",
              "      <td>0.122657</td>\n",
              "      <td>0.014653</td>\n",
              "      <td>0.044878</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>-0.009499</td>\n",
              "      <td>4.946564</td>\n",
              "      <td>25.0</td>\n",
              "      <td>0.007547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.276555</td>\n",
              "      <td>0.242013</td>\n",
              "      <td>0.211087</td>\n",
              "      <td>0.214948</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.124904</td>\n",
              "      <td>0.127787</td>\n",
              "      <td>-0.018291</td>\n",
              "      <td>0.031870</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>-0.006632</td>\n",
              "      <td>4.963635</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.008984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.296420</td>\n",
              "      <td>0.235832</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.233284</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>175</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.204399</td>\n",
              "      <td>0.034291</td>\n",
              "      <td>-0.024319</td>\n",
              "      <td>0.045231</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>-0.010074</td>\n",
              "      <td>4.702388</td>\n",
              "      <td>68.0</td>\n",
              "      <td>0.002775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.270336</td>\n",
              "      <td>0.227319</td>\n",
              "      <td>0.231308</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.223296</td>\n",
              "      <td>4</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>175</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.159127</td>\n",
              "      <td>-0.017549</td>\n",
              "      <td>0.089803</td>\n",
              "      <td>-0.060609</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.012760</td>\n",
              "      <td>4.478359</td>\n",
              "      <td>96.0</td>\n",
              "      <td>0.001965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.285443</td>\n",
              "      <td>0.220161</td>\n",
              "      <td>0.226018</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.227897</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.228703</td>\n",
              "      <td>-0.026604</td>\n",
              "      <td>0.057750</td>\n",
              "      <td>-0.070109</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.014931</td>\n",
              "      <td>4.387952</td>\n",
              "      <td>101.0</td>\n",
              "      <td>0.001868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>0.271711</td>\n",
              "      <td>0.213222</td>\n",
              "      <td>0.214292</td>\n",
              "      <td>0.205012</td>\n",
              "      <td>0.202861</td>\n",
              "      <td>4</td>\n",
              "      <td>0.202861</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>176</td>\n",
              "      <td>342</td>\n",
              "      <td>1</td>\n",
              "      <td>0.215264</td>\n",
              "      <td>-0.005022</td>\n",
              "      <td>0.043309</td>\n",
              "      <td>0.010492</td>\n",
              "      <td>0.202861</td>\n",
              "      <td>-0.002128</td>\n",
              "      <td>4.981760</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.011098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>0.249604</td>\n",
              "      <td>0.216167</td>\n",
              "      <td>0.208797</td>\n",
              "      <td>0.206256</td>\n",
              "      <td>0.208492</td>\n",
              "      <td>4</td>\n",
              "      <td>0.206256</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>222</td>\n",
              "      <td>315</td>\n",
              "      <td>1</td>\n",
              "      <td>0.133961</td>\n",
              "      <td>0.034096</td>\n",
              "      <td>0.012171</td>\n",
              "      <td>-0.010845</td>\n",
              "      <td>0.206256</td>\n",
              "      <td>0.002237</td>\n",
              "      <td>4.796340</td>\n",
              "      <td>48.0</td>\n",
              "      <td>0.003931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>0.236131</td>\n",
              "      <td>0.227124</td>\n",
              "      <td>0.216975</td>\n",
              "      <td>0.216953</td>\n",
              "      <td>0.215080</td>\n",
              "      <td>4</td>\n",
              "      <td>0.215080</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>151</td>\n",
              "      <td>238</td>\n",
              "      <td>1</td>\n",
              "      <td>0.038147</td>\n",
              "      <td>0.044683</td>\n",
              "      <td>0.000101</td>\n",
              "      <td>0.008633</td>\n",
              "      <td>0.215080</td>\n",
              "      <td>-0.001857</td>\n",
              "      <td>4.689921</td>\n",
              "      <td>72.0</td>\n",
              "      <td>0.002620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>0.247247</td>\n",
              "      <td>0.235878</td>\n",
              "      <td>0.226799</td>\n",
              "      <td>0.220653</td>\n",
              "      <td>0.219192</td>\n",
              "      <td>4</td>\n",
              "      <td>0.219192</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>184</td>\n",
              "      <td>243</td>\n",
              "      <td>2</td>\n",
              "      <td>0.045981</td>\n",
              "      <td>0.038493</td>\n",
              "      <td>0.027096</td>\n",
              "      <td>0.006625</td>\n",
              "      <td>0.219192</td>\n",
              "      <td>-0.001452</td>\n",
              "      <td>4.592645</td>\n",
              "      <td>84.0</td>\n",
              "      <td>0.002246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>0.294683</td>\n",
              "      <td>0.226716</td>\n",
              "      <td>0.213306</td>\n",
              "      <td>0.208681</td>\n",
              "      <td>0.206915</td>\n",
              "      <td>4</td>\n",
              "      <td>0.206915</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>216</td>\n",
              "      <td>290</td>\n",
              "      <td>1</td>\n",
              "      <td>0.230645</td>\n",
              "      <td>0.059151</td>\n",
              "      <td>0.021681</td>\n",
              "      <td>0.008461</td>\n",
              "      <td>0.206915</td>\n",
              "      <td>-0.001751</td>\n",
              "      <td>4.874137</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.005717</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>112 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f4fd70e5-8b7e-4736-bd9b-03b500e6bf31')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f4fd70e5-8b7e-4736-bd9b-03b500e6bf31 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f4fd70e5-8b7e-4736-bd9b-03b500e6bf31');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            0         1         2  ...     score  model_rank  proba_selection\n",
              "0    0.297629  0.256342  0.224900  ...  4.946564        25.0         0.007547\n",
              "1    0.276555  0.242013  0.211087  ...  4.963635        21.0         0.008984\n",
              "2    0.296420  0.235832  0.227745  ...  4.702388        68.0         0.002775\n",
              "3    0.270336  0.227319  0.231308  ...  4.478359        96.0         0.001965\n",
              "4    0.285443  0.220161  0.226018  ...  4.387952       101.0         0.001868\n",
              "..        ...       ...       ...  ...       ...         ...              ...\n",
              "107  0.271711  0.213222  0.214292  ...  4.981760        17.0         0.011098\n",
              "108  0.249604  0.216167  0.208797  ...  4.796340        48.0         0.003931\n",
              "109  0.236131  0.227124  0.216975  ...  4.689921        72.0         0.002620\n",
              "110  0.247247  0.235878  0.226799  ...  4.592645        84.0         0.002246\n",
              "111  0.294683  0.226716  0.213306  ...  4.874137        33.0         0.005717\n",
              "\n",
              "[112 rows x 21 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.19921749066561462, 192.0, 292.0, 1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Synthèse\n",
        "\n",
        "df_results = create_df_res(results, temperature = 1)\n",
        "display(df_results)\n",
        "print(\"Meilleur modèle : \" , get_best_model(df_results))\n",
        "display(df_results[df_results[\"model_rank\"] == 1])\n",
        "print(\"Meilleure loss atteinte : \" , df_results[df_results[\"best_dev_loss\"] == df_results[\"best_dev_loss\"].min()][\"best_dev_loss\"])\n",
        "display(df_results[df_results[\"best_dev_loss\"] == df_results[\"best_dev_loss\"].min()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "id": "HEjQp87T5fBH",
        "outputId": "d25fee04-99f3-48c0-eb97-2a51c00c79a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c6022489-eeda-4bec-bd90-5d5360d5c782\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>max_epoch</th>\n",
              "      <th>best_dev_loss</th>\n",
              "      <th>best_dev_loss_epoch</th>\n",
              "      <th>lastEpochImproved</th>\n",
              "      <th>embedding_dim</th>\n",
              "      <th>hidden_dim</th>\n",
              "      <th>num_lstm_layers</th>\n",
              "      <th>loss_decrease_perc_0</th>\n",
              "      <th>loss_decrease_perc_1</th>\n",
              "      <th>loss_decrease_perc_2</th>\n",
              "      <th>loss_decrease_perc_3</th>\n",
              "      <th>score_perf</th>\n",
              "      <th>score_pot</th>\n",
              "      <th>score</th>\n",
              "      <th>model_rank</th>\n",
              "      <th>proba_selection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.297629</td>\n",
              "      <td>0.256342</td>\n",
              "      <td>0.224900</td>\n",
              "      <td>0.221605</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.138718</td>\n",
              "      <td>0.122657</td>\n",
              "      <td>0.014653</td>\n",
              "      <td>0.044878</td>\n",
              "      <td>0.211659</td>\n",
              "      <td>-0.009499</td>\n",
              "      <td>4.946564</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.161123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.276555</td>\n",
              "      <td>0.242013</td>\n",
              "      <td>0.211087</td>\n",
              "      <td>0.214948</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.124904</td>\n",
              "      <td>0.127787</td>\n",
              "      <td>-0.018291</td>\n",
              "      <td>0.031870</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>-0.006632</td>\n",
              "      <td>4.963635</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.322247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.296420</td>\n",
              "      <td>0.235832</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.233284</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>175</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>0.204399</td>\n",
              "      <td>0.034291</td>\n",
              "      <td>-0.024319</td>\n",
              "      <td>0.045231</td>\n",
              "      <td>0.222732</td>\n",
              "      <td>-0.010074</td>\n",
              "      <td>4.702388</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.080562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.270336</td>\n",
              "      <td>0.227319</td>\n",
              "      <td>0.231308</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.223296</td>\n",
              "      <td>4</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>175</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.159127</td>\n",
              "      <td>-0.017549</td>\n",
              "      <td>0.089803</td>\n",
              "      <td>-0.060609</td>\n",
              "      <td>0.210536</td>\n",
              "      <td>0.012760</td>\n",
              "      <td>4.478359</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.053708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.285443</td>\n",
              "      <td>0.220161</td>\n",
              "      <td>0.226018</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.227897</td>\n",
              "      <td>4</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.228703</td>\n",
              "      <td>-0.026604</td>\n",
              "      <td>0.057750</td>\n",
              "      <td>-0.070109</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.014931</td>\n",
              "      <td>4.387952</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.040281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.295962</td>\n",
              "      <td>0.254919</td>\n",
              "      <td>0.248721</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>0.243503</td>\n",
              "      <td>4</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>0.138675</td>\n",
              "      <td>0.024316</td>\n",
              "      <td>0.056733</td>\n",
              "      <td>-0.037906</td>\n",
              "      <td>0.234610</td>\n",
              "      <td>0.008893</td>\n",
              "      <td>4.106723</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.029295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.264929</td>\n",
              "      <td>0.224174</td>\n",
              "      <td>0.228173</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>0.218195</td>\n",
              "      <td>4</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>300</td>\n",
              "      <td>2</td>\n",
              "      <td>0.153832</td>\n",
              "      <td>-0.017838</td>\n",
              "      <td>0.057869</td>\n",
              "      <td>-0.015006</td>\n",
              "      <td>0.214969</td>\n",
              "      <td>0.003226</td>\n",
              "      <td>4.583062</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.064449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.270235</td>\n",
              "      <td>0.228662</td>\n",
              "      <td>0.249313</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>0.211333</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>300</td>\n",
              "      <td>300</td>\n",
              "      <td>3</td>\n",
              "      <td>0.153839</td>\n",
              "      <td>-0.090309</td>\n",
              "      <td>0.163048</td>\n",
              "      <td>-0.012797</td>\n",
              "      <td>0.208663</td>\n",
              "      <td>0.002670</td>\n",
              "      <td>4.731872</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.107416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.257634</td>\n",
              "      <td>0.223843</td>\n",
              "      <td>0.245183</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>0.223688</td>\n",
              "      <td>4</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.131159</td>\n",
              "      <td>-0.095335</td>\n",
              "      <td>0.104335</td>\n",
              "      <td>-0.018609</td>\n",
              "      <td>0.219602</td>\n",
              "      <td>0.004087</td>\n",
              "      <td>4.470503</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.046035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.289596</td>\n",
              "      <td>0.240284</td>\n",
              "      <td>0.235281</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>0.247811</td>\n",
              "      <td>4</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>0.170279</td>\n",
              "      <td>0.020819</td>\n",
              "      <td>0.022889</td>\n",
              "      <td>-0.077928</td>\n",
              "      <td>0.229896</td>\n",
              "      <td>0.017915</td>\n",
              "      <td>4.035327</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.026854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.254877</td>\n",
              "      <td>0.259745</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>0.240172</td>\n",
              "      <td>0.241364</td>\n",
              "      <td>4</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>300</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.019099</td>\n",
              "      <td>0.111744</td>\n",
              "      <td>-0.040966</td>\n",
              "      <td>-0.004965</td>\n",
              "      <td>0.230720</td>\n",
              "      <td>0.001145</td>\n",
              "      <td>4.312846</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.035805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.306085</td>\n",
              "      <td>0.269204</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>0.240201</td>\n",
              "      <td>0.240892</td>\n",
              "      <td>4</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>350</td>\n",
              "      <td>300</td>\n",
              "      <td>3</td>\n",
              "      <td>0.120492</td>\n",
              "      <td>0.127428</td>\n",
              "      <td>-0.022564</td>\n",
              "      <td>-0.002880</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>0.000677</td>\n",
              "      <td>4.244900</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.032225</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c6022489-eeda-4bec-bd90-5d5360d5c782')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c6022489-eeda-4bec-bd90-5d5360d5c782 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c6022489-eeda-4bec-bd90-5d5360d5c782');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           0         1         2  ...     score  model_rank  proba_selection\n",
              "0   0.297629  0.256342  0.224900  ...  4.946564         2.0         0.161123\n",
              "1   0.276555  0.242013  0.211087  ...  4.963635         1.0         0.322247\n",
              "2   0.296420  0.235832  0.227745  ...  4.702388         4.0         0.080562\n",
              "3   0.270336  0.227319  0.231308  ...  4.478359         6.0         0.053708\n",
              "4   0.285443  0.220161  0.226018  ...  4.387952         8.0         0.040281\n",
              "5   0.295962  0.254919  0.248721  ...  4.106723        11.0         0.029295\n",
              "6   0.264929  0.224174  0.228173  ...  4.583062         5.0         0.064449\n",
              "7   0.270235  0.228662  0.249313  ...  4.731872         3.0         0.107416\n",
              "8   0.257634  0.223843  0.245183  ...  4.470503         7.0         0.046035\n",
              "9   0.289596  0.240284  0.235281  ...  4.035327        12.0         0.026854\n",
              "10  0.254877  0.259745  0.230720  ...  4.312846         9.0         0.035805\n",
              "11  0.306085  0.269204  0.234900  ...  4.244900        10.0         0.032225\n",
              "\n",
              "[12 rows x 21 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meilleur modèle :  (0.20809729459385076, 100.0, 275.0, 2.0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-44a53929-9e11-48bf-86de-29e9fd2dd2b3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>max_epoch</th>\n",
              "      <th>best_dev_loss</th>\n",
              "      <th>best_dev_loss_epoch</th>\n",
              "      <th>lastEpochImproved</th>\n",
              "      <th>embedding_dim</th>\n",
              "      <th>hidden_dim</th>\n",
              "      <th>num_lstm_layers</th>\n",
              "      <th>loss_decrease_perc_0</th>\n",
              "      <th>loss_decrease_perc_1</th>\n",
              "      <th>loss_decrease_perc_2</th>\n",
              "      <th>loss_decrease_perc_3</th>\n",
              "      <th>score_perf</th>\n",
              "      <th>score_pot</th>\n",
              "      <th>score</th>\n",
              "      <th>model_rank</th>\n",
              "      <th>proba_selection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.276555</td>\n",
              "      <td>0.242013</td>\n",
              "      <td>0.211087</td>\n",
              "      <td>0.214948</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.124904</td>\n",
              "      <td>0.127787</td>\n",
              "      <td>-0.018291</td>\n",
              "      <td>0.03187</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>-0.006632</td>\n",
              "      <td>4.963635</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.322247</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-44a53929-9e11-48bf-86de-29e9fd2dd2b3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-44a53929-9e11-48bf-86de-29e9fd2dd2b3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-44a53929-9e11-48bf-86de-29e9fd2dd2b3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "          0         1         2  ...     score  model_rank  proba_selection\n",
              "1  0.276555  0.242013  0.211087  ...  4.963635         1.0         0.322247\n",
              "\n",
              "[1 rows x 21 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meilleure loss atteinte :  1    0.208097\n",
            "Name: best_dev_loss, dtype: float64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0e4e6187-738e-4520-b042-20be05618d43\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>max_epoch</th>\n",
              "      <th>best_dev_loss</th>\n",
              "      <th>best_dev_loss_epoch</th>\n",
              "      <th>lastEpochImproved</th>\n",
              "      <th>embedding_dim</th>\n",
              "      <th>hidden_dim</th>\n",
              "      <th>num_lstm_layers</th>\n",
              "      <th>loss_decrease_perc_0</th>\n",
              "      <th>loss_decrease_perc_1</th>\n",
              "      <th>loss_decrease_perc_2</th>\n",
              "      <th>loss_decrease_perc_3</th>\n",
              "      <th>score_perf</th>\n",
              "      <th>score_pot</th>\n",
              "      <th>score</th>\n",
              "      <th>model_rank</th>\n",
              "      <th>proba_selection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.276555</td>\n",
              "      <td>0.242013</td>\n",
              "      <td>0.211087</td>\n",
              "      <td>0.214948</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>0.124904</td>\n",
              "      <td>0.127787</td>\n",
              "      <td>-0.018291</td>\n",
              "      <td>0.03187</td>\n",
              "      <td>0.208097</td>\n",
              "      <td>-0.006632</td>\n",
              "      <td>4.963635</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.322247</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0e4e6187-738e-4520-b042-20be05618d43')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0e4e6187-738e-4520-b042-20be05618d43 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0e4e6187-738e-4520-b042-20be05618d43');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "          0         1         2  ...     score  model_rank  proba_selection\n",
              "1  0.276555  0.242013  0.211087  ...  4.963635         1.0         0.322247\n",
              "\n",
              "[1 rows x 21 columns]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "--wnhgC0GdTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGjJ4Na_xVCD"
      },
      "source": [
        "### A bit of reflection\n",
        "\n",
        "In the report, you can write:\n",
        "- What configurations did you try and which one worked best?\n",
        "\n",
        "More globally:\n",
        "- Out of the three types of models you implemented, which one was the best?\n",
        "- How do you think the/some models could be combined for better performance?\n",
        "\n",
        "And a bit deeper:\n",
        "- Have a look at the predictions of some of your models (1, 2 and/or 3). Do you observe anything interesting? Are the sentence pairs with the worst\\* predictions more challenging, in your opinion? (e.g. they contain rare words)\n",
        "\n",
        "\\* \"worst predictions\" would be those which are farthest away from their corresponding gold value. You can determine this with the Mean Squared Error (MSE)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MyHgRuAnocP"
      },
      "source": [
        "## Team-specific exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjvHjQhJI_tb"
      },
      "source": [
        "\n",
        "\n",
        "### Team A: Spanish STS\n",
        "\n",
        "Your task will consist in addressing STS in Spanish. For this purpose you will download a specific dataset, as well as Spanish word embeddings. You will implement a model like that in (2) for this task. \n",
        "Additionally, you will combine the word embeddings with at least one feature of those used in (1) (yes, it could be simple word overlap)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaWj4PlB280K"
      },
      "source": [
        "#### (Team A) Dataset\n",
        "\n",
        "- For training: the [**2015** data](http://ixa2.si.ehu.es/stswiki/images/9/9d/STS2015-es-test.zip) (751 instances)\n",
        "- For testing: the [**2017 track 3** data](http://ixa2.si.ehu.es/stswiki/images/2/20/Sts2017.eval.v1.1.zip) with its [gold standard](http://ixa2.si.ehu.es/stswiki/images/7/70/Sts2017.gs.zip) (250 instances)\n",
        "\n",
        "\n",
        "Download the datasets, upload them to Colab, get acquainted with them, and load them using the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goKO42yr255i"
      },
      "source": [
        "# Loading team A datasets\n",
        "! unzip STS2015-es-test.zip -d STS2015-es-test\n",
        "! unzip Sts2017.eval.v1.1.zip\n",
        "! unzip Sts2017.gs.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8KBp-2v7Usr"
      },
      "source": [
        "def load_mono_spanish_data():\n",
        "  traindir = \"STS2015-es-test/\"  \n",
        "  data = dict()\n",
        "  for subset in [\"train\",\"test\"]:\n",
        "    data[subset] = dict()\n",
        "    data[subset][\"data\"] = []\n",
        "    data[subset][\"scores\"] = []  \n",
        "  train_data_by_category = dict()\n",
        "  train_scores_by_category = dict()  \n",
        "  for fn in os.listdir(traindir):\n",
        "    _, typ, category, _ = fn.split(\".\")\n",
        "    with open(traindir + fn) as f:      \n",
        "      if typ == \"gs\":\n",
        "        train_scores_by_category[category] = []\n",
        "        for l in f:\n",
        "          train_scores_by_category[category].append(float(l))\n",
        "      elif typ == \"input\":\n",
        "        train_data_by_category[category] = []\n",
        "        for l in f:\n",
        "          train_data_by_category[category].append(l.strip().split(\"\\t\"))\n",
        "\n",
        "  for cat in train_data_by_category:\n",
        "    data['train']['data'].extend(train_data_by_category[cat])\n",
        "    data['train']['scores'].extend(train_scores_by_category[cat])\n",
        "\n",
        "  #### now test data and then we merge them...  \n",
        "  with open(\"STS2017.eval.v1.1/STS.input.track3.es-es.txt\") as f:\n",
        "    for l in f:\n",
        "      data['test']['data'].append(l.strip().split(\"\\t\"))\n",
        "  with open(\"STS2017.gs/STS.gs.track3.es-es.txt\") as f:\n",
        "    for l in f:\n",
        "      data['test']['scores'].append(float(l))\n",
        " \n",
        "  return data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDxHLdKs6hUc"
      },
      "source": [
        "# Load the data\n",
        "\n",
        "mono_data = load_mono_spanish_data()\n",
        "\n",
        "print(\"Score ranges:\")\n",
        "print(min(mono_data['train']['scores']), \"-\", max(mono_data['train']['scores']))\n",
        "print(min(mono_data['test']['scores']), \"-\", max(mono_data['test']['scores']))\n",
        "\n",
        "## Careful! The range of scores is different in the Spanish training set. It goes from 0 to 4.\n",
        "# We will map it to a 0-5 scale\n",
        "\n",
        "mono_data['train']['scores'] = (np.array(mono_data['train']['scores']) / 4) * 5\n",
        "\n",
        "## Also, you may notice that some sentence pairs are repeated within the training set, but have different scores.\n",
        "## We will simply make sure that there is no overlap between the training and the test datasets:\n",
        "\n",
        "\n",
        "found = False\n",
        "tr_sps = [tuple(ins) for ins in mono_data['train']['data']]\n",
        "te_sps = [tuple(ins) for ins in mono_data['test']['data']]\n",
        "for ins in te_sps:\n",
        "  if ins in tr_sps:\n",
        "    found = True\n",
        "    break\n",
        "if found:\n",
        "  print(\"Found some repetition\")\n",
        "else:\n",
        "  print(\"No repetitions found\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdbOOXeTcRhW"
      },
      "source": [
        "#### (Team A) Word embeddings in other languages\n",
        "\n",
        "In this experiment, you will implement a model using word embeddings, similar to what you did in (2). This time you will combine word-embedding-based features with some other feature from the model in (1).\n",
        "\n",
        "For STS in Spanish, we will download Spanish Fasttext word embeddings from [this website](https://fasttext.cc/docs/en/crawl-vectors.html). The link to the Spanish embeddings is [here](https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.bin.gz).\n",
        "You can either download them from the link or using the code below.\n",
        "\n",
        "For convenience, for these word embeddings we will use a different library: `fasttext`. The code below shows how to retrieve the embedding for a word.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFEtcO-MJQmZ"
      },
      "source": [
        "# This will take a while\n",
        "\n",
        "! pip install fasttext\n",
        "import fasttext.util\n",
        "print(\"Installed and loaded the library\")\n",
        "# Download Spanish embeddings using this code or from the link above\n",
        "fasttext.util.download_model('es', if_exists='ignore')\n",
        "print(\"Downloaded embeddings\")\n",
        "# Load them\n",
        "ft = fasttext.load_model('cc.es.300.bin')\n",
        "print(\"Loaded embeddings\")\n",
        "\n",
        "# This is how you get the embedding of a word\n",
        "vector = ft.get_word_vector('hola')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26P9SkfqeL41"
      },
      "source": [
        "#### Spanish specifics\n",
        "\n",
        "Be careful: not all features may readily work for Spanish (for example, WordNet is English). Ideally, you should use a tokenizer that is specific for Spanish. You can find some sample code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhLb6XHzeT1L"
      },
      "source": [
        "! pip install spacy\n",
        "# Downloading Spanish model\n",
        "! python -m spacy download es_core_news_sm\n",
        "import spacy\n",
        "import es_core_news_sm\n",
        "nlp = es_core_news_sm.load()\n",
        "#nlp = spacy.load('es_core_news_sm')\n",
        "text = \"Esto es una frase en español.\"\n",
        "processed_text = nlp(text)\n",
        "for t in processed_text:\n",
        "  print(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1IDrTijtLgr"
      },
      "source": [
        "def assign_FT_vectors(data, vectors, sim_or_dist=True, lowercase=False):\n",
        "  # Build a function similar to assign_distributional_embeddings which works with \n",
        "  # this embedding format. Additionally, this function calculates other features from the model\n",
        "  # you used in (1) - you can reuse code from your <extract_features> function. \n",
        "  ## TO COMPLETE\n",
        " \n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtJ__-L_AtwA"
      },
      "source": [
        "# Obtain train_x, test_x and train_y, test_y\n",
        "## TO COMPLETE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el7O1sCiJjmf"
      },
      "source": [
        "## Extract features, run classifier, evaluate.\n",
        "## TO COMPLETE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdqgHbl5SYwT"
      },
      "source": [
        "### Team B: Cross-lingual English-Spanish STS\n",
        "\n",
        "Your task will consist in addressing cross-lingual STS, where $s_1$ and $s_2$ are in different languages. For this purpose you will download a specific dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onbzblDwsfnp"
      },
      "source": [
        "#### (Team B) Dataset\n",
        "\n",
        "- For training: the **2016** [trial](http://ixa2.si.ehu.es/stswiki/images/f/f4/Sts2016-crosslingual-trial.zip) and [test data](http://ixa2.si.ehu.es/stswiki/images/f/fb/Sts2016-cross-lingual-test.tar.gz) (698 instances)\n",
        "- For testing: the **2017 track 4a and 4b** data (500 instances, link just above, including data for Machine Translation evaluation). It can be downloaded [here](http://ixa2.si.ehu.es/stswiki/images/2/20/Sts2017.eval.v1.1.zip) and [here](http://ixa2.si.ehu.es/stswiki/images/7/70/Sts2017.gs.zip).\n",
        "\n",
        "Download the datasets, upload them to Colab, get acquainted with them, and load them using the code below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blbw-TvWz_gA"
      },
      "source": [
        "# Team B datasets\n",
        "! unzip Sts2017.eval.v1.1.zip\n",
        "! unzip Sts2017.gs.zip\n",
        "! tar -xvzf Sts2016-cross-lingual-test.tar.gz\n",
        "! unzip Sts2016-crosslingual-trial.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-kM1AH_zILI"
      },
      "source": [
        "def load_crosslingual_data():  \n",
        "  data = dict()\n",
        "  for subset in [\"train\",\"test\"]:\n",
        "    data[subset] = dict()\n",
        "    data[subset][\"data\"] = []\n",
        "    data[subset][\"scores\"] = [] \n",
        "\n",
        "  for input_file in [\"STS2016-Crosslingual-Trial/STS.input.crosslingual-trial.txt\", \"STS2016-cross-lingual-test/STS.input.news.txt\", \"STS2016-cross-lingual-test/STS.input.multisource.txt\"]:\n",
        "      with open(input_file) as f:\n",
        "        for l in f:\n",
        "          data['train']['data'].append(l.strip().split(\"\\t\")[:2])\n",
        "  for gs_file in [\"STS2016-Crosslingual-Trial/STS.gs.crosslingual-trial.txt\", \"STS2016-cross-lingual-test/STS.gs.news.txt\", \"STS2016-cross-lingual-test/STS.gs.multisource.txt\"]:\n",
        "    with open(gs_file) as f:\n",
        "      for l in f:\n",
        "        data['train']['scores'].append(float(l))\n",
        "  \n",
        "  # now test data...\n",
        "  for input_file in [\"STS2017.eval.v1.1/STS.input.track4a.es-en.txt\", \"STS2017.eval.v1.1/STS.input.track4b.es-en.txt\"]:\n",
        "      with open(input_file) as f:\n",
        "        for l in f:\n",
        "          data['test']['data'].append(l.strip().split(\"\\t\")[:2])\n",
        "  for gs_file in [\"STS2017.gs/STS.gs.track4a.es-en.txt\", \"STS2017.gs/STS.gs.track4b.es-en.txt\"]:\n",
        "    with open(gs_file) as f:\n",
        "      for l in f:\n",
        "        data['test']['scores'].append(float(l))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d11cmuV0KOH"
      },
      "source": [
        "cl_data = load_crosslingual_data()\n",
        "\n",
        "print(\"Score ranges:\")\n",
        "print(min(cl_data['train']['scores']), \"-\", max(cl_data['train']['scores']))\n",
        "print(min(cl_data['test']['scores']),\"-\", max(cl_data['test']['scores']))\n",
        "\n",
        "## Take note: what sentence comes first, the one in Spanish or the one in English?\n",
        "for i in range(3):\n",
        "  print(cl_data[\"train\"][\"data\"][i])\n",
        "  print(cl_data[\"test\"][\"data\"][i])\n",
        "\n",
        "## Also, you may notice that some sentence pairs are repeated within the training set, but have different scores.\n",
        "## We will simply make sure that there is no overlap between the training and the test datasets:\n",
        "\n",
        "found = False\n",
        "tr_sps = [tuple(ins) for ins in cl_data['train']['data']]\n",
        "te_sps = [tuple(ins) for ins in cl_data['test']['data']]\n",
        "for ins in te_sps:\n",
        "  if ins in tr_sps:\n",
        "    found = True\n",
        "    break\n",
        "if found:\n",
        "  print(\"Found some repetition\")\n",
        "else:\n",
        "  print(\"No repetitions found\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeK8EAIsYeZN"
      },
      "source": [
        "#### (Team B) Translate before training\n",
        "\n",
        "One common way of solving cross-lingual STS actually involves automatically translating sentences from one of the languages to the other and treating it as a monolingual task.\n",
        "\n",
        "Try translating sentences in **at least two different ways** (Spanish -> English, English -> Spanish, using different Machine Translation systems...). You can use the online interfaces of well-known MT systems.\n",
        "\n",
        "Load the translated sentences (you may need to write additional functions to load the data) and train and evaluate any of the models you implemented above on this dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYdXlu72k1yw"
      },
      "source": [
        "#### Spanish specifics\n",
        "\n",
        "Be careful: if you choose to translate English to Spanish, some features may not work (e.g. Wordnet-based features). You should also use a tokenizer that is specific for this language. You can find some sample code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pV5KLRAk1yx",
        "outputId": "e91c8e62-bc33-4f33-fd88-5052bf550779"
      },
      "source": [
        "! pip install spacy\n",
        "# Downloading Spanish model\n",
        "! python -m spacy download es_core_news_sm\n",
        "import spacy\n",
        "import es_core_news_sm\n",
        "nlp = es_core_news_sm.load()\n",
        "#nlp = spacy.load('es_core_news_sm')\n",
        "text = \"Esto es una frase en español.\"\n",
        "processed_text = nlp(text)\n",
        "for t in processed_text:\n",
        "  print(t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Esto\n",
            "es\n",
            "una\n",
            "frase\n",
            "en\n",
            "español\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUxpXQwY0O3_"
      },
      "source": [
        "### Use any of the models implemented in 1-2-3. Prepare it /modify it if necessary\n",
        "## Maybe: TO COMPLETE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7p5eznErJED",
        "outputId": "6a469cf4-8dbd-42b1-dce0-3342389b16f6"
      },
      "source": [
        "# Obtain train_x, test_x and train_y, test_y\n",
        "## TO COMPLETE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done train x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlQTwUFHrstM"
      },
      "source": [
        "## Extract features, run classifier, evaluate.\n",
        "## TO COMPLETE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7UlrE-XnFD_"
      },
      "source": [
        "## A bit of reflection\n",
        "\n",
        "Regardless of the team you have been in, you can reflect upon the following questions for the report:\n",
        "- Was this task harder than the one you tackled in 1-2-3 (i.e., were results worse or better?)? Why do you think this is the case?\n",
        "- Out of the different things you tried, what worked best? Did it meet your expectations?"
      ]
    }
  ]
}